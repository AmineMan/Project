[[[ ID ]]]
1569566755
[[[ INDEX ]]]
540
[[[ TITLE ]]]
The Sensitivity of Compressed Sensing Performance to Relaxation of Sparsity
[[[ AUTHORS ]]]
D. Donoho
G. Reeves
[[[ ABSTR ]]]
Not saved
[[[ BODY ]]]
Not saved
[[[ REFS ]]]
S. Chen
--
“Examples of basis pursuit
----
M. Bayati
--
“The LASSO risk for gaussian matrices
----
E. J. Candes
--
“Decoding by linear programming
----
W. Xu
--
minimization: A uniﬁed geometric framework
----
M. Herman
--
“General deviants: An analysis of perturbations in compressed sensing
----
M. Herman
--
“Mixed operators in compressed sensing
----
D. L. Donoho
--
“Minimax risk over lp balls
----
G. Reeves
--
sampling error in sparse signal reconstruction
[[[ META ]]]
xmlpapertitle -> The Sensitivity of Compressed Sensing Performance to Relaxation of Sparsity
parsed -> yes
xmlabstract -> Many papers studying compressed sensing consider the noisy underdetermined system of linear equations: $y=Ax^0 + z^0$, with $n \times N$ measurement matrix $A$, $n < N$, and Gaussian white noise $z^0$. Both $y$ and $A$ are known, both $x^0$ and $z^0$ are unknown, and we seek an approximation to $x^0$; we let $\delta = n/N \in (0,1)$ denote the undersampling fraction. In the popular strict sparsity model of compressed sensing, such papers further assume that $x^0$ has at most a specified fraction $\eps$ of nonzeros.

In this paper, we relax the assumption of strict sparsity by assuming the vector $x^0$ is close in mean $p$-th power to a sparse signal. We study how this relaxation affects the performance of $\ell_1$-penalized $\ell_2$ minimization, in which the reconstruction $\hxl$ solves $ \min \| y - Ax\|_2^2/2 + \lambda \|x\|_1$.

We study asymptotic mean-squared error (AMSE), the large-system limit of the MSE of $\hxl$. Using recently developed tools based on Approximate Message Passing (AMP), we develop expressions for minimax AMSE $M^*_{\eps,p}(\delta,\xi,\sigma)$ -- max over all approximately sparse signals, min over penalizations $\lambda$, where $\xi$ measures the deviation from strict sparsity. There is of course a phase transition curve $\delta^* = \delta^*(\eps)$; only above this curve, $\delta > \delta^*(\eps)$, can we have exact recovery even in the noiseless-data strict-sparsity setting. It turns out that the minimax AMSE can be characterized succinctly by a coefficient $sens_p^*(\eps,\delta)$ which we refer to as the {\em sparsity-relaxation sensitivity}. We give explicit expressions for $sens^*_p(\eps,\delta)$, compute them, and interpret them. 

Our approach yields precise formulas in place of loose order bounds based on restricted isometry property and instance optimality results. Our formulas reveal that sensitivity is finite everywhere exact recovery is possible under strict sparsity, and that sensitivity to added random noise in the measurements $y$ is {\it smaller} than the sensitivity to adding a comparable amount of noise to the estimand $x^0$. Our methods can also treat the mean $q$-th power loss. The methods themselves are based on minimax decision theory and seem of independent interest.
xmlsessionid -> S12.T9.1
xmlendtime -> 11:50
xmlpaperid -> 1569566755
xmlsession -> S12.T9: L1-Regularized Least Squares and Sparsity
xmldate -> 1341480600000
file -> PapersDataset\1569566755.txt
xmlstarttime -> 11:30
xmlauthors -> David Donoho, Galen Reeves
xmlroom -> Stratton West Lounge (201)
[[[ LINKS ]]]

