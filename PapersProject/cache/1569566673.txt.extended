[[[ ID ]]]
1569566673
[[[ INDEX ]]]
517
[[[ TITLE ]]]
Lossy Common Information of Two Dependent Random Variables
[[[ AUTHORS ]]]
K. Viswanatha
E. Akyol
K. Rose
[[[ ABSTR ]]]
Not saved
[[[ BODY ]]]
Not saved
[[[ REFS ]]]
R. Gray
--
“Source coding for a simple network”
----
R. Ahlswede
--
characteristics of correlated information sources”
----
S. Kamath
--
common information deﬁned via the Gray-Wyner system
----
RX (D1 )
P (Y |Y ) which achieve
are unique channels P (X|X)
˜
˜
in fact sufﬁcient in this case. Again we ﬁrst assume that there
of Theorem 1. The original Gray-Wyner’s characterization is
Proof: The proof follows in very similar lines to the proof
--
RX (D1 ) and R0 + R2 = RY (D2 )
----
Writing similar inequality relations for Y
--
and P (Y |Y ) ∈ PD2 
[[[ META ]]]
xmlpapertitle -> Lossy Common Information of Two Dependent Random Variables
pdf -> PapersDataset\1569566673.pdf
parsed -> yes
xmlabstract -> The two most prevalent notions of common information are due to Wyner and Gács-Körner and both the notions can be stated as two different characteristic points in the lossless Gray-Wyner region. Although these quantities can be easily evaluated for random variables with infinite entropy (eg. continuous random variables), the operational significance underlying their definition is applicable only to the lossless framework. The primary objective of this paper is to generalize these two notions of common information to the lossy Gray-Wyner network, which extends the theoretical intuition underlying their definitions for general sources and distortion measures. We begin with the lossy generalization of Wyner's common information, defined as the minimum rate on the shared branch of the Gray-Wyner network at minimum sum rate when the two decoders reconstruct the sources subject to individual distortion constraints. We derive a complete single letter information theoretic characterization for this quantity and use it to compute the common information of symmetric bivariate Gaussian random variables. We then derive similar results to generalize Gács-Körner's definition to the lossy framework. These two characterizations allow us to carry the practical insight underlying the two notions of common information to general sources and distortion measures.
xmlsessionid -> S3.T8.3
xmlendtime -> 15:40
xmlpaperid -> 1569566673
linked -> yes
xmlsession -> S3.T8: Directed Information, Common Information, and Divergence
xmldate -> 1341235200000
file -> PapersDataset\1569566673.txt
xmlstarttime -> 15:20
xmlauthors -> Kumar Viswanatha, Emrah Akyol, Kenneth Rose
xmlroom -> Stratton (491)
[[[ LINKS ]]]
12 7
----
18 7
----
34 7
----
95 7
----
96 7
----
107 7
----
118 7
----
154 7
----
208 7
----
264 7
----
365 7
----
379 7
----
392 7
----
401 7
----
447 7
----
483 7
----
490 7
----
493 7
----
501 14
----
523 7
----
556 21
----
575 7
----
606 7
