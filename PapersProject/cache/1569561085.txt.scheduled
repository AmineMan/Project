[[[ ID ]]]
1569561085
[[[ INDEX ]]]
83
[[[ TITLE ]]]
Information Divergence is more chi square distributed than the chi squared statistics
[[[ AUTHORS ]]]
P. Harremoës
G. Tusnády
[[[ ABSTR ]]]
Not saved
[[[ BODY ]]]
Not saved
[[[ REFS ]]]
E. Lehman
--
Testing Statistical Hypotheses
----
M. P. Quine
--
ratio goodness-of-ﬁt tests
----
P. Harremo¨ s
--
mity by means of the entropy
----
P. Harremo¨ s
--
“Efﬁciency of entropy testing
----
O. E. Barndorff-Nielsen
--
adjustment of the likelihood-ratio statistic
----
R. R. Sokal
--
statistics in biological research
----
O. E. Barndorff-Nielsen
--
Inference and asymptotics
----
J. Chen
--
mean of Gamma and Poisson distributions
[[[ META ]]]
xmlpapertitle -> Information Divergence is more chi square distributed than the chi squared statistics
parsed -> yes
xmlabstract -> For testing goodness of fit it is very popular to use either the chi square statistic or G square statistics (information divergence). Asymptotically both are chi square distributed so an obvious question is which of the two statistics that has a distribution that is closest to the chi square distribution. Surprisingly, when there is only one degree of freedom it seems like the distribution of information divergence is much better approximated by a chi squared distribution than the chi square statistic. For random variables we introduce a new transformation that transform several important distributions into new random variables that are almost Gaussian. For the binomial distributions and the Poisson distributions we formulate a general conjecture about how close their transform are to the Gaussian. The conjecture is proved for Poisson distributions.
xmlsessionid -> S3.T8.4
xmlendtime -> 16:00
xmlpaperid -> 1569561085
xmlsession -> S3.T8: Directed Information, Common Information, and Divergence
xmldate -> 1341236400000
file -> PapersDataset\1569561085.txt
xmlstarttime -> 15:40
xmlauthors -> Peter Harremoës, Gábor Tusnády
xmlroom -> Stratton (491)
[[[ LINKS ]]]

