[[[ ID ]]]
1569563007
[[[ INDEX ]]]
105
[[[ TITLE ]]]
Quantization Effect On Second Moment of Log-Likelihood Ratio and Its Application to Decentralized Sequential Detection
[[[ AUTHORS ]]]
Y. Wang
Y. Mei
[[[ ABSTR ]]]
Not saved
[[[ BODY ]]]
Not saved
[[[ REFS ]]]
L. Le Cam
--
normality under information loss
[[[ META ]]]
xmlpapertitle -> Quantization Effect On Second Moment of Log-Likelihood Ratio and Its Application to Decentralized Sequential Detection
parsed -> yes
xmlabstract -> It is well known that quantization cannot increase the Kullback-Leibler divergence which can be thought of as the expected value or first moment of the log-likelihood ratio. In this paper, we investigate the quantization effects on the second moment of the log-likelihood ratio. It is shown that quantization may result in an increase in the case of the second moment, but the increase is bounded above by $2/e.$ The result is then applied to decentralized sequential detection problems to provide a simpler sufficient condition for asymptotic optimality theory, and the technique is also extended to investigate the quantization effects on other higher-order moments of the log-likelihood ratio and provide lower bounds on higher-order moments.
xmlsessionid -> S2.T8.1
xmlendtime -> 11:50
xmlpaperid -> 1569563007
xmlsession -> S2.T8: Distributed Detection and Estimation
xmldate -> 1341221400000
file -> PapersDataset\1569563007.txt
xmlstarttime -> 11:30
xmlauthors -> Yan Wang, Yajun Mei
xmlroom -> Stratton (491)
[[[ LINKS ]]]

