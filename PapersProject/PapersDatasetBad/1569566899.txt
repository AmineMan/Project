Title:          C:/Users/MMFF/Backup Dell/ISIT 2012/May 12/ISIT_May18.dvi
Creator:        www.freepdfconvert.com         
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 21:52:27 2012
ModDate:        Tue Jun 19 12:55:47 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      341945 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569566899

Extrinsic Jensen–Shannon Divergence with
Application in Active Hypothesis Testing
Mohammad Naghshvar and Tara Javidi

This paper considers the problem of active M –ary hypothesis testing. A Bayesian decision maker is responsible
to enhance his information about the true hypothesis in a
speedy manner while accounting for the penalty of wrong
declarations. In contrast to the classical hypothesis testing
problem [1], our decision maker can choose one of K available
actions and hence exert some control over the collected
sample’s “information content.” The special cases of active
hypothesis testing naturally arise in a broad spectrum of
applications in cognition, medical diagnosis, communication,
and sensor management (see [2] and references therein).
To achieve the best performance, it is essential that the
decision maker, in each step, selects an action that provides
the “highest amount of information.” This raises the question
as what measure of information is appropriate in this context.
In [3], posing the problem as a POMDP, the optimal notion of
information utility was shown to be nothing but the optimal
value function of the corresponding dynamic programming.
In this fashion, the problem of active sequential hypothesis
testing reduces to a sequence of one–shot problems, in each
of which an optimal action is the one that maximizes the
optimal information utility. Although this result provides a
general and structural characterization of the optimal policy
(Markov and deterministic), in the absence of a closed–form
for the optimal value function, it is of little use in identifying
achievable schemes.
In [3], inspired by Burnashev’s coding scheme [4], we
proposed a two–phase randomized policy for selecting actions
as follows. In the ﬁrst phase, the decision maker iteratively

reﬁnes his belief about the true hypothesis; while the second
phase serves to verify the correctness of the output of phase
one. As a measure of performance, we used the notion of
asymptotic optimality due to Chernoff [5] when the penalty
of wrong declaration, l, increases (l → ∞). It was shown
that the proposed policy is asymptotically optimal in many
special cases of active hypothesis testing such as active
binary hypothesis testing (M = 2), noisy dynamic search,
and channel coding with ﬁnite messages (zero rate) in the
presence of perfect feedback. Later, in [6], we showed that
the asymptotic optimality of the proposed policy remains valid
beyond the above special cases so long as the number of
hypotheses M is ﬁxed (also proved independently in [7]).
In [2], we tackled the asymptotic optimality in M as well
and proposed yet another two–phase randomized policy and
proved its asymptotic optimality as l, M → ∞ for channel
coding with perfect feedback (non–zero rate).
What remains unsatisfying about the above achievability
schemes is that they all rely on randomization and a two–
phase operation exhibiting a disconnect to our earlier structural
characterization of the optimal policy [3] as sequential and
deterministic (Markov) in nature. In this paper, we take a
ﬁrst step toward constructing deterministic one–phase policies
for the problem of active hypothesis testing. Inspired by the
popularity of Jensen–Shannon divergence among practitioners
as a measure of information content of samples [8], [9], we
introduce two heuristic policies. We start with a policy that
at any given time maximizes Jensen–Shannon divergence; but
to address the shortcomings of this approach, we introduce
Extrinsic Jensen–Shannon (EJS) divergence as an alternative
measure1 maximizing which gives rise to our second policy.
Via numerical and asymptotic analysis, the performance of the
proposed policies and the relevance of the EJS divergence in
the context of the active hypothesis testing are investigated.
The remainder of this paper is organized as follows. In
Section II, we formulate the problem of active hypothesis
testing and propose various heuristics for selecting actions.
Section III provides the main results of the paper. Finally, we
conclude the paper and discuss future work in Section IV.
Notation: Let [x]+ = max{x, 0}. A random variable is
denoted by an upper case letter (e.g. X) and its realiza-

The authors are with the Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, CA 92093 USA. (e-mail:
naghshvar@ucsd.edu; tjavidi@ucsd.edu).
This work was partially supported by the industrial sponsors of UCSD
Center for Wireless Communication (CWC), Information Theory and Applications Center (ITA), and Center for Networked Systems (CNS), as well as
NSF Grants CCF-0729060 and CCF-1018722.

1 After the initial submission, it was brought to our attention that EJS divergence is equivalent to the full anthropic correction proposed in the context
of mutual information estimation [10]. In particular, the authors in [10] used
the notion of anthropic correction as an estimator of the mutual information
between signals acquired in neurophysiological experiments where only a
small number of stimuli can be tested. Investigating the deeper connections
to this seemingly unrelated context is an important area of future work.

Abstract—Consider a decision maker who is responsible to
dynamically collect observations so as to enhance his information
in a speedy manner about an underlying phenomena of interest
while accounting for the penalty of wrong declarations.
In this paper, Extrinsic Jensen–Shannon (EJS) divergence
is introduced as a measure of information. Using EJS as an
information utility, a heuristic policy for selecting actions is
proposed. Via numerical and asymptotic optimality analysis, the
performance of the proposed policy, hence the applicability of
the EJS divergence in the context of the active hypothesis testing
is investigated.

I. I NTRODUCTION

1

tion is denoted by a lower case letter (e.g. x). Similarly,
a random column vector and its realization are denoted by
bold face symbols (e.g. X and x). For any set S, |S| denotes the cardinality of S. The entropy function on a vector
ρ = [ρ1 , ρ2 , . . . , ρM ] ∈ [0, 1]M is deﬁned as H(ρ) =
M
1
i=1 ρi log(1/ρi ), with the convention that 0 log 0 = 0. The
Kullback–Leibler (KL) divergence between two probability
density functions q(·) and q (·) is denoted by D(q||q ) where
q(z)
D(q||q ) = q(z) log q (z) dz. Finally, let N (m, σ 2 ) denote a
normal distribution with mean m and variance σ 2 .

are selected (the choice of any of the retire-declare actions
marks the stopping time τ ).
In this paper, we consider two heuristic Markov policies
based on the following two principles. The ﬁrst principle
follows from rewriting (1) as

E[τ + l(1 − max ρj (τ ))],

and then noting that when l(1 − ρi (τ )) ≤ 1, the further
reduction in l(1 − ρi (τ )) is not worth taking one more sensing
action and hence increasing τ by 1. The intuition behind the
second principle is that, by choosing an appropriate measure
of information, the problem of active sequential hypothesis
testing can be reduced to a sequence of one–shot problems in
each of which an optimal sensing action is selected deterministically so as to provide the highest amount of information. In
particular, we ﬁrst consider the following notion of divergence
as a measure of information:

II. P ROBLEM S ETUP AND H EURISTIC P OLICIES
In Subsection II-A, we formulate the problem of active
hypothesis testing, referred to as Problem (P) hereafter. In
Subsection II-B, heuristic policies for selecting actions are
proposed.
A. Problem Formulation
Problem (P): Let Ω = {1, 2, . . . , M }. Let Hi , i ∈ Ω,
denote M hypotheses of interest among which only one holds
true. Let θ be the random variable that takes the value θ = i
on the event that Hi is true for i ∈ Ω. We consider a Bayesian
scenario with prior belief ρ = [ρ1 , ρ2 , . . . , ρM ], i.e., initially
P ({θ = i}) = ρi for all i ∈ Ω. A is the set of all sensing
actions and is assumed to be ﬁnite with |A| = K < ∞.
Z is the observation space. For all a ∈ A, the observation
a
kernel qi (·) (on Z) is the probability density function for
observation Z when action a has been taken and Hi is true. We
a
assume that {qi (·)}i,a are known. Let l denote the penalty for
a wrong declaration. Let τ be the stopping time at which the
decision maker retires. The objective is to ﬁnd a sequence of
sensing actions A(0), A(1), . . . , A(τ − 1), a stopping time τ ,
and a declaration rule d : Aτ −1 ×Z τ −1 → Ω that collectively
minimize the total cost:

E τ + l1{d(Aτ −1 ,Z τ −1 )=θ} ,

(2)

j∈Ω

Deﬁnition 2. The Jensen–Shannon (JS) divergence among
probability density functions q1 , q2 , . . . , qM with respect to
ρ = [ρ1 , ρ2 , . . . , ρM ] is deﬁned as
M

ρi D(qi ||qρ ),

JS(ρ; q1 , q2 , . . . , qM ) =
i=1

where qρ (z) =

M
i=1

ρi qi (z).

Remark 1. JS divergence was ﬁrst introduced in [12]. This divergence measure, which is nothing but the mutual information
between distributions ρ and qρ , i.e. I(ρ; qρ ), has been applied
in many areas such as information theory, image processing,
genome comparison, etc.
Note that as the belief about one of the hypotheses, say
ρi , approaches 1, D(qi ||qρ ) approaches to D(qi ||qi ) = 0;
and consequently, independently of the observation kernels
q1 , q2 , . . . , qM , JS(ρ; q1 , q2 , . . . , qM ) approaches 0. To address this insensitivity to the choice of the observation kernels,
we propose the following modiﬁcation.

(1)

where the expectation is taken with respect to the initial belief
as well as the distribution of observation sequence.

Deﬁnition 3. The Extrinsic Jensen–Shannon (EJS) divergence
among probability density functions q1 , q2 , . . . , qM with respect to ρ = [ρ1 , ρ2 , . . . , ρM ] is deﬁned as

B. Heuristic Markov Policies: Maximizing Divergence
Problem (P) is a partially observable Markov decision
problem (POMDP) where the state is static and observations
are noisy. This problem is equivalent to an MDP whose
information state at time t is the belief vector ρ(t) =
[ρ1 (t), . . . , ρM (t)] where ρi (t) = P ({θ = i}|ρ, At−1 , Z t−1 ).
Accordingly, the information state space is deﬁned as P(Θ) =
M
{ρ ∈ [0, 1]M :
i=1 ρi = 1} where Θ is the σ–algebra
generated by random variable θ.

M

ρi D(qi ||

EJS(ρ; q1 , q2 , . . . , qM ) =
i=1

k=i

ρk
qk ).
1 − ρi

We are now ready to introduce our heuristic policies.
1) Policy πJS is deﬁned as follows:

Deﬁnition 1. Let di , i ∈ Ω, represent an action under
which the decision maker retires and declares Hi as the
true hypothesis. Markov stationary deterministic policy2 is a
mapping from P(Θ) to A ∪ {d1 , . . . , dM } based on which
sensing actions A(t), t = 0, 1, . . . , τ − 1 and stopping time τ

πJS (ρ) =

di
if l(1 − ρi ) ≤ 1, i ∈ Ω
a
a
arg max JS(ρ; q1 , . . . , qM )
otherwise
a∈A

2) Similarly, policy πEJS is deﬁned as follows:

πEJS (ρ) =

2 The

sequence of actions can be selected in a Markovian, stationary, and
deterministic fashion without loss of optimality [11, Theorem 14].

di
if l(1 − ρi ) ≤ 1, i ∈ Ω
a
a
arg max EJS(ρ; q1 , . . . , qM )
otherwise
a∈A

2

Finding optimal policy π ∗ for Problem (P) requires knowledge of V ∗ whose closed–form, in general, is not known. Next
we connect the above dynamic programming characterization
to the construction of πJS and πEJS .
Proposition 1 below shows that the proposed JS and EJS
divergences are nothing but the information utility associated
with the following two candidate functionals:
M
1
• Entropy: H(ρ) =
i=1 ρi log ρi .
M
1−ρi
• Average log–likelihood: U (ρ) =
i=1 ρi log ρi .

Remark 2. Note that as belief about one of the hypotheses,
say ρi , becomes large, πEJS selects action a such that
ρk
a
a
D(qi || k=i 1−ρi qk ) is maximized, i.e. it selects an action
that distinguishes Hi from the collection of alternate hypotheses the most; in contrast to πJS maximization of the mutual
information. As we will see in Section III-C, these different
philosophies result in signiﬁcant performance difference.
III. A NALYTICAL R ESULTS
Subsection III-A provides the results from [3] on the corresponding dynamic programming (DP) equation for Problem (P) and characterizes an optimal policy. We then use
this formation to connect our heuristic policies to the MDP
approach and further provide numerical and analytical results
regarding the performance of the proposed policies.

Proposition 1. The information utilities associated with the
entropy function H(·) and the average likelihood function U (·)
are respectively given by
a
a
IU (a, ρ, H) = JS(ρ; q1 , . . . , qM ),
a
a
IU (a, ρ, U ) = EJS(ρ; q1 , . . . , qM ).

A. Markov Decision Problem

Proof: The proof follows a simple algebraic manipulation
and is omitted in the interest of brevity.
Similarly, Proposition 1 shows that policies πJS and πEJS
are nothing but Markov deterministic policies which maximize
the information utility associated with Shannon entropy and
average log-likelihood. In the next subsection we will discuss
the analytic performance of these policies.

As mentioned in Section II-B, Problem (P) is equivalent
to an MDP with state space P(Θ) whose information state
at time t is the belief vector ρ(t). In one sensing step, the
evolution of the belief vector follows Bayes’ rule and is given
by Φa , a measurable function from P(Θ) × Z to P(Θ) for all
a ∈ A:
q a (z)
q a (z)
q a (z)
Φa (ρ, z) = ρ1 1
, ρ2 2
, . . . , ρM M
, (3)
a
a
a
qρ (z)
qρ (z)
qρ (z)
a
where qρ (z) =

M
i=1

B. Performance Bounds and Asymptotic Optimality
In this section, we ﬁrst ﬁnd an upper bound for the expected
total cost of πEJS under the following technical assumptions.

a
ρi qi (z).

Assumption 1. For any two hypotheses i and j, i = j, there
a
a
exists an action a, a ∈ A, such that D(qi ||qj ) > 0.

Deﬁnition 4. A Markov stationary deterministic policy that
minimizes (1) is referred to as an optimal policy and is denoted
by π ∗ .

Assumption 2. max max sup
i,j

We deﬁne operator Ta , a ∈ A, such that for any measurable
function g : P(Θ) → R,
(Ta g)(ρ) =

a
g(Φa (ρ, z))qρ (z)dz.

j

a∈A

(4)

(5)

< ∞.

Proposition 2. Under Assumptions 1 and 2,

∗



Then V (ρ), referred to as the optimal value function, is equal
to the minimum cost in Problem (P) when the initial belief is ρ.


VπEJS (ρ) ≤ 

As a corollary to Fact 1, we can characterize an optimal
policy π ∗ from (5). To do so, we ﬁrst introduce a notion of
information utility.

max min

λ∈Λ(A)

M

+

ρi
i=1

Deﬁnition 5. Associated with a functional V : P(Θ) → R+ ,
the information utility of action a at information state ρ is
deﬁned as IU(a, ρ, V ) := V (ρ) − (Ta V )(ρ).

ˆ
ρ

a∈A

U (ρ)
a
λa EJS(ˆ; q1 , . . . , qM )
ρ a

1−l−1
l−1
a
λa D(qi ||

+

log
max min

λ∈Λ(A)

ˆ
ρ

a∈A

j=i


 + O(log log M l).


ρj
ˆ
qa )
1−ρi j
ˆ

Proof: The proof is given in Appendix A.
Fact 2 (Proposition 2 in [2]). Under Assumption 1 and for
arbitrary δ ∈ [0, 0.5], there exists K independent of M and l
such that

Together with (5), this results in the following optimal
deterministic policy:

V ∗ (ρ) ≥
+

−1
log 1−l
− log 1−δ
δ
l−1
 H(ρ) − δ log(M − 1) +
1{max ρ ≤1−δ} − K 
a
a
max max I(ˆ ; qρ )
ρ a
max max D(qi ||qj ) i∈Ω i
ˆ

∗

π ∗ (ρ) =

a∈A z∈Z

a
qi (z)
a
qj (z)

Assumption 1 ensures the possibility of discrimination between any two hypotheses. Assumption 2 implies that no two
hypotheses are fully distinguishable using a single observation
sample.
1) Performance Bounds: To continue with our analysis, we
need the following notation. Let Λ(A) denote the collection
of all probability distributions on elements of A, i.e., Λ(A) =
{λ ∈ [0, 1]|A| : a∈A λa = 1}.

Fact 1 (Consequence of Theorems 1, 4 in [11]). Let V ∗ :
P(Θ) → R+ be a functional solving the following ﬁxed point
equation:
V ∗ (ρ) = min 1 + min(Ta V ∗ )(ρ), min(1 − ρj )l .

(6)
(7)

di
if V (ρ) = l(1 − ρi ), i ∈ Ω
arg max IU (a, ρ, V ∗ )
otherwise
a∈A

a∈A

3

ˆ
ρ

i,j∈Ω a∈A

Setting δ = 1 − ρ, we obtain the following lower bound:
˜
V ∗ (ρ) ≥

+
−1
log 1−l
H(ρ)
l−1

+
− O(log log M l) .
a
a
max max I(ˆ ; qρ )
ρ a
max max D(qi ||qj )
ˆ
ˆ
ρ

a∈A

i,j∈Ω a∈A

2) Asymptotic Optimality: The upper and lower bounds
provided by Proposition 2 and Fact 2 can be applied to
establish the asymptotic optimality of πEJS as deﬁned below.

Fig. 1. An image with four segments. Some segments can be observed
simultaneously.

Deﬁnition 6. Let Vπ (ρ) denote the the expected total cost of
policy π when the initial belief is ρ. Policy π is referred to
as asymptotically optimal in l (and M ) if for all ρ ∈ P(Θ),

In our performance comparisons, we also consider policy
πML below whose asymptotic optimality was proved in [3].

Vπ (ρ) − V ∗ (ρ)
= 0.
l→∞
Vπ (ρ)

( lim ) lim
M→∞

a
a
max max D(qi ||qj ) = max min
λ∈Λ(A)

ˆ
ρ

a
λa D(qi ||
a∈A

j=i

Next the performance of policies π ∗ , πJS , πEJS , and πML
are compared numerically for the example of Fig. 1 with fobj =
N (1, 1) and fnoise = N (0, 0.5). Fig. 2 plots the expected total
cost of the candidate policies under uniform initial information
state ρ = [1/4, 1/4, 1/4, 1/4]. Fig. 2 conﬁrms the signiﬁcant
improvement of πEJS over all other heuristics.

ρj
ˆ
q a ); (8)
1 − ρi j
ˆ

and πEJS is asymptotically optimal in l and M if the following
condition holds as well,
max max I(ˆ ; qρ ) = max min
ρ a
ˆ
a∈A

ˆ
ρ

λ∈Λ(A)

ˆ
ρ

a
λa EJS(ˆ ; q1 , . . . , qM ).
ρ a

(9)

a∈A

18

There are many important special cases of the active hypothesis testing for which conditions (8) and (9) hold. Among
these cases are active binary hypothesis testing (M=2) and
noisy dynamic search under which (8) holds establishing the
asymptotic optimality of πEJS in l (see [2] for details). In [2],
we have addressed the relation between the problems of channel coding with perfect feedback and active hypothesis testing
and in [13], we have shown that πEJS achieves asymptotic
optimality in l and M providing the only known deterministic
one–phase policy that achieves the optimal error exponent of
any discrete memoryless channel (DMC) in the presence of
the perfect feedback.
In the next subsection, the performance of policies π ∗ , πJS ,
and πEJS will be compared numerically.

Expected Total Cost

16

14

πJS
πEJS
πM L
π∗

12

10

8

6
50

100

200

500

1000

2000

5000

Penalty l

Fig. 2.

Expected total cost under different policies as l increases.

IV. D ISCUSSION AND F UTURE W ORK
In this paper, we considered the problem of active sequential
hypothesis testing. Using Extrinsic Jensen–Shannon (EJS)
divergence as an information utility, a heuristic policy, referred
to as πEJS , was proposed. Via numerical and asymptotic
analysis, the performance of πEJS was investigated. It was
shown that πEJS is the only known deterministic one–phase
policy that achieves asymptotic optimality in many practically
relevant problems such as noisy dynamic search and channel
coding with perfect feedback. We conjecture that the asymptotic optimality of πEJS remains valid beyond the special
cases discussed in this paper. Proving this conjecture requires
improving the proposed lower and upper bounds and is part
of the current research.

C. Noisy dynamic search
Consider the problem of sequentially searching for one
and only object of interest in an image with M segments.
Let A be the set of all allowable combinations of segments
that the player can select to observe in one time slot. Fig. 1
illustrates an instance of the problem where M = 4 and
A = {1, 2, 3, 4, (1, 2), (3, 4), (1, 3), (2, 4)}. The player collects
visual samples affected by the background noise and is responsible to ﬁnd the object quickly and accurately.
a
Observation kernels {qi (·)}i,a are assumed to be of the
following form:
a
qi (·) =

if l(1 − ρi ) ≤ 1, i ∈ Ω
.
otherwise

i∈Ω

In fact, policy πEJS is asymptotically optimal in l if
i,j∈Ω a∈A

di
arg max ρi

πML (ρ) =

fobj (·)
if i ∈ a
.
fnoise (·) if i ∈ a
/

ACKNOWLEDGMENT
The authors would like to thank M. Raginsky and Y. Polyanskiy for helpful discussions and V. Anantharam for pointing
out the connection to anthropic correction. Authors are indebted to S. Verd´ for suggesting the word extrinsic.
u

In other words, observation samples are distributed as fobj if
the object is in the segment(s) that the player is observing;
otherwise samples have the probability density function fnoise .

4

R EFERENCES

What remains is to ﬁnd upper bounds for the terms in the
right-hand side of (10).
Let Fn be the history of actions and observations up to
time n, i.e. Fn = σ{ρ, A(0), Z(0), . . . , A(n − 1), Z(n − 1)}.
Moreover, let deﬁne the sequence {ζn }, n = 0, 1, . . ., as
follows

[1] V. P. Dragalin, A. G. Tartakovsky, and V. V. Veeravalli, “Multihypothesis
Sequential Probability Ratio Tests. I: Asymptotic Optimality,” IEEE
Transactions on Information Theory, vol. 45, no. 7, pp. 2448–2461,
November 1999.
[2] M. Naghshvar and T. Javidi, “Active Sequential Hypothesis Testing,”
2012, available on arXiv:1203.4626.
[3] M. Naghshvar and T. Javidi, “Performance Bounds for Active Sequential
Hypothesis Testing,” in IEEE International Symposium on Information
Theory (ISIT), 2011, pp. 2666–2670.
[4] M. V. Burnashev, “Data Transmission Over a Discrete Channel with
Feedback Random Transmission Time,” Problemy Peredachi Informatsii, vol. 12, no. 4, pp. 10–30, 1975.
[5] H. Chernoff, “Sequential Design of Experiments,” Ann. Math. Statits.,
vol. 30, pp. 755–770, 1959.
[6] M. Naghshvar and T. Javidi, “Active Hypothesis Testing: Sequentiality
and Adaptivity Gains,” in Conference on Information Sciences and
Systems (CISS), March 2012.
[7] N. K. Vaidhiyan, A. P. Sripati, and R. Sundaresan, “Active Sequential
Hypothesis Testing with Application to a Visual Search Problem,” in
IEEE International Symposium on Information Theory (ISIT), 2012.
[8] I. Grosse, P. Bernaola-Galv´ n, P. Carpena, R. Rom´ n-Rold´ n, J. Oliver,
a
a
a
and H. E. Stanley, “Analysis of Symbolic Sequences Using the Jensen–
Shannon Divergence,” Phys. Rev. E, vol. 65, no. 4, pp. 041905, 2002.
[9] A. J. Arvey, R. K. Azad, A. Raval, and J. G. Lawrence, “Detection of
Genomic Islands via Segmental Genome Heterogeneity,” Nucleic Acids
Research, vol. 37, no. 16, pp. 5255 – 5266, 2009.
[10] M. C. Gastpar, P. R. Gill, A. G. Huth, and F. E. Theunissen, “Anthropic
Correction of Information Estimates and Its Application to Neural
Coding,” IEEE Transactions on Information Theory, vol. 56, no. 2,
pp. 890 –900, February 2010.
[11] S. E. Shreve and D. P. Bertsekas, “Universally Measurable Policies in
Dynamic Programming,” Mathematics of Operations Research, vol. 4,
no. 1, pp. 15–30, February 1979.
[12] J. Lin, “Divergence Measures Based on the Shannon Entropy,” IEEE
Transactions on Information Theory, vol. 37, no. 1, pp. 145 –151, 1991.
[13] M. Naghshvar and T. Javidi, “Optimal Reliability over a DMC with
Feedback via Extrinsic Jensen–Shannon Divergence,” submitted to
International Symposium on Information Theory and its Applications
(ISITA), 2012.

M

ζn = −U (ρ(n)) = −
i=1

E [ζn+1 − ζn |Fn ] = E [U (ρ(n)) − U (ρ(n + 1))|Fn ]
a
a
= max EJS(ρ(n); q1 , . . . , qM )
a∈A

λ∈Λ(A)

∆

a∈A

∗

ζ
Therefore, the sequence { Dn − n}, n = 0, 1, . . . forms a
∗
submartingale with respect to a ﬁltration {Fn } and by Doob’s
stopping theorem,

ζ0
ζτ
˜
≤E
−τ .
˜
D∗
D∗
Rearranging the terms, we obtain
ρ
˜
U (ρ) + log 1−ρ
U (ρ)
ζτ
˜
˜
E[˜] ≤
τ
+E
≤
+ O(1), (11)
D∗
D∗
D∗

where the last inequality follows from the fact that
ζτ ≤ log
˜

ρ
˜
+ O(1).
1−ρ
˜

Next we ﬁnd an upper bound for the second term in the
right–hand side of (10). Note that if ρi (n) ≥ ρ, then
˜
a
a
max EJS(ρ(n); q1 , . . . , qM )
a∈A

j

a
≥ max ρi (n)D(qi ||

τi = min{n : ρi (n) ≥ ρ},
˜
˜

a∈A

τ = min{n : max ρj (n) ≥ 1 − l−1 },

j=i

λ∈Λ(A)

τi = min{n : ρi (n) ≥ 1 − l−1 }.

ˆ
ρ

ρj (n) a
q )
1 − ρi (n) j

a
λa ρD(qi ||
˜

≥ max min

j

a∈A

j=i

VπEJS (ρ) ≤ E[τ ] + 1 = E[˜] + E[τ − τ ] + 1
τ
˜
M

ρi E[τi − τ |θ = i] + 1
˜

E[τi − τi |θ = i] ≤
˜

i=1
M

log

1−l−1
l−1

max min

λ∈Λ(A)

ˆ
ρ

a∈A

− log

ρ
˜
1−ρ
˜

a
λa D(qi ||

j=i

ρj
ˆ
qa )
1−ρi j
ˆ

+ O(1).
(12)

ρi E[τi − τi |θ = i]
˜

For the third term in the right–hand side of (10), we have

i=1

M

M

ρi E[(˜i − τ )1{Zi } |θ = i] + 1,
τ
˜

ρj
ˆ
q a ).
1 − ρi j
ˆ

Constructing a proper submartingale and using Doob’s
stopping theorem, we obtain

From (2), the total cost under policy πEJS satisﬁes

+

ˆ
ρ

=D .

τ = min{n : max ρj (n) ≥ ρ},
˜
˜

= E[˜] +
τ

a
λa EJS(ˆ ; q1 , . . . , qM )
ρ a

≥ max min

Let ρ = 1 − log1 M and τ , τi , τ , τi , i ∈ Ω, be Markov
˜
˜ ˜
stopping times deﬁned as follows:

≤ E[˜] +
τ

1 − ρi (n)
.
ρi (n)

Note that under policy πEJS sensing actions are selected in a
way to maximize the EJS divergence and we obtain,

A PPENDIX A
P ROOF OF P ROPOSITION 2

(a)

ρi (n) log

ρi E[(˜i − τ )1{Zi } |θ = i] ≤
τ
˜

(10)

i=1

i=1

1−ρ
˜
log M.
ρ
˜

(13)

The proof of (13) is omitted in the interest of brevity.
Combining (10)-(13), completes the proof of Proposition 2.

where Zi = {Z ∞ : ρi (˜) < ρ} and (a) follows from the fact
τ
˜
that τ ≤ τi , i ∈ Ω.

5

