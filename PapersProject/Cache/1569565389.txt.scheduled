[[[ ID ]]]
1569565389
[[[ INDEX ]]]
253
[[[ TITLE ]]]
Shannon Entropy Convergence Results in the Countable Infinite Case
[[[ AUTHORS ]]]
J. Silva
P. Parada
[[[ ABSTR ]]]
Not saved
[[[ BODY ]]]
Not saved
[[[ REFS ]]]
T. M. Cover
--
Elements of Information Theory
----
Siu-Wai Ho
--
“On the discontinuity of the
Shannon information measures
----
Siu-Wai Ho
--
“The interplay between entropy
and variational distance
----
I. Csisz´ r
--
Information theory and Statistics: A
a
tutorial
----
L. Devroye
--
Combinatorial Methods in Density Estimation
----
Francisco Piera
--
“On convergence properties of
Shannon entropy
----
S. Kullback
--
“On information and sufﬁciency
----
˜2
Given that µ ∈ H(X)
--
the integral in (30) tends to zero by the dominated
convergence theorem [12
----
which concludes the proof from iii).
Remark 3: It is important to mention that from i)
--
we
have that µ ∈ H(X|µn ) ∀n
[[[ META ]]]
xmlpapertitle -> Shannon Entropy Convergence Results in the Countable Infinite Case
parsed -> yes
xmlabstract -> The convergence of the Shannon entropy in the countable infinity case is revisited and extended in this work. New results are presented that provide necessary and sufficient conditions for the convergence of the entropy in different settings, including scenarios with both finitely and infinitely supported measures. These results show some connections between the Shannon entropy convergence and the convergence in information divergence.
xmlsessionid -> S1.T8.2
xmlendtime -> 11:10
xmlpaperid -> 1569565389
xmlsession -> S1.T8: Information Theoretic Tools and Properties
xmldate -> 1341219000000
file -> C:\Users\Amine\git\Project\PapersProject\PapersDataset\1569565389.txt
xmlstarttime -> 10:50
xmlauthors -> Jorge Silva, Patricio Parada
xmlroom -> Stratton (491)
[[[ LINKS ]]]

