[[[ ID ]]]
1569563725
[[[ INDEX ]]]
111
[[[ TITLE ]]]
Classification with High-Dimensional Sparse Samples
[[[ AUTHORS ]]]
D. Huang
S. Meyn
[[[ ABSTR ]]]
Not saved
[[[ BODY ]]]
Not saved
[[[ REFS ]]]
T. Zhang
--
“Text categorization based on regularized linear
classiﬁcation methods
----
D. Huang
--
“Error exponents for composite hypothesis
testing with small samples
[[[ META ]]]
xmlpapertitle -> Classification with High-Dimensional Sparse Samples
parsed -> yes
xmlabstract -> The task of the binary classification problem is to determine which of two distributions has generated a length-$n$ test sequence. The two distributions are unknown; however two training sequences of length $N$, one from each distribution, are observed. The distributions share an alphabet of size $m$, which is significantly larger than $n$ and $N$. How does $N,n,m$ affect the probability of classification error? We characterize the achievable error rate in a high-dimensional setting in which $N,n,m$ all tend to infinity, under the assumption that probability of any symbol is $O(m^{-1})$. The results are:

There exists an asymptotically consistent classifier if and only if $m=o(\min\{N^2,Nn\})$. This extends the previous consistency result in [1] to the case $N\neq n$.

For the sparse sample case where $\max\{n,N\}=o(m)$, finer results are obtained: The best achievable probability of classification error decays as $-\log(P_e)=J \min\{N^2, Nn\}(1+o(1))/m$ with $J>0$.

A weighted coincidence-based classifier has non-zero generalized error exponent $J$. 

The $\ell_2$-norm based classifier has $J=0$.
xmlsessionid -> S14.T8.1
xmlendtime -> 17:00
xmlpaperid -> 1569563725
xmlsession -> S14.T8: High-Dimensional Inference
xmldate -> 1341499200000
file -> C:\Users\Amine\git\Project\PapersProject\PapersDataset\1569563725.txt
xmlstarttime -> 16:40
xmlauthors -> Dayu Huang, Sean Meyn
xmlroom -> Stratton (491)
[[[ LINKS ]]]

