[[[ ID ]]]
1569565113
[[[ INDEX ]]]
206
[[[ TITLE ]]]
On Lossless Universal Compression of Distributed Identical Sources
[[[ AUTHORS ]]]
A. Beirami
F. Fekri
[[[ ABSTR ]]]
Not saved
[[[ BODY ]]]
Not saved
[[[ REFS ]]]
D. Slepian
--
sources
----
S. Pradhan
--
syndromes (DISCUS): design and construction
----
M. Sartipi
--
rate region
----
A. Beirami
--
compression for ﬁnite-length sequences
----
N. Merhav
--
theorem of universal coding
----
A. Beirami
--
“Memory-assisted universal source coding
----
B. Clarke
--
methods
----
M. Drmota
--
regret
----
R. E. Krichevsky
--
encoding
----
N. Alon
--
“Source coding and graph entropies
[[[ META ]]]
xmlpapertitle -> On Lossless Universal Compression of Distributed Identical Sources
parsed -> yes
xmlabstract -> The problem of data gathering from distributed sources arises in many applications such as sensor networks and Internet multi-homing. Oftentimes, the correlation in such data can be leveraged to reduce the amount of data transmission. Slepian-Wolf theorem is a well-known framework that targets almost lossless compression of (two) data streams with symbol-by-symbol correlation between the outputs of (two) distributed sources. However, this paper considers a different scenario which does not fit in the Slepian-Wolf framework. We consider two identical but spatially separated sources. We wish to study the universal compression of a sequence of length $n$ from one of the sources provided that the decoder has access to (i.e., memorized) a sequence of length $m$ from the other source. Such a scenario occurs, for example, in the universal compression of data from multiple mirrors of the same server. In this setup, the correlation does not arise from symbol-by-symbol dependency of two outputs from the two sources. Instead, the sequences are correlated through the information that they contain about the unknown source parameter. We show that the finite-length nature of the compression problem at hand requires considering a notion of almost lossless source coding, where coding incurs an error probability $p_e$ that vanishes with sequence length $n$. We obtain a lower bound on the average minimax redundancy of almost lossless codes as a function of the sequence length $n$ and the permissible error probability $p_e$ when the decoder has a memory of length $m$ and the encoders do not communicate. Our results demonstrate that a strict performance loss is incurred when the two encoders do not communicate even when the decoder knows the unknown parameter vector.
xmlsessionid -> S4.T1.1
xmlendtime -> 17:00
xmlpaperid -> 1569565113
linked -> yes
xmlsession -> S4.T1: The Slepian-Wolf and CEO Problems
xmldate -> 1341240000000
file -> PapersDataset\1569565113.txt
xmlstarttime -> 16:40
xmlauthors -> Ahmad Beirami, Faramarz Fekri
xmlroom -> Kresge Rehearsal B (030)
[[[ LINKS ]]]
1 10
----
2 10
----
3 10
----
4 10
----
7 10
----
11 10
----
12 10
----
24 40
----
38 10
----
52 10
----
75 10
----
84 10
----
132 10
----
135 10
----
150 10
----
154 10
----
208 10
----
216 10
----
220 20
----
245 10
----
293 10
----
303 10
----
305 10
----
306 10
----
329 10
----
352 10
----
377 10
----
448 10
----
488 10
----
566 10
----
567 10
----
572 10
----
575 10
----
583 10
----
591 10
----
593 10
----
611 10
