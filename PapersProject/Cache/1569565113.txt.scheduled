[[[ ID ]]]
1569565113
[[[ INDEX ]]]
206
[[[ TITLE ]]]
On Lossless Universal Compression of Distributed Identical Sources
[[[ AUTHORS ]]]
A. Beirami
F. Fekri
[[[ ABSTR ]]]
Not saved
[[[ BODY ]]]
Not saved
[[[ REFS ]]]
D. Slepian
--
“Noiseless coding of correlated information
sources
----
S. Pradhan
--
“Distributed source coding using
syndromes (DISCUS): design and construction
----
M. Sartipi
--
“Distributed source coding using short to
moderate length rate-compatible LDPC codes: the entire Slepian-Wolf
rate region
----
A. Beirami
--
“Results on the redundancy of universal
compression for ﬁnite-length sequences
----
N. Merhav
--
“A strong version of the redundancy-capacity
theorem of universal coding
----
A. Beirami
--
“Memory-assisted universal source coding
----
B. Clarke
--
“Information-theoretic asymptotics of Bayes
methods
----
M. Drmota
--
“Precise minimax redundancy and
regret
----
R. E. Krichevsky
--
“The performance of universal
encoding
----
N. Alon
--
“Source coding and graph entropies
[[[ META ]]]
xmlpapertitle -> On Lossless Universal Compression of Distributed Identical Sources
parsed -> yes
xmlabstract -> The problem of data gathering from distributed sources arises in many applications such as sensor networks and Internet multi-homing. Oftentimes, the correlation in such data can be leveraged to reduce the amount of data transmission. Slepian-Wolf theorem is a well-known framework that targets almost lossless compression of (two) data streams with symbol-by-symbol correlation between the outputs of (two) distributed sources. However, this paper considers a different scenario which does not fit in the Slepian-Wolf framework. We consider two identical but spatially separated sources. We wish to study the universal compression of a sequence of length $n$ from one of the sources provided that the decoder has access to (i.e., memorized) a sequence of length $m$ from the other source. Such a scenario occurs, for example, in the universal compression of data from multiple mirrors of the same server. In this setup, the correlation does not arise from symbol-by-symbol dependency of two outputs from the two sources. Instead, the sequences are correlated through the information that they contain about the unknown source parameter. We show that the finite-length nature of the compression problem at hand requires considering a notion of almost lossless source coding, where coding incurs an error probability $p_e$ that vanishes with sequence length $n$. We obtain a lower bound on the average minimax redundancy of almost lossless codes as a function of the sequence length $n$ and the permissible error probability $p_e$ when the decoder has a memory of length $m$ and the encoders do not communicate. Our results demonstrate that a strict performance loss is incurred when the two encoders do not communicate even when the decoder knows the unknown parameter vector.
xmlsessionid -> S4.T1.1
xmlendtime -> 17:00
xmlpaperid -> 1569565113
xmlsession -> S4.T1: The Slepian-Wolf and CEO Problems
xmldate -> 1341240000000
file -> C:\Users\Amine\git\Project\PapersProject\PapersDataset\1569565113.txt
xmlstarttime -> 16:40
xmlauthors -> Ahmad Beirami, Faramarz Fekri
xmlroom -> Kresge Rehearsal B (030)
[[[ LINKS ]]]

