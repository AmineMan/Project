[[[ ID ]]]
1569551541
[[[ INDEX ]]]
4
[[[ TITLE ]]]
Data Processing Inequalities Based on a Certain Structured Class of Information Measures with Application to Estimation Theory
[[[ AUTHORS ]]]
N. Merhav
[[[ ABSTR ]]]
Not saved
[[[ BODY ]]]
Not saved
[[[ REFS ]]]
L. G¨ orﬁ
--
“f –dissimilarity: a generalization of the afﬁnity
y
of several distributions
----
G. Kaplan
--
“Information rates and error exponents
of compound channels with application to antipodal signaling in a fading
¨
environment
----
A. J. Viterbi
--
Principles of Digital Communication and
Coding
----
E. Weinstein
--
“Lower bounds on the mean square
estimation error
----
M. Zakai
--
“A generalization of the rate-distortion theory and
applications
----
J. Ziv
--
“Some lower bounds on signal parameter estimation
----
J. Ziv
--
“On functionals satisfying a data-processing
theorem
----
2
where θ = 2σ 2 /[N0 (1 + 2σ 2 E/N0 )]. On substituting this
channel into IG (U ; Y )
--
we eventually obtain in the high SNR regime (see [10]
for details):


----
which is again signiﬁcantly smaller than our bound. Thus,
we observe that while the WWB
--
when it comes
to channels with fading
[[[ META ]]]
xmlpapertitle -> Data Processing Inequalities Based on a Certain Structured Class of Information Measures with Application to Estimation Theory
parsed -> yes
xmlabstract -> We study data processing inequalities (DPI's) that are derived from a certain class of generalized information measures, where a series of convex functions and multiplicative likelihood ratios are nested alternately. A certain choice of the convex functions leads to an information measure that extends the notion of the Bhattacharyya distance: While the ordinary Bhattacharyya distance is based on the geometric mean of two replicas of the channel's conditional distribution, the more general one allows an arbitrary number of replicas. We apply the DPI induced by this information measure to a detailed study of lower bounds of parameter estimation under additive white Gaussian noise (AWGN) and show that in certain cases, tighter bounds can be obtained by using more than two replicas. While the resulting bound may not compete favorably with the best bounds available for the ordinary AWGN channel, the advantage of the new lower bound, becomes significant in the presence of channel uncertainty, like unknown fading. This is explained by the convexity property of the information measure.
xmlsessionid -> S7.T8.5
xmlendtime -> 16:20
xmlpaperid -> 1569551541
xmlsession -> S7.T8: Information Inequalities
xmldate -> 1341324000000
file -> C:\Users\Amine\git\Project\PapersProject\PapersDataset\1569551541.txt
xmlstarttime -> 16:00
xmlauthors -> Neri Merhav
xmlroom -> Stratton (491)
[[[ LINKS ]]]

