[[[ ID ]]]
1569566673
[[[ INDEX ]]]
517
[[[ TITLE ]]]
Lossy Common Information of Two Dependent Random Variables
[[[ AUTHORS ]]]
K. Viswanatha
E. Akyol
K. Rose
[[[ ABSTR ]]]
Not saved
[[[ BODY ]]]
Not saved
[[[ REFS ]]]
R. Gray
--
“Source coding for a simple network”
----
R. Ahlswede
--
“On common information and related
characteristics of correlated information sources”
----
S. Kamath
--
“A new dual to the Gács - Körner
common information deﬁned via the Gray-Wyner system
----
Proof: The proof follows in very similar lines to the proof
of Theorem 1. The original Gray-Wyner’s characterization is
in fact sufﬁcient in this case. Again we ﬁrst assume that there
˜
˜
are unique channels P (X|X)
P (Y |Y ) which achieve
RX (D1 )
--
D2 ) which lie on both the planes R0 + R1 =
RX (D1 ) and R0 + R2 = RY (D2 )
----
Writing similar inequality relations for Y
--
it follows that for all
X
˜
joint densities satisfying (28) and for which P (X|X) ∈ PD1
Y
˜
and P (Y |Y ) ∈ PD2 
[[[ META ]]]
xmlpapertitle -> Lossy Common Information of Two Dependent Random Variables
parsed -> yes
xmlabstract -> The two most prevalent notions of common information are due to Wyner and Gács-Körner and both the notions can be stated as two different characteristic points in the lossless Gray-Wyner region. Although these quantities can be easily evaluated for random variables with infinite entropy (eg. continuous random variables), the operational significance underlying their definition is applicable only to the lossless framework. The primary objective of this paper is to generalize these two notions of common information to the lossy Gray-Wyner network, which extends the theoretical intuition underlying their definitions for general sources and distortion measures. We begin with the lossy generalization of Wyner's common information, defined as the minimum rate on the shared branch of the Gray-Wyner network at minimum sum rate when the two decoders reconstruct the sources subject to individual distortion constraints. We derive a complete single letter information theoretic characterization for this quantity and use it to compute the common information of symmetric bivariate Gaussian random variables. We then derive similar results to generalize Gács-Körner's definition to the lossy framework. These two characterizations allow us to carry the practical insight underlying the two notions of common information to general sources and distortion measures.
xmlsessionid -> S3.T8.3
xmlendtime -> 15:40
xmlpaperid -> 1569566673
xmlsession -> S3.T8: Directed Information, Common Information, and Divergence
xmldate -> 1341235200000
file -> C:\Users\Amine\git\Project\PapersProject\PapersDataset\1569566673.txt
xmlstarttime -> 15:20
xmlauthors -> Kumar Viswanatha, Emrah Akyol, Kenneth Rose
xmlroom -> Stratton (491)
[[[ LINKS ]]]

