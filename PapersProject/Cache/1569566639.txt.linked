[[[ ID ]]]
1569566639
[[[ INDEX ]]]
505
[[[ TITLE ]]]
Relaxed Gaussian Belief Propagation
[[[ AUTHORS ]]]
Y. El-Kurdi
D. Giannacopoulos
W. Gross
[[[ ABSTR ]]]
Not saved
[[[ BODY ]]]
Not saved
[[[ REFS ]]]
Y. Weiss
--
Gaussian graphical models of arbitrary topology
----
J. Hammersley
--
Lattices
----
have also introduced
demonstrated empirical reductions of up to 12.7 times. We
diagonally dominant inverse covariance matrices. We have
to accelerate GaBP convergence for ill-conditioned weakly
We have presented a new relaxed GaBP algorithm (R-GaBP)
V. C ONCLUSION
--
architectures
[[[ META ]]]
xmlpapertitle -> Relaxed Gaussian Belief Propagation
parsed -> yes
xmlabstract -> "THIS PAPER IS ELIGIBLE FOR THE STUDENT PAPER AWARD"

The Gaussian Belief Propagation (GaBP) algorithm executed on Gaussian Markov Random Fields can take a large number of iterations to converge if the precision matrix is ill-conditioned and weakly diagonally dominant. Such matrices can arise from many practical problem domains. In this study, we propose a relaxed GaBP algorithm that demonstrates a reduction in the number of GaBP iterations (of up to 12.7 times). We also propose a second relaxed algorithm that avoids the need of determining the relaxation factor a priori which can achieve significant iteration reductions using heuristic measures. We illustrate a case for ill-conditioned weakly diagonally dominant precision matrices, where the new relaxed GaBP algorithm can be implemented without any significant increase, over the original GaBP, in both the computational complexity and the memory requirements.
xmlsessionid -> S11.T7.3
xmlendtime -> 10:50
xmlpaperid -> 1569566639
linked -> yes
xmlsession -> S11.T7: Message Passing Algorithms
xmldate -> 1341477000000
file -> PapersDataset\1569566639.txt
xmlstarttime -> 10:30
xmlauthors -> Yousef El-Kurdi, Dennis Giannacopoulos, Warren Gross
xmlroom -> Stratton (407)
[[[ LINKS ]]]
82 9
----
224 9
----
393 9
