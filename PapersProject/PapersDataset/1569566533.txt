Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 16:41:51 2012
ModDate:        Tue Jun 19 12:56:28 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      454578 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566533

Enhancing the Error Correction of Finite Alphabet
Iterative Decoders via Adaptive Decimation
Shiva Kumar Planjery, Bane Vasi´
c

David Declercq

Dept. of Electrical and Computer Eng.
University of Arizona
Tucson, AZ 85721, U.S.A.
Email: {shivap,vasic}@ece.arizona.edu

ETIS
ENSEA/UCP/CNRS UMR 8051
95014 Cergy-Pontoise, France
Email: declercq@ensea.fr

ﬁxing them to these values while continuing to estimate the
remaining bits (see [5] for references). In [5], the decimation
was carried out by the FAID based on messages passed for
some iterations, and a decimation scheme was provided such
that a 7-level DFAID matched the good performance of the
original 7-level FAID while being analyzable at the same time.
In this paper, we show how decimation can be used
adaptively to further increase the guaranteed error correction
capability of FAIDs. The adaptive scheme has only marginally
increased complexity, but can signiﬁcantly improve the errorrate performance compared to the FAIDs. We speciﬁcally
focus on decoders that propagate only 3-bit messages and
column-weight three codes since these enable simple implementations and thus have high practical value. We also provide
some analysis of the decoders which suggests that the failures
are linked to stopping sets of the code. Numerical results are
also provided to validate the efﬁcacy of the proposed scheme.

Abstract—Finite alphabet iterative decoders (FAIDs) for LDPC
codes were recently shown to be capable of surpassing the
Belief Propagation (BP) decoder in the error ﬂoor region on the
Binary Symmetric channel (BSC). More recently, the technique
of decimation which involves ﬁxing the values of certain bits
during decoding, was proposed for FAIDs in order to make
them more amenable to analysis while maintaining their good
performance. In this paper, we show how decimation can be used
adaptively to further enhance the guaranteed error correction
capability of FAIDs that are already good on a given code.
The new adaptive decimation scheme proposed has marginally
added complexity but can signiﬁcantly improve the slope of the
error ﬂoor performance of a particular FAID. We describe the
adaptive decimation scheme particularly for 7-level FAIDs which
propagate only 3-bit messages and provide numerical results for
column-weight three codes. Analysis suggests that the failures of
the new decoders are linked to stopping sets of the code.

I. I NTRODUCTION
The error ﬂoor problem of low-density parity-check (LDPC)
codes under iterative decoding is now a well-known problem,
where the codes suffer from an abrupt degradation in their
error-rate performance in spite of having good minimum
distance. The problem has been attributed to the presence
of harmful conﬁgurations generically termed as trapping sets
[1] present in the Tanner graph, which cause the iterative
decoder to fail for some low-noise conﬁgurations, thereby
reducing its guaranteed error correction capability to an extent
that is far from the limits of maximum likelihood decoding.
More importantly for the BSC, the slope of the error ﬂoor is
governed by the guaranteed correction capability [2].
Recently, a new class of ﬁnite alphabet iterative decoders
(FAIDs) that have a much lower complexity than the BP
decoder, were proposed for LDPC codes on the BSC [3] [4]
and were shown to be capable of outperforming BP in the
error ﬂoor. Numerical results on several column-weight three
codes showed that there exist 7-level FAIDs requiring only
3 bits of precision that can achieve a better guaranteed error
correction ability than BP, thereby surpassing it in the error
ﬂoor region. However, analyzing these decoders for providing
performance guarantees proved to be difﬁcult.
More recently, decimation-enhanced FAIDs [5] were proposed for BSC in order to make FAIDs more amenable to analysis while maintaining their good performance. The technique
of decimation involves guessing the values of certain bits, and

II. P RELIMINARIES
Let G = (V ∪ C, E) denote the Tanner graph of an (n,k)
binary LDPC code C with the set of variable nodes V =
{v1 , · · · , vn } and set of check nodes C = {c1 , · · · , cm }. E is
the set of edges in G. A code C is said to be dv -left-regular if
all variable nodes in V of graph G have the same degree dv .
The degree of a node is the number of its neighbors. dmin is
the minimum distance of the code C.
A trapping set is a non-empty set of variable nodes in G
that are not eventually corrected by the decoder [1].
A multilevel FAID F is a 4-tuple given by F =
(M, Y, Φv , Φc ) [4]. The messages are levels conﬁned to an
alphabet M of size 2s+1 deﬁned as M = {0, ±Li : 1 ≤ i ≤
s}, where Li ∈ R+ and Li > Lj for any i > j. Y denotes the
set of possible channel values that are input to the decoder.
For the case of BSC as Y = {±C : C ∈ R+ }, and for each
variable node vi ∈ V , the channel value yi ∈ Y is determined
by yi = (−1)ri C, where ri is received from the BSC at vi .
Let m1 , · · · , mu denote the incoming messages to a node.
The update function Φc : Mdc −1 → M is used at a check
node with degree dc , and is deﬁned as


dc −1

Φc (m1 , . . . , mdc −1 ) = 

sgn(mj )
j=1

1

min

j∈{1,...,dc −1}

(|mj |)

TABLE I
Φv OF A 7- LEVEL FAID FOR yi = +C FOR A CODE WITH dv = 3
m1 \m2
-L3
-L2
-L1
0
L1
L2
L3

-L3
-L3
-L3
-L2
-L1
-L1
-L1
L1

-L2
-L3
-L1
-L1
0
L1
L1
L3

-L1
-L2
-L1
0
0
L1
L2
L3

0
-L1
0
0
L1
L2
L3
L3

L1
-L1
L1
L1
L2
L2
L3
L3

L2
-L1
L1
L2
L3
L3
L3
L3

the framework of FAIDs before we delve into the details of
adaptive decimation.
Deﬁnition 2: A variable node vi is said to be decimated
at the end of lth iteration if bk is set to b∗ ∀k ≥ l. Also,
i
i
∗
mk (vi , N (vi )) = {(−1)bi Ls }, ∀k ≥ l irrespective of its
incoming messages, i.e., vi always sends the strongest possible
messages.
Remark: If a node vi is decimated, then all its descendants
in the computation tree Tik (G) can be deleted since the node
∗
always sends (−1)bi Ls to its parent.
A decimation rule β : Y × Mdv → {−1, 0, 1} is a function
used by the decoder to decide whether a variable node should
be decimated and what value it should be decimated to. Let
γi denote the output of a decimation rule applied to a node
vi . If γi = 0, then the node is not decimated. If γi = 1, then
b∗ = 0, and if γi = −1, then b∗ = 1
i
i
Remark: The decimation rule is a function of the channel
value and the most recent incoming messages received by the
variable node before the decimation rule is applied.
We shall refer to each instance of applying a decimation
rule on all the variable nodes as a decimation round.
There are two key aspects to note regarding the application
of a decimation rule: 1) the decimation rule is applied after
messages are passed iteratively for some l iterations, and
2) after each instance of applying the decimation rule, all
messages are cleared to zero (which is practically restarting the
decoder except that the decimated nodes remain decimated).
Let Nd denote the number of decimation rounds carried out
by the decoder with a given decimation rule β beyond which
no more variable nodes are decimated.
Deﬁnition 3: The residual graph G is the induced subgraph of the set of variable nodes in G that are not decimated
after Nd decimation rounds.
We can now formally deﬁne the class of adaptive
decimation-enhanced multilevel FAIDs (ADFAIDs) as follows.
A decoder belonging to such a class denoted by F A is deﬁned
as F A = (M, Y, ΦD , Φd , Φr , B, Φc ), where the sets M and
v
v
v
Y, and the map Φc are same as the ones deﬁned for a multilevel FAID. The map ΦD : Y × Mdv −1 × {−1, 0, 1} → M is
v
the update rule used at the variable node. It requires the output
of a decimation rule β as an one of its arguments and also uses
the maps Φd : Y × Mdv −1 → M and Φr : Y × Mdv −1 → M
v
v
to compute its output. For simplicity, we deﬁne it for the case
of dv = 3 as follows.

L3
L1
L3
L3
L3
L3
L3
L3

where sgn denotes the standard signum function.
The update function Φv : Y ×Mdv −1 → M is a symmetric
rule used at a variable node with degree dv and is deﬁned as


dv −1

Φv (yi , m1 , m2 , · · · , mdv −1 ) = Q 

mj + ωi · yi  .
j=1

The function Q is deﬁned based on a threshold set T = {Ti :
1 ≤ i ≤ s + 1} where Ti ∈ R+ and Ti > Tj if i > j, and
Ts+1 = ∞, such that Q(x) = sgn(x)Li if Ti ≤ |x| < Ti+1 ,
and Q(x) = 0 otherwise. The weight ωi is computed at node
vi using a symmetric function Ω : Mdv −1 → {0, 1}. Based
on this, Φv can be described as a linear-threshold (LT) or nonlinear threshold (NLT) function. If Ω = 1 (or constant), then
it is an LT function, else it is an NLT function. Φv can also be
described as a look-up table (LUT) as shown in Table I (for
yi = −C, it can be obtained from symmetry). This rule is an
NLT rule and will be one of the rules used in the proposed
decimation scheme.
Let Tik (G) denote the computation tree of graph G corresponding to a decoder F enumerated for k iterations with
variable node vi ∈ V as its root. A node w ∈ Tik (G) is a
descendant of a node u ∈ Tik (G) if there exists a path starting
from node w to the root vi that traverses through node u.
Deﬁnition 1 (Isolation assumption): Let H be a subgraph
of G induced by P ⊆ V with check node set W ⊆ C. The
computation tree Tik (G) with the root vi ∈ P is said to be
isolated if and only if for any node u ∈ P ∪ W in Tik (G), u
/
does not have any descendant belonging to P ∪ W . If Tik (G)
is isolated ∀vi ∈ P , then the subgraph H is said to satisfy the
isolation assumption in G for k iterations.
Remark: The above deﬁnition is a revised version of the
one given in [3].
The critical number of a FAID F on a subgraph H is the
smallest number of errors for which F fails on H under the
isolation assumption.
Let N (u) denote the set of neighbors of a node u in the
graph G and let N (U ) denote the set of neighbors of all u ∈
U . Let mk (vi , N (vi )) denote the set of outgoing messages
from vi to all its neighbors in the k th iteration. Let bk denote
i
the bit associated to a variable node vi ∈ V that is decided by
the iterative decoder at the end of the k th iteration.

ΦD (yi , m1 , m2 , γi )
v

=

Φd (yi , m1 , m2 ),
v
Φr (yi , m1 , m2 ),
v
γi Ls ,

if γi = 0, p ≤ Nd
if γi = 0, p > Nd
if γi = ±1

where p denotes the pth decimation round completed by the
decoder. The maps Φd and Φr are deﬁned as either LT or NLT
v
v
functions or as look-up tables similar to Φv of a FAID F .
Remark: The new class of decoders proposed in this paper
use two different maps, Φd and Φr , for updating the messages
v
v
on non-decimated variable nodes. Φd is the map used to
v
update messages speciﬁcally during the decimation procedure,

III. A DAPTIVE DECIMATION - ENHANCED FAIDS
We will ﬁrst provide some deﬁnitions and notations related
to the concept of decimation and discuss its incorporation into

2

whereas Φr is the map used to decode the remaining nonv
decimated nodes after the decimation procedure is completed.
Also note that for the case of |M| = 7, we restrict the deﬁnition of Φd to satisfy Φd (C, 0, 0) = L1 , Φd (C, L1 , L1 ) = L2 ,
v
v
v
and Φd (C, L2 , L2 ) = L3 . Φr is also deﬁned similarly.
v
v
Proposition 1: Given a decimation rule β, if the number of
decimated nodes after the pth decimation round is the same as
the number of decimated nodes after the (p − 1)th decimation
round, then no additional nodes will get decimated in the
subsequent decimation rounds.
Remark: This would be the stopping criterion used for the
decimation procedure. In the above case, Nd = p.
The set B is the set of decimation rules used for adaptive
decimation and for any β ∈ B, it satisﬁes the following
properties (speciﬁed for dv = 3).
1) β(C, m1 , m2 , m3 ) = −β(−C, −m1 , −m2 , −m3 )
∀m1 , m2 , m3 ∈ M
2) β(C, m1 , m2 , m3 ) = −1 and β(−C, m1 , m2 , m3 ) = 1
∀m1 , m2 , m3 ∈ M
3) Given m1 , m2 , m3 ∈ M, if β(C, m1 , m2 , m3 ) = 1, then
β(C, m1 , m2 , m3 ) = 1 ∀m1 , m2 , m3 ∈ M such that
m1 ≥ m1 , m2 ≥ m2 , and m3 ≥ m3 .
Remark: Property 2 implies that a node vi can be decimated
to zero only if yi = C and to one only if yi = −C.
Consequently a node initially correct will never be decimated
to a wrong value and a node initially wrong will never be
decimated to the correct value. Then, a necessary condition
for successful decoding is that no node initially in error is
decimated. We shall now restrict our discussion to dv = 3 for
the remainder of the paper.
For a given decimation rule β, a set Ξ can be used to completely specify β, where Ξ is deﬁned as the set of all unordered
triples (m1 , m2 , m3 ) ∈ M3 such that β(C, m1 , m2 , m3 ) =
1. Note that for any unordered triple (m1 , m2 , m3 ) ∈ Ξ,
β(−C, −m1 , −m2 , −m3 ) = −1 by property 1, so Ξ is
sufﬁcient to completely specify β. A β is considered to be a
conservative decimation rule if |Ξ| is small and an aggressive
rule if |Ξ| is large.
Note that the class of decimation-enhanced FAIDs deﬁned
in our previous work [5] is a special case of the newly
proposed decoders where Φd = Φr and B = {β}. In other
v
v
words, only a single non-adaptive decimation rule and a single
map is used for updating messages in the DFAIDs of [5].
For the remainder of the paper, we shall refer to variable
nodes that are initially in error in G as error nodes and variable
nodes that are initially correct as correct nodes.

be decimated as long as none of the error nodes have been
decimated. This is possible since the decimated nodes always
send the strongest message (±Ls ).
Now if a given error pattern is such that the error nodes
are relatively clustered with many interconnections between
them through their neighboring check nodes, then a more
conservative β would have to be used by the decoder to ensure
that none of the error nodes are decimated. However, if the
error pattern is such that the error nodes are more spread
out, then it may be desirable to use a more aggressive β as
there will be many correct nodes in the neighborhood of the
error nodes that can be decimated without decimating the error
nodes, and, in turn, possibly help the decoder to converge. This
is our main motivation for the use of adaptive decimation in
the newly proposed decoders, and we will eventually show
that adaptive decimation can help achieve an increase in the
guaranteed error correction capability of the code.
B. Proposed scheme
We will now describe a particular adaptive decimation
scheme used by the decoder F A in order to enhance the
guaranteed error correction capability. In the proposed scheme,
the set B consists of two decimation rules, namely B =
{β (1) , β (2) }, where Ξ(1) and Ξ(2) are the sets of unordered
triples that completely specify the rules β (1) and β (2) respectively. The rule β (1) is used only once at the end of the third
iteration, and then from that point, β (2) is used after every two
iterations (l = 2). The use of adaptive decimation is carried
out only through β (2) as follows.
We deﬁne a sequence of decimation rules {β (2)[j] }j from
β (2) by considering ordered subsets of Ξ(2) with increasing
size. Let Nβ be the number of rules in the sequence {β (2)[j] }j
and let Ξ(2)[j] denote the set that speciﬁes the rule β (2)[j] .
Then Ξ(2)[j] is deﬁned for each β (2)[j] in a way such that
Ξ(2)[j] ⊂ Ξ(2)[j+1] ∀i ∈ {1, . . . , Nβ −1} with Ξ(2)[Nβ ] = Ξ(2) .
This implies that the sequence of rules are such that β (2)[j+1]
is less conservative than β (2)[j] , with β (2)[1] being the most
conservative and β (2)[Nβ ] = β (2) being least conservative (or
most aggressive). Note that each subset Ξ(2)[j] must be chosen
in a manner that ensures that its corresponding rule β (2)[j]
satisﬁes the properties of β mentioned previously.
For a given error pattern, the decoder starts the decimation
procedure by passing messages using the map Φd and applying
v
the decimation rule β (1) at the end of the third iteration
after which the messages are reset to zero. Then the most
conservative rule in the sequence {β (2)[j] }j , which is β (2)[1] ,
is used after every two iterations (followed by resetting the
messages) until no more nodes can be decimated. The map Φr
v
then is used to decode the remaining non-decimated nodes. If
the decoder still does not converge, then the whole decoding
process is repeated by using a more aggressive rule β (2)[2]
in place of β (2)[1] . This decoding process continues until the
decoder converges or until all rules in the sequence {β (2)[j] }j
have been used. Let Nb denote the number of decimated bits
at the end of a decimation round. The decoding scheme can

A. Motivation for adaptive decimation
Given an error pattern of relatively low weight (≤
), the primary role of decimation is to isolate the
subgraph associated with the error pattern from the rest
of the graph by decimating as many correct nodes outside
this subgraph as possible. The rationale behind resetting the
messages to zero at the end of each decimation round is
to allow more non-decimated correct nodes that are close to
the neighborhood of the decimated correct nodes to possibly
dmin −1
2

3

L3 = 6.6, T1 = 0.8, T2 = 2.8, T3 = 4, C = 1.5. This was
found to be a good rule for decimation.

be summarized as follows. Note that this scheme is devised
particularly for the case of |M| = 7.

D. Analysis

Algorithm 1 Adaptive decimation-enhanced FAID algorithm
1) Set j = 1. Note that Φc will always be used to update
messages at the check node.
2) Initialize γi = 0 ∀vi ∈ V .
3) Start the decimation procedure by passing messages for
three iterations using Φd . If the decoder converges within
v
those three iterations, STOP.
4) Apply decimation rule β (1) for every vi ∈ V . Then reset
all messages to zero and set q = 0.
5) Pass messages for two iterations using Φd for update at
v
the non-decimated nodes. If the decoder converges within
those two iterations, STOP.
6) Apply decimation rule β (2)[j] only on nodes vi for which
γi = 0. Then reset all messages to zero. If Nb > q,
q = Nb and go back to step 5, else go to step 7.
7) Pass messages using Φr on the nodes vi for which γi = 0.
v
8) If decoder converges or has reached maximum allowed
iterations, STOP. Else j = j + 1.
9) If j > Nβ STOP. Else go to step 2.

Due to page constraints, no proofs are provided (but they
will be provided in the journal version of this paper). For the
analysis, we assume that the all-zero codeword is transmitted
which is valid since the decoders considered are symmetric.
Proposition 2: A node vi can receive a ±L3 from its
neighbor cj in the ﬁrst or second iteration after resetting the
messages, only if all nodes in N (cj )\vi have been decimated.
Lemma 1: If β (2)[j] (C, L3 , −L2 , −L2 ) = 1 and if ∀ck ∈
N (vi ) all error nodes in N (ck )\vi are non-decimated, then a
correct node vi will be decimated if it receives an L3 during
a decimation round.
Remark: Note that β (2)[j] will always be deﬁned so that
(2)[j]
β
(C, L3 , −L2 , −L2 ) = 1 for any j as explained in the
next subsection. Also note how resetting messages at the end
of each decimation round can help with decimating more
correct nodes due to the above lemma.
Theorem 1: If β (2)[j] (C, L3 , −L2 , −L2 ) = 1 and no error
node is decimated, then any correct node in the residual graph
G is connected to check nodes that have at least degree-two.
Corollary 1: If Theorem 1 holds and no error node in the
residual graph G is connected to a degree-one check node,
then G is a stopping set.
Remark: Note that if an error node in the residual graph
G is connected to a degree-one check node, it would receive
L3 in every iteration for the remainder of the decoding (again
assuming no error nodes are decimated), and this will most
likely lead to a decoder convergence. Therefore, if no error
node is decimated, the decoder is more likely to fail when the
residual graph G is a stopping set (refer to [1] for details).
The above remark is an important observation since we can
now design the rules β (1) and the sequence {β (2)[j] }j based
on analyzing error patterns whose errors are entirely contained
in the minimal stopping sets of a given code. For instance, if
our goal is to correct up to t-errors, then we consider all error
patterns up to a weight t in the stopping sets in order to design
β (1) and {β (2)[j] }j .
If FAID F with Φv = Φr has a critical number of t+1
v
on a stopping set whose induced subgraph is H, then F A
is guaranteed to correct up to t errors introduced in H on
the code if the residual graph is H. In other words, on a
particular code, Φr is more likely to correct all error patterns
v
up to weight-t whose support lies in a stopping set in the code,
if it has a critical number of t + 1 on the stopping set.

Remarks: 1) The only criterion used by the decoder to
decide when to use a more aggressive rule β (2)[j] on a given
error pattern is whether the decoding has failed. 2) The reason
for applying β (1) at the end of third iteration is that at least
three iterations are required for a ±L3 to be passed. 3)The
reason for the choice of every 2 iterations for applying β (2)[j]
is because 2 iterations is small enough to help prevent the
growth of wrong message strengths but sufﬁcient to allow all
levels in M to be passed.
C. Choice of Φr and Φd
v
v
For the proposed decoders, the map Φr is simply chosen to
v
be the Φv of a particular FAID already known to be good on a
given code, and for which we want to improve the guaranteed
error correction capability. For the numerical results, Φr is
v
chosen to be the Φv of a 7-level FAID deﬁned by Table I.
The choice of Φd on the other hand is non-trivial. It is
v
designed based on analyzing messages that are passed within
dense subgraphs that could potentially be trapping sets for
a given FAID when errors are introduced in them under the
isolation assumption. The rule is chosen under the premise that
the growth of message strengths within the subgraph should
be slow since many correct nodes in the subgraph would most
likely be connected to error nodes, and multiple error nodes
may be interconnected to each other in the subgraph (if the
number of errors introduced is comparable to the size of the
subgraph). Explicit design methods for Φd are not discussed in
v
this paper, but we provide a particular Φd that was designed
v
based on the above philosophy and used for the numerical
results. It is an LT rule (see Section II), so it can be described
by assigning values to elements in M, T , and Y. The map is
deﬁned with the following assignments; L1 = 1.1, L2 = 2.3,

E. Discussion on design of decimation rules β (1) and β (2)
The design of β (1) involves selecting the triples that should
be included in Ξ(1) , which depends on the number of errors
we are trying to correct and the type of harmful subgraphs
present in G. β (1) should be chosen to be conservative enough
so that no error nodes are decimated. On the other hand,
the design of β (2) not only involves selecting the triples that
should be included in Ξ(2) , but also determining a speciﬁc

4

TABLE II
S UBSET Γ OF Ξ(2) DESIGNED FOR (732, 551) CODE
m2
L2
L2
L1
L2
L2

m3
L2
L1
L1
0
−L1

m1
L2
L1
L2
L2

m2
L1
L1
0
L1

m3
0
L1
0
−L1

m1
L1
L2
L2
L1
L1

m2
L1
L2
0
L1
0

Frame error rate (FER)

m1
L2
L2
L2
L2
L2

−1

10

m3
0
−L2
−L1
−L1
0

TABLE III
S UBSET Γ OF Ξ(2) DESIGNED FOR (155, 64) TANNER CODE
m1
L2
L2
L2

m2
L2
L2
L2

m3
L2
L1
0

m1
L2
L2

m2
L1
L1

m3
L1
0

m1
L2
L2
L2

m2
L2
L1
0

BP
7-level FAID
7-level ADFAID

−2

10

−3

10

−4

10

−5

10

−6

10

−7

10

−8

10

−9

10

m3
−L1
−L1
0

−2

10

10

−1

Cross-over probability (α)
Fig. 1.

FER performance comparison on the (155, 64) Tanner code

0

ordering on the triples that will be included in subsets Ξ(2)[j]
which determine the sequence of rules {β (2)[j] }j used starting
from the least conservative rule, and this is dependent on the
structure of the code. Both rules can be designed by analyzing
them on errors introduced in stopping sets of the code.
In order to specify the set Ξ(1) , we just specify the message
triples with the weakest values. For specifying Ξ(2) in a
concise way, we shall introduce some notations. Let Ξ(2) be
divided into two disjoint subsets, i.e., Ξ(2) = Λ∪Γ, where Λ is
a subset that contains all triples (L3 , m2 , m3 ) ∈ M3 such that
m2 , m3 ≥ −L2 . Based on the analysis described previously,
any Ξ(2)[j] deﬁned should always have Λ as its subset,
regardless of the code. The subset Γ, which is dependent on the
code, is an ordered set whose ordering determines the subsets
used to specify the sequence of rules {β (2)[j] }j .

10

BP
7-level FAID
7-level ADFAID

Frame error rate (FER)

−1

10

−2

10

−3

10

−4

10

−5

10

−6

10

−7

10

−8

10

10

−3

10

−2

Cross-over probability (α)
Fig. 2. FER comparison on the (732, 551) structured code with dmin = 12

simulation shown in Fig. 2, which is signiﬁcant since the code
has dmin = 12. This shows that for certain high-rate codes
whose graphs are relatively dense and for which it becomes
difﬁcult to ensure high dmin in the code, the FAIDs with
adaptive decimation can possibly come close to achieving the
guaranteed error correction of maximum likelihood decoding.
Note that the 7-level ADFAIDs are still 3-bit message passing
decoders which have reasonable complexity, and that is still
lower than BP.

IV. N UMERICAL R ESULTS AND D ISCUSSION
Numerical results are provided in Fig. 1 and Fig. 2 for two
codes: the well-known (155, 64) Tanner code and a structured
rate 0.753 (732, 551) code constructed based on latin squares
[6] with dmin = 12. For the Tanner code, the set Ξ(1) contains
all triples (m1 , m2 , m3 ) ∈ M3 such that (m1 , m2 , m3 ) ≥
(L3 , 0, 0) and (m1 , m2 , m3 ) ≥ (L2 , L2 , L1 ) (comparison
is componentwise). For the high-rate structured code, Ξ(1)
contains all triples such that (m1 , m2 , m3 ) ≥ (L3 , L1 , −L3 ),
(m1 , m2 , m3 ) ≥ (L3 , −L1 , −L1 ), and (m1 , m2 , m3 ) ≥
(L2 , L1 , L1 ). |Ξ(1) | = 12 for the Tanner code and |Ξ(1) | = 24
for the (732, 551) code. The Γ sets in Ξ(2) = Λ ∪ Γ
for the structured code and the Tanner code are shown
in Tables II and III respectively. The cardinalities of the
subsets of Ξ(2) used by each of the two codes are
{|Ξ(2)[j] |}j = {24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35}
and {|Ξ(2)[j] |}j = {23, 25, 26, 27, 29} respectively. The maximum number of iterations allowed for BP and 7-level FAID,
and for Φr of the 7-level ADFAID, was 100.
v
The signiﬁcant improvement in the slope of the error ﬂoor
by using the 7-level ADFAID is evident. For the Tanner
code, it was veriﬁed that all 6-error patterns are corrected
by the 7-level ADFAID while the 7-level FAID corrects all
5-errors and BP fails on 5-errors. For the high-rate structured
code, no failed 5-error patterns were found in the region of

ACKNOWLEDGMENT
This work was funded by NSF grants CCF-0830245 and
CCF-0963726, and by Institut Universitaire de France grant.
R EFERENCES
[1] T. Richardson, “Error ﬂoors of LDPC codes,” in Proc. 41st Annual
Allerton Conf. on Commun., Control and Computing, 2003.
[2] M. Ivkovic, S. K. Chilappagari, B. Vasic, “Eliminating trapping sets in
low-density parity-check codes by using Tanner graph covers,” IEEE
Trans. Inf. Theory, vol. 54, no. 8, pp. 3763–3768, 2008.
[3] S. K. Planjery, D. Declercq, S. K. Chilappagari, B. Vasic,“Multilevel decoders surpassing belief propagation on the binary symmetric channel,”
Proc. Int. Symp. Inf. Theory, pp. 769–773, Austin, Jul. 2010.
[4] S. K. Planjery, D. Declercq, L. Danjean, B. Vasic, “Finite alphabet
iterative decoders for LDPC codes surpassing ﬂoating-point iterative
decoders,” Electron. Lett. vol. 47, no. 16, pp. 919–921, Aug. 2011.
[5] S. K. Planjery, B. Vasic, D. Declercq, “Decimation-enhanced ﬁnite
alphabet iterative decoders for LDPC codes on the BSC,” Proc. Int.
Symp. Inf. Theory (ISIT’2011), pp. 2383–2387, St. Petersburg, Jul. 2011.
[6] D. V. Nguyen, B. Vasic, M. Marcellin, S. K. Chilappagari,“On the
construction of structured LDPC codes free of small trapping sets,” IEEE
Trans. Inf. Theory, vol. 58, no. 4, pp. 2280–2302, Apr. 2012.

5

