Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Sun Apr 22 15:49:40 2012
ModDate:        Tue Jun 19 12:54:42 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      441051 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569556091

An Optimal Sampling Technique for Distinguishing
Random S-boxes
Paul Stankovski and Martin Hell
Department of Electrical and Information Technology
Lund University, Sweden
E-mail: {paul,martin}@eit.lth.se

of all outputs. The problem with this vector is that, for large
vector sizes, its probability distribution is infeasible both to
compute and to store. Due to this, a shortcut was used in [2],
namely to consider only the weight of the vector. We show that
this weight probability distribution is equivalent to the optimal
probability distribution and, as a result, it is not possible to
further improve the sampling used in [2]. Finally, we give a
new algorithm for computing the vector weight distribution
that improves the one given in [2]. Our new algorithm uses
much less memory (optimal) and saves 80-85% in time.

Abstract—The nonrandom behavior of the outputs of a random S-box can be exploited when constructing distinguishers
for cryptographic primitives. Different methods of constructing
samples from the outputs have been used in the literature.
However, it has been unclear exactly how these methods differ
and which method is optimal. We analyze four different sampling
techniques. We prove that two of these sampling techniques
result in dependent samples. We further show one sampling
technique that is optimal in terms of error probabilities in the
resulting distinguisher. However, this sampling technique is quite
impractical as it requires very large storage. We further show
a fourth sampling technique that is much more practical, and
we prove that it is equivalent to the optimal one. We also show
an improved algorithm for calculating the associated probability
distributions that are required for the attack.

II. P RELIMINARIES
Two outputs from an S-box of size n are equal with
1
probability (at least) n since the same entry may have been
used twice. This simple observation can be used to construct
a distinguisher for random S-boxes.
We consider an a-to-b-bit random S-box that allows
observations before it is rerandomized. A set of such Sbox observations will be referred to as a chunk, and the
observations themselves are denoted s1 , . . . , s . The S-box
may be regarded as a table with n = 2a entries of b bits
each, and we will restrict ourselves to the case when b = 1,
i.e., we observe bits, but the results carry on to the general
case. The number of -bit chunks is denoted k, and we denote
the total number of observations by N = k · .
As noted above, it is known (see [5]) that the xor sum of
a pair of output bits is biased, and this bias stems from the
fact that the same S-box entry may have been probed for both
outputs. More speciﬁcally, for i = j,
1n−1 1
1
1
1
Pr(si = sj ) =
+ = (1+ ) = +2−(a+1) , (1)
2 n
n
2
n
2
and the bias in (1) can be used to construct a distinguisher.
The main problem we study in this paper is exactly how to
construct this distinguisher when the number of S-box observations is more than two. That is, how should a cryptanalyst
use the observations to construct an optimal distinguisher?
The empirical probability distribution as deﬁned by the
sampling is denoted P ∗ . The corresponding (theoretical) probability distribution of the S-box is denoted P1 while its
uniform distribution is denoted P2 . The optimal hypothesis
test is given by the Neyman-Pearson lemma, see e.g., [1].
Lemma 1 (Neyman-Pearson): Let X1 , X2 , . . . , Xt be iid
random variables according to P ∗ . Consider the decision problem corresponding to the hypotheses P ∗ = P1 vs. P ∗ = P2 .

I. I NTRODUCTION
Random S-boxes can appear in cryptanalysis when observations, e.g., linear sums of keystream bits in stream ciphers,
can be regarded as outputs of a large table. In this paper we
study such random S-boxes. More speciﬁcally, we study how
to perform an optimal distinguisher from the observations. A
random S-box is an a-to-b-bit mapping which can be seen
as a table containing n = 2a random entries of b bits each.
Our work is motivated by the analysis of the HC-128 stream
cipher as performed in [2], [5], but the results are applicable
to all random S-boxes. HC-128 is a stream cipher in the
eSTREAM portfolio, and is thus considered to be one of
the most promising stream ciphers today. Indeed, it is very
fast in software and it has been shown to resist cryptanalytic
attacks very well. There are no attacks (not relying on sidechannel information) that are more efﬁcient than exhaustive
key search. The distinguishing attack given in [2] is currently
the most efﬁcient non-generic attack, and that attack is based
on the attack given in [5]. The improvement comes from a
more efﬁcient sampling technique, reducing the number of
keystream bits required by the distinguisher.
We analyze different sampling techniques. We show that the
sampling technique used in [5] signiﬁcantly underestimates the
number of samples needed by the distinguisher as the samples
are not independent. We further prove that it is not possible to
take two independent biased samples at all, unless the S-box
is reinitialized. The optimal sampling technique is thus to take
one sample containing all information, i.e., to consider a vector
This work was sponsored in part by the Swedish Research Council
(Vetenskapsr˚ det) under grant 621-2006-5249.
a

1

For Q ≥ 0 deﬁne a region
At (Q) =

The rest of the paper is organized as follows. In Sections III-A and III-B we prove that APS and LPS are faulty.
In Sections III-C and III-D we give algorithms for computing
the required distributions for VS and WS, respectively. We
also prove that VS and WS are equivalent in terms of the
performance of the resulting distinguisher. Section IV explicitly compares APS, LPS and WS. The paper is concluded in
Section V.

P1 (x1 , x2 , . . . , xt )
>Q .
P2 (x1 , x2 , . . . , xt )

t
t
Let αt = P1 (Ac (Q)) and βt = P2 (At (Q)) be the error
t
probabilities corresponding to the decision region At . Let Bt
be any other decision region with associated error probabilities
α∗ and β ∗ . If α∗ ≤ α, then β ∗ ≥ β.
If we want the error probabilities to be equal we set Q = 1.
In other words, we decide P ∗ = P1 if

P1 (x1 , . . . , xt )
>1 ⇔
indep.
P2 (x1 , . . . , xt )

t

log
i=1

P1 (xi )
> 0,
P2 (xi )

A. All-Pairs Sampling (APS)
The Neyman-Pearson lemma assumes that all samples are
independent and identically distributed. In APS sampling, all
possible bit pairs in an -bit chunk are taken as samples,
producing in total k 2 samples. It is very easy to prove
that these samples are not independent. Consider a chunk
with = 3, where we take the samples (s1 , s2 ), (s1 , s3 ) and
(s2 , s3 ). If we know the ﬁrst two samples, then we also know
the last sample, i.e.,

(2)

and P ∗ = P2 otherwise. The equivalence in (2) is valid when
the samples x1 , . . . , xt are independent.
In our case, the samples xi will be constructed from the
observations sj . Note that the Neyman-Pearson lemma, which
gives the optimal distinguisher, requires that the samples xi
are independent. By sampling technique we mean how to use
the observations to build the samples used in the distinguisher.
If the samples are very easy to construct from the observations, we can say that the online computational complexity of
the attack is given by the number of terms t in the sum (2).
The ofﬂine complexity is the time needed to compute P1 .

H(S2 ⊕ S3 |S1 ⊕ S2 , S1 ⊕ S3 ) = 0,
where H(·) denotes the entropy function, ⊕ denotes xor and
S1 , S2 and S3 are random variables corresponding to the three
observations. This argument is easily extended to the general
case with arbitrary , which also serves as a direct motivation
for deﬁning and using LPS sampling.
Even though the samples are dependent, APS is very easy
to apply. Computing and storing the P1 requires negligible
memory and can be trivially done by hand, see (1). However,
the large number of samples gives an online computational
complexity of k 2 = O(k 2 ).

III. S AMPLING FROM A R ANDOM S- BOX
We will consider the following four sampling techniques:
• All-Pairs Sampling (APS) Take all pairs of observations
(si , sj ), 1 ≤ i < j ≤
as samples. Let P1 be the
distribution corresponding to (1), i.e., Pr(si = sj ) =
1
1
1
1
2 (1 + n ) and Pr(si = sj ) = 2 (1 − n ). P2 is the uniform
distribution, Pr(si = sj ) = Pr(si = sj ) = 1 .
2
• Linear-Pairs Sampling (LPS) Take the pairs of observations (si , si+1 ), 1 ≤ i < as samples and let P1 and
P2 be as for APS above.
• Vector Sampling (VS) Take the vectors (s1 , s2 , . . . , s )
as samples and perform the hypothesis test with the
corresponding vector probability distributions as P1 and
P2 .
• Weight Sampling (WS) Take the vector weights
(s1 , s2 , . . . , s ) =
i=1 si as samples and perform
the hypothesis test with the corresponding vector weight
probability distributions as P1 and P2 .
It is clear that Vector Sampling (VS) is optimal since it
preserves all information in the samples. The drawbacks are
that the distribution is very large in storage (2 ) and that it is
demanding to compute. APS was applied in [4], [5]. It uses
the easily computed bias in (1) and produces many samples.
For observations, 2 samples are produced. Due to the
dependency between samples, LPS was suggested in [2] and
WS was also applied as an improvement. However, it was an
open question whether it was possible to improve over WS as
it appears that not all sample information is retained in the
vector weight samples.

B. Linear-Pairs Sampling (LPS)
In LPS sampling we take (s1 , s2 ) as the ﬁrst sample and
then only take one new sample for each new observation. This
procedure produces − 1 samples per chunk for a total of
k( −1) samples. In order to show that this sampling technique
also gives dependent samples, for P1 we calculate and compare
Pr(s3 = s2 |s2 = s1 ) and Pr(s3 = s2 |s2 = s1 ) to see that the
probability of pair equality in one sample depends on the pair
equality of the previous one.
We regard the S-box as a table with n entries. The ﬁrst time
we read a speciﬁc entry in the table, we say that we “open” the
entry. Consider Pr(s3 = s2 |s2 = s1 ) ﬁrst. Given that s2 = s1 ,
we must have opened precisely two entries in the table, one 0
and one 1. We can now have s3 = s2 in two different ways,
by reading s3 from either an “old” entry (same as s2 ) or a
“new” previously unopened one. Thus, we have
1 n−2
1
1
= .
Pr(s3 = s2 |s2 = s1 ) = 1 · + ·
n 2
n
2
Calculating Pr(s3 = s2 |s2 = s1 ) divides into two cases.
Case A: s1 and s2 were read from the same entry.
Case B: s1 and s2 were read from different entries.
1
The probability of case A is p = n , while that of case B is
n−1
q = 2n . Given case A, the probability that s3 = s2 is
a=

2

1
n−1
n+1
+
=
.
n
2n
2n

Algorithm I – Vector Distribution (vd)

Given case B, the probability that s3 = s2 is
2
n−2
n+2
b= +
=
.
n
2n
2n
In total we get Pr(s3 = s2 |s2 = s1 ) =

Input: S-box size n, vector length , current depth d, current
probability p, probability distribution container dist of length
2 , vector v, number of opened table entries with zeros a0 ,
number of opened table entries with ones a1 .
Output: probability distribution dist.
Initial recursion parameters: dist zeroized,
(d, p, v, a0 , a1 ) = (0, 1, 0, 0, 0).

p
q
1
2
1
·a+
· b = (1 +
)> ,
p+q
p+q
2
n+1
2
from which we conclude that

if
if
if
if

Pr(s3 = s2 |s2 = s1 ) = Pr(s3 = s2 |s2 = s1 ).
This proves that LPS is also erroneous in assuming independence between samples.
One may further note that the same probabilities are valid
for any other overlapping pair, i.e., for Pr(sk = sj |sj = si )
and Pr(sk = sj |sj = si ) for all distinct indices i, j and k.
This dependency may seem natural since the two samples are overlapping in one of the observations. Collecting samples in a non-overlapping fashion according to
(s1 , s2 ), (s3 , s4 ), (s5 , s6 ), and so on, may at ﬁrst glance seem
better. However, by performing similar calculations we can
also prove that

}

The S-box vector probability distribution P1 can be calculated according to Algorithm I, which is stated recursively for
simplicity. The main idea here is simply to keep track of how
many entries in the S-box that have revealed zeros and ones,
as this information will enable us to compute the associated
probabilities at each stage.
The storage requirement for Algorithm I is precisely 2
(probability entries), and since this amount of memory is
necessary to store the resulting probability distribution, no
other algorithm can do better in terms of memory. The time
complexity of Algorithm I is also exponential in , at most
4 = 22 , because every call at depth d results in at most 4
calls at depth d + 1. By employing dynamic programming,
see e.g., [3], it is possible to improve this time complexity to
O(n2 2 ) at the expense of increased storage, O(n2 2 ), but the
running time must still necessarily be exponential in .
For large , i.e., when many observations are generated
before the S-box is reinitialized, the vector sampling technique
is infeasible since the distribution P1 is both too large to store
and too demanding to compute.

Pr(s4 = s3 |s2 = s1 ) = Pr(s4 = s3 |s2 = s1 ).
The corresponding calculations show that
Pr(s4 = s3 |s2 = s1 )

=

Pr(s4 = s3 |s2 = s1 )

=

1
2
1
2

(d == ) { dist[v] += p; return; }
a0
(a0 > 0) vd(dist, n, , d + 1, p · n , v 0, a0 , a1 ); /* old 0 */
a1
(a1 > 0) vd(dist, n, , d + 1, p · n , v 1, a0 , a1 ); /* old 1 */
(a0 + a1 < n) { /* table not exhausted */
n−(a0 +a1 )
vd(dist, n, , d + 1, p ·
, v 0, a0 + 1, a1 ); /* new 0 */
2n
n−(a0 +a1 )
vd(dist, n, , d + 1, p ·
, v 1, a0 , a1 + 1); /* new 1 */
2n

n−2
and
n2
n2 + 3n − 2
1+ 2
.
n (n + 1)
1+

This means that the probability of pair equality in one sample
depends on the previous one in this case as well. This
immediately generalizes to all non-overlapping pairs, i.e., the
same holds for Pr(sj = si |sv = su ) and Pr(sj = si |sv = su )
for all distinct indices i, j, u and v. Since the overlapping and
non-overlapping cases are exhaustive, we can conclude that
any two samples will be dependent. An intuitive explanation
for this is that a sample leaks information about the entries in
the S-box. This information will affect the probability of the
next sample since we may read the same entries as before. We
summarize this result in Theorem 2.
Theorem 2 (Random S-box Sampling Theorem): It is not
possible to extract more than one independent sample from
a chunk s1 , . . . , s of observations from a random S-box.
The advantage of LPS over APS is that fewer samples are
used. The computational complexity of the online phase of
LPS is k( − 1) = O(k ).

D. Weight Sampling (WS)
Now consider WS, for which we take vector weights
(s1 , s2 , . . . , s ) = i=1 si as samples. The corresponding
vector probability distributions P1 and P2 have domains of
size + 1, which is much more manageable than those for
VS.
For WS we begin with P2 . Every vector is equally likely
in the ideal case, so the resulting vector weight probability
distribution is combinatorially determined by
P2 (w) =

C. Vector Sampling (VS)

w

2−

for all vector weights 0 ≤ w ≤ .
P1 can be calculated according to Algorithm II, which is
just a simple modiﬁcation of Algorithm I. Instead of passing
on a (partial) vector we now pass on the (accumulated) vector
weight. The algorithm is, again, stated recursively for simplicity, but it can also be implemented in a dynamic programming
fashion as detailed in [2]. Upper bound formulas for memory
and computational complexity for handling vectors of size

In order to correctly apply the Neyman-Pearson lemma,
we need to ﬁnd the probability of the complete chunk. Thus,
we collect all observations in one vector (s1 , s2 , . . . , s ). The
vector probability distributions P1 and P2 both have a domain
of size 2 , which determines the storage cost for handling P1
and P2 with VS.
For P2 , all vectors are equally likely, resulting in identical
probability values P2 (v) = 2− for all vectors v.

3

Algorithm II – Weight Distribution (wd)

vp(p, v , a0 , a1 ) (we omit some of the less interesting parameters).
All recursive calls to vp(p, v, a0 , a1 ) and vp(p, v , a0 , a1 )
are identical up to depth i, so it is enough to consider any
two such calls vp(p, vi , a0 , a1 ) and vp(p, vi , a0 , a1 ) at depth
i. We need to show that both these calls give rise to the same
quadruple of function calls at depth i + 2, two levels deeper.
vp(p, vi , a0 , a1 ) splits into
a0
vp(p · , vi+1 , a0 , a1 ) and
n
n − a0 − a1
, vi+1 , a0 + 1, a1 )
vp(p ·
2n
at level i + 1, and then into
a0 a1
vp(p · 2 , vi+2 , a0 , a1 ),
n
a0 (n − a0 − a1 )
vp(p ·
, vi+2 , a0 , a1 + 1),
2n2
(n − a0 − a1 )a1
vp(p ·
, vi+2 , a0 + 1, a1 ) and
2n2
(n − a0 − a1 )(n − (a0 + 1) − a1 )
, vi+2 , . . .)
vp(p ·
(2n)2

Input: S-box size n, vector length , current depth d, current
probability p, probability distribution container dist of length
+ 1, weight w, number of opened table entries with zeros
a0 , number of opened table entries with ones a1 .
Output: probability distribution dist.
Initial recursion parameters: dist zeroized,
(d, p, w, a0 , a1 ) = (0, 1, 0, 0, 0).
if
if
if
if

(d == ) { dist[w] += p; return; }
a0
(a0 > 0) wd(dist, n, , d + 1, p · n , w, a0 , a1 ); /* old 0 */
a1
(a1 > 0) wd(dist, n, , d + 1, p · n , w + 1, a0 , a1 ); /* old 1 */
(a0 + a1 < n) { /* table not exhausted */
n−(a0 +a1 )
wd(dist, n, , d + 1, p ·
, w, a0 + 1, a1 ); /* new 0 */
2n
n−(a0 +a1 )
wd(dist, n, , d + 1, p ·
, w + 1, a0 , a1 + 1); /* new 1 */
2n

}

Algorithm III – Vector Probability (vp)

Input: vector probability prob (accumulated), current probability p, S-box size n, vector v (s to s1 from MSB to LSB),
vector length t, number of opened table entries with zeros a0 ,
number of opened table entries with ones a1 .
Output: vector probability prob.
Initial recursion parameters:
(prob, p, t, a0 , a1 ) = (0, 1, , 0, 0).
if (t == 0) { prob += p; return; }
if (v & 1) {/* next output bit is 1 */
a1
if (a1 > 0) vp(prob, p · n , n, v
if (a0 + a1 < n) /* table not exhausted */
n−(a0 +a1 )
, n, v
vp(prob, p ·
2n
} else {/* next output bit is 0 */
a0
if (a0 > 0) vp(prob, p · n , n, v
if (a0 + a1 < n) /* table not exhausted */
n−(a0 +a1 )
vp(prob, p ·
, n, v
2n
}

1, t − 1, a0 , a1 );

1, t − 1, a0 , a1 + 1);
1, t − 1, a0 , a1 );

at level i + 2. Similarly, vp(p, vi , a0 , a1 ) splits into
a1 a0
vp(p · 2 , vi+2 , a0 , a1 ),
n
a1 (n − a0 − a1 )
, vi+2 , a0 + 1, a1 ),
vp(p ·
2n2
(n − a0 − a1 )a0
vp(p ·
, vi+2 , a0 , a1 + 1) and
2n2
(n − a0 − a1 )(n − a0 − (a1 + 1))
, vi+2 , . . .).
vp(p ·
(2n)2

/* old 1 */

/* new 1 */

/* old 0 */

1, t − 1, a0 + 1, a1 );

2

/* new 0 */

2 2

derived from an S-box of size n are given by n2 and n 4 ,
respectively.
We now explicitly prove that VS and WS are equivalent in
terms of keystream complexity of the resulting distinguisher.
We ﬁrst present Algorithm III which calculates the probability of an S-box outputting a speciﬁc vector – the vector
probability. The correctness of Algorithm III follows from its
relationship to Algorithm I.
Theorem 3 (WS is optimal): WS is equivalent to VS in
terms of the Neyman-Pearson test (Lemma 1).
Proof: The proof follows if we can show that all vectors
of equal weight are equiprobable, because the probability
distributions P1 and P2 for WS can then be derived from
those of VS by grouping all probabilities for vectors of equal
weight. In such a case the Neyman-Pearson test is equal for
both sampling techniques, showing that no information is lost
when considering WS over VS.
It is sufﬁcient to show that the vector probability is invariant
under pairwise bit ﬂips. That is, we need to show that the
vector probability does not change if a neighboring pair of
bits in a vector are ﬂipped from 10 to 01 (or from 01 to 10).
Let v = (s1 , s2 , . . . , s ) be a vector for which si = 0
and si+1 = 1 for some i, and let v be the corresponding
vector with si = 1 and si+1 = 0. Let vj denote the vector
(sj , sj+1 , . . . , s ). We need to show that vp(p, v, a0 , a1 ) =

Here we have vi+2 = vi+2 , so the sets of calls are identical.
A direct consequence of Theorem 3 is that, although VS
is highly impractical to use due to its exponential time- and
memory complexities, WS will provide the same result as
VS at a much lower cost, allowing much larger -values.
The computational complexity of the distinguisher is given
by O(k).
We can also use Theorem 3 to improve the efﬁciency
of computing P1 with Algorithm II. This is also true for
the dynamic programming variant of the algorithm presented
in [2]. Since all vectors of equal weight are equiprobable,
we need only consider vectors of type 00 . . . 011 . . . 1. The
improved algorithm is to calculate the probabilities for all
+1 such vectors by using a dynamic programming version of
Algorithm III. Recall that the time and memory complexities
given in [2] are O(n2 2 ) and O(n2 ), respectively, so memory
usage is limiting in practice. For the new algorithm we need
only O(min(n, )) memory for storing intermediate probability values and O( ) storage to hold the resulting probability
distribution. An additional improvement is to recognize that
the distribution is symmetric, so we need only compute half
of it.
While the time required is still O(n2 2 ), the constants are
better. Our simulations show that we save 80-85% in time, and

4

TABLE I
A COMPARISON OF ONLINE AND OFFLINE COMPUTATIONAL AND MEMORY
LPS

2)

online
mem

ofﬂine
comp

ofﬂine
mem

LPS

−1

10

Error Probability

online
comp

Error Probability

COMPLEXITIES FOR A DISTINGUISHING ATTACK WHEN USING EACH
SAMPLING TECHNIQUE .

APS

−1

10

APS

O(1)

APS

O(k

O(1)

O(1)

LPS

O(k )

O(1)

O(1)

O(1)

VS

O(k)

O(2 )

O(n2 2 )

O(n2 2 )

WS

O(k)

O( )

O(n2

2)

WS
−2

10

−3

10

WS
−2

10

−3

0

2

10

10

4

10
Number of chunks (k)

10

0

10

1

2

10
10
Number of chunks (k)

3

10

O( )
Fig. 1. Success probability as a function of the number of chunks k. Size
of S-box is n = 64 and is 20 (left) and 500 (right)

the memory usage is O( ), i.e., it no longer depends on the
size of the S-box. This is optimal since it equals the length of
the vector.

LPS

IV. C OMPARING S AMPLING T ECHNIQUES
We have shown above that both APS and LPS are erroneous
as the corresponding samples are not taken independently.
Still, both techniques are very simple to apply. The distribution
P1 is very easy to compute in each case, and checking if
(si = sj ) is trivial, but the drawback is that the resulting
distinguisher will not be optimal. For optimality, WS (or VS)
must be used. This optimality comes at the cost of a larger
precomputational complexity, i.e., for computing P1 . Table I
summarizes the important parameters corresponding to each
sampling technique. Note that we assume that the best dynamic programming variant is used to compute the probability
distributions P1 for VS and WS. The actual performance of
the attack using each of the sampling techniques has been
simulated. As VS and WS give the exact same distinguisher
performance, only WS is included in the simulations. For a
fair comparison, we assume that the number of chunks is
the same for all variants, i.e., APS and LPS are allowed
to use many more samples than WS, but all use the same
number of observations. The two plots in Fig. 1 show the
error probabilities for APS, LPS and WS when the size of the
S-box is n = 64 and the number of observations in each chunk
is = 20 and = 500, respectively. Similarly, Fig. 2 shows
the error probabilities when the size of the S-box is n = 512.
From the simulations we can see that both LPS and APS
are outperformed by WS. It is interesting to see that APS is
not very much worse when the same number of chunks is
considered. However, we stress again that APS uses a factor
2 more samples than WS. This clearly shows the problem
of assuming independent samples when they are in fact very
dependent. Also, the factor 2 makes APS impractical for
large .
Speciﬁcally for HC-128, each observation is a linear combination of keystream bits. In this case, the comparison assumes
an equal number of keystream bits for all sampling techniques.
Looking at Fig. 1 and Fig. 2 it seems that the performance of
APS approaches that of WS when the S-box size n increases
and when the chunk size decreases. Thus, for large n and
small their performances are practically equal, while for
small n and large , WS clearly outperforms APS. We have

Error Probability

Error Probability

LPS
−1

10

APS
WS
−2

10

−3

10

−1

10

APS
WS
−2

10

−3

0

5

10

10

10

0

10

Number of chunks (k)

2

10
Number of chunks (k)

4

10

Fig. 2. Success probability as a function of the number of chunks k. Size
of S-box is n = 512 and is 20 (left) and 500 (right). Note that WS and
APS almost coincide for = 20.

simulated many other choices of n and , and all simulations
show this same tendency.
V. C ONCLUSIONS
Four different sampling techniques for random S-box outputs have been considered and analyzed. We have proved that
it is not possible to take two independent samples from one
chunk of a random S-box, which implies that APS and LPS
are sub-optimal as they impose a higher error probability in
the resulting distinguisher. We have also proved that WS is
equivalent to the optimal VS, and that WS is much more practical than VS. WS is thus the preferred sampling technique.
We have also presented an improved algoritm for the ofﬂine
computation of P1 for WS.
Even though APS and LPS are not optimal, they are
very simple to apply. For large S-boxes that are frequently
rerandomized, the APS technique is very close to optimal.
R EFERENCES
[1] T. Cover and J. A. Thomas. Elements of Information Theory. Wiley series
in Telecommunication. Wiley, 1991.
[2] P. Stankovski, S. Ruj, M. Hell, and T. Johansson. Improved Distinguishers
for HC-128. Designs, Codes and Cryptography, pages 1–16.
http://dx.doi.org/10.1007/s10623-011-9550-9.
[3] R. Rivest T. Cormen, C. Leiserson and C. Stein. Introduction to
Algorithms, Third Edition. MIT Press, 2009.
[4] H. Wu. A New Stream Cipher HC-256. In Fast Software Encryption
2004, volume 3017 of Lecture Notes in Computer Science, pages 226–
244. Springer-Verlag, 2004.
[5] H. Wu. The Stream Cipher HC-128. In New Stream Cipher Designs,
volume 4986 of Lecture Notes in Computer Science, pages 39–47.
Springer-Verlag, 2008.

5

