Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 22:36:34 2012
ModDate:        Tue Jun 19 12:55:40 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      500547 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569567691

Recovery Threshold for Optimal Weight
Samet Oymak

1

Minimization

M. Amin Khajehnejad

Babak Hassibi

Department of Electrical Engineering
California Institute of Technology
Pasadena, CA 91125
Abstract

this structure is to modify the regular 1 minimization
algorithm. We will consider a regularized version of (P1)
to tackle this problem.

We consider the problem of recovering a sparse signal
from underdetermined measurements when we have prior
information about the sparsity structure of the signal.
In particular, we assume that the entries of the signal
can be partitioned into two known sets S1 , S2 where
the relative sparsities over the two sets are different. In
this situation it is advantageous to replace classical 1
minimization with weighted 1 minimization, where the
sparser set is given a larger weight. In this paper we
give a simple closed form expression for the minimum
number of measurements required for successful recovery
when the optimal weights are chosen. The formula shows
that the number of measurements is upper bounded by
the sum of the minimum number of measurements needed
had we measured the S1 and S2 components of the signal
separately. In fact, our results indicate that this upper
bound is tight and we actually have equality. Our proof
technique uses the “escape through a mesh” framework
and connects to the Minimax MSE of a certain basis
pursuit denisoing problem.

ˆ
x

I. Introduction
Compressed sensing deals with the recovery of sparse
signals from underdetermined observations [1]. In order
to recover a k sparse signal x ∈ Rn from observations
Ax ∈ Rm , a typical approach is minimizing the 1 norm
of x subject to the constraints, i.e.,
ˆ
x

1

subject to

Aˆ = Ax
x

i∈S1

s.t.

Aˆ = Ax
x

(P2)

i∈S2

Here w is the nonnegative weighting parameter.
Problem Background and Motivation: Non-uniformly
sparse signals naturally appear in various applications
such as Brain fMRI, DNA microarrays and satellite images. In general, whenever there is a priori information
regarding the nonzero locations we have non-uniformity.
Weighted and reweighed algorithms are known to be a
simple but rewarding way of dealing with such signals,
and have recently been the subject of considerable interest
[3]–[8]. An important performance criteria for a sparse
recovery algorithm is its phase transition characteristics
which is the relation between the sparsity k, ambient
dimension n and number of measurements m. In this
work, we investigate phase transitions of the regularized
1 minimization following the approach of [13]. Our
motivation is to characterize the gain obtained by using
optimal weighting for the problem (P2).
Contributions: We ﬁnd out that, for optimal regularization w∗ , the number of required measurements for (P2)
is tightly upper bounded by the total number of measurements, when the regions S1 , S2 are sensed separately and
(P1) is used. We can also provide a simple formula for
the optimal weight that is independent of the sizes of S1
and S2 . Our result implies that although the information
provided by Ax is a nontrivial mixture from regions S1 ,
S2 by using optimal weighting, one can do as good as
if sensing x over S1 and S2 separately, i.e., as if A is a
block diagonal matrix which doesn’t mix S1 and S2 .
More accurately, let the size of the regions be |S1 | = n1
and |S2 | = n2 and sparsities be k1 , k2 . Given n and
k, denote the “minimum number of measurements for
successful recovery” (MoM) via (P1) by mn,k and denote
the upper bound provided by our analysis by mn,k .
ˆ
Similarly, denote MoM for (P2) by mn1 ,k1 ,n2 ,k2 and our
upper bound by mn1 ,k1 ,n2 ,k2 . First, we show that:
ˆ

Index Terms—weighted 1 minimization, compressed
sensing, basis pursuit denoising, sparse recovery

ˆ
min x

|ˆi |
x

|ˆi | + w
x

min

(P 1)

In many cases, one might have extra information about
the structure of the sparse signal x. In this paper, we will
be considering a non-uniformly sparse model, where the
signal is relatively sparser over region S1 and denser over
the remaining region S2 = {1, 2, . . . , n} − S1 . Given this
additional information, a reasonable approach to exploit
This work was supported in part by the National Science Foundation
under grants CCF-0729203, CNS-0932428 and CCF-1018927, by the
Ofﬁce of Naval Research under the MURI grant N00014-08-1-0747,
and by Caltech’s Lee Center for Advanced Networking.

mn1 ,k1 ,n2 ,k2 = (1 + o(1))(mn1 ,k1 + mn2 ,k2 )
ˆ
ˆ
ˆ

1

(1)

sparse over the sets S1 , S2 = [n] − S1 is called
(|S1 |, |S2 |, k1 , k2 ) non-uniform. In particular, we will use
k
a linear model where βi = ni and αi = ni are constants
n
i
for 1 ≤ i ≤ 2 and let n → ∞. Denote (α1 , α2 ) by α
and (β1 , β2 ) by β. Then, we will simply say x is (α, β)
non-uniform. A general signal with sparsity k = βn will
similarly be called β sparse.
Minimum Number of Measurements: Let the measurement matrix A ∈ Rm×n have i.i.d. standard normal
entries. Given a signal x ∈ Rn that is β sparse, η(β)
is the minimum number η so that, x can be recovered via
(P1) asymptotically with high probability (w.h.p.) using
m = (η + o(1))n measurements. Similarly, if x ∈ Rn is
(α, β) non-uniform, η(α, β, w) is the minimum η so that
x can be recovered from m = (η + o(1))n observations
via (P2) w.h.p.
• Remark: It is known that for a Gaussian sensing matrix
A, the required number of measurements for (P1) or (P2)
is independent of the locations or particular values of the
nonzero entries, [2].

While this result is an equality of upper bounds, as a
next step, we relate phase transitions obtained by our
analysis to phase transitions of Approximate Message
Passing (AMP) algorithms which is known to be the same
as the true phase transitions of 1 minimization, [12].
Using this we show, mn,k = mn,k (1 + o(1)).
ˆ
This way, we provably show that optimally weighted
1 minimization is better than or equal to two separately
measured regular 1 minimizations. We believe the converse result is also correct as [19] experimentally shows
mn1 ,k1 ,n2 ,k2 = mn1 ,k1 ,n2 ,k2 . As a side result, we provide
ˆ
the connection between AMP related works [10]–[12]
and works that analyze the phase transitions for convex
optimizations [13]–[17]. Finally, closer to this paper, in
[6], the authors analyze the same problem using the
replica method. While replica method does not provide
a rigorous proof, our results are consistent with [6].

II. Problem Setup
Basic Deﬁnitions: Denote the set {1, 2, . . . , n} by [n].
¯
Given T ⊆ [n], denote [n] − T by T . Further, we denote
the operators that project and collapse a vector on T by
PT (·) : Rn → Rn and RT (·) : Rn → R|T | . In particular,
¯
PT (·) sets the entries over T to 0 whereas RT (·) throws
¯. The support T of a vector x is
away the entries over T
a subset of [n] that is composed of the nonzero locations,
i.e., xi = 0 ⇐⇒ i ∈ T for all i ∈ [n]. sgn(·) : Rn → Rn
is the function that returns entry-wise signs of a vector,
x
i.e., 0 is mapped to 0 and x = 0 is mapped to |x| .
n
The distance between two sets A, B ⊆ R is denoted
by dist(A, B) and deﬁned as mina∈A,b∈B a − b 2 . The
cone of a set A is denoted by cone(A). Given a scalar
λ ≥ 0, denote the set obtained by scaling elements of a
set A by λ by λ · A.
Finally, for the rest of the discussion, we assume the
¯
regions S1 , S2 = S1 ⊆ [n] are ﬁxed and known.

Deﬁnition II.2 (Random Distance). Given a set A ⊆ Rn ,
let d(A) be the scalar random variable statistically identical to dist(g, A) where g ∈ Rn is a vector with i.i.d.
standard normal entries.

III. Main Results
Our main results can be summarized in two theorems.
The ﬁrst theorem is obtained by analyzing sharp upper
bounds for the phase transitions of (P1) and (P2), i.e.,
η(β) and η(α, β, w) and shows the equality (1). It also
provides the optimal weight for (P2) that achieves (1).
Theorem III.1. Let β = (β1 , β2 ) and assume x, x1 , x2 ∈
Rn are (α, β), β1 , β2 sparse vectors respectively. Then,
one has the following:
• η (β1 ), η (β2 ), η (α, β, w) are upper bounds to
ˆ
ˆ
ˆ
η(β1 ), η(β2 ), η(α, β, w) respectively and are given
as:

Deﬁnition II.1. Let x ∈ Rn be a vector with support
T . The set of sub gradients of the 1 norm at x will be
denoted by S(x). In particular, S(x) is given as:
v ∈ Rn PT (v) = sgn(PT (x)),

PT (v)
¯

∞

≤1

η (βi ) = lim n−1 E[d(cone(S(xi )))]2
ˆ

(2)

η (α, β, w) = lim n−1 E[d(cone(Sw (x)))]2
ˆ

Similarly, let · w be the norm given as x w =
i∈S1 |xi | + w
i∈S2 |xi |. The set of subgradients of
· w at x is denoted by Sw (x). v ∈ Rn belongs to
Sw (x) if and only if it satisﬁes:
PS1 ∩T (v) = sgn(PS1 ∩T (x)),

PS1 ∩T (v)
¯

PS2 ∩T (v) = w · sgn(PS2 ∩T (x)),

∞

PS2 ∩T (v)
¯

≤1
∞

(4)

n→∞

n→∞

•

(3)

∗

Let η (α, β) be the upper bound obtained by choosˆ
ing the optimal regularization parameter w = w∗
that minimize η (α, β, w). Then,
ˆ
η ∗ (α, β) = α1 η (β1 ) + α2 η (β2 )
ˆ
ˆ
ˆ

≤w
•

A. Framework
The following deﬁnitions rigorize the non-uniform
signal model that will be built upon.
Non-uniform Sparse Model: A signal x that is k1 , k2

Optimal regularization satisﬁes w∗ =
{λi }2 are given as:
i=1

(5)
λ2
λ1

λi = lim arg min E[d(λ · S(xi ))]
n→∞

2

λ≥0

where
(6)

As the next step, we will provide a connection between
the upper bounds obtained in Theorem III.1 and Approximate Message Passing (AMP) algorithms that are used
for sparse recovery, [10]–[12]. To explain our approach,
we ﬁrst introduce the Basis Pursuit Denoising (BPDN)
problem and its asymptotic minimax mean squared error
(MMSE).

0.9

η(α,β,w)

0.86

Deﬁnition III.1 (BPDN). Assume x ∈ Rn is a k sparse
vector and we observe noisy vector y = x + z where z
has i.i.d. Gaussian entries with variance σ 2 . We consider
the following estimation problem,
ˆ
x∗ = arg min λ x
ˆ
x

1

+

1
ˆ
y−x
2

2
2

0.76
0

•

(8)

4

5

6

7

IV. Graphical Interpretation

(9)

Figure 1 illustrates an example setup where we set
α = (0.4, 0.6) and β = (0.7, 0.3). Based on results of
[5], [19], η(α, β) is plotted as a function of w. Green
line corresponds to the sum of separate measurements.
Keeping Theorems III.1 and III.2 in mind, intuitively it
is not surprising to see that the relation η(α, β, w) ≥
α1 η(β1 ) + α2 η(β2 ) holds and equality is achieved for a
particular w which is the optimal regularizer for (P2). Let
us deﬁne the gain of the optimally weighted algorithm to
be the reduction in the number of measurements compared
to only applying (P1) or equivalently using w = 1. The
gain, in Figure 1 is illustrated as the dashed line. In
particular,

Let η ∗ (α, β) be the amount of measurements for the
optimally regularized (P2), i.e. minw≥0 η(α, β, w).
Combining Theorem III.1 and (9), we have:
η ∗ (α, β) ≤ η ∗ (α, β) = α1 η(β1 ) + α2 η(β2 ) (10)
ˆ

Comments about Theorems III.1 and III.2
• Observe that (10) implies that by using the optimal
regularization, (P2) will asymptotically require less
or equal measurements than the sum of two (P1)’s
applied to S1 , S2 separately.
• While we show η (β) = ηDN (β) in (9), ηDN (β) =
ˆ
η(β) is provided by [12]. The relation of BPDN and
AMP has been studied in a series of papers including
[10]–[12]. Denoting the asymptotic phase transitions
of AMP by ηAM P (β), in [12], it has been shown
that,
ηAM P (β) = η(β) = ηDN (β)
(11)
•

3

Gordon, [18]. By showing η (β) = η(β) = ηDN (β)
ˆ
we connect the series of papers that are based on
Theorem V.1 to results on AMP and BPDN [10]–
[17]. In particular, (9) implies the results obtained
by [13] are indeed tight and it is not surprising that
it matches with exact phase transitions found in [2].
It also provides a strong justiﬁcation for the tightness
of the upper bounds in [14]–[17] for various sparse
recovery problems including block sparsity and low
rank recovery.

For any 0 ≤ β ≤ 1,
η (β) = ηDN (β) = η(β)
ˆ

2

Fig. 1. Solid line shows η(α, β, w) as a function of w for regularized
1 minimization. For this graph, we have α1 = 0.4, α2 = 0.6, β1 =
0.7 and β2 = 0.3. The dashed (horizontal) line shows the required
measurements when regions are sensed separately. As predicted by
Theorem III.1, solid and dashed lines intersect at a single point.

Theorem III.2. η (β) is the same as given in (4), and
ˆ
ηDN (β) is same as in Deﬁnition III.1. Then,
• For a β sparse vector x,

•

1

w (weight)

Our next result relates η (β) to ηDN (β) hence connects
ˆ
upper bounds obtained by Theorem III.1 to BPDN. As it
will be discussed soon, this will also imply η (β) = η(β).
ˆ

λ≥0

0.82

0.78

It is known, [10], that E[ x∗ − x 2 ] is a function of
2
k
k, λ, σ 2 . In the asymptotic setting where β = n and
n → ∞, we deﬁne optimally regularized normalized
MMSE of BPDN to be,
E[ x∗ − x 2 ]
2
(7)
ηDN (β) = lim lim inf
n→∞ σ→0 λ≥0
nσ 2

n→∞

0.84

0.8

(P 3)

ηDN (β) = lim n−1 inf E[d(λ · S(x))2 ]

Weighted Algorithm (P2)
Sum of Two Separate L1
Performance Gain
Optimal Weight

0.88

Gain = η(α1 β1 + α2 β2 ) − η ∗ (α, β)
= η(α1 β1 + α2 β2 ) − α1 η(β1 ) + α2 η(β2 )

(12)

Figure 2 provides a different view point for the improvement provided by optimally weighted (P2). The solid line
is the phase transition η(β) of 1 minimization. Then, for
ﬁxed β1 , β2 , the dashed red line illustrates the required

Our analysis is based on the strong concentration
inequality given in Theorem V.1 which is due to

3

Ax. Let Mw (x) be the set of vectors z such that
x + z w ≤ x w . x is the unique optimal of (P2)
if the null space N (A) of A and Mw (x) have no
intersection. N (A) is an n − m dimensional uniformly
distributed subspace due to Gaussianity of A. Hence,
letting Sw = cone(Mw (x))∩B n−1 and by making use of
Theorem V.1, x will be the unique optimal of (P2) w.h.p.
if the number of measurements satisﬁes,
√
1
m − √ > ω(Sw )
(14)
4 m

1

(measurements)

L1 phase transition
Optimally Weighted L1

Improvement*

{

0.5

0
0

β1*

α1β1+α2β2*

0.5

β2*

Following (14), η (α, β, w) provided by Theorem V.1,
ˆ
takes a simple form. In particular, asymptotically
2
η (α, β, w) = ω(Sw ) (1 + o(1)) will ensure success of
ˆ
n
(P2) w.h.p. Next, we cast the calculation of ω(Sw ) as the
expectation of the following optimization problem where
g is an i.i.d Gaussian vector with unit variance,

1

Fig. 2. The solid line is 1 phase transition. Dashed red line is the
measurements required for optimally weighted (P2) as α1 , α2 varies
and β1 , β2 are ﬁxed.

number of measurements as α1 = 1 − α2 varies. For example, α1 = 1 (P2) will need η(β1 ) measurements which
is hardly surprising. The relation (5) can be translated
as the linearity of the dashed line. Hence, the gain of
the optimal weighting can again be characterized by (12)
and it basically corresponds to the curvature of the η(·)
function.

max g, z
z

subject to

z ∈ cone(Mw (x)) and z

(15)
2

=1

(16)

Following [14], by writing and processing the dual problem and using convexity of Mw (x), it can be shown that
strong duality holds and the result of (15) is equal to,
E[min g − z 2 ]
z

V. Analysis

subject to

z ∈ cone(Sw (x))

Observe
that
the
ﬁnal
line
is
simply
E[dist(g, cone(Sw (x)))] = E[d(cone(Sw (x)))]. Hence:

Due to insufﬁcient space we will be omitting most
of the analytical derivations. Detailed derivations can be
found in [20]. We begin by considering Theorem III.1.
The following section will describe our approach for this
problem.

η (α, β, w) = n−1 E[d(cone(Sw (x)))]2
ˆ

(17)

will be an upper bound to η(α, β, w). η (β) can be found
ˆ
in a similar fashion. Next, let us consider the relation
(5) of Theorem III.1. Recall the deﬁnitions introduced in
Section II. Using (17) and making use of the structure of
the sub-gradient of · w at (α, β) non-uniform x, we
have,

A. Proof Sketch of Theorem III.1
We will make use of an approach that has been
introduced in [13] and used in relevant works [14]–[17].
Deﬁnition V.1. Let S ⊆ Rn and g ∈ Rn be a vector
with i.i.d standard normal entries. Gaussian width of S
is deﬁned as,
ω(S) = E[sup v, g ]
(13)

η (α, β, w) = n−1 E inf d(λ · S(RS1 (x)))2 +
ˆ
λ≥0

d(wλ · S(RS2 (x)))2

v∈S

1/2 2

= n−1 E[ inf (d1 (λ)2 + d2 (wλ)2 )1/2 ]2 (18)

The next theorem provides a strong concentration
result for random subspaces based on Gaussian width
calculation.

λ≥0

where di (c) = d(c · S(RSi (x)))2 . We can similarly
characterize α1 η (β1 ) and α2 η (β2 ). We have,
ˆ
ˆ

Theorem V.1 (Escape Through a Mesh). Let S be a
subset of unit ball B n−1 in Rn . Further, let H be an
n − m dimensional subspace distributed uniformly over
Grassmannian w.r.t Haar measure. Then,
√
1
P(H ∩ S = ∅) ≥ 1 − 3.5 exp(−( m − √ − ω(S))2 )
4 m

αi η (βi ) = E[d(cone(S(RSi (x)))]2
ˆ

(19)

= n−1 E[ inf d(λ · S(RSi (x))]2

(20)

= n−1 E[ inf di (λ)]2

(21)

λ≥0

λ≥0

Based on (18) and (21), what remains to show is whether,
2

Based on Theorem V.1, without losing generality, we
will aim to obtain η (α, β, w). Hence, we will conˆ
sider (P2) and will aim to recover x from observations

E[ inf di (λi )]2 ≈ E[ inf (d1 (λ)2 + d2 (w∗ λ)2 )1/2 ]2
i=1

4

λi ≥0

λ≥0

(22)

for w∗ that minimizes right hand side of (18). Proof
of (22) requires concentration inequalities for Gaussian
λ∗
vectors and is omitted here. Finally w∗ = λ2 where
∗
1
λ∗ = arg minλ≥0 E[di (λ)]. This is not surprising as the
i
left hand side of (22) will use λi = λ∗ and the right
i
hand side should better choose (λ, w∗ λ) = (λ∗ , λ∗ ) for
1
2
equality.

Secondly, we believe weighted sparse recovery algorithms can be analyzed in a more general framework
that is applicable to not only 1 minimization but also
to closely related algorithms, such as 1 / 2 minimization
and nuclear norm minimization, which are used for block
sparse and low rank recovery. Finally, while we show
that η ∗ (α, β) ≤ α1 η(β1 ) + α2 η(β2 ), the converse is
only shown for the upper bounds η ∗ (α, β). Consequently,
ˆ
it would be interesting to establish a deeper connection
between our framework, which is based on Theorem V.1,
and results on Approximate Message Passing algorithms.

B. Proof Sketch of Theorem III.2
In order to estimate ηDN (β) we need to consider
Deﬁnition III.1. It is known that the solution of (P3) is
given as the shrinkage operator applied to y, [10]–[12].

yi − λ if yi ≥ λ

x∗ = shrink(yi ) = 0 if |yi | < λ
(23)
i


yi + λ else

References
[1] D. L. Donoho, “Compressed Sensing,” IEEE Trans. on Information
Theory, 52(4), pp. 1289 - 1306, April 2006.
[2] D. L. Donoho and J. Tanner, “Neighborliness of randomly-projected
simplices in high dimensions,” Proc. National Academy of Sciences,
102(27), pp. 9452-9457, 2005.
[3] N. Vaswani and W. Lu, “Modiﬁed-CS: Modifying compressive
sensing for problems with partially known support,” IEEE Trans.
Signal Processing, vol. 58(9), pp. 4595-4607, Sep. 2010.
[4] L. Jacques, “A short note on compressed sensing with partially
known signal support”, arXiv:0908.0660v2.
[5] M. A. Khajehnejad, W. Xu, A. S. Avestimehr, B. Hassibi, “Weighted
1 minimization for sparse recovery with prior information,” Proc.
Int. Symp. on Information Theory (ISIT) 2009.
[6] T. Tanaka, J. Raymond, “Optimal incorporation of sparsity information by weighted 1 optimization,” Proc. Int. Symp. on Information
Theory (ISIT) 2010.
[7] W. Xu, M. A. Khajehnejad, S. Avestimehr, and B. Hassibi, “Breaking through the thresholds: an analysis for iterative reweighted 1
minimization via the Grassmann angle framework,” ICASSP 2010.
[8] D. Wipf and S. Nagarajan, “Iterative Reweighted l1 and l2 Methods
for Finding Sparse Solutions, IEEE J. Select. Topics Signal Process.,
vol. 4, no. 2, pp. 317329, 2010.
[9] M. A. Khajehnejad, W. Xu, A.S. Avestimehr,and B. Hassibi, “Improved sparse recovery thresholds with two-step reweighted 1
minimization”, Proc. Int. Symp. on Information Theory (ISIT) 2010.
[10] D. L. Donoho, I. Johnstone and A. Montanari, “Accurate Prediction
of Phase Transitions in Compressed Sensing via a Connection to
Minimax Denoising”, available at arXiv:1111.1041v1.
[11] D. L. Donoho, A. Maleki, and A. Montanari, “Message Passing
Algorithms for Compressed Sensing”, Proceedings of the National
Academy of Sciences 106 (2009), 18914-18919.
[12] M. Bayati and A. Montanari, “The LASSO risk for gaussian
matrices”, available at arXiv:1008.2581v1
[13] M. Stojnic, “Various thresholds for 1 - optimization in compressed
sensing”, available at arXiv:0907.3666v1.
[14] V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky,
“The Convex Geometry of Linear Inverse Problems”
[15] N. Rao, B. Recht, and R. Nowak, “Tight measurement bounds for
exact recovery of structured sparse signals”, arXiv:1106.4355, 2011.
[16] M. Stojnic, “Block-length dependent thresholds in block-sparse
compressed sensing”, arXiv:0907.3679v1.
[17] S. Oymak and B. Hassibi, “New Null Space Results and Recovery Thresholds for Matrix Rank Minimization”, available at
arXiv:1011.6326v1.
[18] Y. Gordon, “On Milmans inequality and random subspaces which
escape through a mesh in Rn ”, in Geometric Aspects of Functional
Analysis, volume 1317 of Lecture Notes in Mathematics, pages 84106. Springer, 1988.
[19] A. Krishnaswamy, S. Oymak and B. Hassibi, “A Simpler Approach
to Weighted 1 Minimization”, accepted to ICASSP 2012.
[20] S. Oymak, M. A. Khajehnejad and B. Hassibi, “Recovery Threshold for Optimal Weight 1 Minimization”, Technical Report available at http://www.its.caltech.edu/~soymak/OptWeight.pdf.

Let T denote the support of the sparse vector x. In the
setting σ → 0, for a Gaussian noise z, w.h.p. |xi | ≥ |zi |
for all i ∈ T , and hence it can be shown that w.h.p. x∗
takes the following form,
xi + zi − λsgn(xi ) if i ∈ T
x∗ =
(24)
i
shrink(zi ) else
Recall that we would like to characterize, x∗ − x which
can now be given as,
x∗ − x = PT (z − λsgn(x)) + PT (shrink(z))
¯

(25)

Keeping Deﬁnition II.1 in mind, (25) simply reduces to,
x∗ − x

2
2

= dist(z, λS(x))2 = σ 2 d(σ −1 λS(x))2 (26)

and hence, ηDN (β) ≈ n−1 inf ζ≥0 E[d(ζ·S(x))2 ]. Finally,
we again make use of concentration results to show that
this statement is equal to η (β) given in (21).
ˆ

VI. Further Directions
In this work, we have considered a non-uniform signal
model where there are two distinct regions. In general,
we believe it will not be hard to generalize results of this
paper to a more general model with arbitrary number of
t
regions {Si }t , i=1 Si = [n] where we generalize (P2)
i=1
to,
t

min
ˆ
x

|ˆj |
x

wi
i=1

s.t.

Aˆ = Ax
x

(27)

j∈Si

where w ∈ Rt is a nonnegative weight vector. Such a
model has actually been considered and analyzed in [5],
[6], [19]. Based on the results of this work, denoting the
normalized sizes and sparsities of the regions by {αi }t ,
i=1
{βi }t , and denoting the number of measurements for
i=1
optimally weighted (27) to be η ∗ ({αi }t , {βi }t ), it
i=1
i=1
can be shown that,
t
η ∗ ({αi }t , {βi }t ) =
i=1
i=1

αi η(βi )

(28)

i=1

5

