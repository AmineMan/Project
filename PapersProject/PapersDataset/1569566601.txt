Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 21:22:25 2012
ModDate:        Tue Jun 19 12:56:03 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      538764 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569566601

On Decoding Delay of Intra-session Random Linear
Network Coding for Line Networks
Vahid Shah-Mansouri and Vincent W.S. Wong
Department of Electrical and Computer Engineering
The University of British Columbia, Vancouver, Canada
e-mail: {vahids, vincentw}@ece.ubc.ca
and then waits for the destination to announce how many more
packets it requires for decoding. In order to reduce power
consumption, there is an optimal number of packets to be
transmitted before waiting for the feedback.
The decoding delay of line networks with a pair of source
and destination with multiple intermediate nodes has been
studied in the literature (e.g., [3]–[5], [7]). In some applications, a subset of these intermediate nodes can also be
sources with data to be sent to the same destination. As an
example, in wireless mesh networks (e.g., [8]), all the wireless
mesh routers have data to be transmitted to the same gateway
(destination). In this paper, we consider a line topology where
there is one destination and all other nodes are sources. This
topology can be extended to the case where only a subset of
intermediate nodes are sources.
For a network with multiple sources (or ﬂows), RLNC with
intra-session coding refers to the case that an intermediate
node performs coding operations on the packets of its upstream
ﬂows destined for the destination [9]. A transmitted packet
contains information of all upstream sources. In this paper,
we study the decoding delay of RLNC with intra-session
coding and derive an upper bound on the decoding delay. The
contributions of our paper are as follows:

Abstract— Random linear network coding (RLNC) can improve the reliability of end-to-end communication in the presence
of erasure channels. In RLNC with intra-session coding, an intermediate node creates coded packets by combining the packets
of various sources destined for a destination. Each transmitted
packet contains information of multiple sources. In this paper,
we study the decoding delay of RLNC with intra-session coding
for a line network with S sources and a destination. Given the
generation size K, ￿ show that the expected decoding delay is
we
upper-bounded by η2 K log(S) + Kη1 + η, where η, η1 , η2 are
constants. The analytical results are validated via simulations.
We also compare the results with the case where intra-session
coding is not used.

I. I NTRODUCTION
Network coding can achieve the packet-level capacity of
unicast and multicast sessions in wireless networks in the
presence of lossy links [1], [2]. In random linear network
coding (RLNC), the source groups the packets into blocks
called generations. The intermediate nodes store the packets
received from one generation. The source and intermediate
nodes transmit a new packet by choosing the random coefﬁcients from a Galois ﬁeld and linearly combining the stored
packets using these coefﬁcients. The source and intermediate
nodes transmit packets of a generation until the destination
can fully decode all the packets of that generation. The time
between sending the ﬁrst packet of a generation by the source
and the decoding time of a generation at the destination is
called the decoding delay.
The decoding delay of network coding over line networks
with erasure channels is ﬁrst studied in [3]. Different coding
schemes are presented and the delays are compared. It is
shown that for a two-hop line network with identical erasure
√
probabilities α, the decoding delay is equal to K/α+O( K),
where K is the generation size. The delay analysis of RLNC
for a unicast session on line networks is presented in [4].
When the erasure probability of various links is different, the
decoding delay mainly depends on the link with the minimum
erasure probability. For a two parallel multihop line network,
results in [5] show that the delay gap between network coding
and routing is unbounded as the generation size increases.
The work in [6] presents an upper bound on the expected
decoding delay, which is a constant plus the generation size
divided by the max-ﬂow min-cut of the relay network. RLNC
for time division duplexing line networks with unicast session
is studied in [7]. The source transmits packets consecutively

•

•

For RLNC with intra-session coding, we show that the
expected decoding delay for a line network with S
sources and generation size K is upper-bounded by
￿
η2 K log(S) + Kη1 + η, where η, η1 , η2 are constants.
The analytical models are validated via simulations. Results show that as the number of sources S increases on
the line, the difference between the decoding delay of
RLNC with and without intra-session coding increases
without bound.

Our work is different from the previous works in various
aspects. In [4] and [7], a single source and destination pair
is considered, whereas our work consider multiple sources. In
[7], the line network is half-duplex, whereas our work study
the full-duplex scenario. In [6], the decoding delay for multiple
parallel line networks between a source and destination is
determined. In this paper, we consider the multihop scenario
and study the beneﬁt of using intra-session coding.
The rest of this paper is organized as follows: The system
model is presented in Section II. In Section III, we present the
decoding delay model for RLNC with intra-session coding.

1

Source 1

Source 2




Fig. 1.

various packet transmissions over link s experience erasure
independently with probability 1 − αs , the time between two
consecutive successful packet transmissions for node s is the
time between two packet transmissions (i.e., 1/s) multiplied
by a random variable which follows a geometric distribution.
The time between two successful packet transmissions of node
s is the service time for the virtual queue of node s.
All the sources start transmission of one generation at time
0, with the size of the virtual queue for all the sources being
K. Let τ s denote the time at which node s transmits its last
innovative packet (i.e., the sKth innovative packet). Consider
node s has successfully transmitted us ∈ {sK, sK + 1, . . .}
packets up to time τ s . Time τ s can be written as τ s =
￿ us
j=1 νj , where νj is the time interval between the (j − 1)th
and jth successful packet transmission of node s. Random
variable νj follows a geometric distribution. sK transmissions
out of us successful packet transmissions contain innovative
information for node s + 1. Let Us denote the set of time
intervals which precedes an innovative packet transmission.

Source S Destination






A line network with S sources and destination d.

Performance evaluation and comparisons are presented in
Section IV. Conclusions are given in Section V.
II. S YSTEM M ODEL
We consider a line network1 with S source nodes and one
destination d as shown in Fig. 1. Each source s ∈ {1, . . . , S}
also forwards the packets of other sources with lower indices
(i.e., sources 1, 2, . . . , (s − 1)) towards the destination d.
Source s has one outgoing link with node s + 1 as its
downstream neighbor. We assume that the packet generation
rate for each source is one packet per second. Since source s
forwards packets of sources 1, . . . , (s−1) as well, its aggregate
sending rate is s packets per second. We assume that the
capacity of each link is higher than the aggregate sending rate.
For RLNC with intra-session coding, packets of various
ﬂows towards the destination d are combined at the intermediate nodes. The intermediate nodes store the received packets
of different ﬂows in a single buffer. The coded packets are
created by combining packets of various ﬂows and each packet
transmitted by node s contains information of all the sources
in set {1, . . . , s}.
The ﬂux of innovative packets from source s to destination d
can be modeled as a queueing system [10]. Each intermediate
node can be considered as a server while innovative packets
are customers traveling from servers with queues on a tandem.
We notice that this queue is different from the actual buffer
of the node which stores the received packets. To distinguish
this queue from the buffer of node s, we call it the virtual
queue of node s. The size of the virtual queue is increased
by one when node s receives an innovative packet. A packet
is serviced by the virtual queue of node s and is dequeued
when node s successfully transmits an innovative packet to
node s + 1. We notice that the packet is not removed from the
actual buffer of node s. The size of the virtual queue of node
s at time t is the difference between the number of packets
with independent encoding vectors at the buffers of node s
and downstream node s + 1.

Us

=

{νj | j = 1, . . . , us , and

jth transmitted packet is innovative} . (1)

The total time which takes source node s to transmit sK
innovative packets is the summation of the members of set Us .
￿
Let ζs denote this summation (i.e., ζs = j∈Us νj ). Since νj
is the product of a geometric random variable and 1/s, ζs is
the summation of sK independent geometric random variables
with parameters αs multiplied by 1/s. The summation of sK
independent geometric random variables with parameter αs
constructs a negative binomial distribution with parameters sK
and αs . Thus, for i = sK, sK + 1, sK + 2, . . ., we have
￿ i−1 ￿
i−sK sK
P {ζs = i/s} = sK−1 (1 − αs )
αs .
(2)

For the line network with S sources, the time that the Sth
source node transmits the last innovative packet, τ S , is the
decoding delay of the system. Time τ S can be written in a
recursive manner using two different equations. First, τ S can
be written as the summation of τ S−1 and the time node S
requires to transmit the remaining packets in its virtual queue
when it receives the last innovative packet at time τ S−1 . Let
nS denote the number of packets in the virtual queue of source
S at time τ S−1 . The time it takes node S to successfully
transmit nS packets is the product of 1/S (i.e., the time
between two packet transmissions) and the number of packet
transmissions needed to successfully transmit nS packets. Let
t(nS ) denote the number of packet transmissions needed to
successfully transmit nS packets. t(nS ) is a negative binomial
random variable with parameters nS and αS . Therefore, we
have τ S = τ S−1 + t(nS )/S.
In the second approach for deriving τ S , we use the equation
￿ uS
S
τ = j=1 νj . The decoding delay τ S can be expressed as
the summation of two terms
￿
￿
¯
τS =
νj +
νj , where US = {1, . . . , uS }\US . (3)

III. D ECODING D ELAY A NALYSIS
When RLNC with intra-session coding is employed, each
intermediate source node keeps a shared buffer for different
ﬂows towards the destination. Hence, each node has only one
virtual queue. Node s transmits coded packets periodically
from its buffer at a rate of s packets per second. The time interval between two consecutive packet transmissions is 1/s. The
packet transmission is independent of receiving new packets.
We assume that the packet success probability of the outgoing
link of source s, which is denoted as αs , is given. Since
1 We notice that in case of using single path routing in a mesh network, the
data path from various sources to the destinations are line networks.

j∈US

2

¯
j∈US

￿
In (3), the second term
¯
j∈US νj is the portion of time
within τ S that node s transmits packets without innovative
information. Let cS denote the number of successful packet
transmissions of node S up to time τ S which do not have
￿
innovative information. j∈US νj is the time node S spends
¯
on transmitting cS packets. The time it takes node S to
successfully transmit cS packets is the product of the time
between two packet transmissions 1/S and the number of
packet transmissions which is a negative binomial random
variable with parameters cS and αS . Let t(cS ) denote this
random variable. t(cS ) times 1/S denotes the time node S
￿
requires to transmit cS packets (i.e., t(cS )/S = j∈US νj ).
¯
Therefore, τ S = ζS + t(cS )/S. Using the two equations
derived for τ S , one can obtain
τS

=
=

= τ S−1 + t(nS )
S￿
￿
S−1
max ζS , τ
+ min t(cS ) ,
S

ζS +

t(cS )
S

￿

t(nS )
S

￿

of the nodes γ + 1, . . . , S are in the steady state mode2 . This
guarantees that E[ns ] is constant for s = γ + 1, . . . , S. Let Fs
denote the expectation of t(ns ) in this case. Then we have
￿￿
￿
￿￿
￿
Ens |γ [t(ns )]
S
S
Fs
Eγ
= Eγ
(9)
s=γ+1
s=γ+1 s
s

On the other hand, since ζγ > ζγ−1 , the rate at which node
γ receives innovative packets is higher than the rate at which
it sends innovation packets. Therefore, the virtual queue of
node γ does not reach a steady state until it receives the last
innovative packet. Recall that cγ denotes the number of times a
successfully transmitted packet by node γ does not have new
information. This is equal to the number of times a packet
transmitted when the virtual queue of node γ is empty. If we
model the size of the virtual queue of node γ with a Markov
chain, then cγ is the number of times this Markov chain visits
state 0. Since this queue is unstable, state 0 is a transient state
and the number of times the Markov chain visits state zero is
upper-bounded. Let Dγ denote the expectation of t(cγ ) when
the queue is unstable. Therefore, we have
￿ ￿
￿ E [t(c )] ￿
γ
cγ |γ
D
Eγ
= Eγ γγ .
(10)
γ

. (4)

A similar equation can be written for τ S−1 as
￿
￿
￿
￿
t(cS−1 ) t(nS−1 )
S−1
S−2
τ
= max ζS−1 , τ
+ min
,
.
S−1
S−1
When we substitute τ S−1 to (4), we obtain
τS

=
=
=

t(cS )
S
ζS−1 + t(cS−1 ) + t(nS )
S−1
S
t(nS−1 )
t(nS )
S−2
τ
+ S−1 + S .

Let η denote the upper bound on all values of Fs and Dγ .
Using equations (9) and (10), equation (8) can be written as
￿ ￿
￿￿
￿
￿￿
￿
D
S
S
Fs
1
E[gS ] = Eγ γγ + Eγ
< ηEγ
s=γ+1 s
s=γ s .

ζS +

(5)

Since we have no information about the success probabilities,
the probability that γ = i is￿equal to 1/S for i ∈ {1, . . . , S}.
￿￿
S
1
This results in Eγ
s=γ s = 1 and E[gS ] < η.
S
τ = max {ζS , ζS−1 , . . . , ζ1 } + gS ,
(6)
Next, we present the derivation of an upper bound for
E[max {ζS , ζS−1 , . . . , ζ1 }]. The probability distribution funcwhere
tion of ζs is given in (2). ζs is the product of 1/s and a random
S
variable with negative binomial distribution with parameters
t(cγ ) ￿ t(ns )
gS =
+
, and γ = arg max {ζS , ζS−1 , . . . , ζ1 } sK and α . The probability distribution function of random
s
γ
s
s=γ+1
variable ζs can be approximated with a normal distribution
(7)
￿
￿
To ﬁnd the expected value of the decoding delay, we need to and we have
K
ζs ≈ N αs , K(1−αs ) .
(11)
determine E[gS ] and E[max {ζS , ζS−1 , . . . , ζ1 }]. The followsα2
s
ing lemma presents an upper bound on E[gS ].
If αs is not small and for large values of K, then the
Lemma 1: The expectation of gS is upper-bounded by a
approximation in (11) is accurate enough to derive the upperconstant, which is independent of S and K.
bound. In the rest of the paper, we use the approximation of
Proof: Using equation (7), one can obtain
ζs as ζs . The following theorem presents an upper bound for
￿ S
￿
￿
￿
￿ t(ns )
t(cγ )
the expectation of max {ζS , ζS−1 , . . . , ζ1 }.
E[gS ] = E
+E
Theorem 1: Using the approximation in (11), the expectaγ
s
s=γ+1
￿ S
￿
tion of max {ζS , ζS−1 , . . . , ζ1 } is upper-bounded by η1 K +
￿
￿
￿
￿ Ens |γ [t(ns )]
Ecγ |γ [t(cγ )]
= Eγ
+Eγ
, (8) η2 K log(S), where η1 and η2 are constants.
γ
s
Proof: Equation (11) provides a normal approximation
s=γ+1
˜
for variable ζs . Consider another random variable ζs , which
where Eγ is the expectation with respect to random variable γ. is a normal random variable with mean K/α
and variance
min
Ecγ |γ and Ens |γ are the conditional expectations with respect K(1 − α )/(α2 ), where α
min
min = min{α1 , . . . , αS }. The
min
to random variables cγ and ns , given γ, respectively. The difference between two normal random variables ζ and ζ ,
˜s
s
expectation of t(ns ) is equal to E[ns ]/αs . Note that ns denotes
the number of packets in the virtual queue of node s at time
2 A queue reaches the steady state if its average input rate is less than the
τ s−1 . Since ζγ > ζs for s = γ + 1, . . . , S, the virtual queues service rate of the queue. Otherwise, the queue is unstable.
If τ S−2 is substituted to (5) accordingly and it is continued
until it reaches source 1, one can write τ S as

3

which is then normalized by the generation size K, is also a
normal random variable. That is,
￿
￿
￿￿
˜
ζs − ζs
1
1
1
1 − αmin
1 − αs
∼N
−
,
+
.
2
2
K
αmin
αs Ks
αmin
αs

1.85
RLNC without intra−session coding
RLNC with intra−session coding

1.8

Normalized Decoding Delay

1.75

For the normalized difference of random variables, the mean
is always greater than one while for large values of K, the
variance approaches zero. Therefore, for large values of K,
˜
the probability that ζs is greater than ζs approaches one.
Therefore, we have
￿
￿
˜ ˜
˜
max {ζS , ζS−1 , . . . , ζ1 } ≤ max ζS , ζS−1 , . . . , ζ1 . (12)
￿
￿
˜ ˜
˜
Let random variable YS = max ζS , ζS−1 , . . . , ζ1 . YS is
the maximum of S normal random variables with identical
2
mean µ = K/αmin and variance σ 2 = K(1−αmin )/αmin . Let
−1
F (.) denote the inverse of the cumulative distribution function (CDF) of the normal random variable. Then, F −1 (U 1/S )
is distributed as YS [11], where U is a uniform (0, 1) random
variable. We exploit the fact that eP is a uniform (0, 1)
random variable when P is an exponential random variable
with parameter 1 [11]. Using Taylor series expansion for eP ,
we obtain
￿
￿
P
P2
YS ∼ F −1 (U 1/S ) = F −1 (eP/S ) ≈ F −1 1 − +
.
S
2S 2

1.7
1.65
1.6
1.55
1.5
1.45
1.4
50

100

150

200
250
Generation size K

300

350

400

Fig. 2. The normalized decoding delay versus the generation size K. (λ = 4,
S = 10)

We are now able to present an upper bound on the decoding
delay τ S obtained in (6). Theorem 1 presents an upper bound
on the ﬁrst term in (6) while the second term in (6) is bounded
by a constant η. Therefore, the upper bound for decoding delay
can be written as
￿
E[τ S ] ≤ η2 K log(S) + Kη1 + η.
(16)
IV. P ERFORMANCE E VALUATION

In this section, we validate our analytical models with simulations and perform performance comparison. We consider
a line network with multiple sources and a destination. New
data from upper layer arrives at each source at a rate of one
packet per second. For the packet success probabilities, we
follow the model used in [13]. For the output link of source
s, the packet success probability αs is randomly chosen with
(λ−1)
probability λαs
, where λ is a tunable design parameter.
The mean and variance of the packet success probability are
λ/(λ + 1) and λ/(λ + 2), respectively. The higher the value of
λ, the lower the rate of packet loss in the network. We set λ to
be equal to 4, which results in an average success probability
of 0.8 for all the links. Unless stated otherwise, the Galois
ﬁeld size q = 8, and the number of sources S = 10.
The destination cannot decode the packets of one generation
unless it has enough packets from that generation. Large
values of generation size K imposes long waiting time for the
destination. It is of interest to choose a small K. On the other
hand, the greater the value of K, the higher the maximum
achievable throughput. This trade-off encourages us to ﬁnd
an optimal K. We vary the generation size K from 50 to
400 with steps of 50. Fig. 2 shows the normalized decoding
delay of RLNC with and without intra-session coding3 . The
decoding delay is normalized to the generation size K at each
point. We notice that the normalized decoding delay is the
inverse of the throughput for the line network. For all values
of K, RLNC with intra-session coding has a lower decoding

The inverse of the CDF of a normal random variable can be
written using the inverse of error function as F −1 (x) = µ +
√
σ 2erf−1 (2x − 1). Therefore, we have
￿
￿
√
2P
P2
YS = µ + σ 2erf−1 1 −
+ 2 .
(13)
S
S

An upper bound for the inverse of error function is presented
￿
in [12] as erf−1 (x) < − log(1 − x), for 0 < x < 1. Using
this upper bound, we obtain
￿
￿
√
2P
P2
YS ≈ µ + σ 2erf−1 1 −
+ 2
S
S
￿
￿
￿
√
2P
P2
< µ + σ 2 − log
− 2
S
S
√ ￿
< µ + σ 2 log (S 2 ) − log (2P S − P 2 ) (14)

Random variable P ￿
has an exponential distribution. The
￿
probability that ￿ 2P S ￿ P 2 is less than 0 (i.e.,
log
−
￿ ￿
P log 2P S − P 2 < 0 | S ) quickly approaches zero as S
increases. Therefore, by taking expectation from both sides of
inequality in (14), we obtain
√ ￿
E[YS ] < E[µ + σ 2 log (S 2 ) − log (2P S − P 2 )]
￿
< µ + 2σ log (S),
(15)
￿
2
where µ = K/αmin and σ = K(1 − αmin )/(αmin ). Setting
2
η1 = 1/αmin and η2 = 4(1 − αmin )/αmin , we obtain
￿
E[YS ] < Kη1 + η2 K log(S).

3 For RLNC without intra-session coding, the ﬂows of different sources
towards the destination are transmitted independently at the intermediate
sources. Each intermediate source node keeps separate buffers for different
ﬂows that it relays.

4

390

460
Analytical upper bound
Simulation results

380

420

360

Decoding Delay

Decoding Delay

370

350
340
330

400
380
360
340

320

320

310
300

RLNC without intra−session coding
RLNC with intra−session coding

440

1

5

10
15
Number of sources S

20

300

25

1

5

10
15
Number of sources S

20

25

Fig. 3. Comparison on the analytical upper bound and the simulation results
on decoding delay. (λ = 4, K = 250)

Fig. 4. Comparison between RLNC with and without intra-session coding
on decoding delay. (λ = 4, K = 250)

delay than RLNC without intra-session coding. For values of
K greater than 250, the normalized decoding delay of both
cases approaches a steady state.
Fig. 3 shows that the decoding delay for RLNC with intrasession coding obtained via simulation match the analytical
upper bound. The gap between the analytical model and
simulation results originates from (12), where ζs is replaced
˜
with ζs . Note that the mean of ζs is K/αs whereas the
˜
mean of ζs is K/αmin and αmin = min{α1 , . . . , αS }. The
analytical upper bound will match the simulation results when
α1 = α2 = · · · = αS .
Fig. 4 compares the decoding delay of RLNC with and
without intra-session coding. For values of S less than 5, the
average decoding delay of RLNC without intra-session coding
is slightly less than RLNC with intra-session coding. In RLNC
with intra-session coding, the decoding delay of different
sources is identical and depends on the link with the lowest
packet success rate. Thus, the decoding delay of RLNC with
intra-session coding is higher than RLNC without intra-session
coding when S is small. When intra-session coding is used, the
intermediate sources can opportunistically transmit packets of
other ﬂows when a particular ﬂow does not have innovative
packet. As the number of sources increases in the network,
the aggregate sending rate of intermediate nodes increases.
Thus, using intra-session coding, the number of transmitted
packets which do not have innovative information is reduced as
the number of sources S increases. This consequently reduces
the decoding delay compared to RLNC without intra-session
network coding. As the number of sources S increases, the
difference between the decoding delay of RLNC with and
without intra-session coding is increased without bound.

generation size K, when RLNC with intra-session coding is
used, we showed that the decoding delay is upper-bounded
￿
by η2 K log(S) + Kη1 + η, where η, η1 , η2 are constants.
The analytical model is validated through simulations. We
compared the decoding delay in both cases for varying number
of sources. The results showed that as the number of sources S
increases, the difference between the decoding delay of RLNC
with and without intra-session coding increases without bound.
R EFERENCES
[1] D. S. Lun, M. M´dard, and M. Effros, “On coding for reliable come
munication over packet networks,” in Proc. of 42nd Annual Allerton
Conference on Communication, Control, and Computing, Monticello,
IL, Oct. 2004.
[2] D. S. Lun, M. M´dard, R. Koetter, and M. Effros, “On coding for reliable
e
communication over packet networks,” Physical Communication, vol. 1,
no. 1, pp. 3–20, Mar. 2008.
[3] P. Pakzad, C. Fragouli, and A. Shokrollahi, “Coding schemes for line
networks,” in Proc. of IEEE Int’l Symposium on Information Theory
(ISIT), Adelaide, Australia, Sept. 2005.
[4] T. K. Dikaliotis, A. G. Dimakis, T. Ho, and M. Effros, “On the delay of
network coding over line networks,” in Proc. of IEEE Int’l Symposium
on Information Theory (ISIT), Seoul, Korea, June 2009.
[5] ——, “On the delay advantage of coding in packet erasure networks,”
in Proc. of IEEE Information Theory Workshop (ITW), Dublin, Ireland,
Sept. 2010.
[6] R. Cogill and B. Shrader, “Delay bounds for random linear coding in
multihop relay networks,” in Proc. of Conference on Information Science
and Systems, Baltimore, MD, Mar. 2011.
[7] D. Lucai, M. Stojanovic, and M. M´dard, “Random linear network
e
coding for time division duplexing: When to stop talking and start
listening,” in Proc. of IEEE INFOCOM, Rio de Janeiro, Brazil, Apr.
2009.
[8] A. H. Mohsenian-Rad and V.W.S. Wong, “Joint channel allocation,
interface assignment, and MAC design for multi-channel wireless mesh
networks,” in Proc. of IEEE INFOCOM, Anchorage, Alaska, May 2007.
[9] H. Seferoglu, A. Markopoulou, and K. K. Ramakrishnan, “I2 NC: Intraand inter-session network coding for unicast ﬂows in wireless networks,”
in Proc. of IEEE INFOCOM, Shanghai, China, Apr. 2011.
[10] T. Ho and D. S. Lun, Network Coding: An Introduction. Cambridge
University Press, 2008.
[11] L. Devroye, “Generating the maximum of independent identically distributed random variables,” Comp. and Maths. with Applications, vol. 6,
pp. 305–315, 1980.
[12] M. Chiani and D. Dardari, “Improved exponential bounds and approximation for the Q-function with application to average error probability
computation,” in Proc. of IEEE Globecom, Taipei, Taiwan, Nov. 2002.
[13] Y. Mao, F. R. Kschischang, B. Li, and S. Pasupathy, “A factor graph
approach to link loss monitoring in wireless sensor networks,” IEEE J.
on Selected Areas in Commun., vol. 23, no. 4, pp. 820–829, Apr. 2005.

V. C ONCLUSION
In this paper, we studied random linear network coding
(RLNC) with intra-session coding. We considered a line
network while all the nodes generate packets. RLNC with
intra-session coding requires intermediate sources to perform
coding operation on the packets of all the sources that they
relay. The packets of various ﬂows are decoded at the same
time in the destination. For a line network with S sources and

5

