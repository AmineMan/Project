Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 17:09:54 2012
ModDate:        Tue Jun 19 12:54:40 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      396373 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569551347

Moderate Deviations Analysis of Binary Hypothesis Testing
Igal Sason
sason@ee.technion.ac.il
Department of Electrical Engineering
Technion, Haifa 32000, Israel

deviations regime. This work was followed by a work by
Polynaskiy and Verd´ where they show that a DMC satisﬁes
u
the MDP if and only if its channel dispersion is non-zero, and
also that the AWGN channel satisﬁes the MDP with a constant
that is equal to the channel dispersion. The approach used in
[4] was based on the method of types, whereas the approach
used in [17] borrowed some tools from a recent work by the
same authors in [16].
In [11], the moderate deviations analysis of the SlepianWolf problem for lossless source coding was studied. More
recently, moderate deviations analysis for lossy source coding
of stationary memoryless sources was studied in [22].
These works, including this paper, indicate a recent interest
in moderate deviations analysis in the context of informationtheoretic problems. In the literature on probability theory, the
moderate deviations analysis was extensively studied (see, e.g.,
[10, Section 3.7]), and in particular the MDP was studied in
[9] for continuous-time martingales with bounded jumps.
This paper has the following structure: Section II introduces
brieﬂy some preliminary material related to martingales and
Azuma’s inequality. It then follows by introducing a reﬁned
version of Azuma’s inequality, and a study of its relation to
the moderate deviations principle for i.i.d. random variables.
Section III considers the relation of Azuma’s inequality and the
reﬁned version of this inequality (from Section II) to moderate
deviations analysis of binary hypothesis testing. Section IV
concludes the paper, followed by a discussion on the MDP
that is relegated to an appendix.

Abstract—This work refers to moderate-deviations analysis of
binary hypothesis testing. It relies on a concentration inequality
for discrete-parameter martingales with bounded jumps, which
forms a reﬁnement to the Azuma-Hoeffding inequality. Relations
of the analysis to the moderate deviations principle for i.i.d.
random variables and the relative entropy are considered.
Index Terms—Concentration inequalities, hypothesis testing,
moderate deviations principle.

I. I NTRODUCTION
The moderate deviations analysis in the context of source
and channel coding has recently attracted some interest among
information theorists (see [1], [4], [11], [16], [19] and [22]).
The purpose of this paper is to consider moderate deviations
analysis for binary hypothesis testing.
In the following, related literature on moderate deviations
analysis in information-theoretic aspects is shortly reviewed.
Moderate deviations were analyzed in [1, Section 4.3] for
a channel model that gets noisier as the block length is
increased. Due to the dependence of the channel parameter
in the block length, the usual notion of capacity for these
channels is zero. Hence, the issue of increasing the block
length for the considered type of degrading channels was
examined in [1, Section 4.3] via moderate deviations analysis
when the number of codewords increases sub-exponentially
with the block length. In another recent work [4], the moderate
deviations behavior of channel coding for discrete memoryless
channels was studied by Altug and Wagner with a derivation
of direct and converse results which explicitly characterize the
rate function of the moderate deviations principle (MDP). In
[4], the authors studied the interplay between the probability
of error, code rate and block length when the communication
takes place over discrete memoryless channels, having the
interest to ﬁgure out how the decoding error probability of
the best code scales when simultaneously the block length
tends to inﬁnity and the code rate approaches the channel
capacity. The novelty in the setup of their analysis was the
consideration of the scenario mentioned above, in contrast
to the case where the rate is kept ﬁxed below capacity, and
the study is reduced to a characterization of the dependence
between the two remaining parameters (i.e., the block length n
and the average/ maximal error probability of the best code).
As opposed to the latter case when the code rate is kept
ﬁxed, which then corresponds to large deviations analysis
and characterizes the error exponents as a function of the
rate, the analysis in [4] (via the introduction of direct and
converse theorems) demonstrated a sub-exponential scaling
of the maximal error probability in the considered moderate

II. C ONCENTRATION AND I TS R ELATION TO THE
M ODERATE D EVIATIONS P RINCIPLE
We present here some essential material that is related to
the martingale approach used in this paper for the moderatedeviations analysis of binary hypothesis testing. A background
on martingales is provided in, e.g., [23] where we only rely
here on basic knowledge on martingales.
A. Azuma’s Inequality
Azuma’s inequality1 forms a useful concentration inequality
for bounded-difference martingales [5]. In the following, this
inequality is introduced. The reader is referred to, e.g., [6], [?,
Chapter 2] and [15] for surveys on concentration inequalities
for martingales (including a proof of this inequality).
Theorem 1: [Azuma’s inequality] Let {Xk , Fk }∞ be
k=0
a discrete-parameter real-valued martingale sequence (where
1 Azuma’s inequality is also known as the Azuma-Hoeffding inequality. It
will be named from this point as Azuma’s inequality for the sake of brevity.

1

Let η ∈ ( 1 , 1) be an arbitrary ﬁxed number, and let {an }∞
n=1
2
be the non-negative sequence

F0 ⊆ F1 ⊆ . . . is called a ﬁltration). Assume that for every
k ∈ N, the condition |Xk − Xk−1 | ≤ dk holds a.s. for some
non-negative constants {dk }∞ . Then
k=1
P(|Xn − X0 | ≥ r) ≤ 2 exp −

r
2

an = n1−2η ,

n
k=1

d2
k

so that an → 0 and nan → ∞ as n → ∞. Let α ∈ R+ , and
Γ (−∞, −α] ∪ [α, ∞). Note that, from (4),

∀ r ≥ 0. (1)

n

The concentration inequality stated in Theorem 1 was
proved in [12] for independent bounded random variables, and
it was later derived in [5] for bounded-difference martingales.

so from the moderate deviations principle (MDP)
n→∞

δ

α
d

p
q

(3)

III. M ODERATE D EVIATIONS A NALYSIS FOR B INARY
H YPOTHESIS T ESTING
Binary hypothesis testing for ﬁnite alphabet models was
analyzed via the method of types, e.g., in [7, Chapter 11] and
[8]. It is assumed that the data sequence is of a ﬁxed length
(n), and one wishes to make the optimal decision based on
the received sequence and the Neyman-Pearson ratio test.
Let the RVs X1 , X2 .... be i.i.d. ∼ Q, and consider two
hypotheses:
• H 1 : Q = P1 .
• H 2 : Q = P2 .
For the simplicity of the analysis, let us assume that the RVs
are discrete, and take their values on a ﬁnite alphabet X where
P1 (x), P2 (x) > 0 for every x ∈ X .
In the following, let

1−p
1−q

and D(p||q)
p ln
+ (1 − p) ln
for p, q ∈ [0, 1]
is the divergence (a.k.a. relative entropy or Kullback-Leibler
distance) between the two probability distributions (p, 1 − p)
and (q, 1 − q). If δ > 1, then the probability on the left-hand
side of (2) is equal to zero.
Proof: See [14], [10, Corollary 2.4.7] or [19, Section III].

C. Relation of Theorem 2 with the Moderate Deviations
Principle for i.i.d. RVs
According to the moderate deviations theorem (see, e.g.,
[10, Theorem 3.7.1]) in R, let {Xi }n be a sequence of i.i.d.
i=1
real-valued RVs such that ΛX (λ) = E[eλXi ] < ∞ in some
neighborhood of zero, and also assume that E[Xi ] = 0 and
σ 2 = Var(Xi ) > 0. Let {an }∞ be a non-negative sequence
n=1
such that an → 0 and nan → ∞ as n → ∞, and let
an
n

Zn

L(X1 , . . . , Xn )

∀ n ∈ N.

n

ln
i=1

lim

n→∞

P1 (Xi )
P2 (Xi )

(7)

L(X1 , . . . , Xn )
= −D(P2 ||P1 )
(8)
n→∞
n
where the above assumptions on the probability mass functions
P1 and P2 imply that the relative entropies, D(P1 ||P2 ) and
D(P2 ||P1 ), are both ﬁnite. Consider the case where for some
ﬁxed constants λ, λ ∈ R that satisfy

1
inf x2
2σ 2 x∈Γ0
≤ lim inf an ln P(Zn ∈ Γ)

lim

−

n→∞

≤ lim sup an ln P(Zn ∈ Γ)
n→∞

1
inf x2
2σ 2 x∈Γ

n
P1 (X1 , . . . , Xn )
n (X , . . . , X ) =
P2
1
n

L(X1 , . . . , Xn )
= D(P1 ||P2 )
n
and otherwise, if hypothesis H2 is true, then a.s.

(4)

i=1

Then, for every measurable set Γ ⊆ R,

≤−

ln

designate the log-likelihood ratio. By the strong law of large
numbers (SLLN), if hypothesis H1 is true, then a.s.

n

Xi ,

α2
, ∀ α ≥ 0. (6)
2σ 2

(2)

for every k ∈ {1, . . . , n}. Then, for every α ≥ 0,

σ2
,
d2

i=1

=−

It is demonstrated in Appendix A that, in contrast to Azuma’s
inequality, Theorem 2 gives an upper bound on the probability
n
η
(where n ∈ N and α ≥ 0) which
P
i=1 Xi ≥ αn
coincides with the exact asymptotic limit in (6). The analysis
in Appendix A provides another interesting link between
Theorem 2 and a classical result in probability theory, which
also emphasizes the signiﬁcance of the reﬁnements of Azuma’s
inequality.

|Xk − Xk−1 | ≤ d,

γ

Xi ≥ αnη

lim n1−2η ln P

Var(Xk |Fk−1 ) = E (Xk − Xk−1 )2 | Fk−1 ≤ σ 2

γ
δ+γ
1+γ 1+γ

= P(Zn ∈ Γ)

i=1

n

Theorem 2: Let {Xk , Fk }∞ be a discrete-parameter realk=0
valued martingale. Assume that, for some constants d, σ > 0,
the following two requirements are satisﬁed a.s.

P(|Xn − X0 | ≥ αn) ≤ 2 exp −n D

Xi ≥ αnη

P

B. A Reﬁned Version of Azuma’s Inequality

where

∀n ∈ N

2

(5)

−D(P2 ||P1 ) < λ ≤ λ < D(P1 ||P2 )

0

where Γ and Γ designate, respectively, the interior and closure
sets of Γ.

one decides on hypothesis H1 if L(X1 , . . . , Xn ) > nλ, and
on hypothesis H2 if L(X1 , . . . , Xn ) < nλ. Note that if

2

λ=λ
λ then a decision on the two hypotheses is based
on comparing the normalized log-likelihood ratio (w.r.t. n) to
a single threshold (λ), and deciding on hypothesis H1 or H2
if this normalized log-likelihood ratio is, respectively, above
or below λ. If λ < λ then one decides on H1 or H2 if
the normalized log-likelihood ratio is, respectively, above the
upper threshold λ or below the lower threshold λ. Otherwise,
if the normalized log-likelihood ratio is between the upper and
lower thresholds, then an erasure is declared and no decision
is taken in this case.
Let
(1)
αn

n
P1 L(X1 , . . . , Xn ) ≤ nλ

(9)

(2)
αn

n
P1 L(X1 , . . . , Xn ) ≤ nλ

(10)

(1)
βn

n
P2 L(X1 , . . . , Xn ) ≥ nλ

(11)

(2)
βn

n
P2 L(X1 , . . . , Xn ) ≥ nλ

Accordingly, the conditional probabilities in (9)–(12) are
modiﬁed so that the ﬁxed thresholds λ and λ are replaced
(n)
and
with the above block-length dependent thresholds λ
(n)
λ , respectively. The moderate deviations analysis for binary
hypothesis testing studies the probability of an error event and
the probability of a joint error and erasure event under the two
hypotheses, and it studies the interplay between each of these
probabilities, the block length n, and the related thresholds
that tend asymptotically to the limits in (7) and (8) when the
block length tends to inﬁnity.
In light of the discussion in Section II-C on the MDP for
i.i.d. RVs and the discussion of its relation to Theorem 2 (see
Appendix A), and also motivated by the three recent works in
[1, Section 4.3], [4] and [11], we proceed to consider in the
following moderate deviations analysis for binary hypothesis
testing. Our approach for this kind of analysis is different, and
it relies on concentration inequalities for martingales.
In the following, we analyze the probability of a joint error
and erasure event under hypothesis H1 , i.e., derive an upper
(1)
bound on αn in (9). The same kind of analysis can be adapted
easily for the other probabilities in (10)–(12).
Under hypothesis H1 , let us construct the martingale sequence {Uk , Fk }n where F0 ⊆ F1 ⊆ . . . Fn is the ﬁltration
k=0

(12)

and

(1)

(1)

then αn and βn are the probabilities of either making an
error or declaring an erasure under, respectively, hypotheses
(2)
(2)
H1 and H2 ; similarly αn and βn are the probabilities of
making an error under hypotheses H1 and H2 , respectively.
Let π1 , π2 ∈ (0, 1) denote the a-priori probabilities of the
hypotheses H1 and H2 , respectively, so
(1)
(1)
(1)
Pe,n = π1 αn + π2 βn

F0 = {∅, Ω},

Fk = σ(X1 , . . . , Xk ), ∀ k ∈ {1, . . . , n}

and
n
Uk = EP1 L(X1 , . . . , Xn ) | Fk .

(13)

For every k ∈ {0, . . . , n}

is the probability of having either an error or an erasure, and
(2)
(2)
(2)
Pe,n = π1 αn + π2 βn

n
n
Uk = EP1

(14)

k

=
k

= D(P1 ||P2 ),

i=1

λ

n
n
EP1 ln

i=k+1

P1 (Xi )
P2 (Xi )

P1 (Xi )
+ (n − k)D(P1 ||P2 ).
P2 (Xi )

n

Un =

ln
i=1

(16)

P1 (Xi )
= L(X1 , . . . , Xn )
P2 (Xi )

(17)

and, for every k ∈ {1, . . . , n},

lim λ(n) = −D(P2 ||P1 ).

Uk − Uk−1 = ln
Let
d1

max ln
x∈X

P1 (Xk )
− D(P1 ||P2 ).
P2 (Xk )

(18)

P1 (x)
− D(P1 ||P2 )
P2 (x)

(19)

so d1 < ∞ since by assumption the alphabet set X is ﬁnite,
and P1 (x), P2 (x) > 0 for every x ∈ X . From (18) and (19),
|Uk − Uk−1 | ≤ d1 a.s. for every k ∈ {1, . . . , n}, and due to
the statistical independence of {Xi }

= D(P1 ||P2 ) − ε1 n−(1−η)

(n)

P1 (Xi )
+
P2 (Xi )

Fk

U0 = nD(P1 ||P2 ),

n→∞

(n)

P1 (Xi )
P2 (Xi )

In particular

1
Speciﬁcally, let η ∈ ( 2 , 1), and ε1 , ε2 > 0 be arbitrary
ﬁxed numbers, and consider the case where one decides on
(n)
hypothesis H1 if L(X1 , . . . , Xn ) > nλ , and on hypothesis
H2 if L(X1 , . . . , Xn ) < nλ(n) where these upper and lower
thresholds are set to

λ

ln

=

Suppose that instead of having some ﬁxed upper and lower
thresholds, one is interested to set these thresholds such that
as the block length n tends to inﬁnity, they tend simultaneously
to their asymptotic limits in (7) and (8), i.e.,
lim λ

ln
i=1

−D(P2 ||P1 ) < λ ≤ λ < D(P1 ||P2 ).

n→∞

ln
i=1

is the probability of error.
Based on the asymptotic results in (7) and (8), which
hold a.s. under hypotheses H1 and H2 respectively, the large
deviations analysis refers to upper and lower thresholds λ and
λ which are kept ﬁxed (i.e., these thresholds do not depend on
the block length n of the data sequence) where

(n)

(15)

= −D(P2 ||P1 ) + ε2 n−(1−η)

n
EP1 (Uk − Uk−1 )2 | Fk−1

so that they approach, respectively, the relative entropies
D(P1 ||P2 ) and −D(P2 ||P1 ) in the asymptotic case where
the block length n of the data sequence tends to inﬁnity.

=

P1 (x) ln
x∈X

3

P1 (x)
− D(P1 ||P2 )
P2 (x)

2
2
σ1 . (20)

Let ε1 > 0 and η ∈ ( 1 , 1) be two arbitrarily ﬁxed numbers.
2
Then, under hypothesis H1 , it follows from Theorem 2 and
the above construction of a martingale that
(n)

n
P1

L(X1 , . . . , Xn ) ≤ nλ

2
that, under hypothesis H1 , have zero mean and variance σ1 .
n
Since, by assumption, the sequence {Xi }i=1 are i.i.d., then
n

L(X1 , . . . , Xn ) − nD(P1 ||P2 ) =

)

n
= P1 Un − U0 ≤ −ε1 nη
(η,n)

δ1

≤ exp −nD
where

ε1 n−(1−η)
,
d1

(η,n)

δ1

γ1
1 + γ1

+ γ1
1 + γ1

(22)

2
with d1 and σ1 from (19) and (20).
In the following, we will make use of the following lemma:
Lemma 1:

u+

(1 + u) ln(1 + u) ≥

u+

u2
2 ,
2

u
2

u ∈ [−1, 0]
3

u
6

−

(23)
u≥0

,

where at u = −1, the left-hand side is deﬁned to be zero (it
is the limit of this function when u → −1 from above).
Proof: The proof follows by elementary calculus.
From (22) and the inequality in Lemma 1, it follows that
(η,n)

D
≥

δ1

+ γ1
1 + γ1

γ1
1 + γ1

δ1
γ1

(η,n) 3

(η,n) 2

(η,n)

γ1
1 + γ1

+

δ1
2
2γ1

−

δ1
3
6γ1

(η,n)

+

1
(δ
)2
(η,n)
−δ1
+ 1
γ1
2

(η,n) 2

=
=

δ1
2γ1

(η,n) 3

−

ε2 n−2(1−η)
1
2
2σ1

δ1
2 (1 + γ )
6γ1
1
1−

ε1 d1
1
2
3σ1 (1 + γ1 ) n1−η

(η,n)

provided that δ1
< 1 (which holds for n ≥ n0 for some
n0
n0 (η, ε1 , d1 ) ∈ N that is determined from (22)). By
substituting this lower bound on the divergence into (21), it
follows that
(1)
n
αn = P1 L(X1 , . . . , Xn ) ≤ nD(P1 ||P2 ) − ε1 nη
ε1 d1
1
ε2 n2η−1
1− 2
≤ exp − 1 2
2σ1
3σ1 (1 + γ1 ) n1−η

.

(24)

Consequently, in the limit where n tends to inﬁnity,
(1)
lim n1−2η ln αn ≤ −

n→∞

(26)

and it follows from the one-sided version of the MDP in (6)
that indeed (25) holds with equality. Moreover, Theorem 2
provides, via the inequality in (24), a ﬁnite-length result that
enhances the asymptotic result for n → ∞.
In the considered setting of moderate deviations analysis for
binary hypothesis testing, the upper bound on the probability
(1)
αn in (24), which refers to the probability of either making
an error or declaring an erasure (i.e., making no decision)
under the hypothesis H1 , decays to zero sub-exponentially
with the length n of the sequence. As mentioned above, based
on the analysis in Section II-C and Appendix A, the asymptotic
upper bound in (25) is tight. A completely similar moderatedeviations analysis can be also performed under the hypothesis
(1)
H2 . Hence, a sub-exponential scaling of the probability βn in
(11) of either making an error or declaring an erasure (where
the lower threshold λ is replaced with λ(n) ) also holds under
the hypothesis H2 . These two sub-exponential decays to zero
(1)
(1)
for the probabilities αn and βn , under hypothesis H1 or H2
respectively, improve as the value of η ∈ ( 1 , 1) is increased.
2
On the other hand, the two exponential decays to zero of the
(2)
(2)
probabilities of error (i.e., αn and βn under hypothesis H1
or H2 , respectively) improve as the value of η ∈ ( 1 , 1) is
2
decreased; this is due to the fact that, for a ﬁxed value of n, the
margin which serves to protect us from making an error (either
under hypothesis H1 or H2 ) is increased by decreasing the
value of η as above (note that by reducing the value of η for a
(n)
ﬁxed n, the upper and lower thresholds λ and λ(n) are made
closer to D(P1 ||P2 ) from below and to −D(P2 ||P1 ) from
above, respectively, which therefore increases the margin that
is used for protecting one from making an erroneous decision).
This shows the existence of a tradeoff, in the choice of the
1
parameter η ∈ ( 2 , 1), between the probability of error and the
joint probability of error and erasure under either hypothesis
H1 or H2 (where this tradeoff exists symmetrically for each
of the two hypotheses).
In [4] and [17], the authors consider moderate deviations
analysis for channel coding over memoryless channels. In
particular, [4, Theorem 2.2] and [17, Theorem 6] indicate on
a tight lower bound (i.e., a converse) to the asymptotic result
in (25) for binary hypothesis testing. This tight converse is
indeed consistent with the asymptotic result of the MDP in (6)
for real-valued i.i.d. random variables, which implies that the
asymptotic upper bound in (25), obtained via the martingale
approach with the reﬁned version of Azuma’s inequality in
Theorem 2, holds indeed with equality. Note that this equality
does not follow from Azuma’s inequality, so its reﬁnement
was essential for obtaining this equality. The reason is that,
due to Appendix A, the upper bound in (25) that is equal to
ε2
1
− 2σ2 is replaced via Azuma’s inequality by the looser bound

(21)

2
σ1
2
d1

γ1

Yi ,
i=1

ε2
1
2
2σ1

(25)

2
with σ1 in (20). From the analysis in Section II-C and
Appendix A, it follows that the inequality for the asymptotic
limit in (25) holds in fact with equality. To verify this, consider
the real-valued sequence of i.i.d. RVs

1

Yi

ln

P1 (Xi )
P2 (Xi )

ε2

− D(P1 ||P2 ),

1
− 2d2 (note that, from (19) and (20), σ1 ≤ d1 where in general
1
σ1 may be signiﬁcantly smaller than d1 ).

i = 1, . . . , n

4

for every n ∈ N, and therefore (since, from (3),

IV. S UMMARY
This paper is focused on the moderate deviations analysis
of binary hypothesis testing. The analysis is based on a concentration inequality for discrete-parameter martingales with
bounded jumps, which forms a reﬁned version of Azuma’s
inequality (see [10, Corollary 2.4.7]). The relation of this
concentration inequality to the moderate deviations principle
for i.i.d. random variables is considered. This paper presents in
part the work in [19], and it exempliﬁes the use of a reﬁnement
of Azuma’s inequality in an information-theoretic aspect.
Further information-theoretic applications are considered in,
e.g., [20] and [24]. The slides are available in [21].

R EFERENCES
[1] E. A. Abbe, Local to Global Geometric Methods in Information Theory,
Ph.D. dissertation, MIT, Boston, MA, USA, June 2008.
[2] N. Alon and J. H. Spencer, The Probabilistic Method, Wiley Series in
Discrete Mathematics and Optimization, Third Edition, 2008.
[3] A. N. Arkhangel’skii, “Lower bounds for probabilities of large deviations
for sums of independent random variables,” Theory of Probability and
Applications, vol. 34, no. 4, pp. 565-575, 1989.
[4] Y. Altuˇ and A. B. Wagner, “Moderate deviations analysis of channel
g
coding: discrete memoryless case,” Proceedings 2010 IEEE International Symposium on Information Theory (ISIT 2010), pp. 265–269,
Austin, Texas, USA, June 2010.
[5] K. Azuma, “Weighted sums of certain dependent random variables,”
Tohoku Mathematical Journal, vol. 19, pp. 357–367, 1967.
[6] F. Chung and L. Lu, “Concentration inequalities and martingale inequalities: a survey,” Internet Mathematics, vol. 3, no. 1, pp. 79–127, March
2006.
[7] T. M. Cover and J. A. Thomas, Elements of Information Theory, John
Wiley and Sons, second edition, 2006.
[8] I. Csisz´ r and P. C. Shields, Information Theory and Statistics: A
a
Tutorial, Foundations and Trends in Communications and Information
Theory, vol. 1, no. 4, pp. 417–528, 2004.
[9] A. Dembo, “Moderate deviations for martingales with bounded jumps,”
Electronic Communications in Probability, vol. 1, no. 3, pp. 11–17,
March 1996.
[10] A. Dembo and O. Zeitouni, Large Deviations Techniques and Applications, Springer, second edition, 1997.
[11] D. He, L. A. Lastras-Monta˜ o, E. Yang, A. Jagmohan and J. Chen, “On
n
the redundancy of Slepian-Wolf coding,” IEEE Trans. on Information
Theory, vol. 55, no. 12, pp. 5607–5627, December 2009.
[12] W. Hoeffding, “Probability inequalities for sums of bounded random
variables,” Journal of the American Statistical Association, vol. 58,
no. 301, pp. 13–30, March 1963.
[13] F. den Hollander, Large Deviations, Fields Institute Monographs, American Mathematical Society, 2000.
[14] C. McDiarmid, “On the method of bounded differences,” Surveys in
Combinatorics, vol. 141, pp. 148–188, Cambridge University Press,
Cambridge, 1989.
[15] C. McDiarmid, “Concentration,” Probabilistic Methods for Algorithmic
Discrete Mathematics, pp. 195–248, Springer, 1998.
[16] Y. Polyanskiy, H. V. Poor, and S. Verd´ , “Channel coding rate in ﬁnite
u
blocklength regime,” IEEE Trans. on Information Theory, vol. 56, no. 5,
pp. 2307–2359, May 2010.
[17] Y. Polyanskiy and S. Verd´ , “Channel dispersion and moderate deviau
tions limits of memoryless channels,” Proceedings Forty-Eighth Annual
Allerton Conference, pp. 1334–1339, UIUC, Illinois, USA, October
2010.
[18] L. V. Rozovsky, “Estimate from below for large-deviation probabilities
of a sum of independent random variables with ﬁnite variances,” Journal
of Mathematical Sciences, vol. 109, no. 6, May 2002.
[19] I. Sason, “On reﬁned versions of the Azuma-Hoeffding inequality with
applications in information theory,” last updated on February 2012.
[Online]. Available: http://arxiv.org/abs/1111.1977.
[20] I. Sason, “On the concentration of the crest factor for OFDM signals,”
Proceedings of the 2011 8th International Symposium on Wireless
Communication Systems (ISWCS ’11), pp. 784–788, Aachen, Germany,
November 2011. [Online]. Available: http://arxiv.org/abs/1111.1982.
[21] I. Sason, ”On Concentration of measures and moderate deviations
analysis of binary hypothesis testing,” presentation is online available at
http://webee.technion.ac.il/people/sason/Presentation Technion11.pdf.
[22] V. Y. F. Tan, “Moderate-deviations of lossy source coding for discrete
and Gaussian sources,” http://arxiv.org/abs/1111.2217, November 2011.
[23] D. Williams, Probability with Martingales, Cambridge University Press,
1991.
[24] K. Xenoulis, N. Kalouptsidis and I. Sason, “New achievable rates for
nonlinear Volterra channels via martingale inequalities,” Proceedings of
the 2012 International Symposium of Information Theory, MIT, Boston,
USA, July 2012.

α2 n2η−1
2d2

and therefore
α2
.
(27)
n→∞
2d2
This differs from the limit in (6) where σ 2 is replaced by d2 ,
so Azuma’s inequality does not provide the correct asymptotic
result in (6) (unless σ 2 = d2 , i.e., |Xk | = d a.s. for every k).
2) Analysis related to Theorem 2: From Theorem 2, it
follows that for every α ≥ 0,
lim n1−2η ln P |Sn | ≥ αnη ≤ −

δ +γ
γ
1+γ 1+γ

where γ is introduced in (3), and δ is given by
α
n1−η

= δn−(1−η)
d
due to the deﬁnition of δ in (3). Hence, it follows that
δ

α2
σ2 )

α2
.
(29)
n→∞
2σ 2
Hence, this bound coincides with the exact limit in (6).

A PPENDIX A
A NALYSIS R ELATED TO THE M ODERATE D EVIATIONS
P RINCIPLE F OR I . I . D . RV S (S EE S ECTION II-C)
It is demonstrated in the following that, in contrast to
Azuma’s inequality, Theorem 2 provides an upper bound on
n
η
P
for α ≥ 0, which coincides with
i=1 Xi ≥ αn
the correct asymptotic result in (6). It is proved under the
further assumption that there exists some constant d > 0
such that |Xk | ≤ d a.s. for every k ∈ N (since the RVs
{Xk } are assumed to be i.i.d., it is sufﬁcient to require it for
k = 1). Let us deﬁne the martingale sequence {Sk , Fk }n
k=0
k
where Sk
σ(X1 , . . . , Xk ) for every
i=1 Xi and Fk
k ∈ {1, . . . , n} with S0 = 0 and F0 = {∅, F}.
1) Analysis related to Azuma’s inequality: The martingale
sequence {Sk , Fk }n has uniformly bounded jumps, where
k=0
|Sk − Sk−1 | = |Xk | ≤ d a.s. for every k ∈ {1, . . . , n}. Hence
it follows from Azuma’s inequality that, for every α ≥ 0,

P(|Sn | ≥ αnη ) ≤ 2 exp −n D

=

lim n1−2η ln P |Sn | ≥ αnη ≤ −

Acknowledgment: One of the reviewers pointed out that
the moderate deviations analysis in this work can be done
alternatively by relying on results, e.g., from [3] or [18]. We
thank the reviewer for this note, and we currently study this
line of work.

P (|Sn | ≥ αnη ) ≤ 2 exp −

δ2
γ

(28)

P(|Sn | ≥ αnη )
δ 2 n2η−1
α(1 − γ) −(1−η)
≤ 2 exp −
1+
·n
+ ...
2γ
3γd

5

