Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 22:45:50 2012
ModDate:        Tue Jun 19 12:56:32 2012
Tagged:         no
Pages:          4
Encrypted:      no
Page size:      595.276 x 841.89 pts (A4)
File size:      333663 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566839
1

Design Principles and Speciﬁcations for
Neural-like Computation Under Constraints on
Information Preservation and Energy Costs as
Analyzed with Statistical Theory
William B Levy∗ and Toby Berger †
∗ Departments of Neurosurgery and of Psychology,
University of Virginia,
Charlottesville, Virginia 22903,
wbl@virginia.edu.
† Department of Electrical and Computer Engineering,
University of Virginia,
Charlottesville, Virginia 22903,
tb6n@virginia.edu.

NL element (call it j) in terms of the relevant informationpreservation and in terms of the energetic costs.
Generically characterized, NL computation uses many
asynchronous, analog, computational elements, with large
fan-in and fan-out between elements. Both communication
between elements and computation within each element
has associated costs. Starting with communication between
elements, we assume constant amplitude pulses are used
over the lossy transmission lines which connect individual
elements. Arguably, under this regime of constant amplitude pulses, the most energy-efﬁcient form of communication is continuous-time differential pulse position
modulation in which each pulse signals the end of an
interpulse interval (ipi) and the beginning of the next such
interval. Thus, we have our ﬁrst random variable relative
to j’s transformations, the interval Tj (k) , the time between
pulse k-1 and pulse k.
Throughout most of this work, until we generalize
certain results, we will concern ourselves with the simple
NL element j which receives inputs on lines indexed by i
and, like j, each of these input lines is delivering pulses,
Xi (S) = 1 for a single example, where s denotes a point in
continuous time. Driving these input lines is a random latent process Λj (s) ∈(0,∞). This latent process expresses itself as the parameter of a Poisson process driving j’s inputs.
(This Poisson process is actually an approximation; see
Berger and Levy, 2010.) It is j’s problem to communicate
something about the states of this latent variable. Instead of
the impossible task of communicating a continuous stream
of real numbers, element j will communicate information
concerning the average value (possible weighted) of this
latent variable over an interval deﬁned exactly as a single
ipi. In particular, Λj (k) := t−1 K(tk − S)Λj (S)dS where
k
the integration ranges from the beginning of an ipi to tk
(its end), and K(tk − S) is a, ﬁxed, causal (tk > S ≥ 0 )
function with K (0 ) = 0 that is bounded and continuous
function of tk − S with ﬁnite derivatives at all arguments

Abstract—Given enough physical constraints, the format of
optimal computation may resolve into a rather small set of
options, which we call design speciﬁcations. Our interest centers on computational problems that are so intensive, relative
to the time and energy available, that they can be solved only
in a probabilistic fashion. Here we consider just information
and energy in one particular computational format, called
neural-like (NL), and characterized as massively parallel,
analog computation. Within this format, we consider only the
design of a single NL element and the nature of its inputs.
Importantly, we provide a speciﬁc mathematical format of a
simple NL element. We consider this format to be minimal
and generic and, therefore, extendable to structures composed
of several NL compartments. Secondly, the information and
energy constraints are linked, via Shannon’s entropy, to
classical results from mathematical statistics yielding design
speciﬁcations that go beyond our initial description of a
NL element and its inputs. Critically, for a NL element
to preserve all of its relevant input-information at minimal
energetic cost, it must transform its inputs so as to create
and communicate a minimal sufﬁcient statistic. Then, the
assumptions associated with producing such a statistic become
new design speciﬁcations for NL computing.

I. I NTRODUCTION
In contrast to conventional computing (e.g., TuringChurch limited on a von Neumann architecture) whose
underlying computations are boolean-algebraic in nature,
we are interested in problems so computationally intensive,
relative to two physical constraints, that such problems
must be solved in terms of probabilities (e.g., highdimensional pattern recognition, decision-making, and control); the two physical constraints are (i) the available time
and (ii) the available energy. Here we consider just energy
and establish the importance of one computational format
for the simplest element of a massively parallel, analog
computational device, where such a device is most easily
described by the appellation neural-like (NL). Speciﬁcally,
we will analyze the transformation performed by a simple

1

2

in [0, tk ).
An externally arriving pulse is transformed into an
internal function hi (τni , τk ) = h(τk − τni ) that is scaled
by an amplitude that is dependent on i; here, h(u) is a
bounded, continuous function that equals 0 for u ≤ 0 and
has ﬁnite derivatives at all u > 0. h(τk − τni ) is known
when τk , τni are known. τni is the arrival time of the nth
i
pulse during interval k, and τ k (≥τni ) is the time elapsed
within the interval. The scaled h(τk − τni )’s are combined
to produce j’s internal excitation, Yj (k,τ k ); thus τ k ∈ [0,
tk ). For the kth ipi the transformations are summarized as
Λj (k) → Xj (k) → Yj (k) → Tj (k) where k implies the
full kth ipi, [0,tk ), and where Xj (k) is the list of all pulse
times for each input line i during the kth interval.
The internal excitation is converted into a pulse via
a ﬁrst passage time to a deterministic threshold-function
θ(k,τ k ) that may or may not vary with time. In any event,
knowledge of the ﬁrst passage time tk implies the value
of the internal excitation, Yj (k), at this time point. Thus
communication of tk to a recipient that has knowledge of
the threshold-function is equivalent to communicating {tk ,
Yj (k)}⇔{tk , Yj (k)/tk }.
To complete the model we add band limited noise,
thermal and ampliﬁer associated shot-noise which total
η(k) and which is in addition to the random Poisson-like
arrivals of the individual excitations. For the purpose of
mathematical analysis, we associate this noise, with and
proportionately apportion it among the individual events.
That is, we include noise henceforth via the substitution
hi (tk − τni ) ← (1 − ηni )hi (tk − τni ).
At this point we want to consider j as taking samples in order to make some inference about Λj (k). Call
this set of samples S(k):= {hi (tk − τni ): 0 ≤τni ≤ tk ,
ni ∈ {0, 1, ...}}: i∈ {1,...,N}} or ∅ if the preceding set
is empty, and denote one of its canonical realizations by s.
Now the visualization of the input and j’s transformations
is Λj (k) → Xj (k) → S (k) → Yj (k) → Tj (k). For now,
the function that combines the events in S(k) to produce
Yj (k) is an open question.
We have deﬁned computation by a simple NL element as
a series of transformations. Our design goal is to preserve
the Λj (k)-relevant information at minimum cost, where
cost is average energy expended. We will use, and slightly
extend via Shannon’s entropy, results from mathematical
statistics to infer additional design requirements that are
necessary to achieve this pair of goals.
The main results in terms of additional speciﬁcations are
(1) a probability model of Λj (k) must be in the exponential
class, (2) conditional on Λj (k)=λ, the relationship between
j’s inputs must well-approximate conditional independence,
and (3) Yj (k) must be formed additively from the individual
events in S(k). These results will be inferred from the
preceding deﬁnitions together with the facts that (a) the NL
element must use a sufﬁcient statistic in order to preserve
the Λj (k)-relevant information and (b) that representational
costs (e.g., states of Yj (k) or states of Tj (k)) will be
minimized for such a sufﬁcient statistic only when the
statistic is a minimal sufﬁcient statistic. Finally, additivity
arises from Dynkin’s result concerning minimal sufﬁcient
statistics of ﬁxed dimension. Corollaries will be described

that extend the results to include a simple NL element
that performs Bayesian inference and an extension to nonsimple NL elements.
II. T HE ANALYSIS
Fully consistent with the NL element, one can consider
S(k) to be a data-sampling consisting of the observable
events generated by the random process Λj (k) and noise;
moreover, as a reduction of these data, Yj (k) is a statistic.
As a design goal, we require that the statistic Yj (k) should
contain all the Λj (k)-relevant information arriving at j. The
statistic that preserves all the information from a sampling
is called a sufﬁcient statistic (Fisher, 1922; deﬁned below).
A sufﬁcient statistic can only occur when there is a
family of probability models. First, an essentially zerothorder design speciﬁcation that arises along with these
probability models is that any such model is analytically tractable. Speciﬁcally, analytical tractability here
shall mean that these distributions must be continuous
probability densities that are suitably differentiable and
positive everywhere between their endpoints; furthermore,
these densities must have a support that is both ﬁxed
and known. The densities (or function for any case where
normalization is not possible) under discussion are joint
densities and associated marginal and conditional densities,
e.g.,p(S(k) = s|Λj (k) = λ), p(Λj (k) = λ|S(k) =
s), p(S(k) = s|Yj (k) = y), inter alia. (Note: tk is
implicitly given as a conditioning variable whenever k
appears in a probability.)
Deﬁnition: A statistic Yj (k) (≡Yj (S(k))≡Yj (S([0,tk )))
is sufﬁcient for the model distributions of the data,
p(S (k) |Λj (k) = λ) (where all the distributions of the
model reside in the range of λ), if the data conditioned both
on Λj (k)=λ and on Yj (k) = y, are distributed the same way
as when they are conditioned only on the value of Yj (k).
In our notation, the requirement that qualiﬁes Yj (k) as a
sufﬁcient statistic is
p(S (k) = s|Λj (k) = λ, Yj (k) = y)
=p(S (k) = s|Yj (k) = y), ∀λ ∈ (0, ∞), ∀s.

(1)

Via the deﬁnition of a conditional probability, an equivalent
requirement is
p(Λj (k) = λ|S (k) = s, Yj (k) = y)
=p(Λj (k) = λ|Yj (k) = y), ∀λ ∈ (0, ∞), ∀s.

(2)

Given this deﬁnition, Koopman (1936) places
p(S(k)|Λj (k) = λ) in the exponential family if and
only if there is a sufﬁcient statistic. Furthermore,
this result can be combined with a near simultaneous
observation by Fisher and Neyman (cited in Kagan et
al., 1973) concerning the factorization of the likelihood
that must occur when there is a sufﬁcient statistic and
the range of Λj (k) is ﬁxed: p(s|λ) = g(y|λ)φ(s) where
g(y|λ) contains no terms of s and φ(s) contains no terms
of λ and y.
Theorem 1 (Scalar parameter form): Given the above
assumptions concerning the ﬁxed support of λ and the other
regularity conditions needed (see e.g. Koopman 1936 or
Barankin and Maitra, 1963), and given that λ is a scalar,

2

3

then p(S (k) |Λj (k) = λ) is in the exponential family if
and only if there is a sufﬁcient statistic, e.g., Yj (k) = y,
for any value of the parameter λ. That is, the existence
of a sufﬁcient statistic and the given regularity conditions
require that, for all λ,

Thus, the design speciﬁcation is now extended to require
creation of a minimal sufﬁcient statistic in the transformations performed by j. Furthermore, a minimal sufﬁcient
statistic has a special form when the assumptions of
Theorem 1 hold. That is, additivity of singleton events from
the sample space is required as the functional form of b(y)
when Yj (k) is a minimal sufﬁcient statistic (Dynkin (1950)
or see Zacks (1971)). This additivity result follows from
independent sampling so this independence (conditional on
λ) is also a new design speciﬁcation.
Lemma: A minimal sufﬁcient statistic is additive in the
logarithm of the likelihood.
Consider any singleton data set and apply Theorem 1,

p(S (k) |Λj (k) = λ)
= exp(a(λ) · b(S (k)) + c(S (k)) + η) + d(λ))
= exp(a(λ) · b(y) + c(S (k)) + d(λ))

(3)

where the functions a(λ) and d(λ) do not contain terms
of the data, hi (τni , tk ), and the functions b(S(k)) and
c(S(k)) do not contain terms of λ. (Note that d(λ) is the
normalization term and may require recalculating in some
of the results that follow although we do not modify its
explicit form.)
At this point let’s turn our attention to the average
energy expenditure (cost) and consider a subclass of the
sufﬁcient statistics, the minimal sufﬁcient statistics. To
relate information-preservation and energy-expenditures,
we must complement statistical theory with informationtheoretic considerations.
Consider some space of events deﬁned by sample paths
of a random process that has a probability model consistent
with the existence of a sufﬁcient statistic for the random
variables Yj (k) and Tj (k). Suppose Yj (k) is any minimal
sufﬁcient statistic on this space while Yj + (k) is any nonminimal sufﬁcient statistic for this same space and model
and that all the sufﬁcient statistics are truncated to some
arbitrary precision:
Lemma (encoding inequality): The entropy of a minimal
sufﬁcient statistic Yj (k) is strictly less than the entropy of
Yj + (k). (This result will be proved elsewhere where the
critical insight arises from a fundamental characterization
of entropy; i.e., partitioning an event of positive probability
into two distinct events of positive probability increases the
entropy of the event space.)
Relating an information-preserving minimal sufﬁcient
statistic to energy costs hinges on an exchangeability of
information and energy. Assume a constant minimum cost
for any information that is encoded by j. At the joules-perbit limit, this cost is known: repeating the sense-write-erase
cycle so that a bit of information per cycle (i.e., per pulse)
is generated (remember information is an average), has a
minimum cost of kTloge 2 joules per bit (fractions of a bit
scale accordingly; T. Sagawa and M. Ueda, 2009; just here,
k is Boltzmann’s constant and T is absolute temperature).
Moreover, even if j does not work at this limit, assume
construction of a NL element so that every one of j’s
transformations operates at a ﬁxed multiple of this joulesper-bit limit. Then, combining a ﬁxed cost-per-bit with the
encoding lemma implies
Lemma (cost minimization): (i) A statistic that preserves
information and minimizes cost (average energy use) must
be in the equivalence class of minimal sufﬁcient statistics.
To say it another way, (ii) failure to use a minimal sufﬁcient
statistic leads to information loss and/or excessive cost.
Proof: The ﬁrst part follows from the encoding inequality
and the fact that every extra bit produces extra cost. The
second part follows from the ﬁrst in light of the deﬁnition
of a sufﬁcient statistic.

p(hi (τni , tk )|Λj (k) = λ)
= exp(a(λ) · b(hi (τni , tk )) + c(hi (τni , tk )) + d(λ)).
Then, given (conditional on λ) independent sampling of
each singleton member making up a generic data-set, the
conditional probability of this data set is the product of
the individual probabilities associated with each of the
singleton events; that is,
p(S (k) |Λj (tk ) = λ)
=

p(hi (τni , tk )|Λj (k) = λ)
i

ni

i

ni

exp(a(λ) · b(hi (τni , tk )) + c(hi (τni , tk )) + d(λ)),

=

where the normalization exp[d(λ)] has been re-calculated
= exp(a(λ)

b(hi (τni , tk ))
i

ni

+

c(hi (τni , tk )) + d(λ)),
ni

i

but, assuming the designer is motivated for a particular
function b(·) (and recalling also that any invertible function
of a minimal sufﬁcient statistic is a minimal sufﬁcient
statistic), this can be incorporated into the hi (τni , tk )
function allowing the substitution y= i ni hi (τni , tk ),
= exp(a(λ) · y +

c(hi (τni , tk )) + d(λ)).
i

ni

Putting the foregoing results together yields our main
theorem.
Theorem 2: For arrivals that are conditionally independent given λ for any value tk >max(τni ∈ Xj (k)) and any
ﬁxed data-set S(k), the transformation Xj (k)→Yj (k) that
maximizes Λ-information preservation and minimizes cost,
takes the form y= i ni hi (τni , tk ).
III. E XTENSIONS
These results can be extended to a Bayesian form and
extended to include dependence on other conditioning variables including dependence on the past sequence of ipi’s.
Upgrading the transformations within j for past ipi’s t(k−1) ,
the system of transformations are Λj (k) → Xj (k) →
S(k, t(k−1) ) → Yj (k, t(k−1) ) → Tj (k, t(k−1) ), where we
have left implicit the delayed feedback from Tj (·) to Sj (·)
and Yj (·) necessary for these localized memories to grow
with increasing k.

3

4

Because a sufﬁcient statistic is information equivalent to
the likelihood function and because the likelihood function plus a prior distribution is all that is needed for
Bayesian inference, another extension is easily implied
because the non-Bayesian deﬁnition of sufﬁcient statistic
qualiﬁes as a sufﬁcient statistic within the Bayesian deﬁnition (see Blackwell and Ramamoorthi, 1982); speciﬁcally,
let’s equip j and its recipients with a timer and a prior
distribution.
Bayesian Corollary: Internally, j can make a Bayesian
inference at the time of threshold-crossing and, externally,
a receiver of j’s time of threshold-crossing can perform a
Bayesian inference on λ when we equip j and the recipient with a suitable prior distribution, p(Λj (k)=λ|t(k−1) ),
a timer, and knowledge of the deterministic threshold
function, θ(t(k) ): Speciﬁcally, when Yj (k, t(k−1) )= θ(t(k) ),
for j:

[6] B. O. Koopman, On Distributions Admitting a Sufﬁcient Statistic,
Transactions of the American Mathematical Society, Vol. 39, No.
3., pp. 399-409, May 1936.
[7] T. Sagawa and M. Ueda, Minimal Energy Cost for Thermodynamic
Information Processing: Measurement and Information Erasure,
Physical Review Letters 102, 250602, 2009.
[8] S. Zaks, The Theory of Statistical Inference, John Wiley and Sons,
Inc., New York, 1971.
[9] A. M. Kagan, U. V. Linik, and C. R. Rao, Characterization Problems
in Mathematical Statistics, Wiley, 1973.
[10] E. Gutierriz and P. Muliere, Conjugate priors represent strong preexperimental assumptions, Scand. J. of Stat. v31, 235-246, 2004.

p(Λj (tk ) = λ|Yj ( t(k) ) = θ( t(k) ), t(k) ) ∝
p(Yj (t(k) ) = θ(t(k) )|Λj (tk ) = λ, t(k) )
· p(Λj (k) = λ|t(k−1) ),
and for a recipient of j:
p(Λj (tk ) = λ| t(k) ) ∝
p(tk |Λj (tk ) = λ, t(k−1) ) · p(Λj (k) = λ|t(k−1) ).
(Proportionality becomes equality when each r.h.s. is normalized by division with the appropriate integral).
Using a result of Gutierriz and Muliere (2004) and
our earlier reasoning about energy-efﬁciency, a further
corollary points out that a conjugate prior is necessary to
minimize costs.
The ﬁnal extension moves us from a simple NL element
to a complex NL element. In this case we deﬁne a complex
NL element as being constructed of multiple compartments
where each compartment is predicting a different latent
variable.
Multi-compartment corollary: Consider a NL element
made up of more than one compartment where, for each
compartment, there is a different scalar-valued latent random process driving the inputs to that compartment and
possibly a different sequence of transformations ending
with a scalar variate. Then, for each such compartment,
the information-preserving and cost-minimizing transformations encoding the transmitted scalar value of that
compartment must be a minimal sufﬁcient statistic formed
by summation of conditionally independent inputs.
R EFERENCES
[1] E.W. Barankin and A.P. Maitra, Generalization of the FisherDarrnois-Koopman-Pitman theorem on sufﬁcient statistics, Sankhya,
25, 217-244, 1963.
[2] T. Berger and W. B. Levy, A mathematical theory of energy efﬁcient
neural computation and communication, IEEE Trans. IT, vol. 56,
No. 2, pp. 852-874, February 2010.
[3] D. Blackwell and R. V. Ramamoorthi, A Bayes but Not Classically
Sufﬁcient Statistic, Ann. Statist. Volume 10, Number 3, 1025-1026,
1982.
[4] E. B. Dynkin, On sufﬁcient and necessary statistics for families of
probability distributions (Russian), DokI. Akad. N auk SSSR 75,
161-164, 1950.
[5] R. A. Fisher, On the Mathematical Foundations of Theoretical
Statistics, Phil. Trans. Roy. Soc. London A 22, 309-368, 1922.

4

