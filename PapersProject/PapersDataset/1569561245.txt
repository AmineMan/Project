Title:          C:/Documents and Settings/user/Desktop/I2R/Information Theory/An extremal inequality and the capacity region of degraded compound BC with multiple users/An extremal inequality vs 1.dvi
Creator:        dvips(k) 5.98 Copyright 2009 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 15:35:31 2012
ModDate:        Tue Jun 19 12:56:09 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      595 x 842 pts (A4)
File size:      300408 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569561245

An Extremal Inequality and the Capacity Region of
the Degraded MIMO Compound Gaussian
Broadcast Channel with Multiple Users
Hon Fah Chong and Ying-Chang Liang
1 Fusionopolis Way, #21-01 Connexis (South Tower),
Institute for Infocomm Research, Singapore 138632.
E-mail: chong.hon.fah@ieee.org, ycliang@i2r.a-star.edu.sg

Abstract—Weingarten et al. characterized the capacity region
of a two-user compound MIMO broadcast channel when the two
users exhibit a certain degradedness order. To show that Gaussian
inputs attain the capacity region, they proved a new extremal
inequality and made use of the channel enhancement technique.
In this paper, we prove a generalization of the extremal inequality
considered by Weingarten et al. We then apply the generalized
extremal inequality to characterize the capacity region of the N user compound MIMO Gaussian broadcast channel when the N
users exhibit a degradedness order similar to the two-user case.

where µ > 0, λ11 , λ22 ≥ 0, MS 0, and (S − K∗ ) MS = 0.
i
i
X
Then for every distribution x such that E xxT
S, we have
K2

K1
i1 =1

λ11 h x + z11 |W − µ
i
i

K1

≤

λ11
i
2

i1 =1

i2 =1

λ22 h x + z22 |W
i
i

log (2πe) K∗ + N11
X
i

K2

−µ

I. I NTRODUCTION
In [1], Weingarten et al. considered the two-user compound
MIMO broadcast channel (BC) with two users and two private
message sets. Users 1 and 2 have K1 and K2 possible
realizations, respectively, such that
1
yi1 = x + z11 ,
i

i1 = 1, 2, . . . , K1

2
yi2

i2 = 1, 2, . . . , K2

=x+

z22 ,
i

(1)

2
1
where x is a real input vector of size t × 1; yi1 and yi2 are
real output vectors of size t × 1 corresponding to realizations
i1 and i2 of user 1 and user 2, respectively; z11 and z22 are
i
i
real Gaussian noise vectors with zero mean and covariance
matrices N11 and N22 , respectively.
i
i
There is also a degradedness order between the noise
covariance matrices such that a covariance matrix N∗ exists
where

N11
i

N∗

N22 ,
i

∀i1 , i2 = 1.

(2)

Both receivers know the actual realization but the transmitter
does not. The messages must be decoded successfully regardless of the actual transmission. To characterize the capacity
region of this channel, Weingarten et al. proved a new extremal
inequality [1, Corollary 4]:
Lemma 1: Let 0 K∗
S satisfy the following:
X
K1

λ11
i
i1 =1

K∗
X

+

N11
i

−1

K2

λ22
i

=µ

K∗
X

+

N22
i

−1

i2

λ22
i
log (2πe) K∗ + N22
X
i
2
=1

where W is a random variable independent of z11 and z22 .
i
i
The above extremal inequality is interesting and important
in its own right. It has been used in the proof of the capacity
region of the reversely degraded Gaussian MIMO BC with a
common message and two private messages [2], the capacity
region of the Gaussian MIMO BC with a common message
and a conﬁdential message [3], the capacity region of some
classes of degraded compound multi-receiver wiretap channels
[4], the capacity region of the Gaussian MIMO BC with a
common message and two conﬁdential messages [5] as well
as the capacity-equivocation region of the Gaussian MIMO
wiretap channel [6].
In this paper, we prove a generalization of the above
extremal inequality, which we believe will ﬁnd applications
in the proof of other capacity theorems for multi-user networks. We follow a similar perturbation approach adopted by
Weingarten et al. to prove the above extremal inequality. For
the generalized extremal inequality, we require the additional
use of the data processing inequality. Finally, we apply the
generalized extremal inequality to characterize the capacity
region of the N -user compound MIMO BC, a generalization
of the two-user case.
In the N -user compound MIMO BC, user n ∈ {1, . . . , N }
has Kn possible realizations such that
n
yin = x + zn ,
in

+ MS

i2 =1

(3)

1

(4)

in = 1, 2, . . . , Kn

(5)

n
where x is a real input vector of size t×1; yin is a real output
vector of size t × 1 corresponding to realization in of user n;

and zn is a real Gaussian noise vector with zero mean and
in
covariance matrix Nn .
in
There is also a degradedness order between the noise
covariance matrices such that a covariance matrix N∗ , n =
n
1, 2, . . . , N − 1, exists where
Nn
in

N∗
n

n+1
Nin+1 , ∀in , in+1 .

(6)

Let (u1 , u2 , . . . , uN −1 ) be a random vector independent of
zL,n ∼ N 0, NL,n and zR,n ∼ N 0, NR,n and such that
rn
rn
ln
ln
uN −1 → uN −2 → . . . → u1 → x
forms a Markov chain. Then, for every distribution such that
E xxT
S, we have
L
N −1 Kn

T

Finally, we consider a matrix constraint E xx
S on
the input. As is well-known in the literature, this will allow us
to prove capacity results for total power as well as per antenna
power constraints.
Similar to the two-user case, an alternative view of this
channel is a BC with N groups of users and N -degraded
message sets (W1 , . . . , WN ). All the users in each group n ∈
{1, . . . , N }, are increasingly further away from the transmitter
and we wish to transmit the messages (Wn , . . . , WN ) to all
the users in group n. This is applicable in video broadcasting
where users closer to the transmitter receive a higher quality
version of the video broadcast while users further away receive
a lower quality version.
In the following section, we describe the main results of
this paper: the generalized extremal inequality and the capacity
region of the N -user compound MIMO BC. In Section III, we
go through some mathematical preliminaries before proving
the generalized extremal inequality. In Section IV, we make
use of the generalized extremal inequality to characterize the
capacity region of the N -user compound MIMO BC.
Remark 1: We note that our proof for the N -user case does
not need to make use of the channel enhancement technique. In
fact, one can also readily prove Lemma 1 without the channel
enhancement technique.
II. M AIN R ESULTS
The generalized extremal inequality is presented in the
following theorem and is proved in Section III.
Theorem 1: Let Qn 0, n ∈ {1, . . . , N − 1}, be positive
semi-deﬁnite matrices. For compactness of notation, we deﬁne

n=1 ln =1

λn h x + zL,n |un
ln
ln

R
N −1 Kn

−

n=1 rn =1

L
N −1 Kn

≤

n=1 ln =1

ρnn h x + zR,n |un
r
rn

λn
ln
log (2πe) Sn + NL,n
ln
2

R
N −1 Kn

−

n=1 rn

ρnn
r
log (2πe) Sn + NR,n
rn
2
=1

.

(9)

Remark 2: There is a slight change in notation in Theorem
1 as the extremal inequality is more general than that required
to prove the capacity region of the N -user compound MIMO
BC. We may set NL,n = Nn and NR,n = Nn+1 to prove the
rn
in
in+1
ln
capacity region of the N -user case. However, since extremal
inequalities are often useful in proving capacity theorems
in multi-user networks, we gave the most general extension
we could. We immediately recover Lemma 1 from Theorem
1. We also recover the extremal inequalities in [7, Lem. 5
and Lem. 6], which were necessary to prove the capacity
region of the Gaussian MIMO three-receiver BC with twodegraded message sets. Theorem 1 may also be used to prove,
without the need for channel enhancement, the capacity region
of the N -user degraded Gaussian MIMO BC with private
messages as well as the non-degraded case using the channel
enhancement technique. The proofs of these follow along the
lines for the proof of the N -user compound MIMO BC in this
paper and hence, are omitted due to space limitations.

n

Qi . Let us also suppose that SN −1

Sn

S, and

The capacity region of the N -user compound MIMO BC
is stated in the following theorem and the proof is given in
Section IV.
Theorem 2: The capacity region of the channel given in (5)
and satisfying (6), CDC (S), is given by the set of rate tuples
(R1 , . . . , RN ) such that

i=1
L
Kn

λn Sn + NL,n
ln
ln

−1

+ Mn =

ln =1
R
Kn

ρnn Sn + NR,n
rn
r
rn =1

where λn , ρnn ≥ 0, Mn
r
ln

−1

+ Mn+1 , n = 1, . . . , N − 1 (7)
0, NL,n , NR,n ≻ 0 and
rn
ln

Mn Qn = 0,
(S − SN −1 ) MN = 0.

NL,n
ln

T∗
n

NR,n ,
rn

min

in =1,...,Kn

Sn + Nn
1
in
,
log
2
Sn−1 + Nn
in

n = 1, . . . , N (10)

n

Qi and SN

where we deﬁne Sn

S, for some Qn

0,

i=1

n = 1, . . . , N − 1

Furthermore, there exists positive deﬁnite matrices
n = 1, . . . , N − 1, such that

Rn ≤

(8)
T∗
n

≻ 0,

∀ln = 1, rn = 1.

2

n ∈ {1, . . . , N − 1}, and SN −1

S.

III. P ROOF OF THE GENERALIZED EXTREMAL INEQUALITY
As we shall rely heavily on Fisher information matrices
and Fisher information inequalities to prove the generalized
extremal inequality, we provide some deﬁnitions and auxiliary
results in this section.

A. Mathematical Preliminaries
We ﬁrst begin with the following deﬁnition for the conditional Fisher information matrix:
Deﬁnition 1: Let (u, y) be a pair of jointly distributed
random vectors where, for each u ∈ U , the conditional
probability density function fy|u (y|u) is differentiable. The
conditional Fisher information matrix of y given u is then
deﬁned as
J (y|u) = Eu [J (y|u = u)]
▽ log fy|u (y|u)

= Eu Ey|u

▽ log fy|u (y|u)

T

.
(11)

Next, we need the following conditional form of the CramerRao inequality from [8, Lem. 13].
Lemma 2: Let (u, y) be a pair of jointly distributed random
vectors. Let the conditional covariance matrix of y given u be
Cov (y|u) ≻ 0, then
J (y|u)

Cov (y|u)

−1

.

(12)

As our proof relies on a perturbation approach, we also
need the following Fisher information inequality result from
[9, Lem. 3]:
Lemma 3: Let (x, y) be a pair of independent random
vectors, of length t, where the probability density functions,
fx (x) and fy (y), are differentiable. Then
J (x + y)

T

AJ (x) A + (I − A) J (y) (I − A)

T

(13)

for any matrix A of size t × t.
The conditional form of the above Fisher information inequality can be found in [8, Lem. 15].
Finally, to prove the generalized extremal inequality, we
also need the following inequality on the conditional Fisher
information matrix. This may be proved by the data processing
inequality (see [7, Appen. I]).
Lemma 4: For any positive semi-deﬁnite matrix K
0,
we have
tr [KJ (x + z|v)] ≥ tr [KJ (x + z|u)]

We consider the following function:
L
N −1 Kn

N −1

−

Sn + NL,n J xn,γ + zL,n |un − I
ln
ln

λn tr
ln
n=1 ln =1
R
N −1 Kn

−

Sn + NR,n J xn,γ + zR,n |un − I .
rn
rn

ρnn tr
r
n=1 rn =1

(16)
Next, we lower bound the summands of the ﬁrst sum in the
above equation. By assigning
A = (Sn + T∗ )
n

−1

Sn + NL,n ,
ln

˜
x = xn,γ +zL,n and y = zn ∼ N 0, T∗ − NL,n in Lemma
n
ln
ln
3 (its conditional form), and with some manipulations, we
obtain
tr

Sn + NL,n J xn,γ + zL,n |un − I
ln
ln
(Sn + T∗ ) J (xn,γ + z∗ |un ) (Sn + T∗ )
n
n
n

≥ tr

− (Sn + T∗ )
n

Sn + NL,n
ln

−1

.

A = Sn + NR,n
rn

−1

(Sn + T∗ ) ,
n

Sn + NR,n J xn,γ + zR,n |un − I
rn
rn
(Sn + T∗ ) J (xn,γ + z∗ |un ) (Sn + T∗ )
n
n
n
− (Sn + T∗ )
n

(15)

Sn + NR,n
rn

−1

.

Finally, by combining both bounds, we obtain

where
xn,γ =

√
1 − γx + γxG ,
n

and

n = 1, . . . , N − 1

′

∼N

0, Sn =

(a) N −1

2 (1 − γ) g (γ) ≥

n

xG
n

(17)

where z∗ ∼ N (0, T∗ ).
n
n
Similarly, by assigning

≤ tr

R
Kn

n=1 rn =1

L
N −1 Kn

tr

λn h xn,γ + zL,n |un
ln
ln
ρnn h xn,γ + zR,n |un
r
rn

′

2 (1 − γ) g (γ) =

˜
x = xn,γ + z∗ , where z∗ ∼ N (0, T∗ ) and y = zn ∼
n
n
n
N 0, NR,n − T∗ in Lemma 3 (its conditional form), and
n
rn
with some manipulations, we obtain

B. Main details of proof

n=1 ln =1

G
where wn ∼ N (0, Qn ) is independent of xn−1,γ .
We wish to show that for any distribution uN −1 → . . . →
′
u1 → x satisfying E xxT
S, we must have g (γ) ≥ 0,
∀γ ∈ [ 0, 1 ).
Using the vector form of deBruijn’s identity (see [10, Prop.
7]), and the fact that we may exchange the order of integral
and derivative (see [7, Appen. I]), we obtain

(14)

where u → v → x + z forms a Markov chain.

g (γ) =

is independent of (u1 , . . . , uN −1 , x). We note that xn,γ can
also be expressed as follows:
√ G
xn,γ = xn−1,γ + γwn

Qi

[J (xn,γ +

i=1

3

n=1

tr (Sn + T∗ ) ×
n

z∗ |un ) (Sn
n

+ T∗ ) − I] (Mn+1 − Mn )
n

(18)

(b)

∗
≤ tr Sn−1 + T∗
n−1 J xn−1,γ + zn−1 |un−1 ×

= −tr {T∗ [J (x1,γ + z∗ |u1 ) T∗ − I] M1 }
1
1
1

−
+

−

Sn−1 + T∗
n−1 Mn .

SN −1 + T∗ −1 J xN −1,γ + z∗ −1 |uN −1 ×
N
N

+ tr

SN −1 + T∗ −1 MN
N
tr {(SN −1 + T∗ ) MN }
N
N −1
∗
tr Sn−1 + T∗
n−1 J xn−1,γ + zn−1 |un−1
n=2
Sn−1 + T∗
n−1 Mn
N −1
tr (Sn−1 + T∗ ) J (xn,γ + z∗ |un ) ×
n
n
n=2
(Sn−1 + T∗ ) Mn
n

This follows from Lemma (4). To upper bound the second
√ G
ˆn−1
ˆn−1 =
γwn + z∗
J z∗
term in (22), we note that J
−1
∗
∗
. Hence, we have
Tn − Tn−1
√ G
∗
∗
ˆn−1 Tn − T∗
γwn + z∗
Tn − Tn−1 J
n−1

×

T∗ − T∗
n
n−1 .

N −1

tr

T∗
n

n=2

−

T∗
n−1

Mn .

−tr {T∗
1

n=2

[J (x1,γ +

z∗ |u1 ) T∗
1
1

− I] M1 } ≥ 0.

(20)

To bound the second term and third terms in (19), we note that
−1
(this follows
J xN −1,γ + z∗ −1 |uN −1
S + T∗ −1
N
N
from Lemma 2 and the fact that the conditional covariance
matrix is less than the unconditional covariance matrix).
Hence, we have
SN −1 + T∗ −1
N

J xN −1,γ + z∗ −1 |uN −1 ×
N

SN −1 + T∗ −1
N
= SN −1 + T∗ −1
N

∗
Sn−1 + T∗
n−1 J xn−1,γ + zn−1 |un−1 ×

tr

(19)

where (a) follows from applying the bounds (17)-(18) as
well as condition (7) of Theorem 1 and (b) follows from
expanding the terms appropriately and using the fact that
Sn Mn = Sn−1 Mn , n = 1, 2, . . . , N − 1.
To bound the ﬁrst term, we note that J (x1,γ + z∗ |u1 )
1
J (z∗ ) = T∗−1 . Hence, we have
1
1

SN −1 + T∗ −1 − I
N
S + T∗ −1
N

S + T∗ −1
N

−1

−1

SN −1 + T∗ −1 − I
N

(SN −1 − S) .

(21)

Hence, the second and third terms are lower bounded by zero
since (S − SN −1 ) MN = 0.
Next, we give an upper bound for the summands in the
second summation term in (19). Noting that xn,γ + z∗ and
n
√ G
ˆn
xn−1,γ + γwn +z∗ +ˆ∗ (where z∗ ∼ N 0, T∗ − T∗
n
n−1 zn
n−1 )
have the same distribution, we put
A = (Sn−1 + T∗ )
n

−1

Sn−1 + T∗
n−1

−1

Tn − T∗
and I − A = (Sn−1 + T∗ )
n
n−1 into Lemma 3
(its conditional form), we obtain,
∗
Sn−1 + T∗
Sn−1 + T∗
n−1
n−1 J xn−1,γ + zn−1 |un
√ G
ˆn−1 Tn − T∗
J
+ Tn − T∗
.
γwn + z∗
n−1
n−1
(22)

To upper bound the ﬁrst term above, we note that
Sn−1 + T∗
n−1
Sn−1 + T∗
n−1

J xn−1,γ +
Mn

z∗ |un
n−1

Sn−1 + T∗
n−1 Mn
N −1

−

n=2

tr (Sn−1 + T∗ ) J (xn,γ + z∗ |un ) ×
n
n
(Sn−1 + T∗ ) Mn
n

N −1

tr

+
n=2

T∗ − T∗
n
n−1 Mn ≥ 0.

′

IV. P ROOF OF THE CAPACITY REGION OF THE N - USER
COMPOUND MIMO BC
In the ﬁnal section, we complete the proof of the capacity
region of the N -user compound MIMO BC. We ﬁrst give
a single letter characterization of the discrete case in the
following lemma:
Lemma 5: Consider a discrete memoryless compound BC
with input X and outputs Yin , in ∈ {1, 2, . . . , Kn }, for the nth
n
user, n = 1, 2, . . . , N . There are also N − 1 auxiliary outputs
∗
Yn , n = 1, 2, . . . , N − 1. All outputs are deﬁned by their
∗
conditional probability mass functions: PYin |X and PYn |X .
n
Furthermore, assume that for each in ∈ {1, 2, . . . , Kn }, n =
1, 2, . . . , N − 1, there exists a joint conditional probability
∗
distribution with marginal distributions PYin |X and PYn |X and
n
where the following Markov chain holds:
∗
X → Yi1 → Y1∗ → Yi2 → Y2∗ . . . → YN −1 → YiN .
N
2
1

The capacity region of this channel is given by
min

in =1,...,Kn

where UN
where

I Un−1 ; Yin |Un ,
n

∅ and U0

n = 1, . . . , N

(26)

X and for some (X, U1 , . . . , UN −1 )

UN −1 → . . . → U1 → X →Yin ,
n

UN −1 → . . . → U1 → X

×

(25)

Therefore, g (γ) is non-negative for γ ∈ [ 0, 1 ) and the proof
is complete.

Rn ≤

(Sn−1 + T∗ ) J (xn,γ + z∗ |un ) (Sn−1 + T∗ )
n
n
n

tr

(24)

Finally, using the bounds for the summands in the second
summation of (19), given by (22)-(24), we obtain for the three
summations in (19)

N −1

+

(23)

∗
→Yn ,

∀in , n

n = 1, . . . , N − 1

form Markov chains.
Proof: The proof is omitted due to space constraints.

4

We restrict our attention to zero-mean Gaussian inputs and
consider the following optimization problem (P1-G):

KN

=u

N −1

1 →x

Cov(x) S

N

N
N
βiN h yiN
iN =1

N −1 Kn

max

µn R N

Q1 ,...,QN −1 0
(R1 ,...,RN ) n=1

+
n=1 in =1
n

s.t. Rn ≤

max
→...u

min

in =1,...,Kn

1
log
2

i=1
n−1

N
n
n
βin h yin |un −

Kn

n=2 in =1

(32)

Qi + Nn
in

i=1

n

, n = 1, . . . , N

K1

−

Qi + Nn
in

1
βi1 h

z11
i

i1 =1

(b) N

≤

Kn

n=1 in

n
βin
log
2
=1

N −1

S.

(27)

n=1

n
(c)

where µn ≥ 0, n = 1, . . . , N .
The necessary KKT conditions of (P1-G) are given by:
∗
Lemma 6: The optimal solution to (P1-G), Ri , i ∈
∗
{1, . . . , N }, Qn , n ∈ {1, . . . , N − 1}, must satisfy
Kn

−1

n

Q∗
i

n
βin
in =1

+

+ M∗ =
n

Nn
in

i=1

Kn+1

−1

n

Q∗
i

n+1
βin+1
in+1 =1

+

Nn+1
in+1

+ M∗ ,
n+1

M∗ Q∗ = 0,
n n
N −1

S−

(28)

i=1

where in = 1, . . . , Kn , n = 1, . . . , N − 1, M∗
n

Q∗
n

0, and

M∗ = 0.
N

(29)

n=1

Kn
n
βin = µn ,

n = 1, . . . , N

(30)

in =1
n



1
n
βin  log
2

i=1
n−1
i=1

Q∗
i

+

Nn
in

Q∗ + Nn
i
in




i = 1, . . . , Kn ,
∗
− Rn  = 0, n

n = 1, . . . , N.

(31)

Proof: The result follows directly from the Karush-KuhnTucker (KKT) conditions. However, we will not derive them
due to space constraints. Since the optimization problem (P1G) is non-convex, a set of constraint qualiﬁcations should be
checked to make sure that the KKT conditions hold. We may
check that constraint qualiﬁcation CQ5a in [11] holds here.
Finally, since CDC (S) may be deﬁned by supporting hyperplanes with µn ≥ 0, we note that
N

max

(R1 ,...,RN )∈CDC (S)

max

µn R n
n=1
N Kn

uN −1 →...u1 →x
n=1 in =1
Cov(x) S

N

=

µn
n=1

min

in =1,...,Kn

1
log
2

i=1
n−1
i=1

Q∗ + Nn
i
in
(33)
Q∗ + Nn
i
in

Q∗ + Nn
i
in
(34)
Q∗
i

+

Nn
in

where (a) follows from (30) and (26); (b) from the fact that
the ﬁrst term in (32) is maximized by Gaussian inputs with
Cov (x) = S. Furthermore, since the conditions of Lemma 6
satisfy the conditions of Theorem 1, the second and third terms
are upper bounded by the optimal Gaussian inputs satisfying
the conditions of Lemma 6. Hence, we may upper bound (32)
by (33); and (c) follows from (30) and (31).
Since the upper bound is achievable with zero-mean Gaussian inputs, this concludes the proof of Theorem 2.
R EFERENCES

n = 1, . . . , N − 1

n
and for some βin ≥ 0 where

≤

i=1
n−1
i=1

Qn

(a)

n
n
βin h yin |un−1

n
n
βin I un−1 ; yin |un

5

[1] H. Weingarten, T. Liu, S. Shamai, Y. Steinberg and P. Viswanath,
“The capacity region of the degraded multiple-input multiple-output
compound broadcast channel,” IEEE Trans. Inf. Theory, vol. 55, no. 11,
pp. 5011–5023, Nov. 2009.
[2] J. Chen and C. Tian, “Capacity region of reversely degraded Gaussian
MIMO broadcast channel,” in Proc. IEEE Global Telecomms. Conf.
2009, Honolulu, Hawaii, 30 Nov.-4 Dec. 2009, pp. 1–6.
[3] T. L. H. D. Ly and Y.-B. Liang, “Multiple-Input Multiple-Output
Gaussian broadcast channels with common and conﬁdential messages,”
IEEE Trans. Inf. Theory, vol. 56, no. 11, pp. 5477–5487, Nov. 2010.
[4] E. Ekrem and S. Ulukus, “Degraded compound multi-receiver wiretap
channels,” IEEE Trans. Inf. Theory, submitted for publication. [Online].
Available: http://arxiv.org/pdf/0910.3033v1.pdf
[5] ——, “Capacity region of Gaussian MIMO broadcast channels with
common and conﬁdential messages,” IEEE Trans. Inf. Theory, submitted
for publication. [Online]. Available: http://arxiv.org/pdf/1002.5026v1.pdf
[6] ——, “Capacity-equivocation region of the Gaussian MIMO wiretap
channel,” IEEE Trans. Inf. Theory, submitted for publication. [Online].
Available: http://arxiv.org/pdf/1005.0419v1.pdf
[7] H. F. Chong and Y.-C. Liang, “The capacity region of the class
of Gaussian MIMO three-receiver multilevel broadcast channels with
degraded message sets,” in Proc. 48th Annual Allerton Conf. Commun.
Control Comput., UIUC, Illinois, USA, 29 Sept.-1 Oct. 2010 2010, pp.
94–101.
[8] E. Ekrem and S. Ulukus, “The secrecy capacity region of the Gaussian
MIMO multi-receiver wiretap channel,” IEEE Trans. Inf. Theory, vol. 57,
no. 4, pp. 2083 – 2114, April 2011.
[9] T. Liu and P. Viswanath, “An extremal inequality motivated by multiterminal information-theoretic problems,” IEEE Trans. Inf. Theory, vol. 53,
no. 5, pp. 1839–1851, May 2007.
[10] O. Rioul, “Information theoretic proofs of entropy power inequalities,”
IEEE Trans. Inf. Theory, vol. 57, no. 1, pp. 33 – 55, Jan. 2011.
[11] D. P. Bertsekas and A. E. Ozdaglar, “Pseudonormality and a lagrange
multiplier theory for constrained optimization1,” J. Optimiz. Theory
App., vol. 114, no. 2, pp. 287–343, Aug. 2002.

