Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 19:45:14 2012
ModDate:        Tue Jun 19 12:55:10 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      442423 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566799

Recovery of Sparse 1-D Signals from the
Magnitudes of their Fourier Transform
Kishore Jaganathan

Samet Oymak

Babak Hassibi

Department of Electrical Engineering, Caltech
Pasadena, CA 91125
In this work, we prove that signals can be recovered from
their autocorrelation with arbitrarily high probability under
certain conditions. We prove this using dimension counting,
based on the ideas used in [10], [13] for multidimensional
signals. We also propose two non-iterative recovery algorithms
to extract sparse signals from their autocorrelation. Note that
the phase recovery problem is inherently non-convex, and
relaxations similar to the ones used in [12], [14] are used
to develop a convex-optimization based framework.
The paper is organized as follows. In Section 2, we discuss
some properties of autocorrelation and spectral factorization
which we use for signal extraction. In Section 3, we prove
that signals can be recovered from their autocorrelation with
very high probability under certain conditions. Non-iterative
recovery algorithms are proposed for extraction of the signal
from their autocorrelation in Section 4. Section 5 presents the
simulation results and concludes the paper.

Abstract—The problem of signal recovery from the autocorrelation, or equivalently, the magnitudes of the Fourier transform,
is of paramount importance in various ﬁelds of engineering. In
this work, for one-dimensional signals, we give conditions, which
when satisﬁed, allow unique recovery from the autocorrelation
with very high probability. In particular, for sparse signals, we
develop two non-iterative recovery algorithms. One of them is
based on combinatorial analysis, which we prove can recover
signals upto sparsity o(n1/3 ) with very high probability, and the
other is developed using a convex optimization based framework,
which numerical simulations suggest can recover signals upto
sparsity o(n1/2 ) with very high probability.
1
Index Terms—Autocorrelation, Phase Retrieval, Convex Optimization, Sparse Spectral Factorization

I. I NTRODUCTION
Signal extraction from the autocorrelation, or equivalently,
from the magnitude of the Fourier Transform is known as
phase retrieval. This problem fundamentally arises in many
practical systems such as X-ray crystallography [1], astronomical imaging [2], channel estimation, speech recognition [3]
etc, and has attracted a considerable amount of attention from
researchers over the last few decades [4]. Various algorithms
have been proposed to retrieve phase information [5], [6] and
a comprehensive survey of them can be found in [7], [8].
For one-dimensional signals, since the mapping from signals
to autocorrelation is not one-to-one, unique recovery is not
possible in general. For any given Fourier transform magnitude, every possible phase corresponds to a different signal.
Hence, additional prior information on the signal is required
to limit the number of valid phase combinations. [9] uses
multiple structured illuminations, in which several patterns
using different masks are collected to guarantee uniqueness.
We assume that the signal is sparse, i.e., the number of
non-zero entries in the signal is much less compared to the
length of the signal. This constraint greatly limits the number
of possible phase combinations, and research has been done
recently to exploit this feature [11], [12]. In many applications
of phase retrieval, the signals encountered are naturally sparse.
For example, astronomical imaging deals with the locations of
the stars in the sky, electron microscopy deals with the density
of electrons, and so on.

II. T HEORY
Let x = (x0 , x1 , ....xn−1 ) be a real-valued signal of length
n. Its autocorrelation, denoted by a = (a0 , a1 , ....an−1 ), is
deﬁned as
def

˜
xj xj+i = (x x)i

ai =

(1)

j

˜
where x is the time-reversed version of x. Note that cyclic
indexing scheme is used in this deﬁnition. Rewriting (1) in
the z-domain, we get
A(z) = X(z)X(z −1 )

(2)

where A(z) and X(z) are the z-transforms of a and x
respectively. Since x is real valued, X(z) is a polynomial in
z with real coefﬁcients and hence its zeros occur in conjugate
pairs. Also, since A(z) = A(z −1 ), if z0 is a zero of A(z),
−1
then z0 is also a zero. Hence, the zeros of A(z) appear in
−1 −
quadruples of the form (z0 , z0 , z0 , z0 ).
The extraction of x from a, or equivalently X(z) from A(z),
is known as spectral factorization and deals with the distribution of these quadruples between X(z) and X(z −1 ). For every
−1 −
quadruple (z0 , z0 , z0 , z0 ), we can either assign (z0 , z0 ) to
−1 −
−1 −
X(z) and (z0 ,z0 ) to X(z −1 ), or assign (z0 ,z0 ) to X(z)
−1
and (z0 , z0 ) to X(z ). The total number of different valid
factorizations hence is exponential in the number of such
quadruples.

1 This work was supported in part by the National Science Foundation
under grants CCF-0729203, CNS-0932428 and CCF-1018927, by the Ofﬁce
of Naval Research under the MURI grant N00014-08-1-0747, and by Caltech’s
Lee Center for Advanced Networking.

1

(i) The set Fgh is a manifold of dimension kg + kh − 1.
(ii) Igh has sparsity kgh ≥ kg + kh − 1, with equality iff g
and h have uniform support.
(iii) If f = g ∗ h, where I, the support of f , is a subset of
Igh with sparsity k. The set of such f is a manifold of
dimension kg + kh − 1 − γ, where γ = kgh − k.

Lemma II.1. If two distinct ﬁnite-length real-valued signals f1
and f2 have the same autocorrelation, then there exists ﬁnitelength real-valued signals g and h such that
˜
f2 = g h

f1 = g h

(3)

˜
where h is the time-reversed version of h.
Proof:
Let F1 (z), F2 (z), G(z) and H(z) be the z-transforms of
the signals f1 , f2 , g and h respectively. Since f1 and f2 have
the same autocorrelation, (2) gives us
A(z) = F1 (z)F1 (z

−1

) = F2 (z)F2 (z

−1

)

Proof: We refer the readers to [10] for the proof of (i) and
(iii). (ii) directly follows from the properties of convolution.
Lemma III.3. Suppose f = g ∗ h, with f having non-uniform
support where as g and h have uniform support, also has the
additional property that f has non-uniform support. Then, the
set of such signals is a manifold of dimension strictly lesser
than kg + kh − 1 − γ.

(4)

where A(z) is the z-transform of the autocorrelation of f1 and
−1 −
f2 . For every quadruple (z0 ,z0 ,z0 ,z0 ) which are zeros of
A(z), (z0 , z0 ) has to be assigned to F1 (z) or F1 (z −1 ), and
F2 (z) or F2 (z −1 ). Let P1 (z), P2 (z) and P3 (z) be the polynomials constructed from such conjugate pairs of zeros which
are assigned to (F1 (z), F2 (z)) and (F1 (z), F2 (z −1 )) and
(F1 (z −1 ), F2 (z)) respectively. Note that P2 (z) = P3 (z −1 ).
We have
F1 (z) = P1 (z)P2 (z)

(5)

F2 (z) = P1 (z)P3 (z) = P1 (z)P2 (z −1 )

Proof: The idea of the proof is similar to [10], based on
dimension counting. We saw in Lemma III.2 that the set of
signals f which can be represented as g ∗ h with sparsity k
can be written as a manifold of dimension kg + kh − 1 − γ.
The new set of constraints introduced by terms in f being 0
result in a further reduction in dimension. Hence the set of
such signals belong to a manifold of dimension strictly lesser
than kg + kh − 1 − γ.

(6)

and hence F1 (z) and F2 (z) can be written as
F1 (z) = G(z)H(z)

F2 (z) = G(z)H(z

−1

)

Theorem III.1 (Main Theorem). Signals can be uniquely
recovered from their autocorrelation, or equivalently, from the
magnitudes of their Fourier Transforms almost surely iff they
have non-uniform support.

(7)

where G(z) = P1 (z) and H(z) = P2 (z), or equivalently
f1 = g h

˜
f2 = g h

(8)

Proof: Let Fk be the set of all signals f with non-uniform
support of sparsity k which have another signal f with nonuniform support and same autocorrelation. Note that Fk is the
set of signals of sparsity k which cannot be recovered uniquely
from their autocorrelation. Lemma II.1 showed the existence
of signals g and h such that

˜
in the time domain as the z-transform of h is H(z −1 ).
III. U NIQUE R ECOVERY
In this section, we establish the fact that within the class
of signals with non-uniform support (deﬁned later), there is a
one-to-one mapping between signals and their autocorrelation
almost surely.

f =g∗h

˜
f =g∗h

(9)

From Lemma III.2, we note that the dimension of Fk is less
than or equal to kg + kh − 1 − γ
Case I: kgh > kg + kh − 1: This is the case if g and h
do not have uniform support. In this case, the dimension of
Fk is strictly less than k. Hence from Lemma III.1, we see
that Fk is a set of measure zero in Fk and signals with nonuniform support can be recovered from their autocorrelation
almost surely.
Case II: kgh = kg + kh − 1: In this case, g and h have
uniform support. If f and f have non-uniform support, from
Lemma III.3, we see that the dimension of Fk is strictly lesser
than k = kg +kh −1−γ, and hence can be uniquely recovered
from their autocorrelation almost surely.
Suppose f or f have uniform support, there will be no
additional reduction in dimension. This case is equivalent
to recovering a one-dimensional signal uniquely with no
additional constraints, which is almost surely not possible.

Lemma III.1. If f : A → B is a map from A to B, where A is
a manifold of dimension da and B is a manifold of dimension
db , then the image of f is measure zero in B if da < db .
Note that any signal of length n can be represented as a
vector in Rn . Let f be a ﬁnite-length real-valued signal of
length n. Let I represent its support, deﬁned as the set of
locations where the f can have non-zero entries. We say that
a signal f has uniform support if the indices of the elements
belonging to the support are periodic, i.e., in an arithmetic
progression. The size of the set I denotes the sparsity of f .
Let Fk denote the set of signals with sparsity k. Observe that
Fk is a manifold of dimension k.
Lemma III.2. Suppose g and h are ﬁnite-length real-valued
signals with support set Ig and Ih of sparsity kg and kh
respectively. If Fgh denotes the set of signals g ∗ h, and Igh
its support. Then

2

i=1 j=k

P (dk−1,k − d12 = d1j ))

(11)

j=k

Note that the dij ’s for i = 1, j = k are independent of d12 and
dk−1,k . Hence Lemma IV.2 can be applied and each term in the
s
ﬁrst summation can be upper bounded by n . Since dk−1,k <
dik and d12 > 0, all the terms in the second summation are
zero. The terms in the third summation can be equivalently
written as P (dk−1,k − 2d12 = d2j ), and Lemma IV.2 can be
s
used to upper bound every term by n . Since d1k is the largest
sum, we need not consider it in the summation. Hence, we get
P (dk−1,k −d12 ∈ A) ≤ k 2

s
s3
s3
≤ (1+ )2 ≤ (1+ )
(12)
n
n
n

Lemma IV.4. d12 and dk−1,k can be recovered from the
autocorrelation with very high probability if s = o(n1/3 ).
Proof: The ﬁrst and second highest terms in A are
d1k and d2k respectively since d12 ≤ dk−1,k . Note that
d1k − d2k = d12 , hence d12 can be recovered from the
autocorrelation. The only terms that can be higher than d1,k−1
in A are {d3k , d4k , .....dk−1,k }. Note that d2k − dik = d2i ,
which belongs to A for all i = {3, .....k−1}. So if d2k −d1,k−1
doesn’t belong to A, we can recover d1,k−1 by considering
the highest term which when subtracted from d2k produces a
value which doesn’t belong to A. The probability of failure
can hence be written as P (dk−1,k − d12 ∈ A) which goes
1
to zero if s = o(n 3 ), as seen in Lemma IV.3. Hence both
d12 and dk−1,k can be recovered with very high probability if
s = o(n1/3 ).
With the knowledge of d12 and dk−1,k , we can construct
the sets A1 and A2 . Consider the intersection of A and A1 .
All entries of the form d2i for i = {3, 4, ...k} will survive
trivially for any signal. Similarly, all entries of the form di,k−1
for i = {1....k − 2} will survive the intersection of A and A2
for any signal. If we subtract the survivors of the intersection
of A and A2 from d2,k−1 , we get d2i for i = {3, 4, ...k − 1}.
Hence the elements d2i for i = {3, 4, ...k − 1} will survive
(A ∩ A1 ) ∩ (d2,k−1 − (A ∩ A2 )).

Algorithm 1
Input: The autocorrelation a of the signal.
Output: The sparse signal x which has autocorrelation a
• Extract d12 and dk−1,k from A (Lemma IV.4). Calculate
the sets A1 and A2 .
• Perform (A ∩ A1 ) ∩ (d2,k−1 − (A ∩ A2 )) and identify the
support u of x (Lemma IV.5)
• Construct the graph G (Lemma IV.6) using u, identify
an odd cycle and a path connecting all the vertices and
extract x.
Lemma IV.1. The sparsity k of the signal satisﬁes (1 − )s ≤
k ≤ (1 + )s with very high probability for any > 0, n >
n( ).

Lemma IV.5. No other dij will survive (A ∩ A1 ) ∩ (d2,k−1 −
(A ∩ A2 )) and hence the support can be recovered with very
high probability if s = o(n1/3 )

Proof: Use Chebyshev’s inequality.
Lemma IV.2. For three independent random variables X1 ,
X2 and X3 where X1 and X2 are geometric random variables
s
s
with parameter n , P (X1 − pX2 = qX3 ) ≤ n if s = nα , α <
1 for n > n( ), where p and q are integers.

Proof: Suppose you choose dij such that i and j are
picked at random. The probability that dij is a particular value
1
can be upper bounded by n . For a non-trivial dij in A to
survive A A1 , dij + d12 has to be in A. Similarly, d2k − dij
and d2,k−1 − dij has to be in A for it to survive d2k − A A2 .
Using union bounds, we see that the probability of survival of
some other dij goes to 0 when s = o(n1/3 ). Note that we
have information about dk−1,k upto s = o(n1/3 ).
If no other elements survive, from d2i for i = {3, 4, ...k−1},
we can extract di,i+1 for i = {3, 4, ...k − 2} and since we

Proof: Refer Appendix.
3

Lemma IV.3. P (dk−1,k −d12 ∈ A) ≤ (1+ ) s for any > 0,
n
n > n( ).
Proof: Using union bound, we obtain
P (dk−1,k − d12 = dij ) (10)
i

P (dk−1,k −d12 = dik )
i=1

+

A. Algorithm 1
Algorithm 1 is based on combinatorial analysis. We propose
a method to recover the support of the signal from the support
of the autocorrelation, and prove that recovery is possible with
very high probability if the sparsity of the signal is o(n1/3 ).
Using this support knowledge, we show that signals can be
recovered from the autocorrelation with very high probability.
Suppose x is a signal of length n such that each element
s
in x belongs to the support with a probability n , where
α
s = n , α ≤ 1, independent of each other. Let a denote its
autocorrelation, k denote its sparsity and D = {d1 , d2 , .....dk }
be the set of indices of the elements belonging to the support.
Also, let dij be deﬁned as |di − dj | for (i, j) = {1, 2, ....k}.
If A is the set of indices of elements belonging to the support
of the autocorrelation, then A = { i,j dij }. Note that di,i+1
s
is a geometric random variable with parameter n . Without
loss of generality, let us assume dk−1,k ≥ d12 , otherwise we
could just ﬂip the signal and consider the ﬂipped signal. Deﬁne
A1 = {dij − d12 |dij ∈ A} and A2 = {dij − dk−1,k |dij ∈ A}.
The algorithm for signal recovery is described below. In
what follows, we give a sequence of lemmas to justify various
steps of the algorithm.

P (dk−1,k − d12 ∈ A) ≤

P (dk−1,k −d12 = dij )+

=

IV. R ECOVERY A LGORITHMS
In this section, we develop two non-iterative recovery
algorithms for the extraction of sparse signals from their
autocorrelation.

j

3

ai = 0 ⇒ uj ui+j = 0 for some j

already know d12 and dk−1,k , we have the support of the
signal.

where u is the binary support vector. This is clearly nonconvex as the constraints are non-convex and u is binary.
Deﬁne S = uuT , which is allowed to be positive semideﬁnite,
as it is the smallest convex set containing all rank one matrices.
The entries of S are allowed to be in [0, 1], which is the best
convex relaxation for binary variables. The trace of S is given
by i u2 = i ui = k, the sparsity of the signal. Also, note
i
that i Sij = i ui uj = uj i ui = kuj = ku2 = kSjj and
j
similarly j Sij = kSii . Since ﬂipped version of the support
also satisﬁes all the constraints, a random matrix V is used to
bias the cost. The support estimation problem becomes

Suppose we have the support of the signal, D =
{d1 , d2 , .....dk } being the indices of the elements belonging
to the support. Deﬁne a pair (di , dj ) as a good pair if they are
the only pair separated by |di − dj |. Note that for such a pair,
a|di −dj | = xdi xdj
Lemma IV.6. Consider a graph G with k vertices, each vertex
representing an element of the support. Draw a weighted edge
between every good pair, the weight being the value of the
corresponding autocorrelation. If the graph G has an odd
cycle and is connected, then the signal can be extracted from
the autocorrelation upto a global sign.

minimize

trace(VS)

subject to

Consider an odd cycle with 2r − 1 vertices i1 , i2 , ...i2r−1 .
The term (xi1 i2 xi3 i4 ....xi2r−1 i1 )/(xi2 i3 ...xi2r−2 i2r−1 ) gives
x21 , from which xi1 can be extracted upto a sign, and from
i
it the other terms in the odd cycle can be extracted using
the weight corresponding to the edges. Since the graph is
connected, all the other terms can be calculated.

trace(S) = k

S

Sij = kSjj
i

0
Sij = kSii

j

Si,i+k > 0 iff ak = 0
i

0 ≤ Sij ≤ 1

0 ≤ i, j ≤ m − 1
(15)

Lemma IV.7. The graph G has an odd cycle and is connected
with very high probability for s = o(n1/3 ).

Note that we assume apriori knowledge of the sparsity of
the signal, i.e., the number of non-zero locations of the signal
is known.
2) Signal Recovery: Note that the autocorrelation constraints are non-convex. As we did in the support extraction,
we use the semideﬁnite relaxation X = xxT . We append n
zeros to the signal so that cyclic indexing scheme can be applied, hence a m = 2n order DFT matrix is required. Suppose
T
Mk is the m × m matrix deﬁned by Mk = fk fk , where fk is
th
the k column of the DFT matrix for k = {0, 1, ....m − 1}.
The autocorrelation constraints can be written in the Fourier
domain as

Pick any three vertices randomly. Choose any path of length
k − 3 from one of those vertices to cover all the remaining
vertices randomly. If all the edges exists between the three
vertices and the chosen k−3 length path exists, we are through.
If any of the k edges doesn’t exist, it implies that the distance
between that pair of vertices occurs more than once. Since
there are less than k 2 pairs, the probability of a pair of vertices
2
not having an edge can be union bounded by k . Since there
n
are k edges to be considered, the probability of failure can
3
be upper bounded by k . Hence if s = o(n1/3 ), any chosen
n
triangle and path exists with very high probability.

Yk = trace(Mk X)

B. Algorithm 2

2

k = 0, ......, m − 1

2

(16)

2

where Y = {|y0 | , |y1 | , ......|ym−1 | } is the vector containing
the squared magnitude of the Fourier transform of x. We can
solve for the signal using L1-minimization [15], [16], [17].

Algorithm 2 is developed using a convex optimization based
framework. Semideﬁnite relaxation is used to convert the nonconvex constraints into a set of convex constraints. We break
the problem into two stages. First, the support of the signal is
recovered from the autocorrelation and then we solve for the
signal in the support.
1) Support Recovery: We have to extract u from the
autocorrelation of the signal. We will assume that the support
of the signal is a subset of the support of the autocorrelation.
This is the same as assuming there is no cancellation of support
in the autocorrelation, which is a very weak requirement and
holds with probability one if the coefﬁcients of the signal
are chosen randomly from a non-degenerate distribution. With
this assumption, ai = 0 implies that no two elements in the
support are separated by a distance i, and if ai is non-zero,
there is atleast one pair of elements in the support separated
by a distance i, i.e.,
ai = 0 ⇒ uj ui+j = 0 ∀ j

(14)

minimize
subject to

||X||1
Yk = trace(Mk X),
Xij = 0
X

if

Sij = 0

k = 0, ......m − 1 (17)
0 ≤ i, j ≤ m − 1

0
V. S IMULATION R ESULTS

Figure 1 shows the success rate of signal recovery using
Algorithm 1 as a function of the sparsity of the signal. We see
that signals with s = o(n1/3 ) are recovered successfully with
very high probability. While the algorithm is computationally
very cheap, it is not robust to noise due to error propagation.
Figure 2 demonstrates the performance of Algorithm 2 as a
function of the sparsity of the signal. Numerical simulations
strongly suggest that signals with sparsity upto s = o(n1/2 )
can be recovered using this algorithm. It is also very robust to

(13)

4

B. Proof of Corollary IV.2

1

From Lemma VI.1, we see that

0.9

∞

0.8

P (X1 − pX2 = qX3 ) =

0.7

P (X3 = i)P (X1 − pX2 = qi)
i=0

success

0.6

∞
0.5

≤

0.4

i=0

0.3

∞

P (X3 = i) ≤
i=0

s
n

(19)

[1] R. P. Millane, ”Phase retrieval in crystallography and optics,” J.
Opt. Soc. Am. A 7, 394-411 (1990)
[2] J.C. Dainty and J.R. Fienup,“Phase Retrieval and Image Reconstruction for Astronomy,” Chapter 7 in H. Stark, ed., Image
Recovery: Theory and Application pp. 231-275.
[3] L. Rabiner and B.H. Juang, “Fundamentals of Speech Recognition,” Signal Processing Series, Prentice Hall, 1993.
[4] A. Walther, ”The question of phase retrieval in optics,” Opt.
Acta 10, 4149 (1963).
[5] R. W. Gerchberg and W. O. Saxton. “A practical algorithm for
the determination of the phase from image and diffraction plane
pictures”. Optik 35, 237 (1972).
[6] R. A. Gonsalves. “Phase retrieval from modulus data”. J. Opt.
Soc. Am. 66, 961–964 (1976)
[7] J. R. Fienup, “Phase retrieval algorithms: a comparison”. Appl.
Opt. 21, 2758–2769 (1982).
[8] A.H. Sayed and T. Kailath. “A survey of spectral factorization
methods”. Numerical Linear Algebra with Applications, 8: 467–
496 (2001).
[9] E. J. Candes, Y. Eldar, T. Strohmer, and V. Voroninski. “Phase
retrieval via matrix completion”. arXiv:1109.0573v2
[10] A. Fannjiang. “Absolute Uniqueness of Phase Retrieval with
Random Illumination”. arXiv:1110.5097v3
[11] Y.M. Lu and M. Vetterli. “Sparse spectral factorization: Unicity
and reconstruction algorithms”. Acoustics, Speech and Signal
Processing (ICASSP), 2011 IEEE International Conference on ,
vol., no., pp. 5976–5979, 22-27 May 2011
[12] K. Jaganathan, S. Oymak and B. Hassibi, “Phase Retrieval for
Sparse Signals using Rank Minimization”. Acoustics, Speech
and Signal Processing (ICASSP), 2012 IEEE International Conference.
[13] M. Hayes and J. McClellan, “Reducible Polynomials in more
than One Variable”, Proc. IEEE 70(2): 197 198, (1982)
[14] E. J. Candes, T. Strohmer and V. Voroninski. PhaseLift: exact
and stable signal recovery from magnitude measurements via
convex programming. To appear in Communications on Pure
and Applied Mathematics
[15] E. J. Candes and T. Tao. “Decoding by linear programming”.
IEEE Trans. Inform. Theory, 51 4203–4215.
[16] E. J. Candes and B. Recht. “Exact matrix completion via convex
optimization”. Found. of Comput. Math., 9 717–772.
[17] E. J. Candes and T. Tao. “The power of convex relaxation: Nearoptimal matrix completion”. IEEE Trans. Inform. Theory, 56(5),
2053–2080.

0.1
0
0

50

100

150

k

Fig. 1. Success rate of recovery using Algorithm 1 for n = 8192 (n1/3 ≈
20) for various sparsities

noise and hence more practical. We hope to provide theoretical
guarantees in a future publication.
1
0.9
0.8
0.7
0.6
success

s
s
≤
n
n

R EFERENCES

0.2

0.5
0.4
0.3
0.2
0.1
0
0

5

10

15

20
k

25

30

35

40

Fig. 2. Success rate of recovery using Algorithm 2 for n = 64 (n1/2 = 8)
for various sparsities

VI. A PPENDIX
Lemma VI.1. For a pair of geometric random variables X1
s
s
and X2 with parameter n each, P (X1 − pX2 = c) ≤ n if
α
s = n , α < 1 for n > n( ), where p and c are integers.
A. Proof of Lemma VI.1
∞

P (X1 − pX2 = c) =

P (X2 = i)P (X1 = pi + c) (18)
i=0

∞

=

P (X3 = i)

∞

s s
s
s
s
s
s
(1− )i (1− )c+pi = ( )2 (1− )c
(1− )(1+p)i
n n
n
n
n
n i=0
n
i=0

s
(1 − n )c
s
s
1
s
= ( )2
= ( )2
≤
s
s
s
n 1 − (1 − n )(1+p)
n (1 + p) n + n o(1)
n

for n > n( ).

5

