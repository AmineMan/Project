Title:          htstruct.dvi
Creator:        dvips(k) 5.98 Copyright 2009 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 15:02:51 2012
ModDate:        Tue Jun 19 12:55:48 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      323827 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566267

Hypothesis testing via a comparator
Yury Polyanskiy

correlated in space but not in time). The goal is then to
optimize the communication cost by designing (single letter)
quantizers and a good (single or multi round) protocol for
exchanges between the sensors and the fusion center; see [7]–
[9] and references therein. These problems can again be
restated in the form of constraining the allowable tests similar
to (1).
In this paper we consider tests employing a comparator,
namely those satisfying the constraint:

Abstract—This paper investigates the best achievable performance by a hypothesis test satisfying a structural constraint:
two functions are computed at two different terminals and the
detector consists of a simple comparator verifying whether the
functions agree. Such tests arise as part of study of fundamental
limits of channel coding, but are also useful in other contexts.
A simple expression for the Stein exponent is found and applied
to showing a strong converse in the problem of multi-terminal
hypothesis testing with rate constraints. Connections to the G´ csa
K¨ rner common information and to spectral properties of condio
tional expectation operator are identiﬁed. Further tightening of
results hinges on ﬁnding λ-blocks of minimal weight. Application
of Delsarte’s linear programming method to this problem is
described.

T = 1{f (X n ) = g(Y n )} ,

where the cardinality of the common co-domain of f and g is
unrestricted. This constraint is motivated by the meta-converse
method [1, Section III.E], which proves a lower bound on
probability of error by ﬁrst using a channel code as a binary
hypothesis test and then comparing its performance with that
of an optimal (Neyman-Pearson) test. However, so constructed
test necessarily satisﬁes the structural constraint (2) and thus
it is natural to investigate whether imposing (2) incurs exponential performance loss.
Another situation in which tests of the form (2) occur
naturally is in the analysis of parallel systems, such as in faulttolerant parallel computers, that under normal circumstances
perform a redundant computation of a complicated function
with high probability of agreement, while it is required to
lower bound the probability of agreement when the fault
occurs (modeled as PXY changing to QXY ). Yet another
case is in testing hypotheses of biological nature based on
the observation of zygosity of cells only (in eukaryotes).
The main result is that in the memoryless setting Stein
exponent of tests satisfying (2) can indeed be quite a bit
smaller than D(PXY ||QXY ) and in fact is given by

I. I NTRODUCTION
A classical problem in statistics and information theory is
that of determining which of the two distributions, P or Q,
better ﬁt an observed data vector. As shown by Neyman and
Pearson, the binary hypothesis testing (in the case of simple
hypotheses) admits an optimal solution based on thresholding
the relative density of P with respect to Q (a Radon-Nikodym
derivative). The asymptotic behavior of the tradeoff between
the two types of errors has also been well studied by Stein,
Chernoff, Hoeffding and Blahut. Knowledge of this tradeoff
is important by itself and is also useful for other parts of
information theory, such as channel coding [1, Section III.E]
and data compression [2, Section IV.A].
The problem becomes, however, much more complex with
the introduction of structural constraints on the allowable tests.
For example, it may happen that observations consist of two
parts, say X n = (X1 , . . . , Xn ) and Y n = (Y1 , . . . , Yn ),
which need to be compressed down to nR bits each before
the decision is taken. Even the memoryless case, in which
under either hypothesis the pairs (Xi , Yi ) are independent and
identically distributed (i.i.d.) according to PXY or QXY , is a
notoriously hard problem with only a handful of special cases
solved [3]–[6]. Formally, this problem corresponds to ﬁnding
the best test of the form
T = 1{(f (X n ), g(Y n )) ∈ A} ,

(2)

△

E=

min

VX =PX ,VY =PY

D(VXY ||QXY ) ,

(3)

where D(·||·) is the Kullback-Leibler divergence, and the
optimization is over all joint distributions VXY with marginals
matching those of PXY . In particular, E = 0 if (and only if)
the marginals of QXY coincide with those of PXY .
In fact, for the latter case, the hypothesis testing with
constraint (2) turns out to be intimately related to a problem
of determining the common information C(X; Y ) in the sense
of G´ cs and K¨ rner [10]. Using a technique pioneered by
a
o
Witsenhausen [11] we show that the error probability cannot
decay to zero at all (even subexponentially). Unfortunately,
this is only shown under the condition that the conﬁdence level
is sufﬁciently high. Extending to the general case appears to
be surprisingly hard. For a special case of binary X and Y
we describe a bound based on Delsarte’s linear programming
method [12] and demonstrate promising numerical results.
However, we have not yet been able to identify a convenient

(1)

where optimization is over functions f and g with ﬁnite codomains of cardinality 2nR and critical regions A. Here and
below T = 1 designates the test choosing the distribution P
and T = 0 the distribution Q.
Another rich source of difﬁcult problems is the distributed
case, in which observations are taken by spatially separated
sensors (whose measurements are typically assumed to be
The author is with the Department of Electrical Engineering and Computer
Science, MIT, Cambridge, MA 02139 USA. e-mail: yp@mit.edu.
The research was supported by the Center for Science of Information
(CSoI), an NSF Science and Technology Center, under grant agreement CCF09-39370.

1

have

polynomial, such as found in [13] for the coding in Hamming
space, admitting an asymptotic analysis.
The exponent E has appeared before in the context of
hypothesis testing with rate constraints (1), see [4, Theorems 5
and 8], and distributed detection [8, Theorem 2]. We identify
the reasons for this below and also use this correspondence to
prove the strong converse for the results in [4].

1
n
˜
log βα (PXY , Qn ) = −E ,
XY
n
where E is deﬁned in (3). Moreover, if E = ∞ then there
n
˜
exists n0 (α) such that βα (PXY , Qn ) = 0 for all n ≥ n0 .
XY
Proof: Achievability: Consider functions
lim

n→∞

f (xn ) =
g(y n ) =

II. BACKGROUND AND NOTATION
Consider a distribution PXY on X ×Y. We denote a product
n
distribution on X n × Y n by PXY and by PXY > 0 the fact
that PXY is non-zero everywhere on X × Y.
Fix some PXY and QXY . For each integer n ≥ 1 and
0 ≤ α ≤ 1 the performance of the best possible comparator
hypothesis test of conﬁdence level α is given by

P[f (X n ) = g(Y n )] =
≥

where inﬁmum is over all (perhaps, randomized) maps f :
X n → R and g : Y n → R such that

n
n
Qn [T[PX ] × T[PY ] ] = exp{−nE + o(n)} .
XY

(6)
(7)

(8)

For the case of E < ∞, this has been demonstrated in the
proof of [4, Theorem 5]. For the case E = ∞, we need to
show that for all n ≥ n0 we have

P[T = 1] ≥ α ,
where T is deﬁned in (2). Here and below we follow the
agreement that P and Q denote measures on some abstract
spaces carrying random variables (X n , Y n ) distributed as
n
PXY and Qn , resp..
XY
For a ﬁnite X × Y and a given distribution PXY we
deﬁne a bipartite graph with an edge joining x ∈ X to
y ∈ Y if PXY (x, y) > 0. The connected components of
this graph are called components of PXY and the entropy
of the random variable indexing the components is called the
common information of X and Y , cf. [10]. If the graph is
connected, then PXY is called indecomposable. In particular
indecomposability implies PX > 0 and PY > 0.
We also deﬁne a maximal correlation coefﬁcient S(X; Y )
between two random variables X and Y as

n
n
Qn [T[PX ] × T[PY ] ] = 0 .
XY

Indeed, assuming otherwise we ﬁnd a sequence of typical
pairs (xn , y n ) with positive QXY -probability. But then the
(n)
sequence of the joint types VXY associated to (xn , y n ) belongs
to the closed set of joint distributions {VXY : VXY ≪ QXY }
¯
and by compactness must have a limit point VXY . By the
δ-convention [14, Chapter 2], the accumulation point must
¯
¯
have marginals VX = PX and VY = PY and thus E ≤
¯XY ||QXY ) < ∞ – a contradiction.
D(V
Converse: We reduce to the special case of the theorem,
stated as Theorem 3 below. If E = ∞ then there is nothing
to prove, so assume otherwise and take an arbitrary VXY with
VX = PX , VY = PY and D(VXY ||QXY ) < ∞. Our goal is
to show that
˜
βα (P n , Qn ) ≥ exp{−nD(VXY ||QXY ) + o(n)} . (9)

S(X; Y ) = sup E [f (X)g(Y )]
f,g

supremum taken over all zero-mean functions of unit variance. For ﬁnite X × Y indecomposability of PXY implies
S(X; Y ) < 1 and (under assumption PX > 0,PY > 0) is
equivalent to it.
Finally, we recall [10] that a pair of sets A ∈ X n and
n
n
n
B ∈ Y n is called a λ-block for PXY if PX [A] > 0, PY [B] > 0
and

XY

XY

If VXY > 0 then we can replace VXY with (1 − ǫ)VXY +
ǫPX PY , which is everywhere positive on X × Y, and then
take a limit as ǫ → 0 in (9). Thus we assume VXY > 0.
Denote
△
An = {f (X n ) = g(Y n )} .
By the special case of the theorem we have

P[Y n ∈ B|X n ∈ A] ≥ λ .

n
VXY [An ] ≥ exp{−o(n)} .

An elegant theorem of G´ cs and K¨ rner states
a
o
Theorem 1 ([10]): Let PXY be an indecomposable distribution on a ﬁnite X × Y. Then for every λn ≥ exp{−o(n)}
there exists a sequence νn = o(n) such that for all n any
n
λn -block (A, B) for PXY satisﬁes

(10)

Then, by a standard change of measure argument, we must
have
Qn [An ] ≥ exp{−nD(VXY ||QXY ) + o(n)} .
XY

(11)

Optimizing the choice of VXY in (11) proves (9) and the
Theorem.
It remains to consider the case of matching marginals:
Theorem 3 (Special case E = 0): Let PXY be indecomposable, QXY > 0 and QX = PX , QY = PY . Then for
any 0 < α < 1 we have
˜
βα (P n , Qn ) ≥ exp{−o(n)} .
(12)

[A × B] ≥ exp{−νn } .

III. M AIN

n
n
n
PXY [T[PX ] × T[PY ] ]
1 − o(1) .

On the other hand, using joint-type decomposition it is
n
n
straightforward to show that the set T[PX ] × T[PY ] under the
n
product measure QXY satisﬁes

n
˜
βα (PXY , Qn ) = inf Q[T = 1] ,
XY

n
PXY

(4)
(5)

n
where T[P ] denotes the set of P -typical sequences [14, Chapter
2] over the alphabet of P . Then, on one hand by typicality:

△

P[X n ∈ A|Y n ∈ B] ≥ λ ,

n
1{xn ∈ T[PX ] } ,
n
2 · 1{y n ∈ T[PY ] } ,

RESULTS

A. Stein exponent
Theorem 2: Consider an indecomposable PXY on a ﬁnite
X × Y. Then for an arbitrary QXY and any 0 < α < 1 we

XY

2

XY

0.25

Proof: First we show that any test of level α must
contain a λ-block with λ ≥ α . Indeed, each pair ({f (X n ) =
2
i}, {g(Y n ) = i}) is a λi -block for some λi (chosen to be
maximum possible). Then, by the Bayes rule and max{x, y} ≤
x + y we get

5

n=4
0.2

5

n=2

0

n=1

n=5

λ

0.15

P[f (X n ) = g(Y n ) = i] ≤ λi (P[f (X n ) = i]+P[g(Y n ) = i]) .

0.1

n=2

α
2.

Summing this over i shows that at least one λi ≥
By the G´ cs-K¨ rner effect (Theorem 1) the probability of
a
o
this λ-block is subexponentially large:

0.05

0

P[f (X n ) = g(Y n ) = i] ≥ exp{−o(n)} .

0.01

0.015

0.02

0.025
p=P[A]

0.03

0.035

0.04

0.045

0.05

Corollary 4: Consider a hypothesis testing between an indecomposable PXY and an arbitrary QXY with structural
restriction on tests of the form

(13)
(14)

T = 1{(f (X n ), Y n ) ∈ A}

Thus, the sets {f (X n ) = i} and {g(Y n ) = i} must occupy
n
n
a subexponential fraction of typical sets T[PX ] and T[PY ] . In
view of (8) it is natural to expect that
Q[f (X n ) = g(Y n ) = i] ≥ exp{−o(n)}

0.005

Fig. 1. Linear programming upper bound on λ as a function of p. Uniform
X and Y connected by the BSC(δ), δ = 0.3.

Therefore, in particular we have (since the marginals of X n
and Y n under P and Q coincide)
Q[f (X n ) = i] ≥ exp{−o(n)} ,
Q[g(Y n ) = i] ≥ exp{−o(n)} .

0

(17)

with binary-valued f . Then for any 0 < ǫ < 1 we have
inf Q[T = 1] = exp{−nE + o(n)} ,

(15)

where inﬁmum is over all tests satisfying P[T = 1] ≥ 1 − ǫ
and E is given by (3).
Proof: Clearly, any test with binary-valued f and g of
the form (2) is also a test of the form (17). Thus Theorem 2
establishes the achievability part. Conversely, for any test of
the form (17) we may ﬁnd sets A0 and A1 such that

(note that marginals match and thus E = 0 as per (3)). Under
the assumption QXY > 0 it is indeed straightforward to
show (15) by an application of blowing-up lemma; see [6,
Theorem 3].
Finally, (15) completes the proof because
Q[T = 1] ≥ Q[f (X n ) = g(Y n ) = i] .

P[T = 1] = P[{Y n ∈ A0 , f (X n ) = 0}
∪{Y n ∈ A1 , f (X n ) = 1}]
≥ 1−ǫ.

B. Discussion
It should be emphasized that although intuitively one imag˜
ines that the behavior of βα should markedly depend on how
the connected components of PXY and QXY relate to each
other, Theorem 2 demonstrates that the Stein exponent is not
sensitive to the decomposition of QXY .
The assumption of indecomposability of PXY in Theorem 2, however, is essential. Indeed, consider the case of
X = Y = {0, 1} and X = Y uniform (under PXY ) vs X, Y
independent uniform (under QXY ). Clearly a test {X n = Y n }
demonstrates
n
˜
β1 (PXY , Qn ) ≤ 2−n ,
(16)
XY

(18)
(19)

Then without loss of generality assume that the ﬁrst set in the
union has P-probability larger than 1−ǫ . Deﬁne the following
2
function
g(y n ) = 1{y n ∈ A1 \ A0 } + 21{y n ∈ A0 ∪ A1 }
Then since {f = g} ⊇ {y n ∈ A0 , f (X n ) = 0} we have
1−ǫ
P[f (X n ) = g(Y n )] ≥
,
(20)
2
and thus by Theorem 2 we conclude that Q[T = 1] is at least
exp{−nE + o(n)}.
We remark that the correspondence between the hypothesis
tests with 1-bit compression and those of interest in this
paper (2) does not hold in full generality. In particular, it
was shown in [4, Theorem 5] that the exponent E in (3) is
still optimal in the 1-bit scenario without the requirement of
indecomposability of PXY , while example (16) demonstrates
the contrary for our setup.

while according to the deﬁnition (3) we have E = 0.
We also remark that the case of E = ∞ is possible.
For example, let X, Y be binary with PXY (0, y) = 1 −
2
1
PXY (1, y) = p for p > 1 , and QXY (x, y) = 2 1{x = y}.
2
2
C. Hypothesis testing with a 1-bit communication constraint
The exponent E in (3) is related to hypothesis testing
under the communication constraint (1). In fact, Theorem 2
extends [4, Theorem 5] to the entire range 0 < ǫ < 1,
thereby establishing the full strong converse. This result has
been obtained in [6] under different assumptions on PXY and
QXY 1 .

IV. N ON - VANISHING LOWER BOUNDS
By Theorem 3 in the case when marginals of PXY and
QXY coincide the error cannot decay to zero exponentially.
In fact, we conjecture that in the cases of matching marginals
n
˜
βα (PXY , Qn ) does not vanish at all. In this section we
XY
prove the conjecture under additional assumptions and discuss
potential methods for extending to the general case.

1 Namely, we do not require D(P
XY ||QXY ) < ∞ or positivity of QXY ,
but require indecomposability of PXY .

3

p2 . Nevertheless, even the case of QXY = PX PY is far
from being resolved as we need to extend to the full range
0 < α < 1. We discuss this second problem further.

Theorem 5: Consider a PXY and QXY = PX PY such that
△
S = S(X; Y ) < 1 (under PXY ). Then for any α ∈ (2S, 1]
2
we have
˜
lim βα (P n , Qn ) > 0 .
XY

n→∞

A. More on spectral methods
In a nutshell, the proof of Theorem 5 consisted of two steps.
First, we identiﬁed a Markov chain

XY

Proof: As in the proof of Theorem 3 given a test
{f (X n ) = g(Y n )} of level α we can extract a λ-block
(A, B) with λ > α . We want to show that for some constant
2
n
p = p(α) > 0 and all n at least one of the marginals PX [A]
n
or PY [B] can be bounded away from zero:
n
n
max{PX [A], PY [B]} ≥ p .

F → Xn → Y n → G ,
△

where we denoted F = f (X n ), G = g(Y n ). Note that by the
data-processing for maximal correlation we have

(21)

S(F ; G) ≤ S(X n ; Y n ) = S(X; Y ) .

Indeed, then we have

Second, for large α we showed a lower bound

α
Q[T = 1] ≥
≥ p2
(22)
2
which follows because in a λ-block the smaller of the two
probabilities in (21) should still be larger than the joint probn
ability PXY [A × B] which is ≥ λp. Finally, the estimate (21)
follows from the next result.
n
Lemma 6: Consider a λ-block (A, B) for PXY . Then,
n
n
PX [A]PY [B]

n
n
max{PX [A], PY [B]} ≥ min

1 λ−S
,
2 1−S

,

△

P[F = i]P[G = i] ≥ const > 0

Q[F = G] =
i

under conditions: a) P[F = G] ≥ α and b) S(F ; G) ≤ S. Can
a lower bound be tightened so that it does not vanish for all
α > 0?
The answer is negative. Indeed, consider a distribution PF G
on [M ] × [M ]:
α
1−α
PF G (i, j) =
1{i = j} +
1{i = j} .
(30)
M
M (M − 1)

(23)

whenever S = S(X; Y ) < 1.
n
Proof: Consider an operator Tn : L2 (Y n , PY ) →
n
n
2
L2 (X , PX ) deﬁned as follows:
(Tn h)(xn ) = E [f (Y n )|X n = xn ] ,

(29)

△

1−α
Then we have S(F ; G) = α − M−1 . That is, such PF G satisﬁes the α-constraint and the maximal correlation constraint
whenever α ≤ S(X; Y ) and achieves
1
P[F = i]P[G = i] =
→0
M
i

(24)
n
PXY

where the expectation is over the distribution
. Note
that the second largest singular value of Tn is precisely
the maximal correlation coefﬁcient S(X; Y ) (under PXY ),
see [15]. Thus, for any zero-mean functions h ∈ L2 (X n ) and
h′ ∈ L2 (Y n ) we have

as M → ∞.
It may appear that as a workaround one may consider higher
spectral invariants in addition to S(X; Y ). Formally, to any
n
joint distribution PXY we associate the operator Tn as in (24).
Let the singular values of Tn sorted in decreasing order be

E [h(X n )h′ (Y n )] = (Tn h′ , h) ≤ S(X; Y )||h||2 ||h′ ||2 . (25)

1 = σn,0 ≥ σn,1 ≥ σn,2 ≥ · · · ≥ 0 ,

n
n
Denote pA = PX [A], pB = PY [B] and assume pB ≥ pA . If
1
pB ≥ 2 then there is nothing to prove, so assume otherwise.
Then, we have

⊗n
where σ1,1 = S(X; Y ). Since Tn = T1
the singular
spectrum of Tn consists of all possible products of the form
n
t=1 σ1,jt and in particular

n
≤ PXY [A × B]

(26)

1 = σn,0 ≥ σn,1 = · · · = σn,n = S(X; Y ) .

≤ pA pB + S pA (1 − pA )pB (1 − pB )
≤ p2 + SpB (1 − pB ) ,
B

λpB

(27)
(28)

Moreover, it is easy to show that if one has a Markov
chain (29) then singular values {µj , j = 1, . . .} associated with
n
PF G are related to those of PXY via the following “spectralprocessing” inequalities:

where (26) is by the deﬁnition of a λ-block, (27) is by (25)
applied to h(xn ) = 1{xn ∈ A} − pA and h′ = 1{y n ∈
B} − pB ; and (28) is because pA ≤ pB ≤ 1 . Canceling pB
2
on both sides in (28) we obtain (23).
Next, we discuss what is required to extend Theorem 5 to
full generality. To handle a general QXY one needs a nonvanishing lower bound independent of n on

k

k

µj
j=1

≤

σn,j

k = 1, . . . .

(31)

j=1

Clearly this extends the data-processing for maximal correlation used in the proof of Theorem 5. Does it lead to a lowerbound non-vanishing for all α?
Alas, the answer is negative. Indeed, in the example (30) the
1−α
singular spectrum associated to PF G consists of 1 and M−1
(of multiplicity M − 1). This spectrum satisﬁes (31) as long
as α ≤ S(X; Y ) and M ≤ n + 1. Thus, for α ≤ S(X; Y ),
inequalities (31) can not rule out the possibility that
1
Q[F = G] ≤
.
n+1

λmin (p, Qn ) = min Qn [A × B] ,
XY
XY
A,B

where the minimization is over Qn [A], Qn [B] ≥ p. For
X
Y
n n
QXY = PX PY this problem is void since λmin (p, PX PY ) =
2 The idea to use the maximal correlation to relate marginals and the joint
distribution was ﬁrst proposed by Witsenhausen [11] in the context of a
slightly different problem.

4

where maximum is over all non-negative {αd } such that α0 =
1, βv (α) ≥ 0 and (34).
To give the dual formulation of (35) say that a polynomial
P (x) of degree not larger than n is admissible if

B. λ-blocks of minimal weight
Another method to extend the range of α in Theorem 5 is to
ﬁnd a non-vanishing (as n → ∞) lower bound on the marginal
probability PX n [A] of a λ-block (A, B). In fact, it is enough to
consider the case of PXY with X = Y and PX = PY . Indeed,
consider an arbitrary λ-block (A, B) and construct a Markov
kernel W : X → X as composition W = PX|Y ◦ PY |X ,
namely
W (x1 |x0 ) =

n

P (x) =

and pv ≥ (1 − 2δ)v for all v = 0, . . . , n. Then, we have

PX|Y (x1 |y)PY |X (y|x0 ) .

λ∗ (p) ≤ min 2−n P (0) + (p − 2−n ) max P (x)
n
x=1,...,n

y∈Y

Then distribution PX is a stationary distribution of the Markov
chain associated with W (and operator of conditional expectation (24) is self-adjoint). Moreover, we clearly have
n

W (A|A) ≥

n
PX|Y

n
PY |X (B|A)

2

≥λ .

λ∗ (p) = lim

max

n
n→∞ A:PX [A]≤p

λ∗ (p) ≤ 1 + 2δ(p − 1) ,
n

(32)

n

P (x)

W n (A|A) .

.

where d(x, y) is the Hamming distance. Then,
n

αd (1 − δ)n−d δ d

(33)

d=0
n

Deﬁne βv (α) = x=0 Kv (x)αx , a dual weight distribution of
A, with Kv (x) – Krawtchouk polynomials; e.g. [13, Appendix
A]. By Delsarte’s theorem [12], βv (α) ≥ 0 and in fact by the
cardinality constraint
β0 (α) ≤ p2n .

(34)

Thus, we get the following linear-programming bound
n

λ∗ (p) ≤ max
n

αd (1 − δ)n−d δ d ,

(37)

[1] Y. Polyanskiy, H. V. Poor, and S. Verd´ , “Channel coding rate in the
u
ﬁnite blocklength regime,” IEEE Trans. Inf. Theory, vol. 56, no. 5, pp.
2307–2359, May 2010.
[2] V. Kostina and S. Verd´ , “Fixed-length lossy compression in the ﬁnite
u
blocklength regime,” Arxiv preprint arXiv:1102.3944, 2011.
[3] R. Ahlswede and I. Csisz´ r, “Hypothesis testing with communication
a
constraints,” IEEE Trans. Inf. Theory, vol. 32, no. 4, pp. 533–542, Jul.
1986.
[4] T. S. Han, “Hypothesis testing with multiterminal data compression,”
IEEE Trans. Inf. Theory, vol. 33, no. 6, pp. 759–772, Nov. 1987.
[5] T. S. Han and K. Kobayashi, “Exponential-type error probabilities for
multiterminal hypothesis testing,” IEEE Trans. Inf. Theory, vol. 35, no. 1,
pp. 2–14, Jan. 1989.
[6] H. Shalaby and A. Papamarcou, “Multiterminal detection with zero-rate
data compression,” IEEE Trans. Inf. Theory, vol. 38, no. 2, pp. 254–267,
mar 1992.
[7] J. Tsitsiklis, “Decentralized detection by a large number of sensors,”
Math. Contr. Signals, Syst., vol. 1, no. 2, pp. 167–182, 1988.
[8] H. Shalaby and A. Papamarcou, “A note on the asymptotics of distributed detection with feedback,” IEEE Trans. Inf. Theory, vol. 39, no. 2,
pp. 633–640, Mar. 1993.
[9] W. Tay and J. Tsitsiklis, “The value of feedback for decentralized
detection in large sensor networks,” in Proc. 2011 Int. Symp. Wireless
and Pervasive Comp. (ISWPC), Hong Kong, China, Feb. 2011, pp. 1–6.
[10] P. G´ cs and J. K¨ rner, “Common information is far less than mutual
a
o
information,” Prob. Contr. Inf. Theory, vol. 2, no. 2, pp. 149–162, 1973.
[11] H. Witsenhausen, “On sequences of pairs of dependent random variables,” SIAM J. Appl. Math., vol. 28, pp. 100–113, 1975.
[12] P. Delsarte, “An algebraic approach to the association schemes of coding
theory,” Philips Research Rep. Supp., no. 10, p. 103, 1973.
[13] R. McEliece, E. Rodemich, H. Rumsey, and L. Welch, “New upper
bounds on the rate of a code via the Delsarte-MacWilliams inequalities,”
IEEE Trans. Inf. Theory, vol. 23, no. 2, pp. 157–166, 1977.
[14] I. Csisz´ r and J. K¨ rner, Information Theory: Coding Theorems for
a
o
Discrete Memoryless Systems. New York: Academic, 1981.
[15] O. V. Sarmanov, “A maximal correlation coefﬁcient,” Dokl. Akad. Nauk
SSSR, vol. 121, no. 1, 1958.

1
· |{(x, y) : x ∈ A, y ∈ A, d(x, y) = d}| ,
|A|

=

Kv (x)

R EFERENCES

For the general case, consider an arbitrary set A ⊂ {0, 1}n
of cardinality |A| ≤ p2n . Deﬁne its weight distribution as

W (A|A)

K0 (x) + (1 − 2δ)

As numerical evaluation of (35) shows, see Fig. 1, the
bound (36) can be signiﬁcantly improved. Finding a suitable
admissible polynomial P (x) remains an open problem.

n
whereas on the other hand PX [A] = 2dim A−n . Therefore, the
λ-p tradeoff achievable with linear sets satisﬁes

n

=

v=1

W n (A|A) = PZ n [A] ≤ (1 − δ)n−dim A ,

1
1−δ

(36)

and corresponds to choosing

In fact, for the purpose of extending Theorem 5 we only
need to show λ∗ (0+) = 0.
In the remaining we consider a special case of X = Y =
{0, 1} and W (0|1) = W (1|0) = δ – a binary symmetric
channel, BSC(δ). First, let us consider sets A ⊂ {0, 1}n
which are linear subspaces, then denoting by Z n a vector with
i.i.d. Bernoulli(δ) components, we can easily argue that

λ ≤ plog2

,

where minimum is over all admissible polynomials. The bound
of Lemma 6 states

n
And hence, it is enough to lower bound PX [A] among all A
with the requirement that (A, A) be a λ2 -block for W : X →
X . In other words:
Problem (λ-blocks of minimal weight): Given a
Markov kernel W : X → X with stationary
distribution PX determine

αd =

pv Kv (x) ,
v=0

(35)

d=0

5

