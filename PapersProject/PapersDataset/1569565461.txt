Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 11:14:08 2012
ModDate:        Tue Jun 19 12:54:22 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      566383 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565461

Gaussian Rate-Distortion via Sparse Linear
Regression over Compact Dictionaries
Ramji Venkataramanan

Antony Joseph

Sekhar Tatikonda

Dept. of Electrical Engineering
Yale University, USA
Email: ramji.venkataramanan@yale.edu

Dept. of Statistics
Yale University, USA
Email: antony.joseph@yale.edu

Dept. of Electrical Engineering
Yale University, USA
Email: sekhar.tatikonda@yale.edu

random vectors and matrices. All vectors have length n. The
source sequence is denoted by S (S1 , . . . , Sn ), and the reˆ
ˆ
ˆ
construction sequence by S (S1 , . . . , Sn ). X denotes the
√
2 -norm of vector X, and |X| = X / n is the normalized
version. We use natural logarithms, so entropy is measured
in nats. f (x) = o(g(x)) means limx→∞ f (x)/g(x) = 0;
f (x) = Θ(g(x)) means f (x)/g(x) asymptotically lies in an
interval [k1 , k2 ] for some constants k1 , k2 > 0.
Consider an i.i.d Gaussian source S with mean 0 and
variance σ 2 . A rate-distortion codebook with rate R and
block length n is a set of enR length-n codewords, denoted
ˆ
ˆ
{S(1), . . . , S(enR )}. The quality of reconstruction is measured
through a mean-squared error distortion criterion

Abstract—We study a class of codes for compressing memoryless Gaussian sources, designed using the statistical framework of
high-dimensional linear regression. Codewords are linear combinations of subsets of columns of a design matrix. With minimumdistance encoding we show that such a codebook can attain the
rate-distortion function with the optimal error-exponent, for all
distortions below a speciﬁed value. The structure of the codebook
is motivated by an analogous construction proposed recently by
Barron and Joseph for communication over an AWGN channel.

I. I NTRODUCTION
One of the important outstanding problems in information theory is the development of practical codes for lossy
compression of general sources at rates approaching Shannon’s rate-distortion bound. In this paper, we study the
compression of memoryless Gaussian sources using a class
of codes constructed based on the statistical framework of
high-dimensional linear regression. The codebook consists of
codewords that are sparse linear combinations of columns of
an n × N design matrix or ‘dictionary’, where n is the blocklength and N is a low-order polynomial in n. Dubbed Sparse
Superposition Codes or Sparse Regression Codes (SPARC),
these codes are motivated by an analogous construction proposed recently by Barron and Joseph for communication over
an AWGN channel [1], [2]. The structure of the codebook
enables the design of computationally efﬁcient encoders based
on the rich theory on sparse linear regression and sparse
approximation. Here, the performance of these codes under
minimum-distance encoding is studied. The design of computationally feasible encoders will be discussed in future work.
Sparse regression codes for compressing Gaussian sources
were ﬁrst considered in [3] where some preliminary results
were presented. In this paper, we analyze the performance
of these codes under optimal (minimum-distance) encoding
and show that they can achieve the distortion-rate bound with
the optimal error exponent for all rates above a speciﬁed
value (approximately 1.15 bits/sample). The proof uses Suen’s
inequality [4], a bound on the tail probability of a sum of
dependent indicator random variables. This technique may
be of independent interest and useful in other problems in
information theory.
We lay down some notation before proceeding further.
Upper-case letters are used to denote random variables, lowercase for their realizations, and bold-face letters to denote

1
ˆ
ˆ
dn (S, S) = |S − S|2 =
n

n

ˆ
(Si − Si )2 ,
i=1

ˆ
where S is the codeword chosen to represent the source
sequence S. For this distortion criterion, an optimal encoder
maps each source sequence to the codeword nearest to it in
Euclidean distance. The rate-distortion function R∗ (D), the
minimum rate for which the distortion can be bounded by D
with high-probability, is given by [5]
1
σ2
log
nats/sample.
ˆ
2
D
pS|S :E(S−S)2 ≤D
ˆ
(1)
This rate can be achieved through Shannon-style random
codebook selection: pick each codeword independently as an
i.i.d Gaussian random vector distributed as Normal(0, σ 2 −D).
Such a code has encoding complexity which grows exponentially with block length. Lattice-based codes for Gaussian vector quantization have been widely studied, e.g [6], [7]. There
are computationally efﬁcient quantizers for certain classes
of lattice codes, but the high-dimensional lattices needed to
approach the rate-distortion bound have exponential encoding
complexity [7]. We also note that for sources with ﬁnite alphabet, various coding techniques have been proposed recently
to approach the rate-distortion bound with computationally
feasible encoding and decoding [8]–[11].
R∗ (D) =

min

ˆ
I(S; S) =

II. S PARSE R EGRESSION C ODES
A sparse regression code (SPARC) is deﬁned in terms of a
design matrix A of dimension n × M L whose entries are zero

1

Section 1
M columns

Section 2
M columns

Section L
M columns

Since each codeword in a SPARC is a sum of L columns
of A (one from each section), codewords sharing one or more
common columns in the sum will be dependent. Also, SPARCs
are not linear codes since the sum of two codewords does not
equal another codeword in general.

A:

III. M AIN R ESULT

β:

0,

0, 1,

0, 1, 0,

1, 0,

,0

We begin with some background on error exponents.
The probability of error at distortion-level D of a ratedistortion code Cn with block length n and encoder and
decoder mappings g, h is

T

Fig. 1: A is an n × M L matrix and β is a M L × 1 binary vector.
The positions of the ones in β correspond to the gray columns of A
which add to form the codeword Aβ.

Pe (Cn , D) = P |S − h(g(S))|2 > D .
Deﬁnition 1: The error exponent at distortion-level D of a
sequence of rate R codes {Cn }n=1,2,... is given by

2

−D
mean i.i.d Gaussian random variables with variance a L ,
2
where the constant a will be speciﬁed in the sequel. Here n
is the block length and M and L are integers whose values will
be speciﬁed shortly in terms of n and the rate R. As shown
in Figure 1, one can think of the matrix A as composed of L
sections with M columns each. Each codeword is the sum of L
columns, with one column from each section. More formally, a
codeword can be expressed as Aβ, where β is a binary-valued
M L × 1 vector (β1 , . . . , βM L ) with the following property:
there is exactly one non-zero βi for 1 ≤ i ≤ M , one non-zero
βi for M + 1 ≤ i ≤ 2M , and so forth. Denote the set of all
β’s that satisfy this property by BM,L .
Minimum-distance Encoder: This is deﬁned by a mapping
g : Rn → BM,L . Given the source sequence S n , the
encoder determines the β that produces the codeword closest
in Euclidean distance, i.e.,

r(R, D) = − lim sup
n→∞

∗

r (D, R) =

Decoder: This is a mapping h : BM,L → Rn . On receiving
β ∈ BM,L from the encoder, the decoder produces reconstruction h(β) = Aβ.
Since there are M columns in each of the L sections, the
total number of codewords is M L . To obtain a compression
rate of R nats/sample, we therefore need

− 1 − log

ρ2
σ2

R > R∗ (D)
R ≤ R∗ (D)

(5)

1
ρ2
log .
(6)
2
D
For R > R∗ (D), the exponent in (5) is the Kullback-Leibler
divergence between two zero-mean Gaussian distributions,
the ﬁrst with variance ρ2 and the second with variance σ 2 .
[15] shows that at rate R, we can compress all sequences
which have empirical variance less than ρ2 to within distortion
D with double-exponentially decaying probability of error.
Consequently, the dominant error event is obtaining a source
sequence with empirical variance greater than ρ2 , which has
exponent given by (5).
The main result of our paper is the following.
Theorem 1: Fix a rate R and target distortion D such that
σ 2 /D > x∗ , where x∗ ≈ 4.913 is the solution of the equation
R=

(2)

, and the number of columns M L
n
log n

ρ2
σ2

where ρ2 is determined by

There are several choices for the pair (M, L) which satisfy
this. For example, L = 1 and M = enR recovers the Shannonstyle random codebook in which the number of columns
in the dictionary A is enR , i.e., exponential in n. For our
constructions, we choose M = Lb for some b > 1 so that (2)
implies
L log L = nR/b.
(3)
n
log n

1
2

0

β∈BM,L

Thus L is now Θ

(4)

The optimal error exponent for a rate-distortion pair (R, D)
is the supremum of the error exponents over all sequences of
codes with rate R, at distortion-level D.
The error-exponent describes the asymptotic behavior of the
probability of error; bounds on the probability of error for
ﬁnite block lengths were obtained in [12], [13]. The optimal
error exponent was obtained by Marton [14] for discrete
memoryless sources and was extended to Gaussian sources
by Ihara and Kubo [15].
Fact 1: [15] For an i.i.d Gaussian source distributed as
Normal(0, σ 2 ) and mean-squared error distortion criterion, the
optimal error exponent at rate R and distortion-level D is

g(S) = argmin S − Aβ .

M L = enR .

1
log Pe (Cn , D).
n

1
1
log x = (1 − ).
2
x

b+1

in the dictionary A is now Θ
, a polynomial in
n. This reduction in dictionary complexity can be harnessed
to develop computationally efﬁcient encoders for the sparse
regression code. We note that the code structure automatically
yields low decoding complexity.

3.5R
Fix b > R−(1−D/ρ2 ) , where ρ2 is determined by (6). For every
positive integer n, let Mn = Lb where Ln is determined by
n
(3). Then there exists a sequence C = {Cn }n=1,2,... of rate
R sparse regression codes - with code Cn deﬁned by an n ×

2

Mn Ln design matrix - that attains the optimal error exponent
for distortion-level D given by (5).
Remark: The minimum value of b speciﬁed by the theorem enables us to construct SPARCs with the optimal
error exponent. The proof also shows that we can construct SPARCs which achieve the rate-distortion function for
2.5R
b > R−(1−D/ρ2 ) , with probability of error that decays subexponentially in n when b is less than 3.5R/(R−(1−D/ρ2 )).

From (8), it is seen that

P (E(S) | |S|2 = z 2 ) = P 

(10)
For a ﬁxed S, the Ui (S)’s are dependent. Suppose that the
ˆ
ˆ
codewords S(i), S(j) respectively correspond to the binary
ˆ
ˆ
vectors β(i), β(j) ∈ BM,L . Recall that each vector in BM,L
is uniquely deﬁned by the position of the 1 in each of the L
ˆ
ˆ
sections. If β(i) and β(j) overlap in r of their ‘1 positions’,
ˆ
ˆ
then the column sums forming codewords S(i) and S(j) will
share r common terms.
ˆ
For each codeword S(i), there are L (M − 1)L−r other
r
ˆ
codewords which share exactly r common terms with S(i), for
L
0 ≤ r ≤ L − 1. In particular, there are (M − 1) codewords
ˆ
that are pairwise independent of S(i). We now obtain an upper
bound for the probability in (10) using Suen’s correlation
inequality [4]. First, some deﬁnitions.
Deﬁnition 2 (Dependency Graphs [4]): Let {Ui }i∈I be a
family of random variables (deﬁned on a common probability
space). A dependency graph for {Ui } is any graph Γ with
vertex set V (Γ) = I whose set of edges satisﬁes the following
property: if A and B are two disjoint subsets of I such that
there are no edges with one vertex in A and the other in B,
then the families {Ui }i∈A and {Ui }i∈B are independent.
Fact 2: [4, Example 1.5, p.11] Suppose {Yα }α∈A is a
family of independent random variables, and each Ui , i ∈ I
is a function of the variables {Yα }α∈Ai for some subset
Ai ⊆ A. Then the graph with vertex set I and edge set
{ij : Ai ∩ Aj = ∅} is a dependency graph for {Ui }i∈I .
Remark 1: The graph Γ with vertex set V (Γ) =
{1, . . . , enR } and edge set e(Γ) given by

Due to space constraints, we omit some details in the proof
which will be included in a longer version of this paper. Given
rate R > R∗ (D), let ρ2 be determined by (6). For each
a2 < ρ2 , we will show that there exists a family of SPARCs
a2
1 a2
that achieves the error exponent 2 σ2 − 1 − log σ2 , thereby
proving the theorem.
Code Construction: For each block length n, pick L as
speciﬁed by (3) and M = Lb . Construct an n × M L design
2
−D
matrix A with entries drawn i.i.d Normal(0, a L ). The
codebook consists of all the vectors Aβ, where β ∈ BM,L .
Encoding and Decoding: If the source sequence S is such
that |S|2 ≥ a2 , then the encoder declares error. Else, it ﬁnds
g(S) = argmin S − Aβ .
β∈BM,L

ˆ
ˆ
ˆ
The decoder receives β and reconstructs S = Aβ.
Error Analysis: Denoting the probability of error for this
random code by Pe,n , we have
a2
2

2

P (E(S) | |S|2 = z 2 )dν(z 2 )

Pe,n ≤ 1 · P (|S| ≥ a ) +
0

≤ P (|S|2 ≥ a2 ) +

max P (E(S) | |S|2 = z 2 ).

z 2 ∈(0,a2 )

(7)

ˆ
ˆ
{ij : i = j and S(i), S(j) share at least one common term}

where E(S) is the event that the minimum of |S − Aβ|2 over
β ∈ BM,L is greater than D, and ν(|S|2 ) is the distribution
of the random variable |S|2 . The asymptotic behavior of the
ﬁrst term above is straightforward to analyze and is given by
the following lemma, obtained through a direct application of
Cram´ r’s large-deviation theorem [16].
e
Lemma 1:
lim −

n→∞

1
1
log P (|S|2 > a2 ) =
n
2

a2
a2
− 1 − log 2
σ2
σ

nR

is a dependency graph for the family {Ui (S)}e , for each
i=1
ﬁxed S. This follows from Fact 2 by recognizing that each
Ui is a function of a subset of the columns of the matrix A
and the columns of A are picked independently in the code
construction.
Fact 3 (Suen’s Inequality [4]): Let Ui ∼ Bern(pi ), i ∈ I,
be a ﬁnite family of Bernoulli random variables having a
dependency graph Γ. Write i ∼ j if ij is an edge in Γ. Deﬁne

.

λ=

The rest of the proof is devoted to bounding the second term
in (7). Recall that
P E(S) | |S|2 = z 2
ˆ
= P (|S(i) − S|2 ≥ D, i = 1, . . . , enR | |S|2 = z 2 )

1
0

ˆ
if |S(i) − S|2 < D,
otherwise.

EUi , ∆ =
i∈I

1
2

E(Ui Uj ), δ = max
i∈I j∼i

i∈I

EUk .
k∼i

Then
(8)
P

ˆ
where S(i) is the ith codeword in the sparse regression
codebook. We now deﬁne indicator random variables Ui (S)
for i = 1, . . . , enR as follows:
Ui (S) =

Ui (S) = 0 | |S|2 = z 2  .
i=1

IV. P ROOF OF T HEOREM 1

ˆ
β



enR

Ui = 0
i∈I

≤ exp − min

λ λ λ2
, ,
2 6δ 8∆

.

We now apply this inequality with the dependency graph
speciﬁed in Remark 1 to compute an upper bound for (10).
First term λ/2: Since each codeword is the sum of L
columns of A whose entries are i.i.d Normal(0, a2 − D),

(9)

3

ˆ
ˆ
where the third equality is due to the fact that (S(i), S(j))
ˆ
ˆ
has the same joint distribution as (OS(i), OS(j)) for any
ˆ
ˆ
orthogonal (rotation) matrix O. The (Sk (i), Sk (j)) pairs are
i.i.d across k, and each is jointly Gaussian with zero-mean
vector and covariance matrix

E(Ui (S)) does not depend on i. For any ﬁxed S with |S|2 =
z 2 , we have
enR

E(Ui (S)) = enR P (U1 (S) = 1 | |S|2 = z 2 ). (11)

λ=
i=1

Using the strong version of Cram´ r’s large-deviation theorem
e
by Bahadur and Rao [17], we can obtain the following lemma.
Lemma 2: For all sufﬁciently large n and z 2 ∈ (0, a2 ),

Kr = (a2 − D)

i∈{1,...,enR }

k∼i
L−1

ML
1
λ
= L
=
δ
M − 1 − (M − 1)L
−bL − (1 − L−b )Lb
1−L

ˆ

2tD −

=

1
2ta
+ log(1 − 4γ 2 t + 4γ 4 t2 (1 − α2 ))
2 t(1 + α)
1 − 2γ
2
(18)

(19)

γn = log n/2n + κ/n.

(20)

Here and in the sequel, κ denotes a generic positive constant
whose exact value is not needed. Using AL to denote the set
r
{1/L, 2/L, . . . , (L − 1)/L}, α to denote L , and noting that
L
nR
M = e , we have
λ2
>
∆

enR

E Ui (S)Uj (S) | |S|2 = a2 =

M L exp[−n(log(a2 /D) + γn )]
L
L(1−α) e−nCα (t)
α∈AL Lα (M − 1)

exp[−n(log(a2 /D) + γn )]
>
.
L
(L − 1) maxα∈AL Lα M −αL e−nCα (t)

i=1 j∼i

(21)

L
(M − 1)L−r P (Ui (S) = Uj (S) = 1|Eij (r), |S|2 = a2 ) Substituting M = Lb and taking logarithms, we get
r
(14)
λ2
L

log

ˆ
ˆ
where Eij (r) is the event that S(i), S(j) have exactly r
common terms. We have
P Ui (S) = 1, Uj (S) = 1 | Eij (r), |S|2 = a2

n

ˆ
(Sk (i) − a)2 ≤ D,
k=1

1
n

n

k=1

(15)
|S|2

> min

α∈AL

(bLα − 1) log L − log

Lα
−n(log(a2 /D) + γn − Cα (t)) .

(22)

log(λ2 /∆)
b
1
> min bα + (Cα (t) − log(a2 /D)) −
α∈AL
L log L
R
L

ˆ
(Sk (j) − a)2 ≤ D|Eij (r)

λ2 /∆

∆

Dividing through by L log L and using the relation (3) as well
as the deﬁnition (20) for γn , we get

ˆ
ˆ
= P |S(i) − S|2 ≤ D, |S(i) − S|2 ≤ D | Eij (r), |S|2 = a2

1 Here

ˆ
+t(S(j)−a)2

where

Third Term λ2 /∆: We lower bound λ2 /∆ by obtaining a
lower bound for λ2 using Lemma 2, and an upper bound for
the denominator ∆ as follows.1

1
n

2

exp n(2R − log(a2 /D) − γn )
λ2
>
L−1
∆
enR r=1 L (M − 1)L−r e−nCα (t)
r

L1−b

b

=P

ˆ
(Sk (j) − a)2
≤ D|Eij (r)
n

with γ 2 a2 − D.
Using (17) in (15) and then in (14), and using Lemma 2 to
bound λ2 , we obtain

where we have used M = Lb . Since (1 − L−b )L → e−1 ,
using a Taylor expansion we can show that for L sufﬁciently
large
λ
1
≥ Lb−1 . (13)
≥ −(b−1) 1 −2(b−1)
δ
L
− 4L
+ o(L−2(b−1) )

r=1

k=1

Cα (t) = 2tD − log E et(S(i)−a)

∀i ∈ {1, . . . , enR }

Using this together with the expression for λ in (11), we have

2

n

where κ > 0 is a constant and

L
(M − 1)L−r · P U1 (S) = 1 | |S|2 = z 2
r

r=1

e

k=1

ˆ
(Sk (i) − a)2
≤ D,
n

(17)

= M L − 1 − (M − 1)L P U1 (S) = 1 | |S|2 = z 2 .
(12)

nR L−1

n

2

=

1
∆=
2

(16)

1
≤ √ exp(−nCα (t))
κ n

k∼i

P Uk (S) = 1 | |S|2 = z 2

=

1
log P
n

P Uk (S) = 1 | |S|2 = z 2

max

r
α
, where α =
1
L

ˆ
ˆ
when S(i), S(j) share r common terms. Using a twodimensional Chernoff bound, we have ∀t < 0 and sufﬁciently
large n

P (U1 (S) = 1 | |S|2 = z 2 ) ≥ P (U1 (S) = 1 | |S|2 = a2 )
2
κ
a2
n
1
−n 1 log a + log n + n
2
D
2n
≥ √ e− 2 log D = e
κ n
for some constant κ > 0.
We thus have a lower bound on λ for sufﬁciently large n.
Second term λ/δ: Due to the symmetry of the code construction,
δ

1
α

−

a2 .

we directly lower bound on
for
=
Formally, a
lower bound on λ2 /∆ can be obtained using similar steps for |S|2 = z 2 for
z 2 ∈ (0, a2 ), and it can be shown to be decreasing in z 2 .

4

L
log(L log L) log Lα
κ
−
−
.
2L log L
L log L
L log L
(23)

We need the right side of the above to be positive since we
want λ2 /∆ to grow with L. For this, we need:
3
2L

b>

+
α

L
log (Lα)
κ+log log L
L log L +
L log L
2
+ Cα (t)−log(a /D)
R

∀α ∈ AL .

,

V. C ONCLUSION
We have studied a new ensemble of codes for Gaussian
source coding where the codewords are structured linear
combinations of elements of a dictionary. The size of the
dictionary is a low-order polynomial in the block length. We
showed that with minimum-distance encoding, this ensemble
achieves the rate-distortion function with the optimal error
σ2
exponent for all distortions below 4.91 , or equivalently for
rates higher than 1.15 bits per source sample. This value may
be an artifact of some looseness in our bounding techniques,
especially in analyzing the λ2 /∆ term of Suen’s inequality.
σ2
For distortions between 4.91 and σ 2 , we can show that
SPARCs achieve rates which are slightly larger than the
optimal rate-distortion function. This will be discussed in an
extended version of the paper. We note that for i.i.d Gaussian
sources with mean-squared distortion, SPARCs are universal
codes in the following sense: A SPARC designed to compress
a Gaussian source of variance σ 2 with distortion D can compress all Gaussian sources of variance less than σ 2 to within
distortion D. The ﬁnal goal is to develop computationally
feasible encoding algorithms that rapidly approach the ratedistortion bound with growing block length.

(24)

1
Using t = − 2D(1+α) for Cα (t), we can show that (24) implies
the following simpliﬁed condition for sufﬁciently large L:

2.5R
.
R − (1 − D/a2 )

b > bmin

(25)

When b > bmin , the right side of (23) will be strictly positive
for large enough L.
Minimum Rate Condition: For (25) to be valid, we need the
the denominator on the right side to be positive. Since a2 is
2
any number less than ρ2 where R = 1 log ρ , the condition
2
D
for the denominator to be positive is
a2
D
1
log
> 1 − 2.
2
D
a

(26)

This is satisﬁed whenever a2 /D > x∗ as required by the
theorem.
Thus for large enough L, (23) becomes
log(λ2 /∆)
1
(1 − D/a2 )
> (b − bmin ) 1 −
L log L
L
R

R EFERENCES
.

(27)

[1] A. Barron and A. Joseph, “Least squares superposition codes of moderate dictionary size are reliable at rates up to capacity,” IEEE Transactions
on Information Theory, vol. 58, pp. 2541–2557, Feb 2012.
[2] A. Barron and A. Joseph, “Toward fast reliable communication at rates
near capacity with Gaussian noise,” in 2010 IEEE ISIT. Also Yale Dept.
of Stat. Technical Report, 2011.
[3] I. Kontoyiannis, K. Rad, and S. Gitzenis, “Sparse superposition codes
for Gaussian vector quantization,” in 2010 IEEE Inf. Theory Workshop,
p. 1, Jan. 2010.
[4] S. Janson, Random Graphs. Wiley, 2000.
[5] T. M. Cover and J. A. Thomas, Elements of Information Theory. John
Wiley and Sons, Inc., 2001.
[6] M. Eyuboglu and J. Forney, G.D., “Lattice and trellis quantization with
lattice- and trellis-bounded codebooks-high-rate theory for memoryless
sources,” IEEE Trans. Inf. Theory, vol. 39, pp. 46 –59, Jan 1993.
[7] R. Zamir, S. Shamai, and U. Erez, “Nested linear/lattice codes for
structured multiterminal binning,” IEEE Trans. Inf. Theory, vol. 48,
pp. 1250 –1276, June 2002.
[8] A. Gupta, S. Verdu, and T. Weissman, “Rate-distortion in near-linear
time,” in 2008 IEEE Int. Symp. on Inf. Theory, pp. 847 –851.
[9] I. Kontoyiannis and C. Gioran, “Efﬁcient random codebooks and
databases for lossy compression in near-linear time,” in IEEE Inf. Theory
Workshop on Networking and Inf. Theory, pp. 236 –240, June 2009.
[10] S. Jalali and T. Weissman, “Rate-distortion via Markov Chain Monte
Carlo,” in 2010 IEEE Int. Symp. on Inf. Theory.
[11] S. Korada and R. Urbanke, “Polar codes are optimal for lossy source
coding,” IEEE Trans. Inf. Theory, vol. 56, pp. 1751 –1768, April 2010.
[12] D. Sakrison, “A geometric treatment of the source encoding of a
Gaussian random variable,” IEEE Trans. Inf. Theory, vol. 14, pp. 481 –
486, May 1968.
[13] V. Kostina and S. Verd´ , “Fixed-length lossy compression in the
u
ﬁnite blocklength regime: Gaussian source,” in 2011 IEEE Inf. Theory
Workshop.
[14] K. Marton, “Error exponent for source coding with a ﬁdelity criterion,”
IEEE Trans. Inf. Theory, vol. 20, pp. 197 – 199, Mar 1974.
[15] S. Ihara and M. Kubo, “Error exponent for coding of memoryless
Gaussian sources with a ﬁdelity criterion,” IEICE Trans. Fundamentals,
vol. E83-A, p. 18911897, Oct. 2000.
[16] A. Dembo and O. Zeitouni, Large Deviations Techniques and Applications. Springer, 1998.
[17] R. R. Bahadur and R. R. Rao, “On deviations of the sample mean,” The
Annals of Mathematical Statistics, vol. 31, no. 4, 1960.

Therefore for sufﬁciently large L,
1
D
λ2
> L(b−bmin )(1− R (1− a2 )) .
(28)
∆
Combining the bounds obtained above for each of the three
terms, we have for sufﬁciently large n,

enR

Ui (S) = 0 | |S|2 = z 2 ) ≤ e−n min{T1 ,T2 ,T3 }

P(

(29)

i=1

where
n R− 1 log
2

T1 > e

a2
D

κ
− log n − n
2n

1
D
(b−bmin )(1− R (1− a2

T3 > L

))

,

T2 > Lb−1 ,

(30)

.

Using this in (7), we obtain
Pe,n ≤ P (|S|2 ≥ a2 ) +

max P (E(S) | |S|2 = z 2 )

z 2 ∈(0,a2 )

(31)

< e−nT0 + e−n min{T1 ,T2 ,T3 }
2

2

a
1 a
where T0 = 2 ( σ2 − 1 − log σ2 ) from Lemma 1. Since
1
R = 2 log(ρ2 /D), T1 grows exponentially in n for all
a2 < ρ2 n−2/n When b > 2, T2 = Lb−1 grows faster than
n = bL log L/R. For

(b − bmin ) 1 −

1
(1 − D/a2 )
R

> 1,

T3 also grows faster than n. This corresponds to the minimum
value of b speciﬁed in the statement of the theorem. Therefore,
under this condition, the probability of error for large n is
dominated by the ﬁrst term in (31). This completes the proof.

5

