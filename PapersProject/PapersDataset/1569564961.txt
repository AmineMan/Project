Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 14:16:48 2012
ModDate:        Tue Jun 19 12:56:29 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      351087 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569564961

Selecting Two-Bit Bit Flipping Algorithms for
Collective Error Correction
Dung Viet Nguyen, Bane Vasi´ and Michael W. Marcellin
c
Department of Electrical and Computer Engineering
University of Arizona, Tucson, Arizona 85721
Email: {nguyendv, vasic, marcellin}@ece.arizona.edu

In [1], a class of bit ﬂipping algorithms that employ two bits
for decoding LDPC codes over the BSC was proposed. Compared to serial and parallel bit ﬂipping, a two-bit bit ﬂipping
(TBF) algorithm employs one additional bit at a variable node
and one at a check node. The additional bits introduce memory
in the decoding process, which slows down the decoding when
necessary. Initial results showed that decoders which employ a
group of these algorithms operating in parallel lower the error
ﬂoor while maintaining low complexity. However, in [1] we
have not given a complete failure analysis of these algorithms,
nor have we established the methodology to derive good
algorithms and/or a collection of mutually good algorithms.
In this paper, we provide complete failure analysis for TBF
algorithms. More importantly, we give a rigorous procedure to
select groups of algorithms based on their complementariness
in correcting different error patterns. Decoders that employ
algorithms selected using this procedure have provably good
error performance and, by the nature of bit ﬂipping, high
speed.
As one can expect, a TBF algorithm (like other sub-optimal
graph-decoding algorithms) fails on some low-weight error
patterns due to the presence of certain small subgraphs in the
Tanner graph. In this paper, we characterize a special class
of these subgraphs and refer to them with the common term
“trapping sets.” Our deﬁnition of a trapping set for a given
algorithm readily gives a sufﬁcient condition for successful
decoding. The set of all possible trapping sets of a given
decoding algorithm constitutes the algorithm’s trapping set
proﬁle. A unique property of trapping sets for TBF algorithms is that a trapping set proﬁle may be obtained by a
recursive procedure. The diversity among trapping set proﬁles
of different algorithms allows us to select groups of algorithms
such that they can collectively correct error patterns that are
uncorrectable by individual algorithms.
The rest of the paper is organized as follows. Section II
gives the necessary background. Section III gives motivation.
In Section IV, we deﬁne trapping sets, trapping set proﬁles
and describe the recursive procedure for constructing a trapping set proﬁle. Section V discusses the process of selecting
algorithms. Numerical results are given in Section VI.

Abstract—A class of two-bit bit ﬂipping algorithms for decoding low-density parity-check codes over the binary symmetric
channel was proposed in [1]. Initial results showed that decoders
which employ a group of these algorithms operating in parallel
can offer low error ﬂoor decoding for high-speed applications. As
the number of two-bit bit ﬂipping algorithms is large, designing
such a decoder is not a trivial task. In this paper, we describe
a procedure to select collections of algorithms that work well
together. This procedure relies on a recursive process which
enumerates error conﬁgurations that are uncorrectable by a given
algorithm. The error conﬁgurations uncorrectable by a given
algorithm form its trapping set proﬁle. Based on their trapping
set proﬁles, algorithms are selected so that in parallel, they can
correct a ﬁxed number of errors with high probability.

I. I NTRODUCTION
With the introduction of high speed applications such as
ﬂash memory, ﬁber and free-space optical communications
comes the need for fast and low-complexity error control
coding. Message passing algorithms for decoding low-density
parity-check (LDPC) codes such as the sum-product algorithm
(SPA) offer very attractive error performance, especially for
codes with column-weight dc ≥ 4. However, the complexity of
these algorithms is still high and the decoding speed is limited,
mostly due the fact that the operations at variable and check
nodes must be carried out for every edge in the Tanner graph.
For regular column-weight-three LDPC codes, which allow
lower complexity implementation, message passing algorithms
(as well as other classes of decoding algorithms) usually suffer
from high error ﬂoor. This weakness of message passing algorithms in regular column-weight-three LDPC codes justiﬁes
the search for alternatives which offer better trade-offs between
complexity, decoding speed and error performance.
Among existing decoding algorithms for LDPC codes on
the binary symmetric channel (BSC), bit ﬂipping algorithms
are the fastest and least complex. The check node operations
of these algorithms are modulo-two additions while the variable node operations are simple comparisons. The simplicity
of these algorithms also makes them amenable to analysis.
Many important and interesting results on the error correction
capability of the serial and parallel bit ﬂipping algorithms have
been derived (see [1] for a list of references). Unfortunately,
their error performance is typically inferior. As a result, bitﬂipping-oriented algorithms have been largely considered to
be impractical, even after the introduction of some improved
versions, such as the one in [2].

II. P RELIMINARIES
Let C denote an (n, k) binary LDPC code. C is deﬁned by
the null space of H, an m × n parity-check matrix of C. H is

1

Algorithm 1 TBF Algorithm

the bi-adjacency matrix of G, a Tanner graph representation
of C. G is a bipartite graph with two sets of nodes: n
variable (bit) nodes V (G) = {1, 2, . . . , n} and m check nodes
C(G) = {1, 2, . . . , m}; and a set of edges E(G). A (dv , dc )regular LDPC code has a Tanner graph G in which all variable
nodes have degree dv and all check nodes have degree dc . In
this paper, we only consider (dv , dc )-regular LDPC codes. A
subgraph of a bipartite graph G is a bipartite graph U such
that V (U ) ⊂ V (G), C(U ) ⊂ C(G) and E(U ) ⊂ E(G).
G is said to contain U . Furthermore, if Y is a graph which
is isomorphic to U then G is also said to contain Y . In a
bipartite graph G, the induced subgraph on a set of variable
nodes Vs ⊂ V (G) is a bipartite graph U with V (U ) = Vs ,
C(U ) = {c ∈ C(G) : ∃v ∈ Vs such that (v, c) ∈ E(G)} and
E(U ) = {(v, c) ∈ E(G) : v ∈ Vs }.
A vector x = (x1 , x2 , . . . , xn ) is a codeword if and only if
xH T = 0, where H T is the transpose of H. Assume the transmission of the all-zero codeword over the BSC. Denote by y
the channel output vector and denote by xl = (ˆl , xl , . . . , xl )
ˆ
x1 ˆ 2
ˆn
the decision vector after the lth iteration of the iterative
algorithm, where l is a positive integer. At the end of the
lth iteration, a variable node v is said to be corrupt if xl = 1,
ˆv
otherwise it is correct. For the sake of convenience, we let
x0 = y. A variable node v with x0 = 1 is initially corrupt,
ˆ
ˆv
otherwise it is initially correct. Let sl = (sl , sl , . . . , sl )
m
1 2
denote the syndrome vector of the decision vector after the lth
iteration, i.e., sl = xl H T . A check node c is said to be satisﬁed
ˆ
at the beginning of the lth iteration if sl−1 = 0, otherwise it
c
is unsatisﬁed. TBF algorithms are deﬁned as follows.
Deﬁnition 1: The class F of TBF algorithms is given in
l
l
l
Algorithm 1, where zl = (z1 , z2 , . . . , zm ) gives the states of
the check nodes at the beginning of the lth iteration while
l
l
l
wl = (w1 , w2 , . . . , wn ) gives the states of the variable nodes
at the end of the lth iteration. A variable node v takes its
state from the set Av = {0s , 0w , 1w , 1s }, i.e., it can be strong
zero, weak zero, weak one or strong one. A check node takes
its state from the set Ac = {0p , 0n , 1p , 1n }, i.e., it can be
previously satisﬁed, newly satisﬁed, previously unsatisﬁed or
0
newly unsatisﬁed. The state wv of a variable node v is initialized to ∆v (0) ∈ {0s , 0w } if yv = 0 and to ∆v (1) ∈ {1s , 1w }
1
if yv = 1. The state zc of a check node c is initialized
to ∆c (0) ∈ {0p , 0n } if s0 = 0 and to ∆c (1) ∈ {1p , 1n }
c
m
otherwise. A TBF algorithm F = (f, lF , ∆v , ∆c ) iteratively
l
l
updates z and w until all check nodes are satisﬁed or
m
until a maximum number of iteration lF is reached. The
2
check node update function Φ : {0, 1} → Ac is deﬁned
as follows: Φ(0, 0) = 0p , Φ(0, 1) = 1n , Φ(1, 0) = 0n and
Φ(1, 1) = 1p . The variable node update is speciﬁed by a
function f : Av × Ξdv → Av , where Ξdv is the set of
all ordered 4-tuples ξ = (ξ1 , ξ2 , ξ3 , ξ4 ) such that ξi ∈ N
and i ξi = dv . χl p (v), χl n (v), χl p (v) and χl n (v) give the
0
0
1
1
l
number of check nodes with states zc = 0p , 0n , 1p and 1n ,
respectively, that are connected to v. The function f must be
symmetric with respect to 0 and 1 and must allow every state
of a variable node to be reachable from any other state.
What makes a TBF algorithm novel is that a variable node

0
1
∀v : wv ← ∆v (yv ), ∀c : zc ← ∆c (s0 ), l ← 1
c
l
m
while s = 0 and l < lF do
l
l−1
∀v : wv ← f (wv , χl p (v), χl n (v), χl p (v), χl n (v));
1
1
0
0
l+1
∀c : zc ← Φ(sl−1 , sl );
c
c
l ← l + 1;
end while

has “strength” and a check node’s reliability is evaluated based
on its state in the previous iteration.
III. M OTIVATION
Consider a collection A of iterative decoding algorithms
for LDPC codes. Let us assume for a moment that the set of
all uncorrectable error patterns for each and every algorithm
in A is known. More precisely, in the context of LDPC
codes, we assume that the induced subgraphs on such error
patterns can be enumerated for each decoding algorithm. This
naturally suggests the use of a decoder D which employs
multiple algorithms drawn from A . The basis for this use of
multiple algorithms is rather simple: If different algorithms are
capable of correcting different error patterns, then a decoder
employing a set of properly selected algorithms can achieve
provably better error performance than any single-algorithm
decoder. Disappointingly, the above hypothetical assumption
is not valid for most iterative algorithms. For message passing
algorithms such as the SPA, there is no simple criterion to
verify weather or not an arbitrary error pattern is correctable,
much less an explicit methodology to design a decoder which
employs multiple algorithms in a collaborative manner.
Interestingly, for TBF algorithms, we are able to establish a
framework to analyze and enumerate all uncorrectable error
patterns, and this is the main contribution of this paper.
In particular, we characterize the decoding failures of TBF
algorithms by redeﬁning trapping sets and introducing the deﬁnition of trapping set proﬁles. It is an important property of the
newly deﬁned trapping sets that enable us to enumerate them
using a recursive procedure. We remark that the enumeration
of trapping sets is code independent. More importantly, the
concept and explicit construction of trapping set proﬁles allow
rigorous selections of multiple algorithms which can collectively correct a ﬁxed number of errors with high probability.
Given that the selection of multiple algorithms would become
straightforward once the trapping sets/trapping set proﬁles
have been deﬁned and constructed, we devote a considerable
portion of the paper to introducing these two objects. We also
focus on giving criteria for selecting algorithms rather than
explicitly describing the selection process.
IV. T RAPPING S ETS AND T RAPPING S ET P ROFILES
A. Trapping Sets of TBF Algorithms
Although the term trapping set was originally deﬁned as a
set of variable nodes that are not eventually correctable by an
iterative decoding algorithm [3], in the literature it has been

2

used more frequently to refer to a combinatorially deﬁned
subgraph that may be harmful to decoding. The justiﬁcation
for this less rigorous use of terminology is that the variable
node set of a so-called trapping set (a subgraph) would be
an actual set of non-eventually-correctable variable nodes if
the parallel bit ﬂipping algorithm were used (see [4] for
details). Examples of such trapping sets are ﬁxed sets [4] and
absorbing sets [5]. For TBF algorithms, failure analysis can no
longer solely rely on these combinatorial objects. For certain
TBF algorithms, the smallest subgraphs that cause decoding
failures are neither absorbing sets nor ﬁxed sets. We therefore
(re)deﬁne the notion of a trapping set for TBF algorithms, as
we now explain. We ﬁrst introduce the following deﬁnition on
failures of a TBF algorithm.
Deﬁnition 2: Consider a TBF algorithm F and a Tanner
graph G. Let Ve denote the set of variable nodes that are
initially corrupt and let I denote the induced subgraph on Ve .
m
If the algorithm F does not converge on G after lF iterations,
then we say that F fails on the subgraph I of G.
It can be seen that the decoding failure of F is deﬁned
with the knowledge of the induced subgraph on the set of
initially corrupt variable nodes. To characterize failures of F,
a collection of all induced subgraphs I must be enumerated.
While this is difﬁcult in general, for practically important
cases of small numbers of initial errors (less than 8) and small
column-weight codes (dv = 3 or 4), the enumeration of such
induced subgraphs is tractable.
Consider a given Tanner graph I. Let EI (F) denote a set of
Tanner graphs containing a subgraph J isomorphic to I such
that F fails on J. Since EI (F) is undeniably too general to be
useful, we focus our attention on a subset EIr (F) of EI (F),
described as follows.
Deﬁnition 3: Consider a Tanner graph S1 ∈ EI (F) such
that F fails on the subgraph J1 of S1 . Then, S1 ∈ EIr (F) if
there does not exist S2 ∈ EI (F) such that:
1) F fails on the subgraph J2 of S2 , and
2) there is an isomorphism between S2 and a proper
subgraph of S1 under which the variable node set V (J2 )
is mapped into the variable node set V (J1 ).
Now we are ready to deﬁne trapping sets and trapping set
proﬁles of a TBF algorithm.
Deﬁnition 4: If S ∈ EIr (F) then S is a trapping set of F.
I is called an inducing set of S. EIr (F) is called the trapping
set proﬁle with inducing set I of F.
The following proposition states an important property of a
trapping set.
Proposition 1: Let S be a trapping set of F with inducing
set I. Then, there exists at least one induced subgraph J of S
which satisﬁes the following properties:
1) J is isomorphic to I, and
2) F fails on J of S, and
3) Consider the decoding of F on S with V (J) being
the set of initially corrupt variable nodes. Then, for
any variable node v ∈ V (S), there exist an integer
m
l
0 ≤ l ≤ lF such that wv ∈ {1s , 1w }.

Proof: The proof is omitted due to page limits.
From Proposition 1, one can see that the trapping set proﬁle
EIr (F) of F contains the graphs that are most “compact.” We
consider these graphs most compact because for at least one J
isomorphic to I, the decoding of F on such a graph with V (J)
being the set of initially corrupt variable nodes could be made
successful by removing any variable node of the graph. This
special property of trapping sets is the basis for an explicit
recursive procedure to obtain all trapping sets up to a certain
size, which compensates for the lack of a fully combinatorial
characterization of trapping sets. We remark that for certain
reasonably good algorithms, the necessary condition for a
Tanner graph to be a trapping set can be easily derived. Before
describing the recursive procedure for constructing trapping
set proﬁles, we state the following proposition, which gives a
sufﬁcient condition for the convergence of an algorithm F on
a Tanner graph G.
Proposition 2: Consider decoding with an algorithm F on
a Tanner graph G. Let Ve be the set of initially corrupt variable
nodes and I be the induced subgraph on Ve . Then, algorithm
m
F will converge after at most lF decoding iterations if there
does not exist a subset Vs of V (G) such that Vs ⊃ Ve and the
induced subgraph on Vs is isomorphic to a graph in EIr (F).
Proof: Follows from the deﬁnition of EIr (F).
Remark: Proposition 2 only gives a sufﬁcient condition
because the existence of Vs ⊂ V (G) which satisﬁes the
above-mentioned conditions does not necessarily indicate that
G ∈ EI (F).
B. Constructing a Trapping Set Proﬁle
The recursive procedure for constructing a trapping set
proﬁle EIr (F) relies on Proposition 1. Let us assume that
we are only interested in trapping sets with at most nmax
variable nodes. Consider the decoding of F on a Tanner graph
I with V (I) being the set of initially corrupt variable nodes.
Let nI = |V (I)|. If F fails on the subgraph I of I then
EIr (F) = {I} and we have found the trapping set proﬁle. If
F does not fail on the subgraph I of I, then we expand I by
recursively adding variable nodes to I until a trapping set is
found. During this process, we only add variable nodes that
become corrupt at the end of a certain iteration.
Consider all possible bipartite graphs obtained by adding
one variable node, namely vnI +1 , to the graph I such that
when the decoding is performed on these graphs with V (I)
being the set of initially corrupt variable nodes, the newly
added variable node is a corrupt variable node at the end of
1
the ﬁrst iteration, i.e., wvn +1 ∈ {1w , 1s }. Let OI denote the
I
set of such graphs. Take one graph in OI and denote it by U .
Then, there can be two different scenarios in this step. First,
F does not fail on the subgraph I of U . In this case, U is
certainly not a trapping set and we put U in a set of Tanner
graphs denoted by E1 . Second, F fails on the subgraph I of
I
U . In this case, U can be a trapping set and a test is carried
out to determine if U is indeed one. If U is not a trapping
set then it is discarded. We complete the formation of E1 by
I
repeating the above step for all other graphs in OI .

3

Let us now consider a graph U ∈ E1 . Again, we denote
I
by OU the set of Tanner graphs obtained by adding one
variable node, namely vnI +2 , to the graph U such that when
the decoding is performed on these graphs with V (I) being
the set of initially corrupt variable nodes, the newly added
variable node is a corrupt variable node at the end of the ﬁrst
1
iteration, i.e., wvn +2 ∈ {1w , 1s }. It is important to note that
I
the addition of variable node vnI +2 , which is initially correct,
cannot change the fact that variable node vnI +1 is also corrupt
at the end of the ﬁrst iteration. This is because the addition of
correct variable nodes to a graph does not change the states
of the existing check nodes and the decoding dynamic until
the newly added variable nodes get corrupted. Similar to what
have been discussed before, we now take a graph in OU and
determine if it is a trapping set, or it is to be discarded, or it is
a member of the set of Tanner graph E2 . By repeating this step
I
for all other graphs in E1 , all graphs in E2 can bemax
enumerated.
I
I
(n
−nI )
In a similar fashion, we obtain E3 , E4 , . . . , EI
. For
I
I
0
the sake of convenience, we also let EI = {I}.
At this stage, we have considered one decoding iteration
on I. It can be seen that if S is a trapping set with at most
nmax variable nodes then either S has been found, or S must
(nmax −nI −1) i
contain a graph in i=0
EI . Therefore, we proceed
(nmax −nI −1) i
by expanding graphs in EI = i=0
EI .
(nmax −nI −1) i
Let K denote a Tanner graph in EI = i=0
EI .
We now repeat the above graph expanding process with K
being the input. Speciﬁcally, we ﬁrst obtain OK , which is
deﬁned as the set of all Tanner graphs obtained by adding one
variable node vnK +1 to the graph K such that when decoding
is performed on these graphs with V (I) being the set of
initially corrupt variable nodes, the newly added variable node
is a corrupt variable node at the end of the second iteration,
but not a corrupt variable node at the end of the ﬁrst iteration,
1
2
i.e., wvn +1 ∈ {0w , 0s } and wvn +1 ∈ {1w , 1s }. Graphs in
K
K
OK that are not trapping sets are either discarded or to form
the set E1 . By recursively adding variable nodes, graphs in
K
max
E2 , E3 , . . . , En −nI are enumerated.
K
K
K
One can see that there are two recursive algorithms. The
(nmax −nI ) i
ﬁrst algorithm enumerates graphs in EK = i=0
EK
for a given graph K by recursively adding variable nodes.
The second algorithm recursivelymax the ﬁrst algorithm to
calls
(n
−nI ) i
enumerates graphs in EK = i=0
EK for each graph
max
(n
−nI −1) i
EI . Each recursion of the second
K in EI = i=0
algorithm corresponds to a decoding algorithm. As a result,
m
the trapping set proﬁle is obtained after lF recursions of the
second algorithm.

values of ∆v , and two possible values of ∆c . However, with
a given ∆c , the two sets of algorithms F that correspond to
two possible ∆v are identical (as 0s and 0w , 1s and 1w can
be interchanged). Consequently, if we disregard the maximum
number of iterations, then |F | = 2|Q| ≤ 2(4|Ξdv |+1) . One can
+3
easily show that |Ξdv | = dv3 . Therefore, an upper-bound
on the number of TBF algorithms is:
|F | ≤ 2

2dv 3 +12dv 2 +22dv +15
3

.

(1)

For example, this upper-bound is 281 when dv = 3, and is
2141 when dv = 4.
Due to the huge number of possible algorithms, it is
necessary to focus on a small subset of algorithms. This subset
of algorithms may be obtained by imposing certain constraints
on the function f . One example of such a constraint is as
follows: if f (0s , ξ) ∈ {1w , 1s } then f (0w , ξ) ∈ {1w , 1s }.
This constraint requires that when a strong zero variable
node is ﬂipped with a given combination of check nodes, a
weak variable node is also ﬂipped with the same check node
combination. Other constraints on f are derived by analyzing
possible transitions of variable nodes and check nodes for a
small number of iterations.
B. Selecting a TBF Algorithm
We ﬁrst discuss the main criterion to select one algorithm
among all possible algorithms. Let nmin be the smallest numI,F
ber of variable nodes of Tanner graphs in EIr (F). We would
like to select an algorithm F such that nmin is maximized. The
I,F
justiﬁcation for this selection criterion relies on the following
proposition, whose proof is omitted due to page limits.
Proposition 3: Given three random Tanner graph G, S1 , S2
with 0 < |V (S1 )| < |V (S2 )| < |V (G)|, the probability that
G contains S2 is less than the probability that G contains S1 .
From Proposition 3, one can see that the larger the number
|V (S)| of a given Tanner graph S is, the easier it would be
(if at all possible) to construct a Tanner graph G that does not
contain S. Therefore, a larger nmin means that the sufﬁcient
I,F
condition for the convergence of F can be met with higher
probability. In this sense, an algorithm F with a larger nmin
I,F
is more favorable.
If for two algorithms F1 and F2 , nmin1 = nmin2 , then
I,F
I,F
one can derive other comparison criteria based on EIr (F1 )
and EIr (F2 ), and/or compare F1 and F2 with a different
assumption of I. For example, the probability of a graph G
containing a trapping set S can be also be evaluated based on
|C(S)|.
C. Selecting Multiple TBF Algorithms

V. S ELECTING TBF A LGORITHMS

We now consider the problem of selecting of multiple
algorithms. The basis for this selection is that one should select
good individual algorithms with diverse trapping set proﬁles.
In this paper, we only consider decoder D with algorithms
F1 , F2 , . . . , Fp operating in parallel, i.e., the received vector
of the channel is the input vector for all algorithms. Note
that one can also use trapping set proﬁles to select algorithms
that operate in serial, i.e., the output from one algorithm is

Due to page limits, we only summarize the most important
criteria for selecting TBF algorithms. Let us ﬁrst brieﬂy
discuss the number of possible algorithms.
A. On the Number of Algorithms
Let Q be the set of all functions from Av × Ξdv → Av that
satisfy the symmetry and the irreducibility condition. Due the
symmetry condition, |Q| ≤ 42×|Ξdv | . There are two possible

4

−2

10

the input to another. For a decoder D that employs parallel
algorithms, the concept of trapping sets and trapping set
proﬁles can be deﬁned in the same manner as trapping sets
and trapping set proﬁles for a single TBF algorithm. One can
easily modify the recursive procedures given in Section IV-B
to generate trapping set proﬁles of the decoder D. Then, D can
be designed with the same criterion discussed in the previous
subsection.
Remark: Knowledge on the Tanner graph of a code C can
be used in the selection of algorithms. For example, if it is
known that the Tanner graph of C does not contain a certain
subgraph Y , then all graphs containing Y must be removed
from a trapping set proﬁle.

D - C732
−3

Frame Error Rate (FER)

10

SPA - C732
D - Tanner
SPA - Tanner

−4

10

−5

10

−6

10

−7

10

−8

VI. N UMERICAL R ESULTS

10

As an example, we describe a selection of TBF algorithms
for regular column-weight-three LDPC codes with girth g = 8.
For simplicity, we let ∆v = (0s , 1s ), ∆c = (0p , 1p ) and
m
lF = 30 for all algorithms. By imposing certain constraints
on the functions f , we obtain a set of 21, 962, 496 TBF
algorithms. Out of these, there are 360, 162 algorithms which
can correct any weight-three error pattern. Such an algorithm
is capable of correcting any weight-three error pattern because
its trapping set proﬁle EIr (F) with any inducing set I containing three variable nodes is empty. Since all weight-three
error patterns can be corrected with a single algorithm, our
next step is to select a collection of algorithms which can
collectively correct weight-four and -ﬁve error patterns with
high probability. To achieve this goal, we construct all trapping
set proﬁles with inducing sets containing four and ﬁve variable
nodes for each algorithm. Note that there are 10 possible
inducing sets (Tanner graphs with girth g = 8) containing
four variable nodes and 24 possible inducing sets containing
ﬁve variable nodes. Hence, for each algorithm, we construct a
total of 34 trapping set proﬁles. From the trapping set proﬁles
of all algorithms, we select a collection of 35 algorithms based
on the criterion mentioned in the previous section. Then, we
simulate the performance of a decoder D which employs these
algorithms in parallel. The maximum total number of iterations
of D is 35 × 30 = 1050.
Figure 1 shows the frame error rate (FER) performance of
D on the (155, 64) Tanner code. This code has dv = 3, dc = 5
and minimum distance dmin = 20. For comparison, the FER
performance of the SPA with a maximum of 100 iterations is
also included. It can be seen that the FER performance of D
approach (and might surpasses) that of the SPA in the error
ﬂoor region. It is also important to note that if we eliminate
all trapping sets containing subgraphs that are not present in
the Tanner graph of this code, then all the obtained trapping
set proﬁles are empty. This indicates that D can correct any
error pattern up to weight 5 in the Tanner code.
Figure 1 also shows the FER performance of D on a quasicyclic code C732 of length n = 732, rate R = 0.75 and
minimum distance dmin = 12. The FER performance of the
SPA is also included for comparison. It can be seen that the
slope of the FER curve of D in the error ﬂoor region is higher

−3

10

−2

10
Cross-over probability (α)

Fig. 1: Frame error rate performance of the decoder D.
than that of the SPA. Finally, we remark that the slope of the
FER curve of D in the error ﬂoor region is between 5 and
6, which indicates that D can correct error patterns of weight
4 and 5 with high probability. This also agrees with the fact
that in our simulation, no weight-four error pattern that leads
to decoding failure of D was observed.
We remark that the implementation of TBF algorithms
operating in parallel can be done with a relatively small
number of common logic gates. For example, if a decoder
D employs both the TBFA1 and the TBFA2 given in [1],
then the implementation of the variable node updates require
less than 800 AND-gate inputs and 100 OR-gate inputs. In
comparison, the implementation of a 6-bit adder requires 2196
AND-gate inputs and 355 OR-gate inputs while that of a 6-bit
comparator requires 1536 AND-gate inputs and 190 OR-gate
inputs. One can also expect that the complexity introduced
by an additional algorithm would decrease as the number of
algorithms increases, because many min-terms in the variable
node update logic functions would be already available. More
details will be provided in the journal version of this paper.
ACKNOWLEDGMENT
This work is funded by NSF under the grants CCF-0963726,
CCF-0830245.
R EFERENCES
[1] D. V. Nguyen, M. W. Marcellin, and B. Vasic, “Two-bit bit ﬂipping
decoding of LDPC codes,” in Proc. IEEE Int. Symp. Inform. Theory,
St. Petersburg, Russia, Jul. 31–Aug. 5 2011, pp. 1995–1999.
[2] N. Miladinovic and M. Fossorier, “Improved bit-ﬂipping decoding of lowdensity parity-check codes,” IEEE Trans. Inf. Theory, vol. 51, no. 4, pp.
1594–1606, Apr. 2005.
[3] T. J. Richardson, “Error ﬂoors of LDPC codes,” in Proc. 41st Annual
Allerton Conf. on Communications, Control, and Computing, Allerton
House, Monticello, IL, USA, Oct. 1–3 2003, pp. 1426–1435.
[4] D. V. Nguyen, S. K. Chilappagari, B. Vasic, and M. W. Marcellin, “On
the construction of structured LDPC codes free of small trapping sets,”
IEEE Trans. Inf. Theory, vol. 58, no. 4, pp. 2280–2302, Apr. 2012.
[5] L. Dolecek, Z. Zhang, V. Anantharam, M. J. Wainwright, and B. Nikolic,
“Analysis of absorbing sets and fully absorbing sets of array-based LDPC
codes,” IEEE Trans. Inf. Theory, vol. 56, no. 1, pp. 181–201, Jan. 2010.

5

