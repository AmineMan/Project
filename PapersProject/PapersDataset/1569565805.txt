Creator:         TeX output 2012.05.17:1424
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 15:18:16 2012
ModDate:        Tue Jun 19 12:54:35 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      292019 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565805

Rate Distortion Codes for the Collective Estimation
from Independent Noisy Observations
Tatsuto Murayama

Peter Davis

NTT Communication Science Laboratories, NTT Corporation
Atsugi-shi, Kanagawa 243-0198, Japan
Email: murayama.tatsuto@lab.ntt.co.jp

Telecognix Corporation
Sakyo-ku, Kyoto 606-8314, Japan
Email: davis@telecognix.com

lossy coding of each individual observations enables us to use
as many sensors as possible, since an observation could be
coded and compressed beyond its entropy rate [4].
The latter case involves a new type of tradeoff, i.e., the
tradeoff between the effect of reducing collective sensing error
with increasing number of sensors, and the effect of increasing
aggregation error with increasing number of sensors due to the
need to reduce the amount of information below the entropy
rate. This tradeoff and its implications for optimal size of
systems is the problem that we address in this work. Due to
the ongoing growth of practical network sensing systems it is
expected to be a problem of rapidly increasing relevance and
signiﬁcance [5].
In this paper, we consider a fundamental formulation of the
problem with only one information source and suppose that
all sensors are symmetrical, i.e., exchangeable with respect
to their contributions to the ﬁnal result of aggregation. This
allows us to treat our problem in terms of the theory of large
deviations. Such a class of problems is often termed as the
CEO problem [6]. Here we present a rigorous result by reﬁning
the heuristic argument in our previous work [7]. The paper
is divided into 5 sections. Section II provides mathematical
preliminaries for the following sections. Section III presents
our system model. The relation between the CEO problem
and the problem treated in this work is also mentioned
here. Section IV brieﬂy summarizes our main result and a
brief proof for the proposition. Numerical ﬁndings are also
presented here, including the ﬁgures of optimal data rates and
the corresponding information gain which could be obtained
by using the lossy rate distortion codes. Conclusions are given
in Section V.

Abstract—We present a collective behavior in the optimal
aggregation of noisy observations of a source. The quality of
estimation of the source state involves a difﬁcult tradeoff between
sensing quality which increases by increasing the number of
sensors, and aggregation quality which decreases if the number of
sensors is too large. We analytically study the optimal strategy for
large scale aggregation, and obtain an explicit and exact result by
introducing a basic model. We show that larger scale aggregation
always outperforms smaller scale aggregation at higher noise
levels, while below a critical value of noise, there exist moderate
scale aggregation levels at which optimal estimation is realized.
We also examine the practical tradeoff between the above two
aggregation strategies by applying an iterative encoding to linear
codes.

I. I NTRODUCTION
Device and sensor networks are shaping many activities in
our society. These networks are being deployed in a growing
number of applications as diverse as agricultural management,
industrial controls, crime watch, and military applications.
Indeed, sensor networks can be considered as a promising
technology with a wide range of potential future markets.
Still, for all the promise, it is often difﬁcult to integrate
the individual components of a sensor network in a smart
way. Although we see many breakthroughs in component
devices, advanced software, and power managements, systemlevel understanding of the emerging technology is still weak.
It requires a shift in our notion of ‘what to look for’ [1]. New
properties can be revealed by the study of collective behavior
and resulting trade-offs. This is the issue that we address in
this paper.
In this paper, we consider the problem of reducing sensing
error by collecting observations from many sensors. However,
collecting observations from many sensors usually involves
some cost in terms of network resources, or system communication capacity [2]. If the amount of information transmitted
for each observation is ﬁxed for a given capacity, there would
be a limit on the number of sensors, and on the collective
sensing quality. The number of sensors, and thus the sensing
quality, could increase when it is possible to reduce the
amount of information transmitted for each observation. This
could be done by applying a compression scheme. If we
restrict ourselves to lossless compression of a given set of
observations, the number of sensors is strictly upper bounded
by the system capacity, or the sum rate [3]. On the other hand,

II. M ATHEMATICAL P RELIMINARIES
We brieﬂy review the following two notions before detailing
the system model in Section III. The rate distortion function
deﬁnes optimal data rates for independent noisy observations,
while the large deviation property provides a theoretical basis
for the optimality measure.
A. Rate Distortion Function
Let Xµ be a discrete random variable with alphabet Γ =
{1, −1}. Assume that we have a source that produces a

1

sequence X1 , X2 , . . . , XM , where each symbol is randomly
drawn from the distribution, say

B. Large Deviation Property
Suppose that a collection of Xµ is a Bernoulli(p) sequence for µ = 1, . . . , N . Then the macroscopic behavior of the coin could be captured by a single number;
∑N
MN = (1/N ) i=1 Xµ . This is the average value of the
N Bernoulli(p) trials. Deﬁne PN to be the distribution of
the sample mean MN and assume that a number m is the
mean value of Xµ . Suppose that A(x, ϵ) denotes the interval
(x − ϵ, x + ϵ). Then the law of large numbers says that for any
ϵ > 0,

ρ(x) = pδ(x, 1) + (1 − p)δ(x, −1)
for p in the interval [0, 1]. A sequence of observations Xµ is
called the Bernoulli(p) trials. Suppose that we have a pair of
an encoder and a decoder. The encoder describes the M -bit
state X, Xµ for µ = 1, . . . , M , by an N -bit codeword Zν for
ν = 1, . . . , N . The number M represents the length of a source
sequence, while N represents the length of a codeword. The
ˆ
decoder represents Xµ by an estimate Xµ given a collection
of Zν . The rate is deﬁned by R = N/M in the interval (0, 1].
A distortion function is a mapping d : Γ × Γ → R+
from the set of source alphabet-reproduction alphabet pairs
into the set of non-negative real numbers. Intuitively, the
ˆ
distortion d(Xµ , Xµ ) is a measure of the cost of representing
ˆ
the symbol Xµ by the symbol Xµ . This deﬁnition is quite
general. Hereafter, the Hamming distortion measure is adopted
ˆ
ˆ
as the ﬁdelity criterion; d(Xµ , Xµ ) = 1 − δ(Xµ , Xµ ) where
δ is the Kronecker’s delta function. The distortion measure
is so far deﬁned on a symbol-by-symbol basis. We extend
ˆ
the deﬁnition to sequences, which is deﬁned by d(X, X) =
∑M
ˆ µ ). Therefore, the distortion for a se(1/M ) µ=1 d(Xµ , X
quence is the average distortion per symbol of the elements of
the sequence. Finally, the distortion associated with the code is
ˆ
deﬁned as D = ⟨d(X, X)⟩ where the braket denotes averaging
over random variables Xµ .
A rate distortion pair (R, D) should be achievable if a
ˆ
sequence of rate distortion codes exist with ⟨d(X, X)⟩ ≤ D
in the limit M → ∞. Moreover, the closure of the set of
achievable rate distortion pairs is called the rate distortion
region for a source. Now we can deﬁne a function to describe
the boundary.
Deﬁnition 1 (Rate Distortion Function): The rate distortion function R(D) is the inﬁmum of rates R, so that (R, D) is
in the rate distortion region of the source for a given distortion
D.
We now ﬁnd the description rate R(D) required to describe
the Bernoulli(p) source with an expected proportion of errors
less than or equal to D. In this simpliﬁed case, according to
Shannon, the boundary can be written as follows. The rate
distortion function for a Bernoulli(p) source with Hamming
distortion is given by
{
H2 (p) − H2 (D) (0 ≤ D ≤ p)
R(D) =
,
0
(p ≤ D)

PN {A(m, ϵ)} = P {(X1 , . . . , XN ) ∈ Ω; MN ∈ A(m, ϵ)} → 1
as N → ∞. In other words, if N is large enough, almost
all the realizations of microscopic MN are very close to the
macroscopic mean value m. In contrast, if |x − m| > ϵ then
we observe PN {A(x, ϵ)} → 0 in the limit N → ∞. We reﬁne
the latter statement to claim that the probability decays to zero
exponentially fast. The key concept is given below.
Deﬁnition 2 (Large Deviation Property): Suppose that a
set A does not contain m. The sequence {MN } satisﬁes a
large deviation property if there exists I(x) such that
lim

N →∞

1
PN {A} = − inf I(x) .
x∈A
N

(1)

It is possible to calculate I(x) using the distribution ρ(x) if
the above statement holds. Below we give a brief prescription
for the algebra without a proof. Suppose that t is a real number.
Deﬁne for any t,
∫ ∞
c(t) = ln⟨exp(tXµ )⟩ = ln
dxρ(x) exp(tx) .
−∞

The function c(t) is called the cumulant generating function
of ρ(x). Here it is an easy matter to check that c(t) is convex.
Now assume that c(t) < ∞ for all t. Then it is possible to
give its Legendre transform and write
I(x) = sup{tx − c(x)} .

(2)

t∈R

The function I(x) is a convex function which provides the
exponential rate of decay in the formula (1). Assume that
{Xµ } is a Bernoulli(p) sequence. By setting Γ = {1, −1},
we have c(t) = ln{pet + (1 − p)e−t }. Here c(t) is convex and
thus tx−c(t) is concave. Hence tx−c(t) attains its supremum
at t if and only if
exp (2t) =

where we deﬁne the binary entropy function as H2 (p) =
−p ln p − (1 − p) ln(1 − p). Its inverse function, the distortionrate function D(R), could be also deﬁned since R(D) is
a monotonically decreasing function. We may use either
the distortion-rate function or the rate-distortion function to
describe the optimal boundary, since the two descriptions are
equivalent in the large M limit [8]. The D(R) is known as
the theoretical lower bound of average distortion for a given
rate R.

1−p1+x
.
p 1−x

Then it is straightforward to get the rate function as
(1 − x)
(1 + x) (1 + x)
−
ln p −
ln(1 − p)
Ip (x) = −H2
2
2
2
(3)
for −1 ≤ x ≤ 1 and ∞ otherwise. Notice that the form of
Ip (x) depends on the choice of Γ. By setting Γ = {0, 1}

2

and ρ(x) = pδ(x, 1) + (1 − p)δ(x, 0), we have yet another
representation. That is,

( )
(
)
x ln x + (1 − x) ln 1 − x
(0 ≤ x ≤ 1)
p
1−p
Ip (x) =
.
∞
(otherwise)
(4)

ˆ
collective estimator X. We analyze the behavior of the bit error
probability, denoted pe (p, R; C), in the collective estimate.
First, we assume that the average error due to lossy compression is independent of µ and a, and denoted by D, that
is

In short, if the large deviation property holds, the rate
function Ip (x) could be identiﬁed with the rate of decay.
However, its actual form depends on the choice of alphabets
for the Boolean variable Xµ . In contrast, the form of the rate
distortion function R(D) for the Bernoulli(p) sequence does
not depend on the individual choice of the state space.
On the other hand, as far as {Xµ } is a sequence of i.i.d.
random variables, the next proposition tells us that the property
is actually satisﬁed. It is known as Cram´ r’s Theorem in
e
probability theory [9].
Proposition 3 (Cram´ r’s Theorem): Suppose that c(t) is ﬁe
nite for all real number t. Then the sequence {MN } satisﬁes
the large deviation property (1) with the rate function deﬁned
to be (2).
Since the above statement holds for the Bernoulli(p) trials,
the rate function Ip (x) deﬁned to be (3) provides the decay
rate for the Γ = {−1, 1}. Similarly, we have (4) for the choice
of Γ = {0, 1}. In section IV, we use the latter formula in the
analysis.

Here we used Kronecker’s delta δ and the braket denotes
averaging over random variables. This includes the standard
exchangeable sensor ansatz for our model [6], [10], which
means that all sensors have the same distortion D for a given
rate R. The possible value of the distortion D depends on R,
so we explicitly denote D as D(R). In particular, the smallest
average distortion D(R) is obtained in the limit of M → ∞,
and is called the distortion-rate function for the source Y (a)
(See deﬁnitions in Section II). Now assume hereafter that
D(R) represents the distortion-rate function for Y (a).
ˆ
The combined error probability for Yµ (a), independent of
µ and a, is obtained as

ˆ
⟨δ(Yµ (a), −Yµ (a))⟩ = D .

ρ = (1 − 2p)D(R) + p .

The combined error probability ρ is a function of both p and R.
In particular, the eq. (6) implies that ρ is a decreasing function
of R, since D(R) should be a decreasing function of R. It is
obvious that pe (p, R; C) is a decreasing function of L if ρ is
ﬁxed. However, due to the constraint (5), and the decrease in
distortion D(R) with increase of R, ρ actually increases with
an increase of L, resulting in contrary effects on pe (p, R; C).
Therefore the challenge here is to incorporate consideration of
the distortion D in a way which clariﬁes the interplay between
the contrary effects induced by the constraint (5).

III. P ROBLEM S TATEMENT
This section provides our system model for the sensing and
data aggregation tasks described in Section I. In contrast with
the original CEO problem, the optimal joint decoding task is
replaced by the simple majority vote procedure for a collection
of independently decoded observations. We also deﬁne the
optimality criterion for the system which will be used in the
following analysis.

B. Estimate
In this paper we use symmetric notations, i.e. Xµ , Yµ (a) =
±1 and similarly for their estimates. Assume that the original
Xµ are i.i.d. random variables. That is, the underlying M -bit
ˆ
state X is a Bernoulli sequence. Let Yµ be an L-bit collection
ˆ
ˆ
of µth symbols Yµ (a) of M -bit reproductions Y (a) for a =
ˆ
1, . . . , L. Then the aggregator observes the Yµ and obtain an
estimate Xµ for a µ given. Write the posterior probability
ˆ
ˆ
of Xµ given the L-bit collection Yµ as P (Xµ |Yµ ). Then the
ˆ
average error probability for the estimate Xµ is

A. System Model
Suppose that we have L independent sensors which each independently observe an M -bit state X, Xµ for µ = 1, . . . , M ,
of a common, uniform binary source, and obtain an M bit observation Y (a) (a = 1, . . . , L) where each bit Yµ (a)
has common probability p of error, i.e. differing from the
corresponding source bit Xµ . The value of p speciﬁes the level
of observation noise. Now the sensors independently compress
their M -bit observation into shorter N -bit codewords, Z(a),
and send them to the aggregator. The condition ‘independent’
excludes the possibility of mutual communications between
sensors. We assume the rate R = N/M is common to all the
sensors. In addition, we suppose that the sum total of the rate,
the system capacity C, is ﬁxed, with
C = LR .

(6)

ˆ
1 − Xµ mµ
,
2
∑
ˆ
where we deﬁne a number mµ =
Xµ ±1 Xµ P (Xµ |Yµ ) =
ˆ
ˆ
ˆ
P (+1|Yµ )−P (−1|Yµ ) for any µ. Since Xµ = ±1, the optimal
ˆ
ˆ
estimate Xµ should be Xµ = sign(mµ ). In other words, if
M -bit X is a collection of i.i.d. random variables, the optimal
ˆ
estimate Xµ could be given by calculating mµ for each µ
independently.
Assume for simplicity that the X is a collection of purely
random variables Xµ . Then the Bayes formula says that for
ˆ
every µ the optimal estimate Xµ from the set of aggregated
ˆ
⟨δ(Xµ , −Xµ )⟩ =

(5)

The aggregator then decodes every N bit codeword indeˆ
pendently to obtain L separate M -bit reproductions Y (a)
ˆ
(a = 1, . . . , L). Finally, the Y (a) are used to obtain a single

3

ˆ
values Yµ could be obtained by the majority vote operation.
That is,
{∑
}
L
ˆ
ˆ
Xµ = sgn
Yµ (a) .

for a = 1, . . . , L. Since the variables p, D are in the interval
[0, 1/2), it follows that ρp (R) is also in [0, 1/2). Consider the
sample average deﬁned to be

(7)

1∑
Zµ (a) .
L a=1
L

Mµ,L =

a=1

In the rest of this paper, we only consider the case that Xµ
obey the Bernoulli(1/2) distribution.

Since the expectation ⟨Zµ (a)⟩ = ρp (R) is ﬁnite, we know
that Mµ,L is approaching ρp (R) by the law of large numbers.
However the value of interest is the error probability P {Xµ ̸=
ˆ
Xµ } for the majority vote procedure, which is identical to
P {Mµ,L ≥ 1/2} for the estimate (7). For 0 ≤ ρ < 1/2 and
thus |ρ − 1/2| > 0, the vanishing P {Mµ,L ≥ 1/2} is called
the large deviation probability.
Consider the rate function of Zµ (a). Since
Zµ (1), Zµ (2), . . . , Zµ (L)
are
the
L
independent
Bernoulli(ρp (R)) random variables, the Legendre transform
gives the rate function Iρp (R) (z) for the sample average Mµ,L
as

C. Optimality Measure
Assume that a network capacity C is given. Consider that
the common rate R is ﬁrst allocated to all the sensors. The
number of sensors L is thus determined as the maximum value
of L satisfying the sum rate constraint RL ≤ C. Since C (and
also L) should be a sufﬁciently large number, it is natural to
think that (5) holds in the limit. In our system model, it is
ˆ
obvious to say that P {Xµ ̸= Xµ } → 0 as C → ∞. As
is shown in Section IV, it is not hard to reﬁne the above
ˆ
statement of convergence and to prove that P {Xµ ̸= Xµ }
decays to 0 exponentially fast as C → ∞. By analogy with
large deviation theory [9], we deﬁne the exponential rate of
decay by

Iρp (R) (z) = z ln

1−z
z
+ (1 − z) ln
ρp (R)
1 − ρp (R)

for 0 ≤ z ≤ 1 and ∞ otherwise (See equation (4) in
Section II). Since the number of sensors L is given by C/R,
changing the variable C = LR yields
{
}
1
z
1−z
(1)
Ip,R (z) =
z ln
+ (1 − z) ln
.
R
ρp (R)
1 − ρp (R)

1
ˆ
ln P {Xµ ̸= Xµ } (0 < R ≤ 1) .
C→∞ C

Ip (R) = − lim

The decay rate Ip (R) describes the limiting behavior of
the system from a macroscopic level, on which the rate R
could be used as a control parameter. The case of R = 1
reduces to a naive aggregation scheme in which the sensors
just send their noisy observations to the observer. For this
smallest aggregation, we aggregate data from only L = C
sensors. Hereafter, we call this scheme the level-1 aggregation.
For a given R > 0, the level-R aggregation is deﬁned in
which every sensor encodes its observations at the rate of R
independently. As an extension of the deﬁnition of Ip (R) for
R > 0, we could naturally deﬁne the level-0 decay rate as
Ip (0) = limR→0 Ip (R).

Since the set Aµ = {Mµ,L ≥ 1/2} is closed and does not
contain ρp (R), the large deviation property (1) tells that
1
(1)
ln P {Aµ } = − min Ip,R (z) .
z∈Aµ
C→∞ C
lim

(1)

(1)

Then it is an easy matter to check that Ip,R (z) = Ip,R (1/2).
(1)
Write Ip (R) = Ip,R (1/2) for the convenience. For a given R
we conclude that

IV. L ARGE S YSTEM A NALYSIS

Ip (R) = − lim

C→∞

In this section, we demonstrate a collective behavior in the
large scale aggregation of independent noisy observations by
using a simple majority vote model introduced in Section III.
We ﬁrst prove the main statement by using the concepts
described in Section II, and then resort to numerical analysis
of the given formula.

1
ln P {Aµ } .
C

This completes the proof for the following proposition, which
is the main result of this paper.
Proposition 4: We have
Ip (R)
{
}

− 1 ln 2 + ln ρp (R) + ln(1 − ρp (R))
R
2
2
=

(1 − 2p)2 ln 2

A. Statement of Exact Result
Assume that D(R) denotes the distortion rate function,
which is the inverse function of R(D). Write the error
ˆ
indicator function δ(Xµ , −Yµ (a)) as Zµ (a). For a given µ,
this is a Bernoulli random variable that takes the value 1 with
probability

(8)
(0 < R ≤ 1)
(R = 0)

The maximum of Ip (R) is of great interest from an engineering point of view. That is, we prefer larger values of Ip (R).
Below, we numerically examine the optimal levels deﬁned by
R∗ = argmax0≤R≤1 Ip (R). The optimal aggregation, for a
given p, is called the level-R∗ aggregation.

ρp (R) = (1 − 2p)D(R) + p .

4

.

0.025

1
theory
data
0.8

0.020

0.6

0.015

0.4

0.010

0.2

0.005

0
0

0.1

0.2

p

0.3

0.4

0
0

0.5

0.1

0.2

0.3

0.4

0.5

p
Fig. 2. Information gain. Solid line denotes the difference Ip (R∗ ) − Ip (1).
Ip (R∗ ) is the decay rate with lossy compression at the optimal aggregation
rate R∗ , while Ip (1) is the decay rate without compression. Circles (k = 2)
and squares (k = 3) indicate practical performance of iterative encoding by
the linear codes when C = 50, respectively.

Fig. 1. Optimal aggregation levels for ensemble of independent sensors in
noisy environment. The p is a given noise level. The solid line denotes the
optimal data rate R∗ per sensor, which maximizes the exponential decay rate
Ip (R) of vanishing error probability with increase of system capacity C. For
comparison, the circles represent the numerical simulations with C = 50 for
the optimal R(D).

still room for improvement by using alternative techniques.
B. Onset of Collective Behavior

V. C ONCLUSIONS

We now examine the behavior of formula (8) which gives
the optimal levels R∗ for the noise p. As is seen in Figure 1, the
optimal aggregation scale diverges, i.e., the optimal data rate
R∗ per sensor diverges for noise levels larger than the critical
point p0 = 0.211. In this noisy region, we want the system
to be as large as possible. The larger the system we have,
the smaller the error probability. By deﬁnition, the optimal
aggregation is said to be level-0. In contrast, we can always
ﬁnd the non-zero optimal levels below p0 . In particular, if the
noise level is below p1 = 0.024, our investigations indicate
that the level-1 aggregation is optimal. Moderate aggregation
levels could be optimal in the intermediate noise levels between the two critical points. It is also worth noticing that the
behavior of R∗ of p is reminiscent of that of order parameters
at a continuous phase transition in statistical mechanics [11].
The analytical results presented here are also consistent with
numerical simulations for the system size C = 50.
Since the optimal levels R∗ are unique values for each noise
p, we can evaluate the optimal decay rate Ip (R∗ ) as is given
in Figure 2. The optimal rate Ip (R∗ ) describes the limiting beˆ
havior of the smallest error probability P {Xµ ̸= Xµ } in terms
of macroscopic variables. Clearly, it is a strongly decreasing
function of the noise p. The discrepancy between the optimal
Ip (R∗ ) and Ip (1) implies the possible information gain by our
system-level control. Figure 2 also shows typical results from
a numerical experiment for the average values of per-bit error
probability, pe (p, R; C), obtained using a linear code with an
iterative encoder [12]. The linear codes are deﬁned by a class
of sparse matrices having k ones (’1’) per row and c ones per
column, respectively, where R = k/c = N/M . However, the
gain is less than that obtained for R(D), telling that there is

The results show that the optimal aggregation for a system
of independent sensors with constrained communication capacity exhibits a kind of threshold behavior with respect to
the observation noise level, both theoretically and in practice.
R EFERENCES
[1] P. W. Anderson, “More Is Different,” Science, vol. 177, no. 4047, pp.
393–396, 1972.
[2] I. Akyildiz, W. Su, Y. Sankarasubramaniam, and E. Cayirci, “A survey
on sensor networks,” IEEE Communications Magazine, vol. 40, no. 8,
pp. 102–114, 2002.
[3] D. Slepian and J. Wolf, “Noiseless coding of correlated information
sources,” Information Theory, IEEE Transactions on, vol. 19, no. 4, pp.
471–480, 1973.
[4] C. Shannon, “Coding theorems for a discrete source with a ﬁdelity
criterion,” IRE Nat. Conv. Rec, vol. 4, pp. 142–163, 1959.
[5] D. Culler and H. Mulder, “Smart sensors to network the world.”
Scientiﬁc American, vol. 290, no. 6, pp. 84–91, 2004.
[6] T. Berger, Z. Zhang, and H. Viswanathan, “The CEO problem,” IEEE
Transactions on Information Theory, vol. 42, no. 3, pp. 887–902, 1996.
[7] T. Murayama and P. Davis, “Universal behavior in large-scale aggregation of independent noisy observations,” EPL (Europhysics Letters),
vol. 87, p. 48003, 2009.
[8] T. Cover and J. Thomas, Elements of information theory. Wiley New
York, 1991.
[9] R. Ellis, Entropy, Large Deviations and Statistical Mechanics. Springer,
1985.
[10] Y. Oohama, “The rate-distortion function for the quadratic Gaussian
CEO problem,” IEEE Transactions on Information Theory, vol. 44, no. 3,
pp. 1057–1070, 1998.
[11] R. Monasson, R. Zecchina, S. Kirkpatrick, B. Selman, and L. Troyansky, “Determining computational complexity from characteristic’phase
transitions’,” Nature, vol. 400, no. 6740, pp. 133–137, 1999.
[12] T. Murayama, “Thouless-Anderson-Palmer approach for lossy compression,” Physical Review E, vol. 69, no. 3, p. 35105, 2004.

5

