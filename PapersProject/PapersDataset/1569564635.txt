Creator:         TeX output 2012.05.07:0218
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Mon May  7 02:18:32 2012
ModDate:        Tue Jun 19 12:54:40 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      271596 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569564635

Second-Order Achievable Rates in Random Number
Generation for Mixed Sources
Ryo NOMURA

Te Sun HAN

School of Network and Information
Senshu University, Japan.
Email: nomu@isc.senshu-u.ac.jp

National Institute of Information and
Communications Technology (NICT), Japan
Email: han@aoni.waseda.jp

been established by Nomura and Han [10]. In particular, for
i.i.d. sources [9] has calculated the corresponding second-order
optimal achievable rates by using the asymptotic normality in
both problems.
In this paper, we address the computation problem concerning the second-order formulas for resolvability and intrinsic
randomness problems for mixed sources. In either the resolvability problem or the intrinsic randomness problem the degree
of approximation is measured in terms of variational distance.
Note that mixed sources are typical cases of nonergodic
sources. Nonetheless, we show that we can use still the
two-peak asymptotic normality to exploit the second-order
achievable rates for mixed sources.
Related works include, e.g., Polyanskiy, Poor and Verd´
u
[11] that has developed the second-order optimal capacity rates
for Gilbert-Elliott channel from the viewpoint of a mixture of
two memoryless channels.

Abstract—The second-order achievable rates in typical random
number generation problems are considered. In these problems,
several researchers have derived the ﬁrst-order and the secondorder achievability rates for general sources using the information spectrum methods. Although these formulas are general,
their computation are quite hard. Hence, an attempt to address
explicit computation problems of achievable rates is meaningful.
In this paper, we consider mixed sources of two i.i.d. sources and
compute the second-order achievable rates explicitly.

I. I NTRODUCTION
The problem of random number generation is one of the
main topics in information theory [1]–[3]. There are several
problem settings in random number generations. The resolvability problem and the intrinsic randomness problem are
representative of them. The resolvability problem is formulated
as follows [4]. We ﬁrst use the term of “general source” to
denote a sequence X = {X n }∞ of random variables X n inn=1
dexed by n (taking values in countably inﬁnite sets), typically,
n-dimensional random variables. Given an arbitrary general
source X = {X n }∞ (called the target random number),
n=1
we generate or approximate it by using a discrete uniform
random number whose size is requested to be as small as
possible. Han and Verd´ [1], and Steinberg and Verd´ [5] have
u
u
determined the inﬁma of achievable uniform random number
rates by using the information spectrum methods. On the
other hand, the intrinsic randomness problem is formulated as
follows [2]. Given an arbitrary general source X = {X n }∞
n=1
(called the coin source), we try to generate or approximate,
by using X = {X n }∞ , a uniform random number with as
n=1
large rates as possible. Vembu and Verd´ [2], and Han [4]
u
have determined the suprema of achievable uniform random
number generation rates, again by invoking the information
spectrum methods. Since the class of general sources is quite
large, their results are very basic and quite fundamental. All
the formulas established here may be said to be ones of the
ﬁrst-order.
On the other hand, the ﬁner evaluation of the achievable
rates, called the second-order achievable rates, have been investigated in several contexts [6]–[8]. Among others, Hayashi
[9] has shown the second-order achievability theorems for
the intrinsic randomness problem as well as for the ﬁxedlength source coding problem with general sources, while the
second-order resolvability problem for general sources has

II. S ECOND -O RDER A SYMPTOTICS
We ﬁrst give the necessary notations and deﬁnitions. In the
sequel, let Y = {Y n }∞ be a general source with values in
n=1
countably inﬁnite sets Y n . Let Z be a countably inﬁnite set
and let Z, Z be random variables with values in Z. Denote
by d(Z, Z) the variational distance
∑
PZ (z) − PZ (z) ,
d(Z, Z) ≡
z∈Z

where PX (·) denotes the probability distribution of random
variable X. Moreover, set UM ≡ {1, 2, · · · , M } and let UM
denote the random variable uniformly distributed on UM .
A. Resolvability
Deﬁnition 2.1: Rate R is said to be (a, δ)-achievable if
there exists a mapping ψn : UMn → Y n such that
1
Mn
lim sup √ log na ≤ R and lim sup d(Y n , ψn (UMn )) ≤ δ.
e
n
n→∞
n→∞
Deﬁnition 2.2 ((a, δ)-resolvability):
Sr (a, δ|Y) = inf {R |R is (a, δ)-achievable } .
Then, we have
Theorem 2.1 (Nomura and Han [10]):
}
{
δ
(0 ≤ ∀δ < 2),
Sr (a, δ|Y) = inf R Fa (R) ≤
2

1

where

{

Fa (R) = lim sup Pr
n→∞

1
1
R
log
≥a+ √
n
PY n (Y n )
n

Lemma 3.1 (Han [4]): Let {zn }∞ be any real-valued sen=1
quence. Then for the mixed source Y deﬁned in Section 2, it
holds that, for i = 1, 2,
{
}
− log PY n (Yin )
√
Pr
≥ zn
n
{
}
√
− log PYin (Yin )
√
≥ Pr
≥ zn + γn − e− nγn ,
n

}
.

(1)

B. Intrinsic Randomness
Deﬁnition 2.3: Rate R is said to be (a, δ)-achievable if
there exists a mapping ϕn : Y n → UMn such that
1
Mn
lim inf √ log na ≥ R and lim sup d(UMn , ϕn (Y n )) ≤ δ.
n→∞
e
n
n→∞

and

}
− log PY n (Yin )
√
Pr
≥ zn
n
{
}
− log PYin (Yin )
√
≤ Pr
≥ zn − γn ,
n
√
where γn > 0 satisﬁes γ1 > γ2 > · · · > 0, γn → 0, nγn → ∞.

Deﬁnition 2.4 ((a, δ)-intrinsic randomness):
Sι (a, δ|Y) = sup {R |R is (a, δ)-achievable } .
Then, we have
Theorem 2.2 (Hayashi [9]):
{
}
δ
Sι (a, δ|Y) = sup R Ga (R) ≤
(0 ≤ ∀δ < 2),
2
where

{

Ga (R) = lim sup Pr
n→∞

1
1
R
log
≤a+ √
n
PY n (Y n )
n

IV. (a, δ)- RESOLVABILITY
In this section we shall establish Sr (a, δ|Y) for mixed
sources. First, we introduce two fundamental lemmas due to
Han [4]: Lemma 4.1 and Lemma 4.2, below.
Before describing lemmas, we need to deﬁne two sets. Let
X = {X n }∞ and Y = {Y n }∞ be arbitrary general
n=1
n=1
sources, then, given a sequence {zn }∞ , deﬁne Sn (zn ) and
n=1
Tn (zn ):
{
}
1
1
Sn (zn ) = x ∈ X n √ log
≥ zn ,
PX n (x)
n
{
}
1
1
Tn (zn ) = y ∈ Y n √ log
≤ zn .
PY n (y)
n

}
.

III. S ECOND - ORDER ASYMPTOTICS FOR MIXED SOURCES
In the previous section, we have reviewed the general formulas for typical second-order asymptotic problems of random
number generation with any general source Y = {Y n }∞ .
n=1
However, computation of these general formulas is quite
hard in general. Therefore, in this section we consider to
introduce a class of tractable sources Y for which the general
formulas are computable but still of independent interest. One
of such source classes would be the case where Y is a mixed
source of two i.i.d. sources. In the sequel, we now focus on
the computation problem of the second-order asymptotics for
mixed sources consisting of two i.i.d. sources.
Let us begin with the formal deﬁnition of mixed sources.
Let Y = {0, 1, · · · } (countably inﬁnite) be a discrete source
alphabet and y = y1 y2 · · · yn ∈ Y n denote a sequence emitted
from the source of length n. Let Y n denote a random variable:
a source sequence of length n.
We consider a mixed source consisting of two i.i.d. sources
Yi = {Yin }∞ , where i = {1, 2}. Then, the mixed source
n=1
Y = {Y n }∞ is deﬁned by
n=1
PY n (y) = w(1)PY1n (y) + w(2)PY2n (y),

{

Lemma 4.1: Let X = {X n }∞ and Y = {Y n }∞ be
n=1
n=1
arbitrary general sources, where X n and Y n are random
variables taking values in X n and Y n , respectively. Then,
for an arbitrary sequence {zn }∞ and γ > 0, there exists
n=1
a mapping φn : X n → Y n such that
d(φn (X n ), Y n )
√
(
)
≤ 2 max Pr{X n ∈ Sn (zn +γ)}, Pr{Y n ∈ Tn (zn )} +2e− nγ .
/
/
Lemma 4.2: Let X = {X n }∞ and Y = {Y n }∞ be
n=1
n=1
arbitrary general sources, where X n and Y n are random
variables taking values in X n and Y n , respectively. Then,
for an arbitrary sequence {zn }∞ , γ > 0 and any mapping
n=1
φn : X n → Y n it holds that

(2)

d(φn (X n ), Y n ) ≥ 2 Pr{Y n ∈ Tn (zn +γ)}
/

where w(i) are constants satisfying w(1) + w(2) = 1 and
w(i) > 0 (i = 1, 2). Since two i.i.d. sources Yi (i = 1, 2)
are completely speciﬁed by giving just the ﬁrst component Yi
(i = 1, 2), we may write simply as Yi = {Yi } (i = 1, 2) and
deﬁne the variances:
Deﬁnition 3.1 (variance):

− 2 Pr{X n ∈ Sn (zn )} − 2e−

√
nγ

.

The above lemmas are useful for the random number
generation problem to approximate a probability distribution
Y = {Y n }∞ by using an another probability distribution
n=1
X = {X n }∞ [4]. Clearly, this problem includes the resolvn=1
ability problem as a special case: the resolvability problem is
the case of X n = UMn . So, in this case the condition in the
above lemmas leads to
{
1
0 zn ≤ √n log Mn
n
(3)
Pr{X ∈ Sn (zn )} =
/
1
1 zn > √n log Mn .

2

2
σi = E (− log PYi (Yi ) − H(Yi )) (i = 1, 2),

where we assume that these variances are ﬁnite, and deﬁne
∑
the entropy by H(Yi ) = − y∈Y PYi (y) log PYi (y).
The following lemma plays the key role in the sequel
including the proofs of Theorem 4.1 and Theorem 5.1.

2

1
∗
∗
Denote the solution of this equation by Rn = n log Mn and
set as
(
)
b
1
∗
Rn = a + √ + o √
(a, b are constants),
n
n

In the sequel, we consider the case that 0 ≤ δ < 2 and
δ
δ
w(1) = 2 hold (cf. Remark 4.1 for the case of w(1) = 2 ).
Then, given 0 ≤ δ < 2 we divide the problem into three cases.
Here, without loss of generality, we assume that H(Y1 ) ≥
H(Y2 ) holds:
I
H(Y1 ) = H(Y2 ) holds.
δ
II
H(Y1 ) > H(Y2 ) and w(1) > 2 hold.
δ
III
H(Y1 ) > H(Y2 ) and w(1) < 2 hold.
In Case I, we shall establish Sr (H(Y1 ), δ|Y) (Obviously, this
is equal to Sr (H(Y2 ), δ|Y)). In Case II and Case III we shall
show Sr (H(Y1 ), δ|Y) and Sr (H(Y2 ), δ|Y), respectively. Now
we have one of the main results:

which, substituted into (5), yields
2
∑
i=1

n(a − H(Yi )) + b + o(1)
σi

)
=

δ
.
2

Then, it is not difﬁcult to verify by letting n → ∞ that, given a
and δ, the corresponding solution b = b∗ (a, δ) of this equation
coincides with T1 , T2 and T3 , respectively, according to Cases
I, II, III. Notice here that the equation (5) subsumes Remark
4.1 too. Thus, it is concluded that b∗ (a, δ) is nothing but the
second-order resolvability Sr (a, δ|Y), and hence Theorem 4.1
is equivalent to the equation (5).
∗
Summarizing up, we can write the optimal size Mn as
√
√
∗
log Mn = na + nb∗ (a, δ) + o( n),

Theorem 4.1: Given 0 ≤ δ < 2, the following holds.
Case I:
Sr (H(Y1 ), δ|Y) = T1 ,
where T1 is speciﬁed by
[ 2]
∫ ∞
2
∑
δ
1
z
√ exp −
=
w(i)
dz.
T1
2
2
2π
σ
i=1

(√
w(i)Φ

(4)

with a = H(Y1 ) or a = H(Y2 ) depending on the value of δ,
that is, if H(Y1 ) = H(Y2 ) then
(
)
δ
δ
a = H(Y1 ) and b∗ (a, δ) = σ1 Φ−1 2w(1)
if w(1) > 2 ;
(
)
δ
a = H(Y2 ) and b∗ (a, δ) = σ2 Φ−1 δ−2w(1)
if w(1) < 2 ,
2w(2)

i

Case II:
Sr (H(Y1 ), δ|Y) = T2 ,
where T2 is speciﬁed by
[ 2]
∫ ∞
z
δ
1
√ exp −
= w(1)
dz.
T2
2
2
2π
σ

∗
which enables us to evaluate how large size of Mn is needed as
a function of block length n and variational distance δ. Notice
here that in this case b∗ (a, δ) can be written as the simple
inverse function Φ−1 of the Gaussian distribution function, and
also that b∗ (a, δ) can be negative, for example, b∗ (a, δ) < 0
if δ > 1 + w(1), so that in this case the ﬁrst-order δresolvability Sr (δ|Y) is H(Y2 ) but the optimal achievable rate
∗
Rn approaches it from below. In other words, it is possible to
make necessary rates to be below the δ-resolvability at ﬁnite
block length n. The nonergodic channel counterpart of these
equations has been provided by Polyanskiy, Poor and Verd´
u
[11], who have observed for the Gilbert Elliott channel the
same kind of non-asymptotic phenomena as here.

1

Case III:
Sr (H(Y2 ), δ|Y) = T3 ,
where T3 is speciﬁed by
[ 2]
∫ ∞
δ
z
1
√ exp −
= w(1) + w(2)
dz.
T3
2
2
2π
σ
2

Proof: See Appendix.
Remark 4.1: It is easy to check that T2 = −∞, T3 = +∞
δ
for w(1) = 2 . Also, it will turn out from the way of proving
the above theorem that the second-order asymptotics gets
trivial if a = H(Y1 ) and a = H(Y2 ), because this case
necessarily implies that δ = 0 or δ = 2w(1) or δ = 2,
depending on the value of a; then, accordingly, we can
formally set as Sr (a, δ|Y) = −∞.
Remark 4.2: Theorem 4.1 can be restated more intuitively
1
but equivalently as follows. Let Rn ≡ n log Mn denote the
size rate of resolvability, and consider the following asymptotic equation for Rn :
)
(√
2
∑
n(Rn − H(Yi ))
δ
= ,
(5)
w(i)Φ
σi
2
i=1

V. (a, δ)- INTRINSIC RANDOMNESS
Let us now turn to the computation problem of the (a, δ)intrinsic randomness formula for mixed sources. Without loss
of generality, we consider the following three cases:
I
II
III

H(Y1 ) = H(Y2 ) holds.
H(Y1 ) > H(Y2 ) and w(2) >
H(Y1 ) > H(Y2 ) and w(2) <

δ
2
δ
2

hold.
hold.

In Case I, we shall establish Sι (H(Y2 ), δ|Y) (Obviously this
is equal to Sι (H(Y1 ), δ|Y)). In Case II and Case III we shall
show Sι (H(Y2 ), δ|Y) and Sι (H(Y1 ), δ|Y) respectively. Then,
we have:
Theorem 5.1: Given 0 ≤ ∀δ < 2, the following holds.
Case I:
Sι (H(Y2 ), δ|Y) = T4 ,

where Φ(·) is the Gaussian cumulative distribution function
deﬁned by
∫ +∞
x2
1
e− 2 dx.
Φ(x) = √
2π x

3

It is easy to see that letting n → ∞ in (6) yields the
following non-asymptotic equation to determine the secondorder resolvability b = b∗ (a, δ):
( )
∫
∫
b
δ
Φ
w(dθ) +
w(dθ) = ,
(7)
σθ
2
Λ0 (a)
Λ1 (a)

where T4 is speciﬁed by
[ 2]
∫ T4
2
∑
σi
δ
1
z
√ exp −
=
w(i)
dz.
2
2
2π
−∞
i=1
Case II:
Sι (H(Y2 ), δ|Y) = T5 ,

which implies∫that a given δ is speciﬁed by the following
∫
δ
conditions: If Λ0 (a) w(dθ) = 0 then Λ1 (a) w(dθ) = 2 (b =
∫
−∞); If Λ0 (a) w(dθ) > 0 then
∫
∫
∫
δ
δ
w(dθ) +
w(dθ) ≥ .
w(dθ) < , and
2
2
Λ0 (a)
Λ1 (a)
Λ1 (a)

where T5 is speciﬁed by
[ 2]
∫ T5
σ2
δ
1
z
√ exp −
= w(2)
dz.
2
2
2π
−∞
Case III:

VII. C ONCLUDING R EMARKS

Sι (H(Y1 ), δ|Y) = T6 ,

We have considered the second-order achievability to evaluate the ﬁner structure of random number generation for mixed
sources. The class of mixed sources is very important, because
all of stationary sources can be regarded as forming mixed
sources obtained by mixing stationary ergodic sources with
respect to a probability measure. However, in general, mixed
sources do not have an asymptotic normality. So, our result
is also meaningful because we have demonstrated that the
analysis based on the two-peak asymptotic normality is still
effective also for sources whose self-information spectrum
does not have a single asymptotic normality such as sources
with general mixture.
Finally, although throughout this paper, we only consider
the mixture of i.i.d. sources, we can extend our results to the
general mixture of Markovian sources with ﬁnite alphabet. As
for the details, see Nomura and Han [10].

where T6 is speciﬁed by
[ 2]
∫ T6
σ1
1
z
δ
√ exp −
= w(2) + w(1)
dz.
2
2
2π
−∞
Proof: It sufﬁces to proceed in parallel with the arguments
as made in the proof of Theorem 4.1, while taking account of
the duality between resolvability and intrinsic randomness.
VI. (a, δ)-R ESOLVABILITY FOR GENERAL MIXED S OURCES
In this section, we shall extend our results to more general
mixed sources. A mixed source Y = {Y n }∞ with general
n=1
mixture is deﬁned by
∫
PY n (y) =
PYθn (y)w(dθ),
Λ

where w(dθ) is an arbitrary probability measure on the parameter space Λ, and Yθ = {Yθn }∞ (θ ∈ Λ) are i.i.d. sources
n=1
with ﬁnite alphabet.
With this deﬁnition, we have the following formal extension
of Theorem 4.1:
Theorem 6.1: For a source Y with general mixture w(dθ)
of i.i.d. sources Yθ with ﬁnite alphabet, the optimal size
1
∗
∗
rate Rn = n log Mn is given as the solution for Rn of the
asymptotic equation:
(√
)
∫
n(Rn − H(Yθ ))
δ
Φ
w(dθ) = ;
σθ
2
Λ

A PPENDIX
P ROOF OF T HEOREM 4.1
A simplest way to prove Theorem 4.1 is to ﬁrst apply
Theorem 2.1 to the present case of mixed sources and to
proceed to the computation of necessary quantities. Here,
however, more basically we start along with Lemma 4.1 and
Lemma 4.2 in order to reveal the fundamental logic underlying
the whole process of random number generation.
Proof of Case I: The proof consists of two parts.
1) Direct Part:
√
Set Mn = enH(Y1 )+T1 n+γ , where γ > 0 is an arbitrarily
small number. Then, trivially it holds that

and, furthermore, the second-order resolvability Sr (a, δ|Y) is
given as the solution b = b∗ (a, δ) of the asymptotic equation
(√
)
∫
n(a − H(Yθ )) + b + o(1)
δ
Φ
w(dθ) = , (6)
σθ
2
Λ

lim sup
n→∞

log Mn − nH(Y1 )
√
≤ T1 .
n

Thus, it is enough to show that there exists a mapping φn such
that lim supn→∞ d(φn (UMn ), Y n ) ≤ δ. √
√
On the other hand, set zn = nH(Y1 )+T1 n −γ, then zn +γ ≤
n
log Mn
√
holds. Thus, from Lemma 4.1 and (3) there exists a
n
mapping φn such that

2
where σθ is the variance of − log PYθ (Yθ ).
Proof: The second inequality of Lemma 3.1 is not necessarily valid in the case with general mixture. Nevertheless,
we can slightly modify it so as to be applicable to this general
case. As for the details, see the proof of Han [4, Lemma 1.4.4].

1
d(φn (UMn ), Y n )
2
{
}
√
√
nH(Y1 )+T1 n
1
1
√
≤ Pr
+e− nγ .
−γ < √ log
n)
PY n (Y
n
n

Remark 6.1: Let us deﬁne the subsets Λ0 (a) and Λ1 (a) of
Λ as follows:
Λ0 (a) = {θ ∈ Λ|H(Yθ ) = a}, Λ1 (a) = {θ ∈ Λ|H(Yθ ) > a}.

4

Moreover, from Lemma 3.1 and the deﬁnition of the mixed
source, there exists a mapping φn such that

Thus, from Lemma 3.1 and the deﬁnition of the mixed source,
for any mapping φn we have

1
lim sup d(φn (UMn ), Y n )
n→∞ 2
2
∑ { nH(Y1 )+T1 √n
√
−γ
≤ lim sup
Pr
n
n→∞
i=1

1
lim inf d(φn (UMn ), Y n )
n→∞ 2
}
2
∑ { nH(Y1 )+T √n
1
1
√ 1
Pr
+2γ < √ log
≥ lim inf
n→∞
PY n (Yin )
n
n
i=1

1
1
< √ log
PY n (Yin )
n

≤

2
∑
i=1

lim sup Pr
n→∞

}

· w(i)

{
}
−log PYin (Yin )−nH(Y1 )
√
≥
lim inf Pr
> T1 +3γ w(i),
n→∞
n
i=1
(8)

w(i)

2
∑

{
}
−log PYin (Yin )−nH(Y1 )
√
> T1 −2γ w(i),
n

because γ > γn holds for sufﬁciently large n. Then, by virtue
of the asymptotic normality, for i = 1, 2 it holds that
{
}
− log PYin (Yin ) − nH(Y1 )
T + 3γ
√
> 1
lim inf Pr
n→∞
σi
nσ
[ 2 ]i
[ 2]
∫ ∞
∫ ∞
1
z
1
z
√ exp −
= T +3γ √ exp −
dz >
dz,
T1
1
2
2
2π
2π
σ
σ

because γn > 0 is speciﬁed in Lemma 3.1, and so γn <
γ holds for sufﬁciently large n. Then, noting that H(Y1 ) =
H(Y2 ) holds, we have from the asymptotic normality
{
}
− log PYin (Yin ) − nH(Y1 )
T1 − 2γ
√
lim sup Pr
>
σi
nσi
n→∞
T1
[ 2]
[ 2]
∫ σ
∫ ∞
i
1
z
1
z
√ exp −
√ exp −
=
dz +
dz.
T1
T1−
2γ
2
2
2π
2π
σ
σ
i

i

because we can let T1 + 3γ < T1 hold by letting γ → 0, and
by substituting the above inequality into (8) for any mapping
φn , we have

i

Here, by the continuity of the normal distribution,
[ 2]
∫ T1
σi
z
1
√ exp −
dz → 0
T1−
2γ
2
2π
σ

1
δ
lim inf d(φn (UMn ), Y n ) > .
n→∞ 2
2
This is a contradiction and the proof has been completed.

i

as γ → 0. Thus, since γ > 0 is an arbitrarily small, we have
1
lim sup d(φn (UMn ), Y n )
n→∞ 2
∫
2
∑
≤
w(i)
i=1

∞
T1
σi

Proofs of Case II and Case III:
The proofs of Case II and Case III are similar to the proof
of Case I. Hence, we omit the proofs.

[ 2]
1
z
δ
√ exp −
dz = .
2
2
2π

R EFERENCES
[1] T. S. Han and S. Verd´ , “Approximation theory of output statistics,”
u
IEEE Trans. Inf. Theory, vol. 39, no. 3, pp. 752–772, 1993.
[2] S. Vembu and S. Verd´ , “Generating random bits from an arbitrary
u
source: Fundamental limits,” IEEE Trans. Inf. Theory, vol. 41, no. 5,
pp. 1322–1332, 1995.
[3] T. S. Han, “Folklore in source coding: Information-spectrum approach,”
IEEE Trans. Inf. Theory, vol. 51, no. 2, pp. 747–753, 2005.
[4] ——, Information-Spectrum Methods in Information Theory. Springer,
New York, 2003.
[5] Y. Steinberg and S. Verd´ , “Simulation of random processes and rateu
distortion theory,” IEEE Trans. Inf. Theory, vol. 42, no. 1, pp. 63–86,
1996.
[6] I. Kontoyiannis, “Second-order noiseless source coding theorems,” IEEE
Trans. Inf. Theory, vol. 43, no. 4, pp. 1339–1341, 1997.
[7] M. Hayashi, “Information spectrum approach to second-order coding
rate in channel coding,” IEEE Trans. Inf. Theory, vol. 55, no. 11, pp.
4947–4966, 2009.
[8] Y. Polyanskiy, H. Poor, and S. Verd´ , “Channel coding rate in the ﬁnite
u
blocklength regime,” IEEE Trans. Inf. Theory, vol. 56, no. 5, pp. 2307–
2359, 2010.
[9] M. Hayashi, “Second-order asymptotics in ﬁxed-length source coding
and intrinsic randomness,” IEEE Trans. Inf. Theory, vol. 54, no. 10, pp.
4619–4637, 2008.
[10] R. Nomura and T. S. Han, “Second-order resolvability, intrinsic randomness, and ﬁxed-length source coding for mixed sources: Information
spectrum approach,” arXiv:1106.1879, submitted to IEEE Trans. Inf.
Theory, 2011.
[11] Y. Polyanskiy, H. V. Poor, and S. Verd´ , “Dispersion of the Gilbertu
Elliott Channel,” IEEE Trans. Inf. Theory, vol. 57, no. 4, pp. 1829–1848,
2011.

Therefore, Direct Part has been proved.
2) Converse Part:
We consider a constant T1 < T1 . Assuming that T1 is
(H(Y1 ), δ)-achievable, we shall show a contradiction. Since
we assume that T1 is (H(Y1 ), δ)-achievable, there exists a
mapping φn such that lim supn→∞ d(φn (UMn ), Y n ) ≤ δ, and
lim sup
n→∞

i

log Mn −nH(Y1 )
√
≤ T1 ,
n

which means that there exists a constant γ > 0 satisfying
log Mn − nH(Y1 )
√
< T1 + γ < T 1 ,
n
for sufﬁciently large n.
√
nH(Y1 )+T1 n
√
+ γ. Then zn >
On the other hand, set zn =
n
log Mn
√
holds. Thus, from Lemma 4.2 and (3), for any mapping
n
φn it holds that
1
d(φn (UMn ), Y n )
2
{
}
√
√
nH(Y1 )+T1 n
1
1
√
≥ Pr
−e− nγ .
+2γ < √ log
n)
PY n (Y
n
n

5

