Title:          RanganFletcherGoyalSchniter_HyGAMP.pdf
Author:         vivek
Creator:        Adobe Acrobat 9.5.1
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 20:36:35 2012
ModDate:        Tue Jun 19 12:55:00 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      315874 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566773

Hybrid Generalized Approximate Message Passing
with Applications to Structured Sparsity
Sundeep Rangan, Alyson K. Fletcher, Vivek K Goyal, and Philip Schniter
linear mixing. The basic idea is that when factors depend
on large numbers of variables, the dependencies are often
through aggregates of small, linearizable contributions. In
the proposed framework, these weak, linear interactions are
identiﬁed by partitioning the graph edges into weak and
strong edges, with the dependencies on the weak edges being
described by a linear transform. Under the assumption that the
components of the linear transform are small, it is argued that
the computations for the messages of standard loopy BP along
the weak edges can be signiﬁcantly simpliﬁed. Approximate
messages along the weak edges are integrated with standard
messages on the strong edges.
The Hybrid-GAMP methodology can be applied to any
variant of loopy BP, including the sum-product algorithm for
inference (e.g., computation of a posterior mean) and the
max-sum algorithm for optimization (e.g., computation of a
posterior mode). For the sum-product loopy BP algorithm, we
show that the messages along the weak edges can be approximated as Gaussian random variables and the computations for
these messages can be simpliﬁed via the central limit theorem.
For max-sum loopy BP, we argue that one can use quadratic
approximations of the messages and perform the computations
via a simple least-squares solution.
These approximations can dramatically simplify the computations. The complexity of standard loopy BP generically
grows exponentially with the maximum degree of the factor
nodes. With the GAMP approximation, however, the complexity is exponential only in the maximum degree from the
strong edges, while it is linear in the number of weak edges.
As a result, Hybrid-GAMP algorithms on a graphical model
with linear mixing can remain tractable even with very large
numbers of weak, linearizable interactions.
Gaussian and quadratic approximations for message passing
algorithms with linear dependencies are not new. The purpose
of this paper is to provide a systematic and general framework
for these approximations that incorporates and extends many
earlier algorithms. Many previous works have considered
Gaussian approximations of loopy BP for the problem of
estimating vectors with independent components observed
through noisy, linear measurements [3]–[9]. In the terminology
of this paper, these algorithms apply to graphs where all
the non-trivial edges are weak. By enabling graphs that have
mixes of both strong and weak edges, the framework of this
paper signiﬁcantly generalizes these methods. For example,
instead of the unknown vector simply having independent
components, the presence of strong edges can enable the vector
to have any prior describable with a graphical model.

Abstract—Gaussian and quadratic approximations of message
passing algorithms on graphs have attracted considerable attention due to their computational simplicity, analytic tractability,
and wide applicability in optimization and statistical inference
problems. This paper summarizes a systematic framework for
incorporating such approximate message passing (AMP) methods
in general graphical models. The key concept is a partition of
dependencies of a general graphical model into strong and weak
edges, with each weak edge representing a small, linearizable
coupling of variables. AMP approximations based on the central
limit theorem can be applied to the weak edges and integrated
with standard message passing updates on the strong edges. The
resulting algorithm, which we call hybrid generalized approximate message passing (Hybrid-GAMP), can yield signiﬁcantly
simpler implementations of sum-product and max-sum loopy
belief propagation. By varying the partition between strong and
weak edges, a performance–complexity trade-off can be achieved.
Structured sparsity problems are studied as an example of this
general methodology where there is a natural partition of edges.

I. I NTRODUCTION
Message passing algorithms on graphical models have become widely-used in high-dimensional optimization and inference problems in a range of ﬁelds [1], [2]. The fundamental
principle of graphical models is to factor high-dimensional
problems into sets of problems of lower dimension. The
factorization is represented via a graph where the problem
variables and factors are represented by the graph vertices,
and the dependencies between them represented by edges.
Message passing methods such as loopy belief propagation
(BP) use this graphical structure to perform approximate
inference or optimization in an iterative manner. In each
iteration, inference or optimization is performed “locally” on
the sub-problems associated with each factor, and “messages”
are passed between the variables and factors to account for
the coupling between the local problems.
Although effective in a range of problems, loopy BP is only
as simple as the problems in the constituent factors; if the factors themselves are of high dimensions, exact implementation
of loopy BP will be computationally intractable.
To reduce the complexity of loopy BP, this paper presents
a hybrid generalized approximate message passing (HybridGAMP) algorithm for what we call graphical models with
S. Rangan (email: srangan@poly.edu) is with Polytechnic Institute of New
York University, Brooklyn, NY.
A. K. Fletcher (email: alyson@eecs.berkeley.edu) is with the Department of
Electrical Engineering and Computer Sciences, Univ. of California, Berkeley.
V. K. Goyal (email: v.goyal@ieee.org) is with the Research Laboratory of
Electronics, Massachusetts Institute of Technology.
P. Schniter (email: schniter@ece.osu.edu) is with the Department of Electrical and Computer Engineering, The Ohio State University.

1

x1

The approach here of combining approximate message
passing methods and standard graphical models with linear
mixing is closest to the methods developed in [10]–[13]
for wavelet image denoising and turbo equalization. These
works also considered graphical models that had both linear
and nonlinear components, and applied approximate message
passing techniques along the lines of [7], [8] to the linearizable portions while maintaining standard BP updates in the
remainder of the graph. The use of approximate message
passing methods on portions of a factor graph has also been
applied with joint parameter estimation and decoding for
CDMA multiuser detection in [14]; in a wireless interference
coordination problem in [15], and proposed in [16, Section 7]
in the context of compressed sensing. The framework presented here uniﬁes and extends all of these examples and thus
provides a systematic procedure for incorporating Gaussian
approximations of message passing in a modular manner in
general graphical models.
The remainder of this paper develops only the sum-product
case; the reader is referred to [17] for parallel development of
the max-sum case as well as proofs omitted for brevity and
more examples and details.

f1 ( xD (1) , z1 )

x2
f2 (xD (2) , z2 )

x3
f m ( xD ( m) , z m )

xn

Fig. 1. Factor graph representation of the linear mixing estimation and
optimization problems. The variable nodes (circles) are connected to the factor
nodes (squares) either directly (strong edges) or via the output of the linear
mixing matrix A (weak edges).

will be called a weak edge, since the approximations we
will make in the algorithms below assume that Aij is
small. The set of weak edges into the factor node i will
be denoted β(i).
Together α(i) and β(i) comprise the set of all indices j
such that the variable node xj is connected to the factor node
fi (·) in the graph G. The union ∂(i) = α(i) ∪ β(i) is thus
the neighbor set of fi (·). Similarly, for any variable node xj ,
we let α(j) be the set of indices i such that that the factor
node fi (·) is connected to xj via a strong edge, and let β(j)
be the set of indices i such that there is a weak edge. We let
∂(j) = α(j) ∪ β(j) be the union of these sets, which is the
neighbor set of xj .
Given these deﬁnitions, we are interested in the Expectation
problem P-EXP: Given a function F (x, z) of the form (2), a
matrix A, and scale factor u > 0, deﬁne the joint distribution

II. G RAPHICAL M ODEL P ROBLEMS WITH L INEAR M IXING
Let x and z be real-valued block column vectors
x = (x∗ , . . . , x∗ )∗ ,
1
n

z = (z∗ , . . . , z∗ )∗ ,
1
m

(1)

and consider a function of these vectors of the form
F (x, z) :=

m
i=1

fi (xα(i) , zi ),

(2)

where, for each i, fi (·) is a real-valued function; α(i) is a subset of the indices {1, . . . , n}; and xα(i) is the concatenation of
the vectors {xj , j ∈ α(i)}. We are interested in computations
subject to linear constraints of the form
zi =

n
j=1

Aij xj = Ai x,

(3)

where each Aij is a real-valued matrix and Ai is the block
column matrix with components Aij . We will also let A be
the block matrix with components Aij so that we can write
the linear constraints as z = Ax.
The function F (x, z) is naturally described via a graphical
model as shown in Fig. 1. Speciﬁcally, we associate with
F (x, z) a bipartite factor graph G = (V, E) whose vertices
V consist of n variable nodes corresponding to the (vectorvalued) variables xj , and m factor nodes corresponding to the
factors fi (·) in (2). There is an edge (i, j) ∈ E in the graph
if and only if the variable xj has some inﬂuence on the factor
fi (xα(i) , zi ). This inﬂuence can occur in one of two mutually
exclusive ways:
• The index j is in α(i), so that the variable xj directly
appears in the sub-vector xα(i) in the factor fi (xα(i) , zi ).
In this case, (i, j) will be called a strong edge, since xj
can have an arbitrary and potentially-large inﬂuence on
the factor.
• The matrix Aij is nonzero, so xj affects fi (xα(i) , zi )
through its linear inﬂuence on zi in (3). In this case, (i, j)

p(x) := (1/Z(u)) exp [uF (x, z)] ,

z = Ax,

(4)

where Z(u) is a normalization constant called the partition
function (it is a function of u). For this distribution, compute
the expectations
x = E[x],

z = E[z].

(5)

Also, for each j, compute the log marginal
∆j (xj ) := (1/u) log

exp [uF (x, z)] dx\j ,

(6)

where the integral is over all variables xr for r = j.
P-EXP arises naturally in statistical inference: Suppose we
are given a probability distribution p(x) of the form (4) for
some function F (x, z). The function F (x, z) may depend
implicitly on some observed vector y, so that p(x) represents
the posterior distribution of x given y. In this context, the
solution (x, z) to the problem P-EXP is precisely the minimum
mean squared error (MMSE) estimate when u = 1. The
function ∆j (xj ) is the log marginal distribution of xj .

2

the factors fi (·). The local expectation problems may be
signiﬁcantly lower in dimension than the global problem. In
particular, the factor fi (xα(i) , zi ) is a function of di = |∂(i)|
variables xj , either through one of the |α(i)| strong edges or
|β(i)| weak edges. For each j ∈ ∂(i), the factor node update
involves an integration over di − 1 variables. The complexity
in general grows exponentially in di . Thus, standard loopy BP
is only typically tractable when the degrees di of the factor
nodes are small or the factors have some convenient form.

In the analysis below, we will assume that, for all factor
nodes fi (·), the strong and weak neighbors, α(i) and β(i),
are disjoint. This assumption introduces no loss of generality:
If an edge (i, j) is both weak and strong, we can modify the
function fi (xα (i), zi ) to move the inﬂuence of xj from the
term zi into the direct term xα(i) [17].
Even when the dependence of a factor fi (xα(i) , zi ) on a
variable xj is only through the linear term zi , we may still
wish to move the dependence to a strong edge to improve
accuracy at the expense of greater computation.
Since Aij = 0 only when j ∈ β(i), we may sometimes
write the summation (3) as
zi =

j∈β(i)

Aij xj = Ai,β(i) xβ(i) ,

IV. H YBRID -GAMP
The Hybrid-GAMP algorithm reduces the cost of loopy
BP by exploiting complexity-reducing approximations of the
cumulative effect of the weak edges. We saw in the previous
section that the loopy BP update at each factor node fi (·)
has a cost that may be exponential in the degree d of the
node, which consists of |α(i)| strong edges and |β(i)| weak
edges. The Hybrid-GAMP algorithm with edge partitioning
uses the linear mixing property to eliminate the exponential
dependence on the |β(i)| weak edges, so the only exponential
dependence is on the |α(i)| strong edges. Thus, the edge
partitioning makes Hybrid-GAMP computationally tractable
as long as the number of strong edges is small. The number
of weak edges can be arbitrary. In particular, the mixing matrix
A can be dense.
The basis of the Hybrid-GAMP approximation is to assume
that the matrix Aij is small along any weak edge (i, j). Under
this assumption, one can apply a Gaussian approximation of
the weak edge messages and use the central limit theorem at
the factor nodes. A heuristic derivation of the Hybrid-GAMP
approximations is given in [17, App. B].
We need additional notation: The Hybrid-GAMP algorithm
produces a sequence of estimates xj (t) and zi (t) for the
variables xj and zi . Several other intermediate variables pi (t),
si (t) and rj (t) are also produced. Associated with each of the
variables are matrices Qx (t), Qz (t), . . ., that represent certain
j
i
covariances. When we need to take the inverse of the matrices,
we will use the notation Q−x (t) to mean (Qx (t))−1 . Finally,
j
j
for any positive deﬁnite matrix Q and vector a, we will let
a 2 = a∗ Q−1 a, which is a weighted two norm.
Q

(7)

where xβ(i) is the sub-vector of x with components j ∈ β(i)
and Ai,β(i) is the corresponding portion of the ith block-row
of A.
III. R EVIEW OF L OOPY B ELIEF P ROPAGATION
The sum-product loopy BP algorithm is based on iteratively
passing estimates of the log marginals ∆j (xj ) in (6). We
index the iterations by t = 0, 1, . . ., and denote the estimate
“message” from the factor node fi to the variable node xj
in the tth iteration by ∆i→j (t, xj ) and the reverse message
by ∆i←j (t, xj ). The messages in loopy BP are equivalent up
to a constant factor. That is, adding any constant term that
does not depend on xj to either the message ∆i→j (t, xj ) or
∆i←j (t, xj ) has no effect on the algorithm. We will thus use
the notation
∆(x) ≡ g(x)

⇐⇒

∆(x) = g(x) + C,

for some constant C that does not depend on x. Similarly, we
write p(x) ∝ q(x) when p(x) = Cq(x) for some constant
C. We ﬁx the scale factor u > 0 and, for any function ∆(·),
we will write E[g(x) ; ∆(·)] to denote the expectation of g(x)
with respect to a distribution speciﬁed indirectly by ∆(·):
E[g(x) ; ∆(·)] =

g(x)p(x) dx,

(8)

where p(x) is the probability distribution
p(x) = (1/Z(u)) exp [u∆(x)]

Algorithm 1: Hybrid-GAMP:

Consider

the

problem

P-EXP for some function F (x, z) of the form (2), matrix A,

and Z(u) is a normalization constant.
For each edge (i, j) ∈ E, the factor node update is a
computation of ∆i→j (t, xj ) by integration over variables xr
with r ∈ ∂(i) and r = j. The variable node update is a
computation of ∆i←j (t + 1, xj ) by combining inﬂuences of
∆ℓ→j (t, xj ) for ℓ ∈ ∂(j) \ i followed by computation of a
point estimate xj (t+1) as a scalar expectation.
When the graph G is acyclic, the sum-product loopy BP
algorithm converges to the exact solution of P-EXP. For graphs
with cycles, the loopy BP algorithm is, in general, only
approximate; see, for example, [2], [18], [19].
Brute force solutions to P-EXP involve an expectation over
all n variables xj . Loopy BP reduces this “global” problem
to a sequence of “local” problems associated with each of

and scale factor u > 0.
1) Initialization: Set t = 0 and select some initial values
∆i→j (t − 1, xj ) for all strong edges (i, j) and values
rj (t−1) and Qr (t−1) for all variable node indices j.
j
2) Variable node update, strong edges: For all strong edges
(i, j), compute
∆i←j (t, xj ) ≡
−

ℓ∈α(j)=i ∆ℓ→j (t−1, xj )
2
1
2 rj (t−1) − xj Qr (t−1) .
j

(9)

3) Variable node update, weak edges: For all variable nodes
j, compute
x
∆j (t, xj ) ≡ Hj (t, xj , rj (t−1), Qr (t−1)),
j

3

(10)

x
Hj (t, xj , rj , Qr )
j

=

i∈α(j)

∆i→j (t−1, xj ) −

1
2

2
Qr ,
j

rj − x j

(11)

xj (t)

=

E (xj ; ∆j (t, ·)) ,

(12)

Qx (t)
j

=

u var (xj ; ∆j (t, ·)) .

(13)

4) Factor node update, linear step: For all factor nodes i,
compute
zi (t)

=

pi (t)

=

Qp (t)
i

j∈β(i)

Aij xj (t),

zi (t) − Qp (t)si (t−1),
i

(14b)

x
∗
j∈β(i) Aij Qj (t)Aij ,

=

Fig. 2. Graphical model for the group sparsity problem with overlapping
groups. The group dependencies between components of the vector x are
modeled via a set of binary latent variables ξ.

(14a)
(14c)

Method
Group-OMP [24]
Group-Lasso [20], [21], [25]
Relaxed BP with vector components [26]
Hybrid-GAMP with vector components
Hybrid-GAMP with scalar components

where, initially, we set si (−1) = 0.
5) Factor node update, strong edges: For all strong edges
(i, j), compute:
1
u

pi→j (t, xα(i) , zi )dxα(i)\j dzi
(15)
where the integral is over zi and all components xr
with r ∈ α(i) \ j, and pi→j (0, xj ) is the probability
distribution function
∆i→j (t, xj ) ≡

log

TABLE I
C OMPLEXITY COMPARISON FOR GROUP SPARSITY ESTIMATION OF A
SPARSE VECTOR WITH K GROUPS , EACH GROUP OF DIMENSION d. T HE
NUMBER OF MEASUREMENTS IS m AND THE SPARSITY RATIO IS ρ.

pi→j (t, xα(i) , zi ) ∝

all variables connected to the factor node i. In the HybridGAMP algorithm, these computations are replaced by (15),
where the expectation is over the strong edge variables α(i). If
the number of edges is large, the computational savings can be
dramatic. The other steps of the Hybrid-GAMP algorithms are
all linear least-squares operations, or componentwise nonlinear
functions on the individual variables.
For illustration, we have only presented one form of the
Hybrid-GAMP procedure. Variants with discrete distributions
and other message scheduling are possible.

z
exp uHi→j (t, xα(i) .zi , pi (t), Qp (t)) . (16)
i

6) Factor node update, weak edges: For all factor nodes i,
compute
z
Hi (t, xα(i) , zi , pi , Qp ) := fi (xα(i) , zi )
i

+

r∈α(i)

∆i←r (t, xr ) −

1
2

zi − p i

2
.
Qp
i

(17)

Then, let
z0 (t) = E(zi ),
i

Qz (t) = u var(zi ),
i

(18)

V. A PPLICATION TO S TRUCTURED S PARSITY

where zi is the component of the pair (xα(i) , zi ) with
the joint distribution

Hybrid-GAMP is a ﬂexible and general methodology. To
enable comparisons against existing algorithms, we consider
the group sparse estimation problem [20], [21]. This is a
highly-structured problem in which dependencies beyond linear mixing are captured by binary latent variables ξk as shown
in Fig. 2. The setting and the specialization of Algorithm 1 to
this case are described in detail in [17].
In addition to its generality, the Hybrid-GAMP procedure is
among the most computationally efﬁcient. Consider the special
case when there are K non-overlapping groups of d elements
each; that is, each ξk is connected to d variables xj , and the
sets are disjoint. In this case, the total vector dimension for
x is n = Kd. We consider the non-overlapping case since
there are many algorithms that apply to this case that we
can compare against. For non-overlapping uniform groups,
Table I compares the computational cost of the Hybrid-GAMP
algorithm to other methods. Because of the block separability
of this special case, the analysis in [22] applies. For d = 2,
correlated variables can be considered the real and imaginary
parts of a complex variable, enabling the methods of [23].
Of course, a complete comparison requires that we consider
the number of iterations, not just the computation per iteration.

pi (t, xα(i) , zi ) ∝
z
exp uHi (t, xα(i) , zi , pi (t), Qp (t)) .
i

(19)

Then, compute
si (t)
Qs (t)
i

=

Q−p (t) z0 (t) − pi (t) ,
i
i

=

Q−p (t)
i

−

(20a)

Q−p (t)D−z (t)Q−p (t). (20b)
i
i
i

7) Variable node update, linear step: For all variables nodes
j compute
Q−r (t)
j
rj (t)

=
=

i∈β(j)

x(t) +

A∗ Qs (t)Aij ,
ij i

Qr (t)
j

∗
i∈β(j) Aij si (t).

Complexity
O(ρmn2 )
O(mn) per iteration
O(mn2 ) per iteration
O(mnd) per iteration
O(mn) per iteration

(21a)
(21b)

Increment t and return to step 2 for some number of
iterations.
Although the Hybrid-GAMP algorithm above appears more
complicated than standard loopy BP, Hybrid-GAMP can be
computationally dramatically cheaper. The main computational difﬁculty of loopy BP is the factor update; this involves
an expectation over |∂(i)| variables, where ∂(i) is the set of

4

5

[1] B. J. Frey, Graphical Models for Machine Learning and Digital Communication. MIT Press, 1998.
[2] M. J. Wainwright and M. I. Jordan, Graphical Models, Exponential
Families, and Variational Inference, ser. Foundations and Trends in
Machine Learning. NOW Publishers, 2008, vol. 1.
[3] J. Boutros and G. Caire, “Iterative multiuser joint decoding: Uniﬁed
framework and asymptotic analysis,” IEEE Trans. Inform. Theory,
vol. 48, no. 7, pp. 1772–1793, Jul. 2002.
[4] T. Tanaka and M. Okada, “Approximate belief propagation, density
evolution, and neurodynamics for CDMA multiuser detection,” IEEE
Trans. Inform. Theory, vol. 51, no. 2, pp. 700–706, Feb. 2005.
[5] D. Guo and C.-C. Wang, “Asymptotic mean-square optimality of belief
propagation for sparse linear systems,” in Proc. IEEE Inform. Theory
Workshop, Chengdu, China, Oct. 2006, pp. 194–198.
[6] ——, “Random sparse linear systems observed via arbitrary channels:
A decoupling principle,” in Proc. IEEE ISIT, Jun. 2007, pp. 946–950.
[7] S. Rangan, “Estimation with random linear mixing, belief propagation
and compressed sensing,” arXiv:1001.2228v1 [cs.IT]., Jan. 2010.
[8] D. L. Donoho, A. Maleki, and A. Montanari, “Message-passing algorithms for compressed sensing,” Proc. Nat. Acad. Sci., vol. 106, no. 45,
pp. 18 914–18 919, Nov. 2009.
[9] M. Bayati and A. Montanari, “The dynamics of message passing on
dense graphs, with applications to compressed sensing,” IEEE Trans.
Inform. Theory, vol. 57, no. 2, pp. 764–785, Feb. 2011.
[10] P. Schniter, “Turbo reconstruction of structured sparse signals,” in Proc.
Conf. on Inform. Sci. & Sys., Mar. 2010.
[11] J. Ziniel, L. C. Potter, and P. Schniter, “Tracking and smoothing of timevarying sparse signals via approximate belief propagation,” in Conf. Rec.
Asilomar Conf. on Signals, Syst. & Computers, Nov. 2010.
[12] S. Som, L. C. Potter, and P. Schniter, “Compressive imaging using
approximate message passing and a Markov-tree prior,” in Conf. Rec.
Asilomar Conf. on Signals, Syst. & Computers, Nov. 2010.
[13] P. Schniter, “A message-passing receiver for BICM-OFDM over unknown clustered-sparse channels,” in Proc. IEEE Workshop Signal
Process. Adv. Wireless Commun., Jun. 2011.
[14] G. Caire, A. Tulino, and E. Biglieri, “Iterative multiuser joint detection
and parameter estimation: a factor-graph approach,” in Proc. IEEE
Inform. Theory Workshop, Sep. 2001, pp. 36–38.
[15] S. Rangan and R. K. Madan, “Belief propagation methods for intercell
interference coordination,” in Proc. IEEE Infocom, Apr. 2011.
[16] A. Montanari, “Graphical models concepts in compressed sensing,”
arXiv:1011.4328v3 [cs.IT], Mar. 2011.
[17] S. Rangan, A. K. Fletcher, V. K. Goyal, and P. Schniter, “Hybrid
approximate message passing with applications to structured sparsity,”
arXiv:1111.2581 [cs.IT], Nov. 2011.
[18] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of
Plausible Inference. Morgan Kaufmann, 1988.
[19] J. S. Yedidia, W. T. Freeman, and Y. Weiss, “Understanding belief
propagation and its generalizations,” in Exploring Artiﬁcial Intelligence
in the New Millennium. Morgan Kaufmann, 2003, pp. 239–269.
[20] M. Yuan and Y. Lin, “Model selection and estimation in regression with
grouped variables,” J. Royal Statist. Soc., vol. 68, pp. 49–67, 2006.
[21] P. Zhao, G. Rocha, and B. Yu, “The composite absolute penalties family
for grouped and hierarchical variable selection,” Ann. Stat., vol. 37, no. 6,
pp. 3468–3497, 2009.
[22] D. Donoho, I. Johnstone, and A. Montanari, “Accurate prediction of
phase transitions in compressed sensing via a connection to minimax
denoising,” arXiv:1111.1041v1 [cs.IT]., Nov. 2011.
[23] A. Maleki, L. Anitori, Z. Yang, and R. G. Baraniuk, “Asymptotic
analysis of complex LASSO via complex approximate message passing
(CAMP),” arXiv:1108.0477v1 [cs.IT], Aug. 2011.
´
[24] A. C. Lozano, G. Swirszcz, and N. Abe, “Group orthogonal matching
pursuit for variable selection and prediction,” in Proc. Neural Information Process. Syst., Dec. 2008.
[25] S. J. Wright, R. D. Nowak, and M. Figueiredo, “Sparse reconstruction by
separable approximation,” IEEE Trans. Signal Process., vol. 57, no. 7,
pp. 2479–2493, Jul. 2009.
[26] J. Kim, W. Chang, B. Jung, D. Baron, and J. C. Ye, “Belief propagation
for joint sparse recovery,” arXiv:1102.3289, Feb. 2011.
[27] S. Rangan et al., “Generalized approximate message passing,”
SourceForge.net
project
gampmatlab,
available
on-line
at
http://gampmatlab.sourceforge.net/.
[28] S. Rangan, “Generalized approximate message passing for estimation
with random linear mixing,” arXiv:1010.5141v1 [cs.IT]., Oct. 2010.

0

Normalized MSE (dB)

−5
−10
−15
−20
−25
−30
−35
50

LMMSE
Grp LASSO
Grp OMP
Hybrid−GAMP
100
150
Num measurements (m)

200

Fig. 3. Comparison of performances of various estimation algorithms for
group sparsity with n = 100 groups of dimension d = 4 with a sparsity
fraction of ρ = 0.1.

This comparison requires further study beyond the scope of
this paper. However, it is possible that the Hybrid-GAMP
procedure will be favorable in this regard. Our simulations
below show good convergence after only 10–20 iterations.
Fig. 3 shows the results of a simple simulation comparison
of algorithms. The simulation used a vector x with n = 100
groups of size d = 4 and i.i.d. Bernoulli variables ξk with
ρ = Pr(ξk = 1) = 0.1. The matrix was i.i.d. Gaussian and the
observations were with AWGN noise at an SNR of 20 dB. The
number of measurements m was varied from 50 to 200, and
the plot shows the MSE for each of the methods. The HybridGAMP method was run with 20 iterations. In group LASSO,
at each value of m, the algorithm was simulated with several
values of the regularization parameter and the plot shows the
minimum MSE. In Group-OMP, the algorithm was run with
the true value of the number of nonzero coefﬁcients. It can be
seen that the Hybrid-GAMP method is consistently as good
or better than the other methods. All code for the simulations
can be found in the SourceForge open website [27].
VI. C ONCLUSIONS
A general model for optimization and statistical inference
based on graphical models with linear mixing was presented.
The linear mixing components of the graphical model account
for interactions through aggregates of large numbers of small,
linearizable contributions. Gaussian and second-order approximations are shown to greatly simplify the implementation
of loopy BP for these interactions, and the Hybrid-GAMP
framework presented here enables systematic incorporation of
these approximations in general graphical models. Simulations
were presented for group sparsity where the Hybrid-GAMP
method has equal or superior performance to existing methods.
However, the generality of the method will enable HybridGAMP to be applied to much more complex models where
few algorithms are available. In addition to experimenting with
such models, future work will focus on establishing rigorous
theoretical analyses along the lines of [9], [28].

5

