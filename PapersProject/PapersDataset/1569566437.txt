Title:          JSCC
Subject:        
Keywords:       
Author:         Victoria Kostina
Creator:        Preview
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Sat May 19 03:30:10 2012
ModDate:        Tue Jun 19 12:55:15 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      488599 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569566437

Lossy joint source-channel coding
in the ﬁnite blocklength regime
Victoria Kostina, Sergio Verd´
u
Dept. of Electrical Engineering, Princeton University, NJ, 08544, USA
the achievability bound in [8] to lossy compression, and extend
the dispersion result of [9] to sources with abstract alphabets
and Gaussian channels.
The rest of the paper is organized as follows. Section II
summarizes basic deﬁnitions and notation. Section III and IV
introduce the new converse and achievability bounds to the
maximum achievable coding rate, respectively. A Gaussian
approximation analysis of the new bounds is presented in
Section V. The evaluation of the bounds and the approximation
is performed for two important special cases: the transmission
of a binary memoryless source (BMS) over a binary symmetric
channel (BSC) with bit error rate distortion (Section VI) and
the transmission of a Gaussian memoryless source (GMS) with
mean-square error distortion over an AWGN channel with a
total power constraint (Section VII).

Abstract—This paper shows new tight ﬁnite-blocklength
bounds for the best achievable lossy joint source-channel code
rate, and demonstrates that joint source-channel code design
brings considerable performance advantage over a separate one
in the non-asymptotic regime. A joint source-channel code maps
a block of k source symbols onto a length−n channel codeword,
and the ﬁdelity of reproduction at the receiver end is measured
by the probability that the distortion exceeds a given threshold
d. For memoryless sources and channels, it is demonstrated that
the parameters of the best joint source-channel code must satisfy
p
nC − kR(d) ≈ nV + kV(d)Q−1 ( ), where C and V are the
channel capacity and dispersion, respectively; R(d) and V(d) are
the source rate-distortion and rate-dispersion functions; and Q
is the standard Gaussian complementary cdf.
Index Terms—Achievability, converse, ﬁnite blocklength
regime, joint source-channel coding, lossy source coding, memoryless sources, rate-distortion theory, Shannon theory.

I. I NTRODUCTION
II. D EFINITIONS

For a large class of sources and channels, in the limit
of large blocklength, the maximum achievable joint sourcechannel coding (JSCC) rate compatible with vanishing excess
C
distortion probability is characterized by the ratio R(d) [1],
attainable by separate source-channel coding (SSCC). However, at ﬁnite blocklengths not only is the fundamental limit
C
R(d) no longer achievable but separation ceases to be optimal.
Quantifying, as a function of blocklength and excess distortion
C
probability, the required backoff from R(d) as well as the
increase in the rate of source symbols per channel use afforded
by an optimal joint design bears great practical, as well as
conceptual, interest.
Prior research in this direction includes the work of Csisz´ r
a
[2], [3] who demonstrated that the error exponent of joint
source-channel coding outperforms that of separate sourcechannel coding. For discrete source-channel pairs with average
distortion criterion, Pilc’s achievability bound [4], [5] applies.
For the transmission of a Gaussian source over a discrete
channel under the average mean square error constraint,
Wyner’s achievability bound [6], [7] applies. Most recently,
Tauste Campo et al. [8] showed a number of ﬁnite-blocklength
random-coding bounds applicable to the almost-lossless JSCC
setup. For sources and channels with ﬁnite alphabets, Wang et
al. [9] recently found the dispersion of JSCC. In this paper, we
show new nonasymptotically tight converse bounds, generalize

A lossy source-channel code is a (possibly randomized)
ˆ
pair of mappings f : M → X and c : Y → M. A distortion
ˆ → [0, +∞) is used to quantify the
measure d : M × M
performance of a lossy code.
ˆ
Deﬁnition 1. A (d, ) code for {M, X , Y, M, PS , PY |X ,
d} is a source-channel code with P [d (S, c(Y )) > d] ≤
where f(S) = X.
If the channel has input cost constraints, the additional
requirement in Deﬁnition 1 will be f (S) ∈ F, where F is the
set of the allowable channel inputs, F ⊆ X . The special case
d = 0 and d(s, z) = 1 {s = z} corresponds to almost-lossless
compression. If, in addition, PS is equiprobable on an alphabet
of cardinality M , a (0, ) code in Deﬁnition 1 corresponds to
an (M, ) channel code (i.e. a code with M codewords and
average error probability ). On the other hand, if the channel
is an identity mapping on an alphabet of cardinality M , a
(d, ) code in Deﬁnition 1 corresponds to an (M, d, ) lossy
compression code (as e.g. deﬁned in [10]).
Deﬁnition 2. In the conventional ﬁxed-to-ﬁxed (or block)
setting in which X and Y are the n−fold Cartesian products
ˆ
of alphabets A and B, M and M are the k−fold Cartesian
ˆ and dk : S k × S k → [0, +∞),
ˆ
products of alphabets S and S,
n
n
n
ˆn , PS k , PY n |X n , dk } is
a (d, ) code for {S , A , B , S
called a (k, n, d, ) code.

This work was supported in part by the National Science Foundation (NSF)
under Grant CCF-1016625 and by the Center for Science of Information
(CSoI), an NSF Science and Technology Center, under Grant CCF-0939370.
The ﬁrst author was supported in part by the Natural Sciences and Engineering
Research Council of Canada.

Deﬁnition 3. Fix , d and the channel blocklength n. The maximum achievable source blocklength and coding rate (source

1

symbols per channel use) are deﬁned by, respectively
k (n, d, ) = max {k : ∃(k, n, d, ) code}
k (n, d, )
R(n, d, ) =
n
Denote, for a given PY |X ,

denotes the information density of the joint distribution PSZ
at (s, z). We can deﬁne the right side of (11) for a given
(PZ|S , PZ ) even if there is no PS such that the marginal of
PS PZ|S is PZ . We use the same notation ıS;Z for that more
general function. To extend Deﬁnition 5 to the lossless case,
for discrete random variables we deﬁne 0-tilted information as

(1)
(2)

C = sup I(X; Y )

(3)

S (s, 0) = ıS (s)

PX

where

ˆ
and, for a given PS and the distortion measure d : M × M →
[0, +∞),
RS (d) =
inf
I(S; Z)
(4)

ıS (s) = log

III. C ONVERSES
A. Converses using d-tilted information

We impose the following basic restrictions on PY |X , PS and
the distortion measure.
(a) RS (d) is ﬁnite for some d, i.e. dmin < ∞, where

Our ﬁrst result is a general converse bound.
Theorem 1 (Converse). Fix a positive integer T and a
partition {Xt , t = 1, . . . , T } of the set of the allowable channel
inputs F ⊆ X . Deﬁne the function T : X → [1, . . . , T ],
T (x) = t if x ∈ Xt . Any (d, ) code for source S and channel
PY |X must satisfy

(5)

(b) The inﬁmum in (4) is achieved by a unique PZ |S .
(c) The supremum in (3) is achieved by a unique PX .
The dispersion, which serves to quantify the penalty on the
rate of the best JSCC code induced by the ﬁnite blocklength,
is deﬁned as follows.

≥ sup
γ>0

Deﬁnition 4. Fix d ≥ dmin . The rate-dispersion function
of joint source-channel coding (source samples squared per
channel use) is deﬁned as
V(d) = lim lim sup n
→0 n→∞

C
R(d)

− R(n, d, )

Q−1 ( )

− T exp (−γ) +

sup E inf P S (S, d) − ıX;YT (x) (x; Y ) ≥ γ | S
x∈F
T
{PYt }t=1
(14)

2

(6)

where the conditional probability is with respect to Y distributed according to PY |X=x (independent of S), and

where C and R(d) are the channel capacity and source ratedistortion function, respectively.1

ıX;Yt (x; y) = log

Deﬁnition 5 (d−tilted information [10]). For d > dmin , the
d−tilted information in s is deﬁned as2
1
S (s, d) = log
(7)
E [exp {λ d − λ d(s, Z )}]

E [exp {λ d − λ d(S, z) + S (S, d)}] ≤ 1

(8)

Theorem 2 (Converse). If there exists a PY such that the
¯
distribution of

(9)
(10)

ıX;Y (x; y) = log
¯

where (9) holds for PZ -almost every z, while (10) holds for
ˆ
all z ∈ M, and
ıS;Z (s; z) = log

dPZ|S=s (z)
dPZ (z)

(15)

When we apply Theorem 1 to ﬁnd the dispersion of JSCC,
we will let T be the number of channel input types. For
symmetric channels (in a sense to be formalized) T = 1
already yields a tight bound. Moreover, symmetry signiﬁcantly
simpliﬁes the computation of (14), as the following result
details.

The following properties of d−tilted information, proven in
[11], are used in the sequel.
S (s, d) = ıS;Z (s; z) + λ d(s, z) − λ d

PY |X (y|x)
PYt (y)

and the supremum is over γ > 0 and all collections
{PY1 , . . . , PYT } of probability distributions on the channel
output alphabet Y parameterized by the partition index t.

where the expectation is with respect to the unconditional
distribution of Z , and
λ = −RS (d)

(13)

is the information in outcome s ∈ M.

PZ|S :
E[d(S,Z)]≤d

dmin = inf {d : RS (d) < ∞}

1
PS (s)

(12)

PY |X (y|x)
PY (y)
¯

(16)

(according to PY |X=x ) does not depend on the choice of x ∈
F , then any (d, ) code for source S and channel PY |X with
subset F ⊆ X of allowable channel inputs must satisfy

(11)

≥ sup

1 While for memoryless sources and channels, C = C and R(d) = R (d)
S
given by (3) and (4) evaluated with single-letter distributions, it is important
to distinguish between the operational deﬁnitions and the extremal mutual
information quantities, since the core results in this paper allow for memory.
2 All log’s and exp’s are in an arbitrary common base.

γ>0

P S (S, d) − ıX;Y (x; Y ) ≥ γ | X = x
¯
− exp (−γ)

2

(17)

for an arbitrary x ∈ F.

(d, ) lossy codes is included in the set of all list codes with
list error probability ≤ and list size

Proof: In Theorem 1, let T = 1 and observe that under
our channel symmetry assumption, the conditional probability
in (14) is the same regardless of the choice of x ∈ F.
Remark 1. With the substitution (12), Theorems 1 and 2 still
hold in the case d = 0 and d(x, y) = 1 {x = y}, which
corresponds to almost-lossless data compression.

L = max Q [d(S, z) ≤ d]
ˆ
z∈M

Via (21), any converse for list coding implies a converse for
lossy coding. Keeping this in mind, we state our converses in
this section for list decoding.
Denote by

Remark 2. Our converse for lossy source coding in [10,
Theorem 7] can be viewed as a particular case of the result
in Theorem 2. Indeed, if X = Y = {1, . . . , M } and
1
PY |X (m|m) = 1, PY (1) = . . . = PY (M ) = M , then (17)
¯
¯
becomes
≥ sup P [S (S, d) ≥ log M + γ] − exp (−γ)

βα (P, Q) =

(18)

which is precisely [10, Theorem 7].
B. Converses using hypothesis testing
To show the converses in this section, we use the equivalent
list decoding setting, the idea originally put forward by Csisz´ r
a
[3]. In list decoding, the encoder is the random transformation
PX|S , where X takes values on the subset of allowable channel
inputs, F ⊆ X , and the decoder is the random transformation
˜
PS|Y , where S ∈ M(L) , and M(L) consists of all lists
˜
containing no more than L elements drawn from M. The error
probability with list decoding is the probability that the source
output S is not on the decoder output list for Y :
s
x∈X y∈Y s∈M(L) s∈˜
˜

inf sup β1− (PS PX|S PY |X , QS PX|S PY ) ≤ L
¯

PX|S P ¯
Y

(22)

(23)

where the supremum is over all probability measures PY
¯
deﬁned on the channel output alphabet Y.
Unfortunately, computing the inﬁmum over all encoders
in (23) in general is not feasible. However, if the channel is symmetric (in a sense formalized in Theorem 4),
β1− (PS PX|S PY |X , US PX|S PY ) is independent of PX|S , and
¯
the following result holds.

(19)

While traditionally list decoding has only been considered
in the context of ﬁnite alphabet sources, we generalize the
setting to sources with abstract alphabets.

Theorem 4 (Converse). Fix a probability measure PY . Assume
that the distribution of ıX;Y (x, Y ) according to both PY |X=x
¯
and PY does not depend on x ∈ F, where F ⊆ X is the
¯
subset of allowable channel inputs. Then, any ( , L, QS ) list
code must satisfy

Deﬁnition 6. (list decoder) The (L, QS ) list decoder is the
˜
random transformation PS|Y , where S takes values on QS ˜
measurable sets with QS -measure not exceeding L:
˜
QS S ≤ L

Q [W = 1]

Theorem 3 (Converse). Any ( , L, QS ) list code, where QS
is σ-ﬁnite, must satisfy

PS|Y (˜|y)PY |X (y|x)
s
˜

· PX|S (x|s)PS (s)

min

PW |X :
P[W =1]≥α

the optimal performance achievable among all randomized
tests PW |X : X → {0, 1} between probability distributions P
and Q on X is denoted by (1 indicates that the test chooses
P )3 . Note that Q need not be a probability measure, it just
needs to be σ-ﬁnite in order for the Neyman-Pearson lemma
and related results to hold.
The hypothesis testing converse for channel coding [12,
Theorem 27] can be generalized to joint source-channel coding
with list decoding as follows.

γ>0

= 1−

(21)

(20)

β1− (PS PY |X=x , QS PY ) ≤ L
¯

Deﬁnition 7. (list code) An ( , L, QS ) list code is a pair of
random transformations (PX|S , PS|Y ) such that (20) holds
˜
and the list error probability does not exceed .

(24)

where x ∈ F is arbitrary.
Remark 3. In the case of ﬁnite channel input and output
alphabets, the channel symmetry assumption of Theorem
4 holds, in particular, if the rows of the channel’s transition probability matrix are permutations of each other, and
PY n is the equiprobable distribution on the (n-dimensional)
¯
channel output alphabet, which, coincidentally, is also the
capacity-achieving output distribution. For Gaussian channels
with equal power constraint, which corresponds to F =
x : |x|2 = nP for some P > 0, any spherically-symmetric
PY n satisﬁes the assumption of Theorem 4.
¯

Of course, letting QS = US , where US is the counting
measure on S, we recover the conventional list decoder
deﬁnition. The traditional almost-lossless JSCC is described
by L = 1, QS = US . If the source alphabet is the real line, it
is reasonable to measure the list size in terms of its Lebesgue
measure.
To draw a connection with lossy joint source-channel coding, observe that any (d, ) lossy code can be converted to a
list code with list error probability not exceeding by feeding
the lossy decoder output to a function that outputs the list
of all source outcomes s within distortion d from the output
ˆ
z ∈ M of the original lossy decoder. Therefore, the set of all

3 Throughout, P , Q denote distributions, whereas P, Q are used for the
corresponding probabilities of events on the underlying probability space.

3

IV. ACHIEVABILITY
We generalize the recent achievability bound by Tauste
Campo et al. [8] applicable to almost-lossless JSCC to the
case with nonzero tolerable distortion d.
Denote the distortion d-ball around s ∈ M by
ˆ
Bd (s) = {z ∈ M : d(s, z) ≤ d}

Theorem 7 (Gaussian approximation). Under restrictions (i)–
(iv), the optimal (k, n, d, ) code satisﬁes
nC − kR(d) =

V(d) = Var [S (S, d)]
V = Var [ıX;Y (X ; Y )]

(25)

Theorem 5 (Achievability). There exists a (d, ) sourcechannel code with
≤

inf

PZ ,PX ,M,γ>0

M

− log HM − log γ| }] + E (1 − PZ (Bd (S)))

+e

Remark 4. Using Theorem 7, the rate-dispersion function of
JSCC is found as (recall Deﬁnition 4),

−γ

R(d)V + CV(d)
(32)
R3 (d)
Remark 5. If the channel and the data compression codes are
designed separately, we can invoke channel coding [12] and
lossy compression [10] results to show that,
√
nC − kR(d) ≤ min
nV Q−1 (η) + kV(d)Q−1 (ζ)

(26)

V(d) =

where the expectations are with respect to PS PX PY |X PZ , and
M

HM =

1
m
m=1

(27)

is the M −th harmonic number.

η+ζ≤

Given an (M, ζ) channel code and an (M, d, η) source code,
we may concatenate them to obtain a separate source-channel
code. The performance of such codes, optimized over M ,
is easily evaluated using the recent results in [10], [12], as
outlined in the following Theorem.

+ O (log(n + k))

M

∗

(M, d)}

(33)

Comparing (33) to (29), observe that if either the channel or
the source (or both) have zero dispersion, the joint sourcechannel coding dispersion can be achieved by separate coding.
In that special case, the asymptotics kick in fast enough to
neutralize the performance gain achieved by joint code design.

Theorem 6 (Achievability, SSCC). Fix PY |X , d and PS .
Denote by (M ) the minimum achievable maximal error
probability among all transmission codes of size M , and the
minimum achievable probability of exceeding distortion d with
a source code of size M by ∗ (M, d).
Then, there exists a (d, ) source-channel code with
≤ min{ (M ) +

(30)
(31)

X , Y are the capacity-achieving input and output random
variables.

E[exp{−|ıX;Y (X; Y ) + log PZ (Bd (S))
+

nV + kV(d)Q−1 ( ) + O (log(n + k))
(29)

VI. L OSSY TRANSMISSION OF A BMS OVER A BSC
In this section we particularize the bounds in Sections III,
IV and the approximation in Section V to the transmission of
a BMS with bias p over a BMS with crossover probability δ.
The distortion measure is bit error rate. The source and the
channel dispersions are given by [10], [12]:

(28)

V. G AUSSIAN A PPROXIMATION

1−δ
(34)
δ
1−p
V(d) = p(1 − p) log2
(35)
p
where note that (35) does not depend on d. A source of fair
coin ﬂips has zero dispersion, and as anticipated in Remark 5,
JSSC does not bring much of a gain in the ﬁnite blocklength
regime (Fig. 1). The situation is different if the source is
biased, with JSCC showing signiﬁcant gain over SSCC (Fig.
2).

In addition to the basic conditions (a)-(c) of Section II, in
this section we impose the following restrictions.
(i) The channel is stationary and memoryless, PY n |X n =
PY|X × . . . × PY|X , either discrete with ﬁnite input
alphabet, or Gaussian with a maximal power constraint.

V = δ(1 − δ) log2

(ii) The source is stationary and memoryless, PS k = PS ×
. . . × PS , and the distortion measure is separable,
k
1
d(sk , z k ) = k i=1 d(si , zi ).
(iii) The distortion level satisﬁes dmin < d < dmax , where
dmin is deﬁned in (5), and dmax = inf z∈C E [d(S, z)],
ˆ
where the average is with respect to the unconditional
distribution of S. The excess-distortion probability satisﬁes 0 < < 1.
(iv) E d9 (S, Z ) < ∞ where the average is with respect to
PS ×PZ and PZ is the output distribution corresponding
to the minimizer in (4).
Conditions (i) and (ii) are standard in the memoryless joint
source-channel coding problem setup. The technical condition
(iv) ensures applicability of the Gaussian approximation in
Theorem 7 below.

VII. T RANSMISSION OF A GMS
OVER AN AWGN CHANNEL
2
Consider the transmission of a GMS with variance σS over
an AWGN channel with signal to noise ratio P and the meansquare error distortion measure. The source and the channel
dispersions are given by [10], [12]:
1
1
1−
2
(P + 1)2
1
V(d) = log2 e
2
V =

4

log2 e

(36)
(37)

1

1

C
R(d)

Converse (17)

0.9

C
R(d)

Converse (17)
Converse (24)
Approximation, JSCC (29)

0.9

0.8
0.8
0.7
0.7

Converse (24)

0.5
0.4

R

R

0.6

Approximation, JSCC = SSCC (29)

0.6

Achievability, JSCC (26)

0.5

0.3

Approximation, SSCC (33)

0.4
0.2

Achievability, SSCC (28)

0

Achievability, SSCC (28)

0.3

0.1

0

100

200

300

400

500

600

700

800

900

0.2

1000

n

0

100

200

300

400

500

600

700

800

900

1000

n

Fig. 1. Rate-blocklength tradeoff for the transmission of a fair BMS over a
BSC with crossover probability δ = d = 0.11 and = 10−2 .

Fig. 3. Rate-blocklength tradeoff for the transmission of a GMS with
0.5 over an AWGN channel with P = 1,

Converse (17)
C
Converse (24)
R(d)
Achievability, JSCC (26)
2
Approximation, JSCC (29)

=

d
2
σs

=

10−2 .

zero dispersion, joint code design offers signiﬁcant advantage
in the ﬁnite blocklength regime.
R EFERENCES
[1] C. E. Shannon, “Coding theorems for a discrete source with a ﬁdelity
criterion,” IRE Int. Conv. Rec., vol. 7, pp. 142–163, Mar. 1959, reprinted
with changes in Information and Decision Processes, R. E. Machol, Ed.
New York: McGraw-Hill, 1960, pp. 93-126.
[2] I. Csisz´ r, “Joint source-channel error exponent,” Prob. Contr. & Info.
a
Theory, vol. 9, no. 5, pp. 315–328, 1980.
[3] ——, “On the error exponent of source-channel transmission with a
distortion threshold,” IEEE Transactions on Information Theory, vol. 28,
no. 6, pp. 823–828, 1982.
[4] R. Pilc, “Coding theorems for discrete source-channel pairs,” Ph.D.
dissertation, M.I.T., 1967.
[5] ——, “The transmission distortion of a discrete source as a function of
the encoding block length,” Bell Syst. Tech. J., vol. 47, pp. 827–885,
1968.
[6] A. D. Wyner, “Communication of analog data from a Gaussian source
over a noisy channel,” Bell Syst. Tech. J., vol. 47, no. 5, pp. 801–812,
May/June 1968.
[7] ——, “On the transmission of correlated Gaussian data over a noisy
channel with ﬁnite encoding block length,” Information and Control,
vol. 20, no. 3, pp. 193–215, April 1972.
[8] A. Tauste Campo, G. Vazquez-Vilar, A. Guill´ n i F` bregas, and A. Mare
a
tinez, “Random-coding joint source-channel bounds,” in IEEE International Symposium on Information Theory, Saint-Petersburg, Russia, Aug.
2011.
[9] D. Wang, A. Ingber, and Y. Kochman, “The dispersion of joint sourcechannel coding,” in 49th Annual Allerton Conference on Communication, Control and Computing, Monticello, IL, Sep. 2011.
[10] V. Kostina and S. Verd´ , “Fixed-length lossy compression in the ﬁnite
u
blocklength regime,” IEEE Transactions on Information Theory, vol. 58,
no. 6, pp. 3309–3338, June 2012.
[11] I. Csisz´ r, “On an extremum problem of information theory,” Studia
a
Scientiarum Mathematicarum Hungarica, vol. 9, no. 1, pp. 57–71, 1974.
[12] Y. Polyanskiy, H. V. Poor, and S. Verd´ , “Channel coding rate in ﬁnite
u
blocklength regime,” IEEE Transactions on Information Theory, vol. 56,
no. 5, pp. 2307–2359, May 2010.
[13] H. Poor, An introduction to signal detection and estimation. Springer,
1994.
[14] W. Feller, An introduction to probability theory and its applications,
2nd ed. John Wiley & Sons, 1971, vol. II.

R

1.5

1

Achievability, SSCC (28)
0.5

Approximation, SSCC (33)
0

0

100

200

300

400

500

600

700

800

900

1000

n

Fig. 2. Rate-blocklength tradeoff for the transmission of a BMS with bias
p = 0.11 over a BSC with crossover probability δ = p = 0.11 and d = 0.05,
= 10−2 .

While the bounds are not as tight as in Section VI, they
sufﬁce to show that JSCC noticeably outperforms SSCC in
the displayed region of blocklengths (Fig. 3).
VIII. C ONCLUSION
To estimate the maximum source coding rate sustainable
as a function of channel blocklength and the probability of
exceeding a given distortion level, we have shown a new
converse bound (Theorem 1) and a new achievability bound
(Theorem 5) applicable in full generality. As evidenced by the
numerical results, the converse in Theorem 4, which applies
to those channels satisfying a certain symmetry condition,
can outperform the general converse in Theorem 1. The
asymptotic analysis of the new bounds leads to the Gaussian
approximation in Theorem 7, which holds for both discrete and
Gaussian channels. Unless either the source or the channel has

5

