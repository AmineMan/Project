Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May  9 23:00:11 2012
ModDate:        Tue Jun 19 12:54:20 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      491462 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566999

Achievable Rates of Gaussian Channels with
Realistic Duty Cycle and Power Constraints
Hui Li

Dongning Guo

Dept. of Electronic Engineering and Information Science
University of Science and Technology of China
Hefei, Anhui 230027, China

Dept. of Electrical Engineering and Computer Science
Northwestern University
Evanston, IL 60208, USA

of a pulse spans exactly one symbol interval. However this
assumption is unrealistic because pulse waveform required
for band-limited channels is usually larger than one symbol
interval. As such, during a transition between zero and nonzero
symbol, the pulse waveform of the nonzero symbol leaks
into the interval of the zero symbol. A realistic duty cycle
constraint must include the extra transmission time due to
transitions between zero and nonzero symbols. In this paper
we investigate the optimal signaling for achieving the capacity
subject to the realistic duty cycle and power constraints.
The mathematical model of the input-constrained channels is
described in Section II.
The capacity of a discrete-time channel subject to various
input constraints is a classical problem. It is well-known that
Gaussian signaling achieves the capacity of a Gaussian channel
with average input power constraint only. Zamir [4] also shows
that the achievable rate using a white Gaussian input never
incurs a loss of more than half a bit per sample with respect to
the power constrained capacity. The interesting result of Smith
[5] indicates that the capacity-achieving distribution for scalar
AWGN channels under both average power and peak power
constraint is discrete and consists of a ﬁnite number of mass
points. A list of channels with discrete capacity-achieving
measures is provided in [6], including Poisson channels and
Rayleigh fading channels (see also [7]–[11]).
The main results of this paper are summarized in Section III.
As long as all costs associated with the constraints can be
decomposed into per-letter costs, the optimal input is i.i.d.. The
challenge in this work is that the realistic duty cycle concerns
consecutive symbols, so that the capacity-achieving input is no
longer i.i.d. and is hard to compute. In this paper, we develop
a good lower bound of the input-output mutual information as
a function of the input distribution. It is proved that, under the
realistic duty cycle constraint, a discrete ﬁrst-order Markov
input maximizes the lower bound, the distribution of which
can be efﬁciently computed.
We devote Section V to the numerical methods and results.
In order to compute the achievable rate of the Markov input, a
Monte Carlo method is introduced to numerically compute the
differential entropy rate of hidden Markov processes. Numerical results show that the rate achieved by the Markov input is
substantially higher than that achieved by any i.i.d. input and
that by deterministically scheduled Gaussian signaling.

Abstract—Many wireless communication systems are subject
to duty cycle constraint, that is, a radio only actively transmits
signals over a fraction of the time. For example, it is desirable
to have a small duty cycle in some low-power systems; a halfduplex radio cannot keep transmitting if it wishes to receive
useful signals; and a cognitive radio needs to listen to the channel
frequently to detect primary users. Zhang and Guo have shown
that the capacity of a Gaussian channel subject to an idealized
duty cycle constraint as well as average transmission power
constraint is achieved by discrete independent and identically
distributed (i.i.d.) on-off signaling in lieu of Gaussian signaling.
This paper extends the previous results by considering a more
realistic duty cycle constraint where the extra cost of transitions
between transmissions and nontransmissions due to pulse shaping
is accounted for. The capacity-achieving input is no longer
independent over time and is hard to compute. A lower bound of
the input-output mutual information as a function of the input
distribution is developed, which is shown to be maximized by
a ﬁrst-order Markov process, the distribution of which is also
discrete and can be computed efﬁciently. Simulation results show
that the Markov input is superior to i.i.d. inputs for the Gaussian
channel subject to the realistic duty cycle and average power
constraints.

I. I NTRODUCTION
In many wireless communication systems, radios are designed to be active only for a fraction of the time, which is
known as the duty cycle. For example, in the ultra-wideband
system in [1], short bursts of signals are transmitted to trade
bandwidth for power savings. Another example, the physical
half-duplex constraint also requires a radio to stop transmission over a frequency band from time to time if it wishes
to receive useful signals over the same band. Thus wireless
relays are subject to duty cycle constraint, so do cognitive
radios which have to listen to the channel frequently to avoid
causing interference to primary users [2].
The impact of duty cycle constraint on capacity-achieving
signaling has been studied in [3], where the duty cycle constraint is regarded as a requirement on the minimum fraction of
zero symbols in each codeword. A unique discrete independent
and identically distributed (i.i.d.) input is shown to achieve the
capacity of an additive white Gaussian noise (AWGN) channel
subject to duty cycle and power constraints. The results of
[3] rely on a crucial idealization that the analog waveform
1 This material is based on work supported by the NSF under Grant CCF0644344 and by the UNSRP of Anhui, China under Grant KJ2011A275.

1

n
In this paper, let Xk denote the subsequence
∞
(Xk , Xk+1 , · · · , Xn ), where Xk
= (Xk , Xk+1 , · · · ).
n
n
We also use shorthand X = X1 . The capacity of the
AWGN channel (1) with duty cycle constraint (q, c) and
power constraint γ is

II. S YSTEM M ODEL
We consider the equivalent baseband discrete-time model.
The received signal over a block of n symbols can be described
by
Yi = Xi + Ni
i = 1, · · · , n
(1)
where Xi denotes the transmitted symbol at time i and
N1 , · · · , Nn are independent standard Gaussian random variables. For simplicity, we assume no inter-symbol interference
is at receiver. Each symbol modulates a continuous-time pulse
waveform for transmission. If the width of a pulse were exactly
of one symbol interval, which is denoted by T , the duty cycle
is equal to the fraction of nonzero symbols in a codeword. In
practice, however, the pulse is usually wider than T , so that
the support of the transmitted waveform is greater than the
sum of the intervals corresponding to nonzero symbols. To
be speciﬁc, suppose the width of a pulse is (1 + 2c)T , then
each transition between zero and nonzero symbols incurs an
additional cost up to c T in terms of actual transmission time.
Let 1 − q denote the maximum duty cycle allowed. In this
paper, we require every codeword (x1 , x2 , · · · , xn ) to satisfy
1
n

n

C(γ, q, c) = lim

n→∞

1
n

max

PX n ∈Λn (γ,q,c)

I(X n ; Y n ).

(4)

The capacity is in fact achieved by a stationary input
process. This is justiﬁed in Section IV-A by showing that any
nonstationary input process has a stationary counterpart with
equal or greater input-output mutual information per symbol.
Let us denote the set of stationary distributions which satisfy
duty cycle constraint (q, c) and power constraint γ by
2
Λ(γ, q, c) = µ : µ is stationary, Eµ X1 ≤ γ,

µX1 ({0}) − 2 c µX1 ,X2 ({0} × (R\{0})) ≥ q . (5)
Theorem 1: For any µ ∈ Λ(γ, q, c), let
∞
L(µ) = I(X; Y ) − I(X1 ; X2 )

(6)

where I(X; Y ) is the mutual information of the additive white
Gaussian noise channel between the input symbol X, which
follows distribution µX1 , and the corresponding output Y . The
following properties hold:
a) L(µ) is a lower bound of the channel capacity;
b) The maximum of L(·) is achieved by a discrete ﬁrst-order
Markov process, denoted by µ∗ ;
c) Deﬁne Bi = 1{Xi =0} , i = 1, 2, · · · . Then µ∗ satisﬁes the
following property: for every i, conditioned on Bi and
Bi+1 , the variables Xi and Xi+1 are independent, and

n

1
1{xi =0} + 2c
1{xi =0,xi mod n +1 =0} ≤ 1 − q (2)
n i=1
i=1

where 1{·} is the indicator function, and the number of
transitions from zero to nonzero symbols is equal to the
number of transitions from nonzero to zero symbols. For
subsequent convenience, (2) is deﬁned in a circular manner
using the modular operation. As a consequence, (xn , x1 ) is
also regarded as a transition if xn = 0 and x1 = 0. This of
course has vanishing impact as n → ∞ and thus no impact
on the capacity. From now on, we refer to (2) as duty cycle
constraint (q, c). Note that the constraint assumed in [3] is
1
the special case of duty cycle constraint (q, 0). If c ∈ [0, 2 ],
then the left hand side of (2) is equal to the actual duty
cycle. If c > 1 , the left hand side of (2) is an overestimate
2
of the duty cycle. Nonetheless, we use the simple constraint
(2) because it concerns only pairs of consecutive symbols. In
addition, we consider the usual average input power constraint,
n
1
2
i=1 xi ≤ γ.
n

L(µ∗ ) = I(X; Y ) − I(B1 ; B2 ).

(7)

The optimal input distribution should exhaust the power
budget γ. This is because that increasing the input power by
scaling the input linearly not only maintains its duty cycle, but
also increases the mutual information.
IV. P ROOF OF T HEOREM 1
A. Stationarity of the Capacity-achieving Input Distribution
We ﬁrst establish the fact that a stationary distribution
achieves the capacity of the AWGN channel with the realistic duty cycle constraint and power constraint. Because the
AWGN channel (1) is memoryless, over n uses of the channel,

III. M AIN RESULTS
Let µ denote the probability distribution of the process
X1 , X2 , · · · . We use µXi to denote the marginal distribution
of Xi , and µXi ,Xj to denote the joint probability distribution
of (Xi , Xj ). Denote the set of n-dimension distribution which
satisfy duty cycle constraint (q, c) and power constraint γ by

n

pY n |X n (y n |xn ) =

pY |X (yi |xi )

(8)

i=1

Λn (γ, q, c) =
µ:

1
n

n

where pY |X (·|·) is the conditional probability density function
(pdf) of the output given the input of the AWGN channel.
Proposition 1: A stationary distribution1 achieves
µXi ({0})−2 c µXi ,Xi mod n+1({0}×(R\{0})) ≥ q,

i=1

Eµ

1
n

max

n
2
Xi

µ∈Λn (γ,q,c)

≤γ

(3)

1 The

i=1

I(X n ; Y n ).

(9)

stationarity of distribution ν on X n satisﬁes
νXs ,··· ,Xt = νXs+k ,··· ,Xt+k

where µXi ,Xj ({0} × (R\{0})) = P (Xi = 0, Xj = 0) denotes
the probability of a zero-to-nonzero transition.

for any index s, t, k satisﬁed 1 ≤ s ≤ t ≤ n and 1 ≤ s + k ≤ t + k ≤ n.

2

Proof: Let Tk (·) as a k-cyclic-shift operator on µ ∈
Λn (γ, q, c), deﬁned as

Property (a): Using the fact that processing reduce
relative entropy and µ is a stationary distribution, we have

Tk (µ) = µXk+1 ,··· ,Xn ,X1 ,···Xk

1
D(PY n PY1 ×PY2 ×· · ·×PYn )
n
1
≤ D(PX n PX1 ×PX2 ×· · ·×PXn )
n
n
1
k
I(X1 ; X2 ).
=
n

(10)

where k = 1, · · · , n − 1, and speciﬁcally T0 (µ) = µ. Deﬁne
a stationary distribution ν on X n as
ν=

1
n

n−1

Tk (µ).

(11)

k=0

Therefore
1
∞
lim D(PY n PY1 ×PY2 ×· · ·×PYn ) = I(X1 ; X2 ) (20)
n→∞ n
k
using the fact that the Ces´ ro mean of sequence I(X1 , X2 ) is
a
∞
I(X1 ; X2 ). Applying (12), (14) and (20),

B. The Input-output Mutual Information

∞
L(µ) = I(X; Y ) − I(X1 ; X2 ) ≤ I(µ) ≤ C(γ, q, c). (21)

Proposition 2: Let the input follows distribution µ ∈
Λ(γ, q, c). The limit of the input-output mutual information
per symbol as a function of µ can be expressed as

Thus Property (a) is established.
Property (b): For ∀µ ∈ Λ(γ,q,c), which is not Markov in
general, its ﬁrst-order Markov approximation ν is deﬁned by

(12)

νX1 ,··· ,Xn = µX1 µX2 |X1 µX3 |X2 · · · µXn |Xn−1 .

where I(X; Y ) is the mutual information of the AWGN
channel between the input X, which follows distribution µX1
and the corresponding output Y , h(Y ) is the differential
entropy of Y and h(Y ) is the differential entropy rate of
output process {Yi }.
Proof: The mutual information between X n and Y n can
be expressed using relative entropies
I(X n ; Y n ) = D(PY n |X n PY n |PX n )

∞
∞
I(Z1 ; Z2 ) = I(Z1 ; Z2 ) = I(X1 ; X2 ) ≤ I(X1 ; X2 ) (23)

(13)

where equality holds if and only if {Xi } is a ﬁrst-order
Markov process. By (23), L(ν) ≥ L(µ). So for any µ which
maximizes L(µ), ν can be deﬁned by (22) with L(ν) ≥ L(µ).
L(µ) must be maximized by a ﬁrst-order Markov process.
Property (c): Suppose ν is a stationary ﬁst-order Markov
process, sufﬁciently denote as ν = (X , PX2 |X1 ), where X
is the state space of ν and PX2 |X1 is the transition probability distribution. Let S1 = X \ {0}, α = PX2 |X1 (0|0),
β = P (X2 ∈ S1 |X1 ∈ S1 ) and η = P (X ∈ S1 ). Deﬁne a
new ﬁrst-order Markov process ν from ν as follows.
¯
Deﬁnition 1: Let ν , deﬁned on the same state space X as
¯
ν, be a ﬁrst-order Markov process denoted by (X , PZ2 |Z1 ),

where
α
z1 = 0 z2 = 0,




1 − β
z1 = 0 z2 = 0,
(24)
PZ2 |Z1 (z2 |z1 ) = 1−α

 η PX (z2 ) z1 = 0 z2 = 0,


β
 P (z )
z1 = 0 z2 = 0.
η X 2

− D(PY n PY1 ×···×PYn ) (14)
(15)

Then
1
I(X n ; Y n ) = I(X; Y )−h(Y ) + h(Y ). (16)
n→∞ n
Proposition 2 is established.
When the input is an i.i.d. random process, the output is also
i.i.d., h(Y ) = h(Y ). This implies the following corollary.
Corollary 1: Among all i.i.d. input, the one that maximizes
the mutual information under duty cycle constraint (q, c) and
power constraint γ can be solved by
I(µ) = lim

maximize
PX

I(X; Y )

subject to PX (0) − 2cPX (0)(1 − PX (0)) ≤ q,

(22)

Evidently, ν and µ have identical marginal distributions: νXi =
µXi , and also identical joint distributions of any consecutive
pairs: νXi ,Xi+1 = µXi ,Xi+1 . Since µ ∈ Λ(γ, q, c), we have
ν ∈ Λ(γ, q, c). Let {Xi } follow distribution µ and {Zi } follow
distribution ν. Then

= D(PY n |X n PY1 × · · · × PYn |PX n )
= nI(X; Y ) − nh(Y ) + h(Y n ).

(19)

k=2

Based on the concavity of the mutual information and (8) it
is easy to prove I(ν) ≥ I(µ) for any µ in Λn (γ, q, c).
According to Proposition 1, for any n, I(X n ; Y n ) is maximized by a stationary distribution. Therefore with n → ∞, the
capacity in (4) is achieved by a stationary input distribution.

I(µ) = I(X; Y ) − h(Y ) + h(Y )

(18)

(17)

2

E{X } ≤ γ.
In the special case of no transition cost, i.e., c = 0, the result
of (17) is equal to the result in [3].

It is easy to prove that the stationary distributions PZ = PX .
Moreover, ν satisﬁes the same power and duty cycle con¯
straint as ν, i.e., ν ∈ Λ(γ, q, c). Let Bi = 1{Xi =0} , then
¯
PB2 |B1 (0|0) = α and PB2 |B1 (1|1) = β. Let bi = 1{zi =0} . Since

C. Proof of Theorem 1
The mutual information expressed by (12) is hard to optimize, even if the input is restricted to Markov processes. To
simply the matter, we introduce a lower bound of I(µ), which
is given by L(µ) in (6).

PZ2 |Z1 (z2 |z1 ) = PB2 |B1 (b1 |b2 )

3

PX (z2 )
,
PB2 (b2 )

(25)

0
distribution of X0 given the past observations Y−∞ . In order
0
to estimate PX0 |Y−∞ , ﬁrst deﬁne the log-likelihood ratio:

Zi and Zi+1 are independent given Bi = 1{Zi =0} and Bi+1 =
1{Zi+1 =0} . Based on (24) and (25), it is easy to see that
I(Z1 ; Z2 ) = I(B1 ; B2 ) ≤ I(X1 ; X2 ).

(26)

(i)
Ln = log

The inequality follows on data-processing inequality [12].
The discreteness of the optimized input distribution is
proved in the following. According to the above proof lower
bound L(·) is maximized by a ﬁrst-order Markov process. The
transition probability distribution PX2 |X1 can be expressed as
PX2 |X1 (x2 |x1 ) = PB2 |B1 (b2 |b1 )

PX (x2 )
PB2 (b2 )

m−1

(i)

q0

(i)

PXn |Y n (X (i) |Y n ) = eLn

(27)

eL n

0
0
and when n → ∞, (34) converges to PX0 |Y−∞ (X (i) |Y−∞ ).
(i)

In addition, Ln+1 can be calculated from Ln iteratively as
(i)

I(X; Y )

PX ,PX (0)=q0

Ln+1 = R(i) (Yn+1 ) + F (i) (Ln )

(29)

IB (q0 ) = minimize I(B1 ; B2 )
q0 − 2cq0 PB2 |B1 (1|0) ≥ q

(31)

where PB = PB1 = PB2 . Since given any q0 , IX (q0 )−IB (q0 )
can be maximized by the maximum of IX (q0 ) and the
minimum of IB (q0 ) respectively, the maximization of (28)
must be achieved by PX , which maximizes I(X; Y ) for given
q0 . Therefore given q0 , the maximization in (29) is similar
to the problem in [3]. The difference to [3] is that in (29)
the distribution PX satisﬁes PX (0) = q0 , however in [3]
the distribution PX satisﬁes PX (0) ≥ q. Following a similar
development as the proof in [3], PX can be proved to be
discrete. Therefore, the maximum of the lower bound L(·) is
achieved by a discrete ﬁrst-order Markov process.
Based on Theorem 1, in order to ﬁnd the lower bound of
the capacity, we can maximize L(µ) and obtain an optimized
discrete ﬁrst-order Markov input µ∗ in Λ(γ, q, c). Let µ0
denote the capacity-achieving distribution, then
∗

∗

I(µ0 ) ≥ I(µ ) ≥ L(µ ).

(35)

where R(i) (·) and F (i) (·) are speciﬁed functions relative to
the Markov chain. Due to space limitation, the details of
the deduction is not given in this paper. For the hidden
Markov processes observed through the AWGN channel (1),
the entropy of HMPs can be computed as [15]

(30)

P (B2 |B1 )
PB (0)=q0

(34)

i=0

(28)

subject to IX (q0 ) = maximize

(33)

where m is the number of the states of Markov Chain,
X (i) ∈ X is the ith state and X is the state space of
(0)
Markov Chain. It is obviously that Ln ≡ 0. Then given
(0)
(1)
(m−1)
Ln = {Ln , Ln , · · · , Ln
},

where bi = 1{xi =0} , PX = µX and PX2 |X1 = µX2 |X1 . Then
the maximum of L(µ) can be achieved by the optimization
maximize IX (q0 ) − IB (q0 )

PXn |Y n (X (i) |Y n )
, i = 0, · · · , m − 1
PXn |Y n (X (0) |Y n )

h(Y ) = lim −
n→∞

r(y, ln ) log r(y, ln ) dy dPLn(ln ) (36)

m−1

m−1
(i)

φ(y − x )

r(y, Ln ) =
i=0

k=0

(i)

eLn

m−1 L(i)
n
i=0 e

PX2 |X1 (x(i) |x(k) )

2
1
φ(t) = √ e−t /2 .
2π

In order to compute the entropy rate of HMPs based on (36),
the key is to estimate the probability distribution of Ln , PLn .
we can evolve the distribution of Ln based on (35) from any
initial distribution PL0 . Due to space limitations, the detailed
algorithm is presented in [16].
B. Numerical Results
Based on the results in Section IV and V-A, we ﬁrst seek
a discrete Markov chain with ﬁnite alphabet that maximizes
L(µ). Once the optimal Markov distribution µ∗ is determined,
we compute the achievable rate I(µ∗ ) according to (12).
Fig. 1 shows the stationary (marginal) distribution for suboptimal Markov input. In order to compensate the transition
cost, additional fraction of zero symbol should be transmitted,
PX (0) > q. As the SNR increases, more and more weights are
put on distant constellation points, where less and less weights
are put on the zero letter.
In Fig. 2, the rates achieved by various optimized input
distributions are plotted against the SNR. The rate achieved
by the optimized Markov input is larger than that of suboptimal i.i.d. input calculated by formula (17) with duty cycle
constraint (q, c). The lower bound L(µ) is quite tight and can
be regarded as a good approximation of mutual information
of ﬁrst-order Markov inputs.
Fig. 3 and 4 demonstrate the sensitivity of the achievable
rates to the duty cycle q and the transition cost c, respectively.

(32)

V. N UMERICAL METHODS AND RESULTS
A. Computation of the entropy of Hidden Markov Processes
In order to numerically calculate the mutual information (12), it is important to compute the differential entropy
rate of a HMP generated by Markov input through the AWGN
channel. Computing the (differential) entropy rate of HMPs is
a hard problem. Most works in this area focus on the entropy
rate of the binary Markov input through various channels.
Reference [13] and [14] presented two different methods to
approximate the entropy of the HMP output generated by
binary Markov input. Based on these existing studies, a Monte
Carlo algorithm is provided in this paper to compute the
differential entropy rate of HMPs generated from a m-state
Markov chain (m ≥ 3) through the AWGN channel. We sketch
the main ideas in our algorithm in this subsection.
Based on Blackwell’s work [15], the entropy rate of HMPs
can be expressed in terms of the distribution of the conditional

4

30

2
0.01

Probability mass points

0.01

20

0.02

15

0.02

0.02

0.03

0.03

Achievable rate (bits per channel use)

1.8

25

0.02

0.03

0.03

0.01
0.03

0.03
0.02

0.01

5
0.10

0

0.10

0.10

0.81

0.81

−4

−2

0.81

0

0.02

0.03

0.04

0.05

0.04

0.04

0.05

0.05

0.04

0.04

0.03

10

0.08

0.06

0.05

0.05

0.04

0.09

0.07

0.10

0.09

0.10
0.80

0.79

0.77

0.75

0.73

0.70

0.67

0.65

0.62

0.60

2

4

6
8 10
SNR(dB)

12

14

16

18

Achievable rate (bits per channel use)

1

0.5

5

10

15

20

SNR(dB)

Fig. 2. The achievable rate vs. the SNR. Duty cycle ≤ 0.5, transition cost
c = 1.0.
1.8

Achievable rate (bits per channel use)

1.6
1.4
1.2
1
0.8
0.6

Markov input
i.i.d. input, duty cycle constraint (q,0)
i.i.d. input, duty cycle constraint (q,c)
Deterministic schedule
0.2

0.4

0.4
0.6
duty cycle constraint q

Markov input
i.i.d. input
0.5

1
transition cost c

1.5

2

R EFERENCES

1.5

0
0

0.6

[1] D. Julian and S. Majumdar, “Low power personal area communication,”
in Proc. Inform. Theory Appl. Workshop, La Jolla, CA, USA, 2011.
[2] W.-Y. L. I. F. Akyildiz and e. M. C. Vuran, “Next generation/dynamic
spectrum access/cognitive radio wireless networks: A survey,” Computer
Networks, vol. 50, pp. 2127–2159, 2006.
[3] L. Zhang and D. Guo, “Capacity of gaussian channels with duty cycle
and power constraints,” in Proc. IEEE Int. Symp. Information Theory,
(St Petersburg, Russia), pp. 424–428, July 2011.
[4] R. Zamir, “A gaussian input is not too bad,” IEEE Trans. Inf. Theory,
vol. 50, pp. 1362–1367, June 2004.
[5] J. G. Smith, “The information capacity of amplitude and varianceconstrained scalar Gaussian channels,” Inf. Contr., vol. 18, pp. 203–219,
1971.
[6] T. H. Chan, S. Hranilovic, and F. R. Kschischang, “Capacity-achieving
probability measure for conditionally Gaussian channels with bounded
inputs,” IEEE Trans. Inf. Theory, vol. 51, pp. 2073–2088, June 2005.
[7] S. Shamai (Shitz), “Capacity of a pulse amplitude modulated direct
detection photon channel,” Proc. IEE Communications, Speech and
Vision, vol. 137, pp. 424–430, Dec. 1990.
[8] S. Shamai (Shitz) and I. Bar-David, “The capacity of average and peakpower-limited quadrature Gaussian channels,” IEEE Trans. Inf. Theory,
vol. 41, pp. 1060–1071, July 1995.
[9] I. C. Abou-Faycal, M. D. Trott, and S. Shamai (Shitz), “The capacity of
discrete-time memoryless Rayleigh-fading channels,” IEEE Trans. Inf.
Theory, vol. 47, pp. 1290–1301, May 2001.
[10] M. Katz and S. Shamai (Shitz), “On the capacity-achieving distribution
of the discrete-time noncoherent and partially coherent AWGN channels,” IEEE Trans. Inf. Theory, vol. 50, pp. 2257–2270, Oct. 2004.
[11] M. C. Gursoy, H. V. Poor, and S. Verd´ , “The noncoherent Rician fading
u
channel-part I: Structure of the capacity-achieving input,” IEEE Trans.
Wireless Commun., vol. 4, pp. 2193–2206, Sep. 2005.
[12] T. M. Cover and J. A. Thomas, Elements of information theory. New
Jersey: Wiley-Interscience, 2006.
[13] E. Ordentlich and T. Weissman, “Approximations for the entropy rate of
a hidden markov process,” in Proc. IEEE Int. Symp. Information Theory,
pp. 2198 –2202, Sept. 2005.
[14] J. Luo and D. Guo, “On the entropy rate of hidden markov processes
observed through arbitrary memoryless channels,” IEEE Trans. Inf.
Theory, vol. 55, pp. 1460 –1467, April 2009.
[15] D. Blackwell, “The entropy of functions of ﬁnite-state markov chains,”
in Trans. First Prague Conf. Information Theory, Statistical Decision
Functions, Random Processes, (Prague, Czechoslovakia), pp. 13–20,
1957.
[16] L. Zhang, H. Li, and D. Guo, “Capacity of gaussian channels with duty
cycle and power constraints.” submitted to the IEEE Trans. Inf. Theory,
2012.

2

0.2

0.8

case in [3], i.i.d. input is not a good choice under the realistic
duty cycle constraint.

2.5

0.4

1

Fig. 4. The achievable rate vs. transition cost, with SNR = 10 dB and
q = 0.5.

Markov input
Capacity of AWGN channel
i.i.d. input, duty cycle constraint (q,0)
i.i.d. input, duty cycle constraint (q,c)
Lower bound L(µ*)

0

1.2

0
0

20

3.5

0
−5

1.4

0.2

Fig. 1. The marginal distribution of the stationary Markov input. Duty cycle
≤ 0.5, transition cost c = 1.0.

3

1.6

0.8

1

Fig. 3. The achievable rate vs. the duty cycle, SNR = 10 dB and transition
cost c = 1.0.

The performance of Markov inputs is superior to i.i.d. inputs
as well as Gaussian signaling with deterministic schedule. Fig
3 shows that the performance of i.i.d. input is similar to the
deterministic schedule, which implies that different from the

5

