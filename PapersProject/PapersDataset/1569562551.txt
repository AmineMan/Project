Creator:         TeX output 2012.05.09:0601
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May  9 06:01:36 2012
ModDate:        Tue Jun 19 12:54:53 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      275428 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569562551

Information Spectrum Approach to Overﬂow
Probability of Variable-Length Codes with
Conditional Cost Function
Ryo NOMURA

Toshiyasu MATSUSHIMA

School of Network and Information
Senshu University, Japan.
E-mail: nomu@isc.senshu-u.ac.jp

School of Fundamental Science and Engineering
Waseda University, Japan.

threshold, which means that there exists a variable-length
code, whose overﬂow probability is smaller than or equal
to ε. Then, we ﬁrst determine the inﬁmum of ε-achievable
threshold for general sources. Next, we consider the secondorder achievable threshold for general sources. The ﬁner
evaluation of the achievable rates, called the second-order
achievable rates, have been analyzed in several contexts [7]–
[10]. Analogously to these settings, we deﬁne the second-order
achievable threshold on the overﬂow probability and derive the
inﬁmum of the second-order achievable threshold. Nomura
and Matsushima [11] have ﬁrst derived this quantity in the
case that the cost function is assumed to be memoryless. In
this paper, we treat the conditional cost function to generalize
this result. As a result, the similar analysis with [11], which
is on the basis of the information-spectrum methods, is also
efﬁcient in this case. These results show that the informationspectrum methods are still effective in analyses of the overﬂow
probability in variable-length coding problem.

Abstract—Lossless variable-length source coding with unequal
cost function is considered for general sources. In this problem,
the codeword cost instead of codeword length is important. The
inﬁmum of average codeword cost has already been determined
for general sources. We consider the overﬂow probability of
codeword cost and determine the inﬁmum of achievable overﬂow
threshold. Our analysis is on the basis of information-spectrum
methods and hence valid through the general source.

I. I NTRODUCTION
Lossless variable-length coding problem is quite important
not only from the theoretical viewpoint but also from the
viewpoint of its practical applications. The most fundamental
criterion to evaluate the performance of variable-length codes
is the average codeword length. The overﬂow probability
of codeword length is one of other criteria, which denotes
the probability of codeword length per symbol being above
a threshold R > 0. The overﬂow probability has been
investigated in several contexts. Merhav [1] has determined
the optimal exponent of the overﬂow probability given R for
uniﬁlar sources.
On the other hand, as is well known, if we impose unequal
costs on code symbols, we have to consider the codeword cost,
instead of codeword length. The average codeword cost, which
is a generalization of the average codeword length, have been
ﬁrst analyzed by Shannon [3]. Karp [4] has given the variablelength code, which minimizes the average codeword cost. The
inﬁmum of the average codeword cost has been determined by
Krause [5] for i.i.d. sources and extended to general sources
by Han and Uchida [6]. Among others, Uchida and Han [2]
have proposed the overﬂow probability of codeword cost. They
have considered the overﬂow probability as the probability
of codeword cost per symbol being above a threshold. Then,
they have shown the inﬁmum of achievable threshold, where
achievable threshold means that there exists a code whose
overﬂow probability of codeword cost decreases with given
exponent r.
In this paper, we also deal with the overﬂow probability
of codeword cost. In particular, we consider the ε-achievable

II. P RELIMINARIES
A. Variable-length codes with cost for general source
The general source is deﬁned as an inﬁnite sequence
)}∞
{
(
(n) (n)
(n)
X = X n = X 1 X2 · · · Xn
n=1

of n-dimensional random variables X n , where each compo(n)
nent random variable Xi takes values in a countable set
X.
The variable-length codes are characterized as follows. Let
ϕn : X n → U ∗ , ψn : {ϕn (x)}x∈X n → X n ,
be a variable-length encoder and a decoder, respectively, where
U = {1, 2, · · · , K} is called the code alphabet and U ∗ is the
set of all ﬁnite-length strings over U excluding the null string.
We consider the situation that there are unequal costs on
code symbols. Let us deﬁne the cost function c : U ∗ →
(0, +∞) considered in this paper. The cost c(ul ) of a sequence
ul ∈ U l is deﬁned by
c(ul ) =

This research was supported in part by JSPS Grant-in-Aid for Young
Scientists (B) No. 23760346.

l
∑
i=1

1

c(ui |ui−1 ),
1

where c(ui |ui−1 ) is a conditional cost of ui given ui−1 such
1
1
i−1
that 0 < c(ui |ui−1 ) < ∞ (∀i,∀ui ∈ U,∀u1 ∈ U i−1 ). The
1
conditional cost capacity αc (ui−1 ) given ui−1 is deﬁned by
1
1
the positive unique root α of the equation
∑
i−1
K −αc(ui |u1 ) = 1.
(1)

On the other hand, the ﬁner evaluation of the achievable
rates, called the second-order achievable rates, has been investigated in several contexts [7]–[10]. Analogously to their analyses, we evaluate the overﬂow probability in the second-order
√
sense. To do so, we consider the second case: ηn = na+L n
for all n = 1, 2, · · · . Hereafter, if √ consider the overﬂow
we
probability in the case ηn = na + L n, we call it the secondorder overﬂow probability given a in this paper, while in the
ﬁrst case it is called the ﬁrst-order overﬂow probability. The
second-order overﬂow probability given a of variable-length
encoder ϕn with threshold L is written as
{
}
(
√ )
c(ϕn (X n )) − na
√
εn ϕn , na + L n = Pr
>L .
n

ui ∈U

In this paper, we assume that the conditional cost capacity
αc (ui−1 ) is independent on ui−1 , more exactly, αc (ui−1 ) = α
1
1
1
holds for all ui−1 ∈ U i−1 .
1
Such a class of cost function has been ﬁrst considered in
[12]. Han and Uchida [6] also have treated this type of cost
function.We denote
cmax =

max

i,ui ∈U ,ui−1 ∈U i−1
1

c(ui |ui−1 )
1

In the ﬁrst case, we are interested in the inﬁmum of
threshold R that we can achieve.
Deﬁnition 2.3: Given 0 ≤ ε < 1, R is called an εachievable threshold for the source if there exists a variablelength encoder ϕn such that

for short. The variable-length codes considered in this paper,
satisﬁes the preﬁx condition:
∑
K −αc c(ϕn (x)) ≤ 1.
(2)
x∈X n

lim sup εn (ϕn , nR) ≤ ε.

Throughout this paper, the logarithm is taken to the base K.

n→∞

B. Overﬂow Probability of Codeword Cost

Deﬁnition 2.4:

The overﬂow probability of codeword length is deﬁned as
follows:
Deﬁnition 2.1: Given a threshold R, the overﬂow probability of the variable-length encoder ϕn is deﬁned by
}
{
1
l(ϕ(X n )) > R ,
εn (ϕn , R) = Pr
n

R(ε|X)

=

inf {R|R is an ε-achievable threshold} .

Also, in the analysis of second-order overﬂow probability,
we deﬁne the achievability:
Deﬁnition 2.5: Given 0 ≤ ε < 1 and 0 < a < ∞, L is
called an (ε, a)-achievable threshold for the source, if there
exists a variable-length encoder ϕn such that
(
√ )
lim sup εn ϕn , na + L n ≤ ε.

where l(·) denotes the length of a string.
In this paper, we generalize the above overﬂow probability
not only to the case for unequal costs on code symbols but
also for ﬁner evaluation of the overﬂow probability. To this
end, we consider the overﬂow probability of codeword cost as
follows:
Deﬁnition 2.2: Given some sequence {ηn }∞ , where 0 <
n=1
ηn < ∞ for each n = 1, 2, · · · , the overﬂow probability of
the variable-length encoder ϕn is deﬁned by
εn (ϕn , ηn ) = Pr {c(ϕn (X n )) > ηn } .

def

n→∞

Deﬁnition 2.6:
def

L(ε, a|X) = inf {L|L is an (ε, a)-achievable threshold} .
III. K EY L EMMAS
We show two lemmas that have important roles to derive
our main theorems. These lemmas are derived using the
information-spectrum method and hence valid through the
general source.
Lemma 3.1: For any general sources X and any sequence
of positive number {ηn }∞ , there exists a variable-length
n=1
encoder ϕn that satisﬁes
{
}
εn (ϕn , ηn ) < Pr zn PX n (X n ) ≤ K−αc ηn + zn K αc cmax +1 ,

(3)

{ηn }∞
n=1

is an arbitrary sequence, the above deﬁnition
Since
is general. In particular, we shall consider the following two
types of overﬂow probability in this paper:
1) ηn = nR, √
2) ηn = na + nL.
Remark 2.1: If we set ηn = nR for all n = 1, 2, · · · , the
overﬂow probability can be written as
{
}
1
n
εn (ϕn , nR) = Pr
c(ϕn (X )) > R .
n

for n = 1, 2, · · · , where {zn }∞ is a given sequence of an
n=1
arbitrary number satisfying zi > 0 for i = 1, 2, · · · and αc
denotes the cost capacity deﬁned in (1).
Proof: We use the code proposed by Han and Uchida [6].
Then, from the property of the code, it holds that

Thus, in the case that ηn = nR, the overﬂow probability
deﬁned by (3) means the probability that the codeword cost per
symbol exceeds some constant R. This is a natural extension
of the overﬂow probability of codeword length analyzed in [1]
or [2] to the overﬂow probability of codeword cost.

c(ϕ∗ (x)) ≤ −
n

1
log 2
log PX n (x) +
+ cmax ,
αc
αc

(4)

for all n = 1, 2, · · · , where ϕ∗ denotes the encoder of the
n
code. Furthermore, we set the decoder as the inverse mapping

2

of ϕ∗ .Note that the code is a uniquely decodable code for
n
general sources with countably inﬁnite source alphabet .
Next, we shall evaluate the overﬂow probability of this code.
Set
{
}
An = x ∈ X n zn PX n (X n ) ≤ K −αc ηn ,

Sn = {x ∈ X n |c(ϕn (x)) > ηn } .
Then, we have
{
}
Pr PX n (X n ) ≤ zn K −αc ηn
∑
∑
=
PX n (x) =
PX n (x) +

Sn = {x ∈ X n |c(ϕ∗ (x)) > ηn } .
n

≤

Then, the overﬂow probability of this code is given by
εn (ϕ∗ , ηn )
n

∑

PX n (x) +

PX n (x)

∑

PX n (x),

≤ εn (ϕn , ηn ) +

PX n (x) < K −αc (ηn −cmax )+log 2 ,

x∈X n

≥

K −αc (ηn −cmax )+log 2

Here, from the deﬁnition of An , for ∀x ∈

c
x∈Sn

c
K −αc ηn = |Sn | K −αc ηn .

c
c
|Bn ∩ Sn | ≤ |Sn | ≤ K αc ηn

it holds that

(11)

Hence, substituting (11) into (10), we have
{
}
Pr PX n (X n ) ≤ zn K −ηn ≤ εn (ϕn , ηn )+K αc ηn zn K −αc ηn
= εn (ϕn , ηn ) + zn .

Thus, we have
∑
∑ K −αc ηn
K −αc ηn
1≥
= |Ac |
.
PX n (x) >
n
zn
zn
c
c

Therefore, we have proved the lemma.
IV. I NFIMUM OF ε- ACHIEVABLE THRESHOLD

x∈An

In this section, we determine R(ε|X) for general sources.
Before showing the theorem, we deﬁne the function F (R) as:
{
}
1
1
def
F (R) = lim sup Pr
log
≥R .
nαc
PX n (X n )
n→∞

This mean that
|Sn ∩ Ac | ≤ |Ac | < zn K αc ηn .
n
n

∑

This mean that
(6)

K −αc ηn
.
PX n (x) >
zn

x∈An

(10)

c
x∈Sn

x∈Sn ∩Ac
n

Ac ,
n

c
x∈Bn ∩Sn
c
|Bn ∩ Sn | zn K −αc ηn .

Here, from (2), we have
∑
∑
1≥
K −αc c(ϕx (x)) ≥
K −αc c(ϕx (x))

for ∀x ∈ Sn . Substituting the above inequality into (5), we
have

= Pr {X ∈ An } + |Sn ∩ Ac | K −αc (ηn −cmax )+log 2 .
n

(9)

Thus, we have
{
}
Pr PX n (X n ) ≤ zn K −αc ηn
∑
≤ εn (ϕn , ηn ) +
zn K −αc ηn

1
log 2
log PX n (x) +
+ cmax > ηn .
αc
αc
Thus, we have

n

PX n (x).

PX n (x) ≤ zn K −αc ηn .

−

∑

∑

On the other hand, for ∀x ∈ Bn it holds that

(5)

where Ac denotes the complement set of the set A.
Since (4) holds, for ∀x ∈ Sn , we have

< Pr {X n ∈ An } +

PX n (x)

c
x∈Bn ∩Sn

x∈Sn ∩Ac
n

εn (ϕ∗ , ηn )
n

∑

PX n (x)

c
x∈Bn ∩Sn

c
x∈Bn ∩Sn

≤ εn (ϕn , ηn ) +

x∈Sn ∩Ac
n

≤ Pr {X n ∈ An } +

∑

x∈Sn

Pr {X n ∈ Sn }
∑
=
PX n (x) +

=

x∈Sn ∩An

x∈Bn ∩Sn

x∈Bn

∑

(8)

(7)

Substituting (7) into (6), we have
εn (ϕ∗ , ηn ) < Pr {X n ∈ An }+zn K αc ηn K −αc (ηn−cmax )+log 2
n

The following theorem is one of our main results:
Theorem 4.1: For 0 ≤ ∀ε < 1, it holds that

≤ Pr {X n ∈ An } + zn K αc cmax +1 ,

R(ε|X) = inf {R |F (R) ≤ ε } .

because log 2 ≤ 1 holds. Therefore, we have proved the
lemma.
Lemma 3.2: For any variable-length code and any sequence
{ηn }∞ , it holds that
n=1
{
}
εn (ϕn , ηn ) ≥ Pr PX n (X n ) ≤ zn K −αc ηn − zn ,

Proof: The proof consists of two parts.
(Direct Part) Let R0 be as
R0 = inf {R |F (R) ≤ ε } ,
for short. Then, in this part we prove that

for n = 1, 2, · · · where {zn }∞ is a given sequence of an
n=1
arbitrary number satisfying zi > 0 for i = 1, 2, · · · .
Proof: Let ϕn be an encoder of variable-length code. Set
{
}
Bn = x ∈ X n PX n (x) ≤ zn K −αc ηn ,

R(ε|X) ≤ R0 + γ,
for any γ > 0 by showing that R0 is an ε-achievable overﬂow
threshold for the source. Let ηn be as ηn = n(R0 + γ), then

3

from Lemma 3.1 there exists a variable-length encoder ϕn that
satisﬁes

for any variable-length encoder. Hence, we have
lim sup εn (ϕn , nR1 )
n→∞
{
}
1
1
≥ lim sup Pr
log
≥ R1 + γ > ε,
n
PX n (X n )
n→∞

εn (ϕn , nR0 )
{
}
< Pr zn PX n (X n ) ≤ K−nαc (R0 +γ) + zn K αc cmax +1 ,
for n = 1, 2, · · · . Thus, we have
{
}
1
1
log zn
εn (ϕn , nR0 ) < Pr
log
≥ R0 +γ +
nαc
PX n (X n )
nαc
αc cmax +1
+ zn K
,

where the last inequality is derived from (13) and the deﬁnition
of F (R). This is a contradiction and the converse part has been
proved.
Remark 4.1: In the analysis of ε-ﬁxed-length source coding
for general sources, the function
{
}
1
1
F (R) = lim sup Pr
log
≥R .
nαc
PX n (X n )
n→∞

for n = 1, 2, · · · . Notice that zn > 0 is an arbitrary number.
√
Set zn = K − nγ , then we have
{
√ }
1
nγ
1
εn (ϕn , nR0 ) < Pr
log
≥ R0 +γ −
nαc
PX n (X n )
nαc

is used in the case that αc = 1 [13] (see, also [14, Theorem
1.6.1]). This suggests a deep relationship between the overﬂow
probability of codeword length in variable-length coding and
the error probability in ﬁxed-length coding. The similar relationship in the second-order case is also clariﬁed by Theorem
5.1 in the following section and [8, Theorem 3].
From the above theorem, we can show a corollary. Before
describing the corollary, we deﬁne the spectral sup-entropy
rate [14]1 :

√

+ K − nγ+αc cmax +1
{
}
1
1
< Pr
log
≥ R0
nαc
PX n (X n )
+ K−

√
nγ+αc cmax +1

,

γ
for sufﬁciently large n, because γ > √nαc as n → ∞.
Thus, since αc and cmax are positive constants, by taking
lim supn→∞ , we have

def

H(X) = p- lim sup

lim sup εn (ϕn , nR0 )
n→∞
{
}
1
1
≤ lim sup Pr
log
≥ R0 .
nαc
PX n (X n )
n→∞

n→∞

Then, the following corollary holds.
Corollary 4.1:

Hence, from the deﬁnition of R0 we have
lim sup εn (ϕn , nR0 )
n→∞

R(0|X) =

≤ ε.

(14)

So far, we have considered the ﬁrst-order achievable threshold. In this section, we consider the second-order achievability.
In the second-order case, the inﬁmum (ε, a)-achievable overﬂow threshold for general sources is also determined by using
Lemma 3.1 and Lemma 3.2.
We deﬁne the function Fa (R) given a as follows, which is
correspondence with the function F (R) in the ﬁrst-order case.
{
}
− log PX n (X n ) − nαc a
def
√
Fa (L) = lim sup Pr
≥L .
nαc
n→∞

(12)

is an ε-achievable overﬂow threshold. Then we shall show a
contradiction.
Let ηn be as ηn = nR1 . Then, from Lemma 3.2 for any
sequence {zn }∞ ( zi > 0 i = 1, 2, · · · ) and any variablen=1
length encoder we have
{
}
εn (ϕn , nR1 ) > Pr PX n (X n ) ≤ zn K −nαc R1 − zn ,

Then, we have
Theorem 5.1: For 0 ≤ ∀ε < 1, it holds that

for n = 1, 2, · · · . Thus, for any variable-length encoder it holds
that for each n = 1, 2, · · ·
}
{
1
log zn
1
log
≥ R1 −
−zn .
εn (ϕn , nR1 ) > Pr
nαc
PX n (X n )
n

L(ε, a|X) = inf {L |Fa (L) ≤ ε } .
Proof: The proof is similar to the proof of Theorem 4.1.
Hence, we omit the proof.
The above theorem is valid for general sources. Hence
Theorem 5.1 is a quite general result. However, in general to
compute the function L (ε, a|X) is hard. Next, we consider a
simple case such as an i.i.d. source and we address the above
quantity speciﬁcally.

Set zn = K −nγ , where γ > 0 is a small constant that satisﬁes
R1 + γ < inf {R |F (R) ≤ ε } .

1
H(X).
αc

V. I NFIMUM OF (ε, a)- ACHIEVABLE THRESHOLD

Therefore, the direct part has been proved.
(Converse Part)
Assuming that R1 satisfying
R1 < inf {R |F (R) ≤ ε } ,

1
1
log
.
n
PX n (X n )

(13)

Since we assume that (12) holds, it is obvious that there exists
γ > 0 that satisﬁes the above inequality. Then, we have
}
{
1
1
log
≥ R1 +γ − K −nγ ,
εn (ϕn , nR1 ) > Pr
nαc
PX n (X n )

1 For any sequence {Z }∞
n n=1 of real-valued random variables, we deﬁne
the limit superior in probability of {Zn }∞ by p- lim supn→∞ Zn =
n=1
inf {β| limn→∞ Pr{Zn > β} = 0} (cf. [14]) .

4

For an i.i.d. source, from Corollary 4.1, we are interested
(
)
1
in L ε, αc H(X) X . To specify this quantity for an i.i.d.
source, we need to introduce the variance of self-information
as follows:
def

VI. C ONCLUSION
We have so far dealt with the overﬂow probability of
variable-length coding with cost function for general sources.
The overﬂow probability is important not only from the theoretical viewpoint but also from the engineering point of view.
However, there is few research on the overﬂow probability of
variable-length coding for general sources, even though the
equal cost function is assumed. Hence, our attempt to analyze
the overﬂow probability is meaningful.
We have revealed that, as shown in the proofs of the present
paper, the information-spectrum approach is substantial in
the analysis of the overﬂow probability of variable-length
coding with cost function. An application to more general cost
function is a future work.
Finally, as described in Remark 4.1 there is a deep relationship between the overﬂow probability of codeword length in
variable-length coding and the error probability in ﬁxed-length
coding. It is an interesting problem to consider the relation of
the overﬂow probability of codeword cost to the ﬁxed-length
coding.

2

σ 2 (X) = E (− log PX (X) − H(X)) .
In this paper, we assume that the above variance exists. Then,
from Theorem 5.1 we obtain the following theorem.
Theorem 5.2: For any i.i.d. source, it holds that
(
)
1
1√ 2
L ε, H(X) X =
σ (X)Φ−1 (1 − ε),
αc
αc
where Φ−1 denotes a inverse function of Φ and Φ(T ) is the
Gaussian cumulative distribution function with mean 0 and
variance 1, that is, Φ(T ) is given by
[ 2]
∫ T
y
1
√ exp −
Φ(T ) =
dy.
(15)
2
2π
−∞
Proof: From the deﬁnition of Fa (L), we have
FH(X)/αc (L)

{

= lim sup Pr
n→∞

{

}
− log PX n (X n ) − nH(X)
√
≥L
nαc

= lim sup Pr
n→∞

− log PX n (X n ) − H(X)
Lαc
√
≥√
2 (X)
nσ
σ 2 (X)

R EFERENCES
}

[1] N. Merhav, “Universal coding with minimum probability of codeword
length overﬂow,” IEEE Trans. Inf. Theory, vol. 37, no. 3, pp. 556–563,
1991.
[2] O. Uchida and T. S. Han, “The optimal overﬂow and underﬂow
probabilities of variable-length coding for general source,” IEICE Trans.
Fundamentals, vol. E84-A, no. 10, pp. 2457–2465, 2001.
[3] C. E. Shannon, “A mathematical theory of communication,” Bell Syst.
Tech. J., vol. 27, pp. 379–423, 623–656, 1948.
[4] R. S. Karp, “Minimum-redundancy coding for the discrete noiseless
channel,” IRE Trans. Information Theory, vol. IT-7, pp. 27–38, 1961.
[5] R. M. Krause, “Channels which transmit letter of unequal duration,”
Inform. and Controls, vol. 5, pp. 13–24, 1962.
[6] T. S. Han and O. Uchida, “Source code with cost as a nonuniform
random number generator,” IEEE Trans. Inf. Theory, vol. 46, no. 2, pp.
712–717, 2000.
[7] I. Kontoyiannis, “Second-order noiseless source coding theorems,” IEEE
Trans. Inf. Theory, vol. 43, no. 4, pp. 1339–1341, 1997.
[8] M. Hayashi, “Second-order asymptotics in ﬁxed-length source coding
and intrinsic randomness,” IEEE Trans. Inf. Theory, vol. 54, no. 10, pp.
4619–4637, 2008.
[9] Y. Polyanskiy, H. Poor, and S. Verd´ , “Channel coding rate in the ﬁnite
u
blocklength regime,” IEEE Trans. Inf. Theory, vol. 56, no. 5, pp. 2307–
2359, 2010.
[10] R. Nomura and T. S. Han, “Second-order resolvability, intrinsic randomness, and ﬁxed-length source coding for mixed sources: Information
spectrum approach,” arXiv:1106.1879, submitted to IEEE Trans. Inf.
Theory, 2011.
[11] R. Nomura and T. Matsushima, “Overﬂow probability of variable-length
codes with unequal cost on code symbols,” in Proc. The 34th Symposium
on Information Theory and its Applications, 2011, pp. 25–30.
[12] T. S. Han and A. Kato, “Kolmogorov complexity with cost and stochastic
processes,” IEICE Trans. Fundamentals, vol. J80-A, no. 3, pp. 524–531,
1997.
[13] Y. Steinberg and S. Verd´ , “Simulation of random processes and rateu
distortion theory,” IEEE Trans. Inf. Theory, vol. 42, no. 1, pp. 63–86,
1996.
[14] T. S. Han, Information-Spectrum Methods in Information Theory.
Springer, New York, 2003.

.

On the other hand, since we consider the i.i.d. source, from
the asymptotic normality (due to the central limit theorem) it
holds that
{
}
−log PX n (X n )−nH(X)
√
lim Pr
≤U
n→∞
nσ 2 (X)
[ 2]
∫ U
1
y
√ exp −
=
dy.
2
2π
−∞
Thus, we have
FH(X)/αc (L) =
(

∫

∞
√Lαc

)

σ 2 (X)

[ 2]
y
1
√ exp −
dy.
2
2π

1
Thus, L ε, αc H(X) X is given by
(
)
1
L ε, H(X) X
αc


[ 2]
 ∫ ∞

1
y
√ exp −
= inf L
dy ≤ ε


2
2π
√Lαc
σ 2 (X)
)
}
{
(
Lαc
≤ε
= inf L 1 − Φ √
σ 2 (X)
(
)
Since Φ √Lαc
is a continuous function and monotoni2
σ (X)

cally increases as L increases, we have
(
)
1
L ε, αc H(X) X αc
√
= Φ−1 (1 − ε).
2 (X)
σ
Therefore, the proof has been completed.

5

