Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 17:18:38 2012
ModDate:        Tue Jun 19 12:55:35 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      468682 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566727

Polar Codes: Robustness of the Successive
Cancellation Decoder with Respect to Quantization
S. Hamed Hassani and R¨ diger Urbanke
u
School of Computer and Communication Sciences, EPFL
Email: {seyedhamed.hassani, rudiger.urbanke}@epﬂ.ch
1

Abstract—Polar codes provably achieve the capacity of a
wide array of channels under successive decoding. This assumes
inﬁnite precision arithmetic. Given the successive nature of the
decoding algorithm, one might worry about the sensitivity of the
performance to the precision of the computation.
We show that even very coarsely quantized decoding algorithms lead to excellent performance. More concretely, we show
that under successive decoding with an alphabet of cardinality
only three, the decoder still has a threshold and this threshold is
a sizable fraction of capacity. More generally, we show that if we
are willing to transmit at a rate δ below capacity, then we need
only c log(1/δ) bits of precision, where c is a universal constant.

C(W, Q)

0

I

1

The maximum achievable rate, call it C(W, Q), of a simple
three message decoder, called the decoder with erasures, as a function
of the capacity of the channel for different channel families. From top
to bottom: the ﬁrst curve corresponds to the family of binary erasure
channels (BEC) where the decoder with erasures is equivalent to the
original SC decoder and, hence, the maximum achievable rate is the
capacity itself. The second curve corresponds to the family of binary
symmetric channels (BSC). The third curve corresponds to the family
of binary additive white Gaussian channels (BAWGN). The curve at
the bottom corresponds to a universal lower bound on the achievable
rate by the decoder with erasures.
Fig. 1.

I. I NTRODUCTION
Since the invention of polar codes by Arikan, [1], a large
body of work has been done to investigate the pros and cons
of polar codes in different practical scenarios (for a partial list
see [2]-[9]).
We address one further aspect of polar codes using successive decoding. We ask whether such a coding scheme is robust.
More precisely, the standard analysis of polar codes under successive decoding assumes inﬁnite precision arithmetic. Given
the successive nature of the decoder, one might worry how
well such a scheme performs under a ﬁnite precision decoder.
A priori it is not clear whether such a coding scheme still
shows any threshold behavior and, even if it does, how the
threshold scales in the number of bits of the decoder.
We show that in fact polar coding is extremely robust with
respect to the quantization of the decoder. In Figure 1, we
show the achievable rate using a simple successive decoder
with only three messages, called the decoder with erasures,
when transmission takes place over several important channel
families. As one can see from this ﬁgure, in particular for
channels with high capacity, the fraction of the capacity that
is achieved by this simple decoder is close to 1, i.e., even this
extremely simple decoder almost achieves capacity. We further
show that, more generally, if we want to achieve a rate which
is δ below capacity by δ > 0, then we need at most c log(1/δ)
bits of precision (all the logarithms in this paper are in base
2).
The signiﬁcance of our observations goes beyond the pure
computational complexity which is required. Typically, the
main bottleneck in the implementation of large high speed
coding systems is memory. Therefore, if one can ﬁnd decoders which work with only a few bits per message then

this can make the difference whether a coding scheme is
implementable or not.
A. Basic setting and deﬁnitions
Let W : X → Y be a binary memoryless symmetric (BMS)
channel, with input alphabet X = {0, 1}, output alphabet Y,
and the transition probabilities {W (y | x) : x ∈ X , y ∈ Y}.
Also, let I(W ) denote the capacity of W .
Let G2 = [ 1 0 ]. The generator matrix of polar codes is
11
deﬁned through the Kronecker powers of G2 , denoted by
GN = G⊗n . Throughout the paper, the variables N and n
2
are related as N = 2n . Let us review very brieﬂy how the
generator matrix of polar codes is constructed. Consider the
N × N matrix GN and let us label the rows of the matrix
GN from top to bottom by 0, 1, · · · , N − 1. Now assume that
we desire to transmit binary data over the channel W at rate
R < I(W ) with block-length N . One way to accomplish this
is to choose a subset I ⊆ {0, · · · , N − 1} of size N R and to
N
construct a vector U0 −1 = (U0 , · · · , UN −1 ) in a way that it
contains our N R bits of data at positions in I and contains,
at positions not in I, some ﬁxed value (for example 0) which
is known to both the encoder and decoder. We then send the
N
N
codeword X0 −1 = U0 −1 GN through the channel W . We
refer to the set I as the set of chosen indices or information

1

indices and the set I c is called the set of frozen indices. We
explain in Section II-A how the good indices are chosen. At
the decoder, the bits u0 , · · · , uN −1 are decoded one by one.
That is, the bit ui is decoded after u0 , · · · ui−1 . If i is a frozen
index, its value is known to the decoder. If not, the decoder
N
estimates the value of ui by using the output y0 −1 and the
estimates of u0 , · · · , ui−1 .

close to 1, very little is lost by quantizing. The methods used
here are extendable to other quantized decoders.
The rest of the paper is devoted to proving the ﬁrst part
of Theorem 1. Due to space limitation, we have omitted the
proof of the second part of theorem 1 as well as the proofs
of the lemmas stated in the sequel and we refer the reader to
[10] for more details.

B. Quantized SC decoder

II. G ENERAL F RAMEWORK FOR THE A NALYSIS

Let R∗ = R ∪ {±∞} and consider a function Q(x) :
∗
R → R∗ that is anti-symmetric (i.e., Q(x) = −Q(−x)).
We deﬁne the Q-quantized SC decoder as a version of the SC
decoder in which the function Q is applied to the output of
any computation that the SC decoder does. We denote such a
decoder by SCDQ .
Typically, the purpose of the function Q is to model the
case where we only have ﬁnite precision in our computations
perhaps due to limited available memory or due to other
hardware limitations. Hence, the computations are correct
within a certain level of accuracy which the function Q
models. Thus, let us assume that the range of Q is a ﬁnite
set Q with cardinality | Q | . As a result, all the messages
passed through the decoder SCDQ belong to the set Q.
In this paper we consider a simple choice of the function
Q that is speciﬁed by two parameters: The distance between
levels ∆, and truncation threshold M . Given a speciﬁc choice
of M and ∆, we deﬁne Q as follows:
 x
x ∈ [−M, M ],
 ∆ + 1 ∆,
2
(1)
Q(x) =

sign(x)M,
otherwise.
Note here that | Q | = 1 +

A. Equivalent tree channel model and analysis of the probability of error for the original SC decoder
Since we are dealing with a linear code, a symmetric channel and symmetric decoders throughout this paper, without loss
of generality we conﬁne ourselves to the all-zero codeword
(i.e., we assume that all the ui ’s are equal to 0). In order to
better visualize the decoding process, the following deﬁnition
is handy.
Deﬁnition 2 (Tree Channels of Height n): For each i ∈
{0, 1, · · · , N − 1}, we introduce the notion of the i-th tree
channel of height n which is denoted by T (i). Let b1 . . . bn
be the n-bit binary expansion of i. E.g., we have for n = 3,
0 = 000, 1 = 001, . . . , 7 = 111. With a slight abuse of
notation we use i and b1 · · · bn interchangeably. Note that for
our purpose it is slightly more convenient to denote the least
(most) signiﬁcant bit as bn (b1 ). Each tree channel consists
of n + 1 levels, namely 0, . . . , n. It is a complete binary tree.
The root is at level n. At level j we have 2n−j nodes. For
1 ≤ j ≤ n, if bj = 0 then all nodes on level j are check
nodes; if bj = 1 then all nodes on level j are variable nodes.
Finally, we give a label for each node in the tree T (i): For
each level j, we label the 2n−j nodes at this level respectively
from left to right by (j, 0), (j, 1), · · · , (j, 2n−j − 1).
All nodes at level 0 correspond to independent observations
of the output of the channel W , assuming that the input is 0.
An example for T (3) (that is n = 3, b = 011 and i = 3) is
shown in Fig. 2.

2M
∆ .

C. Summary of results
Theorem 1 (Main Statement): Consider transmission over a
BMS channel W of capacity I(W ) using polar codes and
a SCDQ with message alphabet Q. Let C(W, Q) denote the
maximum rate at which reliable transmission is possible for
this setup.
(i) Let |Q| = 3. Then there exists a computable decreasing sequence {Un }n∈N (see (19)) and a computable
increasing sequence {Ln }n∈N (see (20)), so that Ln ≤
C(W, Q) ≤ Un and

T (3)
(3, 1)

(2, 0)

(1, 0)

lim Ln = lim Un .

n→∞

(2, 1)

(1, 1)

(1, 2)

(1, 3)

n→∞

In other words, Un is an upper bound and Ln is a lower
bound on the maximum achievable rate C(W, Q) and for
increasing n these two bounds converge to C(W, Q).
(ii) To achieve an additive gap δ > 0 to capacity I(W ), it
sufﬁces to choose log |Q| = c log(1/δ).
Discussion: In Figure 1 the value of C(W, Q), |Q| = 3, is
plotted as a function of I(W ) for different channel families
(for more details see Section II-D2). A universal lower bound
for the maximum achievable rate is also given in Figure 1. This
suggests that even for small values of |Q| polar codes are very
robust to quantization. In particular for channels with capacity

(0, 0) (0, 1)

W

W

(0, 2) (0, 3)

W

W

(0, 4) (0, 5)

W

W

(0, 6) (0, 7)

W

W

Fig. 2.
Tree representation of the tree-channel T (3). The 3-bit binary
expansion of 3 is b1 b2 b3 = 011 (note that b1 is the most signiﬁcant bit).
The pair beside each node is the label assigned to it.
N
Given the channel output vector y0 −1 and assuming
that the values of the bits prior to ui are given, i.e.,
u0 = 0, · · · , ui−1 = 0, we now compute the probabilities
N
N
i−1
p(y0 −1 , ui−1 | ui = 0) and p(y0 −1 , u0 | ui = 1) via a
0
simple message passing procedure on the equivalent tree

2

channel T (i). We attach to each node in T (i) with label (j, k)
a message1 mj,k and we update the messages as we go up
towards the root node. We start with initializing the messages
at the leaf nodes of T (i). For this purpose, it is convenient
to represent the channel in the log-likelihood domain; i.e., for
the node with label (0, k) at the bottom of the tree which
corresponds to an independent realization of W , we plug in the
log-likelihood ratio (llr) log( W (yk | 0) ) as the initial message
W (yk | 1)
m0,k . That is,
m0,k = log(

W (yk | 0)
).
W (yk | 1)

evolve via the following relations. At the leaf nodes of the
tree we plug in the message m0,k = Q(log( W (yk | 0) )), and
ˆ
W (yk | 1)
the update equation for m(j,k) is
ˆ
mj,k = Q(mj−1,2k + mj−1,2k+1 ),
ˆ
ˆ
ˆ
if the node (j, k) is a variable node and
mj−1,2k
ˆ
mj−1,2k+1
ˆ
) tanh(
))),
2
2
(8)
if the node (j, k) is a check node. One can use the density
evolution procedure to recursively obtain the densities of the
messages mj,k .
ˆ
ˆ
Finally, let Ei denote the event that we make an error in
decoding the i-th bit, with a further assumption that we have
correctly decoded the previous bits u0 , · · · , ui−1 . In a similar
way as in the analysis of the original SC decoder, we get
mj,k = Q(2 tanh−1 (tanh(
ˆ

(2)

Next, the SC decoder recursively computes the messages
(llr’s) at each level via the following operations: If the nodes
at level j are variable nodes (i.e., bj = 1), we have
mj,k = mj−1,2k + mj−1,2k+1 ,

(3)

and if the nodes at level j are check nodes (i.e., bj = 0), the
message that is passed up is
mj−1,2k+1
mj−1,2k
) tanh(
)). (4)
mj,k = 2 tanh−1 (tanh(
2
2
In this way, it can be shown that ([1]) the message that we
obtain at the root node is precisely the value
mn,0 = log(

N
p(y0 −1 , ui−1 | ui = 0)
0
).
N −1
p(y0 , ui−1 | ui = 1)
0

(7)

1
ˆ
ˆ
Pr(Ei ) = Pr(mn,0 < 0) + Pr(mn,0 = 0).
ˆ
2

(9)

Hence, one way to choose the information bits for the algorithm SCDQ is to choose the bits ui according to the least
ˆ
values of Pr(Ei ).
An important point to note here is that with the decoder
SCDQ , the distribution of the messages in the trees T (i) is
different than the corresponding ones that result from the
original SC decoder. Hence, the choice of the information
indices is also speciﬁed by the choice of the function Q as
well as the channel W .
Note here that, since all of the densities takes their value
in the ﬁnite alphabet Q, the construction of such polar codes
can be efﬁciently done in time O( | Q | 2 N log N ). We refer
the reader to [1] for more details.

(5)

N
Now, given (y0 −1 , ui−1 ), the value of ui is estimated as
0
follows. If mn,0 > 0 we let ui = 0. If mn,0 < 0 we let
ui = 1. Finally, if mn,0 = 0 we choose the value of ui to
be either 0 or 1 with probability 1 . Thus, denoting Ei as the
2
event that we make an error on the i-th bit within the above
setting, we obtain

1
Pr(Ei ) = Pr(mn,0 < 0) + Pr(mn,0 = 0).
(6)
2
Given the description of mn,0 in terms of a tree channel, it
is now clear that we can use density evolution [2] to compute
the probability density function of mn,0 . In this regard, at
each level j, the random variables mj,k are i.i.d. for k ∈
{0, 1, · · · , 2n−j − 1}. The distribution of the leaf messages
m0,k is the distribution of the variable log( W (Y | 0) ), where
W (Y | 1)
Y ∼ W (y | 0). One can recursively compute the distribution
of mj,k in terms of the distribution of mj−1,2k , mj−1,2k+1 and
the type of the nodes at level j (variable or check) by using
the relations (3), (4) with the fact that the random variables
mj−1,2k and mj−1,2k+1 are i.i.d.

C. Gallager Algorithm
Since our aim is to show that polar codes under successive
decoding are robust against quantization, let us investigate
an extreme case. The perhaps simplest message-passing type
decoder one can envision is the Gallager algorithm. It works
with single-bit messages. Does this simple decoder have a nonzero threshold? Unfortunately it does not, and this is easy to
see. We start with the equivalent tree-channel model. Consider
an arbitrary tree-channel T (i). Since messages are only a
single bit, the “state” of the decoder at level j of T (i) can
be described by a single non-negative number, namely the
probability that the message at level j is incorrect. It is an
easy exercise to show that at a level with check nodes the
state becomes worse and at a level with variable nodes the
state stays unchanged and hence no progress in the decoding
is achieved, irrespective of the given tree. In other words, this
decoder has a threshold of zero. The problem is the processing
at the variable nodes since no progress is achieved there. But
since we only have two possible incoming messages there is
not much degree of freedom in the processing rules.

B. Quantized density evolution
Let us now analyze the density evolution procedure for
the quantized decoder. For each label (j, k) in T (i), let mj,k
ˆ
represent the messages at this label. The messages mj,k take
ˆ
their values in the discrete set Q (range of the function Q). It
is now easy to see that for the decoder SCDQ the messages
1 To simplify notation, we drop the dependency of the messages m
j,k to
the position i whenever it is clear from the context.

3

stochastic process Dn starts from the r.v. D0 = Q(L(Y ))
deﬁned as

w.p. p = Pr(L(Y ) > 0),
 ∞,
0,
w.p. e = Pr(L(Y ) = 0),
D0 =
(14)

−∞, w.p. m = Pr(L(Y ) < 0),

D. 1-Bit Decoder with Erasures
Motivated by the previous example, let us now add one
message to the alphabet of the Gallager decoder, i.e., we
also add the possibility of having erasures. In this case Q(x)
becomes the sign function2 , i.e.,

 ∞, x > 0,
0,
x = 0,
Q(x) =
(10)

−∞, x < 0.

and for n ≥ 0
Dn+1 =

As a result, all messages passed by the algorithm SCDQ take
on only three possible values: {−∞, 0, ∞}. In this regard, the
decoding procedure takes a very simple form. The algorithm
starts by quantizing the channel output to one of the three
values in the set Q = {−∞, 0, ∞}. At a check node we
take the product of the signs of the incoming messages
and at a variable node we have the natural addition rule
(0 ← ∞ + −∞, 0 ← 0 + 0 and ∞ ← ∞ + ∞, ∞ ← ∞ + 0
and −∞ ← −∞ + −∞, −∞ ← −∞ + 0 ). Note that on
the binary erasure channel, this algorithm is equivalent to the
original SC decoder.
Our objective is now to compute the maximum reliable rate
that the decoder SCDQ can achieve for a BMS channel W .
We denote this quantity by C(W, Q). The analysis is done in
three steps:
1) The density evolution procedure: To analyze the performance of this algorithm, ﬁrst note that since all our messages
take their values in the set Q, then all the random variables
that we consider have the following form

w.p. p,
 ∞,
0,
w.p. e,
(11)
D=

−∞,
w.p. m.

1
(f (D+ ) + f (D− )) = f (D).
2
With such a function f , the process {f (Dn )}n≥0 is a martingale and consequently we have Pr(D∞ = (1, 0, 0)) = f (D0 ).
Therefore, by computing the deterministic quantity f (D0 ) we
obtain the value of C(W, Q). However, ﬁnding a closed form
for such a function seems to be a difﬁcult task3 . Instead, the
idea is to look for alternative functions, denoted by g : D → R,
such that the process g(Dn ) is a super-martingale (submartingale) and hence we can get a sequence of upper (lower)
bounds on the value of Pr(D∞ = (1, 0, 0)) as follows. Assume
we have a function g : D → R such that g((0, 1, 0)) = 0 and
g(1, 0, 0) = 1 and for any D ∈ D,
1
(g(D+ ) + g(D− )) ≤ g(D).
(16)
2
Then, the process {g(Dn )}n≥0 is a super-martingale and for
n ≥ 0 we have
Pr(D∞ = (1, 0, 0)) ≤ E[g(Dn )].

(13)

(17)

The quantity E[g(Dn )] decreases by n and by using Lemma 4
we have

In order to compute the distribution of the messages mn,0
ˆ
at a given level n, we use the method of [1] and deﬁne
the polarization process Dn as follows. Consider the random
variable L(Y ) = log( W (Y | 0) ), where Y ∼ W (y | 0). The
W (Y | 1)
2 Note

(15)

where the plus and minus operations are given in (12), (13).
2) Analysis of the process Dn : Note that the output of
process Dn is itself a random variable of the form given in
(11). Hence, we can equivalently represent the process Dn
with a triple (mn , en , pn ), where the coupled processes mn , en
and pn are evolved using the relations (12) and (13) and we
always have mn + en + pn = 1. Following along the same
lines as the analysis of the original SC decoder in [1], we
ﬁrst claim that as n grows large, the process Dn will become
polarized, i.e., the output of the process Dn will almost surely
be a completely noiseless or a completely erasure channel.
Lemma 4: The random sequence {Dn = (pn , en , mn ), n ≥
0} converges almost surely to a random variable D∞ such that
D∞ takes its value in the set {(1, 0, 0), (0, 1, 0)}.
We now aim to compute the value of C(W, Q) = Pr(D∞ =
(1, 0, 0)), i.e., the highest rate that we can achieve with the
1-Bit Decoder with Erasures. In this regard, a convenient
approach is to ﬁnd a function f : D → R such that
f ((0, 1, 0)) = 0 and f (1, 0, 0) = 1 and for any D ∈ D

Here, the numbers p, e, m are probability values and p + e +
m = 1. Let us now see how the density evolves through the
tree-channels. For this purpose, one should trace the output
distribution of (7) and (8) when the input messages are two
i.i.d. copies of a r.v. D with pdf as in (11).
Lemma 3: Given two i.i.d. versions of a r.v. D with distribution as in (11), the output of a variable node operation (7),
denoted by D+ , has the following form

w.p. p2 + 2pe,
 ∞,
+
0,
w.p. e2 + 2pm,
(12)
D =

−∞, w.p. m2 + 2em.
Also, the check node operation (8), yields D− as

w.p. p2 + m2 ,
 ∞,
−
0,
w.p. 1 − (1 − e)2 ,
D =

−∞,
w.p. 2pm.

1
+
Dn , w.p. 2 ,
1
−
Dn , w.p. 2 ,

Pr(D∞ = (1, 0, 0)) = lim E[g(Dn )].
n→∞

(18)

3 The function f clearly exists as one trivial candidate for it is f (D) =
Pr(D∞ = (1, 0, 0)), where D∞ is the limiting r.v. that the process
{Dn }n≥0 with starting value D0 = D converges to.

here that we have further assumed that M = ∆ and ∆ → 0.

4

In a similar way, on can search for a function h : D → R
such that for h with the same properties as g except that the
inequality (16) holds in opposite direction and in a similar
way this leads us to computable lower bounds on C(W, Q). It
remain to ﬁnd some suitable candidates for g and h. Let us ﬁrst
note that a density D as in (11) can be equivalently represented
as a simple BMS channel given in Fig. 3. This equivalence
+1

p

{D:E(D)=

D0 =

m

U1 =

?
m

The equivalent channel for the density D given in (11).

stems from the fact that for such a channel, conditioned on
the event that the symbol +1 has been sent, the distribution
of the output is precisely D. With a slight abuse of notation,
we also denote the corresponding BMS channel by D. In
particular, it is an easy exercise to show that the capacity
(I(D)), the Bhattacharyya parameter (Z(D)) and the error
probability (E(D)) of the density D are given as
p
I(D) = (m + p)(1 − h2 (
)),
p+m
e
√
Z(D) = 2 mp + e, E(D) = 1 − p − ,
2
where h2 (·) denotes the binary entropy function. Since
the function Q is a not an injective function, we have
I(D + )+I(D − )
≤ I(D). This implies that the process In =
2
I(Dn ) is a bounded supermartingale. Furthermore, since
I(D = (1, 0, 0)) = 1 and I(D = (0, 1, 0)) = 0, we deduce
from Lemma 4 that In converges a.s. to a 0 − 1 valued r.v.
I∞ and hence

ACKNOWLEDGMENT
The authors wish to thank anonymous reviewers for their
valuable comments on an earlier version of this manuscript.
The work of Hamed Hassani was supported by Swiss National
Science Foundation Grant no 200021-121903.
R EFERENCES
[1] E. Arıkan, “Channel polarization: A method for constructing capacityachieving codes for symmetric binary-input memoryless channels,” IEEE
Trans. Info. Theory, vol. 55, no. 7, pp. 3051–3073, Jul. 2009.
[2] R. Mori and T. Tanaka, “Performance and construction of polar codes on
symmetric binary-input memoryless channels,” in Proc. ISIT 2009, Seoul,
South Korea, pp. 1496–1500, 2009.
[3] I. Tal and A. Vardy, “How to construct polar codes,” [online] Available:
arXiv:1105.6164v1 [cs.IT].
[4] C. Leroux, I. Tal, A. Vardy and W. J. Gross, “Hardware architectures for
successive cancellation decoding of polar codes,” in Proc. ICASSP 2011,
Prague, Czech Republic, pp. 1665-1668, 2011.
[5] R. Pedarsani, S. H. Hassani, I. Tal and E. Telatar, “On the construction
of polar codes,” in Proc. ISIT 2011, St. Petersberg, Russia, pp. 11-15,
2011.
[6] I. Tal and A. Vardy, “List decoding of polar codes,” in Proc. ISIT 2011,
St. Petersberg, Russia, pp. 1-5, 2011.
[7] S. B. Korada, A. Montanari, E. Telatar and R. Urbanke, “An empirical
scaling law for polar codes,” in Proc. ISIT 2010, Austin, Texas, USA,
pp. 884-888, 2010.
[8] S. H. Hassani, K. Alishahi and R. Urbanke, “On the scaling of polar
codes: II. The behavior of un-polarized channels,” in Proc. ISIT 2010,
Austin, Texas, USA, pp. 879–883, 2010.
[9] S. H. Hassani, S. B. Korada and R. Urbanke, “The compound capacity
of polar codes,” in Proc. 47th Annual Allerton Conference on Communication, Control, and Computing, pp. 16–21, 2009.
[10] S. H. Hassani and R. Urbanke, “Polar Codes: Robustness of the Successive Cancellation Decoder with Respect to Quantization,” In preparation.

(19)

(20)

Given a BMS channel W , one can numerically compute
C(W, Q) with arbitrary accuracy using the sequences Ln and
Un (see Figure 1). Also, for a channel W with capacity I(W )
and error probability E(W ), we have
E(W ) ≤

1 − I(W )
.
2

+−
I(D0 )

Hence, for transmission rate R < C(W, Q) and block-length
N = 2n , the probability of error of SCDQ , denoted by
β
log 3
Pe,Q (N, R) satisﬁes Pe,Q (N, R) = o(2−N ) for β < 2 2 .

for n ∈ N. In a similar way, one can obtain a sequence of
lower bounds for C(W, Q).
√
Lemma 5: Deﬁne the function F (D) as F (D) = p−4 pm
for D ∈ D. We have F (D = (1, 0, 0)) = 1, F (D =
+
−
(0, 1, 0)) = 0 and F (D )+F (D ) ≥ F (D).
2
Hence, the process Fn = F (Dn ) is a submartingale and for
n ∈ N we have
Ln .

−+
−−
F (D0 ) + F (D0 )

3

Now, from the fact that In is a supermartingale, we obtain

C(W, Q) ≥ E[Fn ]

(22)

Z(D− ) ≤ 2Z(D) and Z(D+ ) ≤ 2(Z(D)) 2 .

C(W, Q) = Pr(D∞ = (1, 0, 0)) = Pr(I∞ = 1) = E(I∞ ).

Un ,

+
−
I(D0 )+I(D0 )
= .5 and
2
++
+−
F (D0 ) + F (D0 ) +

++
I(D0 )

−1

C(W, Q) ≤ E[In ]

∞,
w.p. 1 − = 0.89,
−∞,
w.p. = 0.11.

= −0.075,
4
−+
−−
+
+ I(D0 ) + I(D0 )
U2 =
= 0.498.
4
Continuing this way, one can ﬁnd L10 = 0.264, U10 = 0.474
and L20 = 0.398, U20 = 0.465 and so on.
♦
3) Scaling behavior and error exponent: In the last step,
we need to show that for rates below C(W, Q) the block-error
probability decays to 0 for large block-lengths.
Lemma 7: Let D ∈ D. We have
L2 =

e

Fig. 3.

C(D, Q) ≤ C(W, Q), which

Therefore, we get L0 = F (D0 ) = −0.361 and U0 = I(D0 ) =
+
−
F (D0 )+F (D0 )
= −0.191,
0.5. We can also compute L1 =
2

+1

p

1−I(W )
}
2

leads to the universal lower bound obtained in Figure 1.
Example 6: Let the channel W be a BSC channel with
cross over probability = 0.11 (hence I(W ) ≈ 0.5). Using
(22) we obtain

e

−1

inf

Therefore,

(21)

5

