Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 23:24:43 2012
ModDate:        Tue Jun 19 12:55:19 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      382882 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566245

Universality in Polytope Phase Transitions
and Iterative Algorithms
Mohsen Bayati

Marc Lelarge

Andrea Montanari

Graduate School of Business
Stanford University

INRIA and ENS
Paris

Department of Electrical Engineering and
Department of Statistics
Stanford University

State evolution. The entries of xt are asymptotically Gaussian
with zero mean, and variance that can be explicitly computed
through a recursion, known as state evolution
Phase transitions in polytope geometry. As an application,
we use state evolution to prove universality of a phase transition on polytope geometry, with connections to compressed
sensing. This solves a conjecture put forward by David
Donoho and Jared Tanner [6], [12].
We think that the ﬁrst two results provide a useful tool to
establish a variety of universality in contexts in which the
iteration (1) would not a priori seem relevant. The third of
the above contributions demonstrates this. In fact, we shall
consider a more general setting than the one just described,
see Sections I-A and I-B.
Iterations of the form (1) emerge in a number of contexts.
In particular, approximate message passing algorithms (AMP)
for compressed sensing reconstruction [8], [4] can be recast in
the form (1) for suitable choices of the matrix A and functions
f . Generalizations of this algorithm were developed by several
groups [17], [15], [14] Here, we will use the term AMP for the
generalized recursion (1). We also notice that the universality
of compressed sensing phase transitions can be conjectured
from the results of the non-rigorous replica method [13], [16].
In the rest of this section we will state formally our results.
Due to their length, we defer proofs to the journal version of
this paper. We limit ourselves to present one important idea
(namely the representation of message passing as sum over
tree embeddings) in Section II.

Abstract—We consider a class of nonlinear mappings FA,N in
RN indexed by symmetric random matrices A ∈ RN ×N with
independent entries. Within spin glass theory, special cases of
these mappings correspond to iterating the TAP equations and
were studied by Erwin Bolthausen. Within information theory,
they are known as ‘approximate message passing’ algorithms.
We study the high-dimensional (large N ) behavior of the iterates
of F for polynomial functions F, and prove that it is universal,
i.e. it depends only on the ﬁrst two moments of the entries of A.
As an application, we prove the universality of a certain phase
transition arising in polytope geometry and compressed sensing.
This solves a conjecture by David Donoho and Jared Tanner.

I. M AIN R ESULTS
N ×N

Let A ∈ R
be a random Wigner matrix, i.e. a
symmetric random matrix with i.i.d. entries (Aij )i≤j satisfying
E{Aij } = 0 and E{A2 } = 1/N . A considerable effort has
ij
been devoted to studying the distribution of the eigenvalues of
such a matrix [3]. The universality phenomenon is a striking
recurring theme in these studies. Roughly speaking, many
asymptotic properties of the joint eigenvalues distribution are
independent of the entries distribution as long as it matches the
ﬁrst two moments, and satisﬁes certain tail conditions. We refer to [3] and references therein for a selection of such results.
Universality is extremely useful because it allows to compute
asymptotics for one entries distribution (e.g. Gaussian) and
then export the results to a broad class of distributions.
In this paper we are concerned with random matrix universality, albeit we do not focus onto eigenvalues properties.
Given A ∈ RN ×N , and an initial condition x0 ∈ RN
independent of A, we consider the sequence (xt )t∈N deﬁned
by letting, for t ≥ 0,

A. Universality
We will consider here and below a setting that is somewhat
more general than the one described so far. We will indeed
generalize the iteration (1) to take place in the vector space
Vq,N ≡ (Rq )N
RN ×q . Given a vector x ∈ Vq,N , we shall
most often regard it as an N -vector with entries in Rq , namely
x = (x1 , . . . , xN ), with xi ∈ Rq . Components of xi ∈ Rq will
be indicated as (xi (1), . . . , xi (q)) ≡ xi .
Given a matrix A ∈ RN ×N , we let it act on Vq,N in the
natural way, namely letting, for v ∈ Vq,N , v = Av be given by
N
N
vi = j=1 Aij vj for all i ∈ [N ], i.e. vi (s) = j=1 Aij vj (s)
for s ∈ [q]. In other words we identify A with the Kronecker
product A ⊗ Iq×q .

xt+1 = A f (xt ; t) − bt f (xt−1 ; t − 1) ,
(1)
1
bt ≡
div(f (x; t)) x=xt .
N
Here, div denotes the divergence operator and, for each
t ≥ 0, f ( · ; t) : RN → RN is a separable function, i.e.
f (z; t) = (f1 (z1 ; t), . . . , f2 (zN ; t)) with the fi ( · ; t) : R → R
polynomial functions of bounded degree. In particular bt =
N
N −1 i=1 fi (xt ; t).
i
The present paper is concerned with the asymptotic distribution of xt as N → ∞ with t ﬁxed, and establishes the
following results:
Universality. As N → ∞, the ﬁnite-dimensional marginals
of the distribution of xt are asymptotically insensitive to the
distribution of the entries of Aij .

Deﬁnition I.1. An AMP instance is a triple (A, F, x0 ) where:
1) A ∈ RN ×N is a symmetric matrix with Ai,i = 0 for all
i ∈ [N ].

1

2) F = {f k : k ∈ [N ]} is a collection of mappings f k :
Rq × N → Rq , (x, t) → f k (x, t) that are Lipschitz in
their ﬁrst argument;
3) x0 ∈ Vq,N is an initial condition.
Given F = {f k : k ∈ [N ]}, we deﬁne f ( · ; t) : Vq,N → Vq,N
by letting v = f (v; t) be given by vi = f i (vi ; t) for i ∈ [N ].

B. State evolution

Deﬁnition I.2. The approximate message passing orbit corresponding to the instance (A, F, x0 ) is the sequence of vectors
{xt }t≥0 , xt ∈ Vq,N deﬁned as follows, for t ≥ 0,

Deﬁnition I.5. We say that the sequence of AMP instances
{(A(N ), FN , x0,N )}N ≥0 is polynomial converging (or simply
converging) if it is (C, d)-regular and there exists: (i) An
integer k; (ii) A symmetric matrix W ∈ Rk×k with non˜
negative entries; (iii) A function g : Rq × Rq × [k] × N → Rq ;
˜
(iv) k probability measures P1 , . . . , Pk on Rq , with Pa a ﬁnite
mixture of (possibly degenerate) Gaussians for each a ∈ [k];
N
N
N
(v) For each N , a ﬁnite partition C1 ∪C2 ∪· · ·∪Ck = [N ];
0
0
q×q
(vi) k positive semideﬁnite matrices Σ1 ,. . . Σk ∈ R , such
that the following happens.
N
(1) For each a ∈ [k], we have limN →∞ |Ca |/N = ca ∈
(0, 1).
N
(1) For each N ≥ 0, each a ∈ [k] and each i ∈ Ca , we
i
have f (x, t) = g(x, Y (i), a, t) where Y (1), . . . , Y (N ) are
independent random variables with Y (i) ∼ Pa whenever i ∈
N
Ca for some a ∈ [k].
(2) For each N , the entries {Aij (N )}1≤i<j≤N are independent subgaussian random variables with scale factor C/N ,
N
N
EAij = 0, and, for i ∈ Ca and j ∈ Cb , E{A2 } = Wab /N .
ij
(3) For each a ∈ [k], almost surely,

xt+1 = A f (xt ; t) − Bt f (xt−1 ; t − 1) .

In order to characterize the high-dimensional limit, we need
to make some assumptions on the collection of functions FN .
We consider a setting in which the instance (A(N ), FN , x0 )
has a block structure: this generality is necessary for applications e.g. to polytope phase transitions.

(2)

Here Bt : Vq,N → Vq,N is the linear operator deﬁned by
letting, for v = Bt v,
A2 Jf j (xt , t) vi ,
j
ij

vi =
j∈[N ]

with Jf j denoting the Jacobian matrix of f j ( · ; t) : Rq → Rq .
The above deﬁnition can be summarized by the following
expression for the evolution of a single coordinate under AMP
xt+1 =
i

Aji f j (xt , t) −
j

j∈[N ]

t−1
A2 Jf j (xt , t)f i (xi , t − 1) .
ij

j∈[N ]

Recall that a centered random variable X is subgaussian
σ 2 λ2
with scale σ 2 if, for all λ > 0, we have E eλX ≤ e 2 .
Deﬁnition I.3. Let {(A(N ), FN , x0,N )}N ≥1 be a sequence
of AMP instances indexed by the dimension N , with A(N )
a random matrix and x0,N a random vector. We say that the
sequence is (C, d)-regular (or, for short, regular) polynomial
sequence if
(1) For each N , the entries (Aij (N ))1≤i<j≤N are independent
centered random variables. Further they are subgaussian with
common scale factor C/N .
i
(2) For each N , the functions fr ( · ; t) in FN (possibly random,
as long as independent from A(N ), x0,N ) are polynomials
with maximum degree d and coefﬁcients bounded by C.
(3) For each N , A(N ) and x0,N are independent. Further, we
N
have i=1 exp{ x0,N 2 /C} ≤ N C almost surely.
2
i

lim

N →∞

N
i∈Ca

Our next result establishes that the low-dimensional
marginals of {xt } are asymptotically Gaussian. State evolution
characterizes the covariance of these marginals. For each
t ≥ 1, state evolution deﬁnes a set of k positive semideﬁnite
matrices Σt = (Σt , Σt , . . . , Σt ), with Σt ∈ Rq×q . These are
a
1
2
k
obtained by letting, for each t ≥ 1
k

Σt
a

cb Wab Σt−1
b

=

(4)

b=1

Σt
a

Theorem
I.4.
Let
(A(N ), FN , x0,N )N ≥1
and
0,N
˜
(A(N ), FN , x )N ≥1 be any two (C, d)-regular polynomial
sequences of instances, that differ only in the distribution of
˜
the random matrices A(N ) and A(N ).
Denote by {xt }t≥0 , {˜t }t≥0 the corresponding AMP orx
bits. Assume further that for all N and all i < j,
˜
E{A2 } = E{A2 }. Then, for any set of Lipschitz functions
ij
ij
{ψN,i }N ≥0,1≤i≤N ψN,i : Rq → R, with Lip(ψN,i ) +
ψi,N ∞ ≤ B for all i, N , we have

t
t
= E g(Za , Ya , a, t)g(Za , Ya , a, t)T

,

(5)

t
for all a ∈ [k]. Here Yb ∼ Pb , Zb ∼ N (0, Σt ), and Σ0 = Σ0 .
b

Theorem I.6. Let (A(N ), FN , x0 )N ≥0 be a converging sequence of AMP instances, and denote by {xt }t≥0 the corresponding AMP sequence. Then for each t ≥ 1, each a ∈ [k],
and each locally Lipschitz function ψ : Rq × R → R such that
|ψ(x, y)| ≤ K(1 + y 2 + x 2 )K , we have, in probability,
2
2
1
N
N →∞ |Ca |

ψ(xt , Y (i)) = E{ψ(Za , Ya )} ,
j

lim

N

EψN,i (xt ) − EψN,i (˜t ) = 0 .
xi
i

g(x0 , Y (i), a, 0)g(x0 , Y (i), a, 0)T = Σ0 .
i
i
a

Notice that, for each r ∈ [q], a ∈ [k], t ∈ N, the functions
gr ( · , a, t) must be polynomials with maximum degree d and
coefﬁcients bounded by C.

The next theorem establishes that the behavior of the AMP
sequence {xt }t≥0 is, in the high dimensional limit, insensitive
to the distribution of the entries of the random matrix A.

1
lim
N →∞ N

1
N
|Ca |

(3)

N
j∈Ca

where Za ∼ N(0, Σt ) independent of Ya ∼ Pa .
a

i=1

2

(6)

Theorem I.8 (Donoho, 2005). There exists a function ρ∗ :
(0, 1) → (0, 1) such that the following holds. Fix δ ∈ (0, 1).
For each n ∈ N, let m(n) = nδ and deﬁne A(n) ∈ Rm(n)×n
to be a random matrix with i.i.d. Gaussian entries. Then, the
sequence of polytopes {A(n)C n }n≥0 has weak neighborliness
ρ∗ (δ) almost surely.

C. Universality of polytope neighborliness
A polytope Q is said to be centrosymmetric if x ∈ Q implies
−x ∈ Q. Following [6], [7] we say that such a polytope is
k-neighborly if every subset of k vertices of Q which does not
contain an antipodal pair, spans a (k − 1) dimensional face.
The neighborliness of Q is the largest value of k for which
this condition holds. The prototype of neighborly polytope is
the 1 ball C n ≡ {x ∈ Rn : x 1 ≤ 1}, whose neighborliness
is indeed equal to n.
It was shown in a series of papers [6], [7], [10], [9],
[11] that polytope neighborliness has tight connections with
the geometric properties of random point clouds, and with
sparsity-seeking methods to solve underdetermined systems of
linear equations. The latter are in turn central in a number of
applied domains, including model selection for data analysis
and compressed sensing.
Intuitive images of low-dimensional polytopes suggest that
‘typical’ polytopes are not neighborly: already selecting k = 2
vertices, does lead to a segment that connects them and passes
through the interior of Q. This conclusion is spectacularly
wrong in high dimension. Natural random constructions lead
to polytopes whose neighborliness scales linearly in the dimension. Motivated by the above applications, and following
[6], [7], [10], [9], we focus here on a weaker notion of
neighborliness. Roughly speaking, this corresponds to the
largest k such that most subsets of k vertices of Q span a
(k − 1)-dimensional face. In order to formalize this notion, let
F(Q; ) be the number of
-dimensional faces of Q.

A characterization of the curve δ → ρ∗ (δ) was provided in
[6], but a more explicit expression will be given below.
The proof of Theorem I.8 is based on exact expressions for
the number of faces F(A(n)C n ; ). These are in turn derived
from earlier work in polytope geometry by Affentranger
and Schneider [2] and by Vershik and Sporyshev [18]. This
approach relies in a fundamental way on the invariance of the
Gaussian distribution of A(n) under rotations.
Motivated by applications to data analysis and signal processing, Donoho and Tanner [12] carried out extensive numerical simulations for random polytopes of the form A(n)C n for
several choices of the distribution of A(n). They formulated a
universality hypothesis according to which the conclusion of
Theorem I.8 holds for a far broader class of random matrices.
Here we establish the ﬁrst rigorous result indicating universality of polytope neighborliness for a broad class of
random matrices. Deﬁne the curve (δ, ρ∗ (δ)), δ ∈ (0, 1),
parametrically by letting, for α ∈ (0, ∞):
2φ(α)
αΦ(−α)
, ρ=1−
,
(7)
α + 2(φ(α) − αΦ(−α))
φ(α)
√
2
where φ(z) = e−z /2 / 2π is the Gaussian density and
x
Φ(x) ≡ −∞ φ(z) dz. Explicitly, if the above functions on
the right-hand sides in Eqs. (7) are denoted by fδ (τ ), fρ (τ ),
−1
then ρ∗ (δ) ≡ fρ (fτ (δ)).
Here we extend the scope of Theorem I.8 from Gaussian
matrices, to matrices with independent subgaussian entries (not
necessarily identically distributed).
δ=

Deﬁnition I.7. Let Q = {Qn }n≥0 be a sequence of centrosymmetric polytopes indexed by the number of vertices 2n,
and denote by m = m(n) their dimension: Qn ⊆ Rm . We say
that Q has weak neighborliness ρ ∈ (0, 1) if for any ξ > 0,
F(Qn ; m(n)ρ(1 − ξ))
n→∞ F(C n ; m(n)ρ(1 − ξ))
F(Qn ; m(n)ρ(1 + ξ))
lim
n→∞ F(C n ; m(n)ρ(1 + ξ))
lim

=

1,

=

0.

Theorem I.9. Fix δ ∈ (0, 1). For each n ∈ N, let m(n) =
nδ and deﬁne A(n) ∈ Rm(n)×n to be a random matrix with
independent subgaussian entries, with mean 0, unit variance,
and common scale factor s independent of n. Then the
sequence of polytopes {A(n)C n }n≥0 has weak neighborliness
ρ∗ (δ) almost surely.

If the sequence Q is random, we say that Q has weak
neighborliness ρ (in probability) if the above limits hold in
probability.
In other words, a sequence of polytopes Qn has weak
neighborliness ρ, if Qn has close to the maximum possible
number of k faces, for all k < mρ(1 − ξ).
The existence of such sequences is clear when m(n) = n
(since in this case we can take Qn = C n , with ρ = 1), but it
is highly non-trivial when m is only a fraction of n. It comes
indeed as a surprise that this is instead a generic situation
as demonstrated by the following construction. For a matrix1
A ∈ Rm×n , and S ⊆ Rn , let AS ≡ {Ax ∈ Rm : x ∈
S}. In particular, AC n is the centrosymmetric m-dimensional
polytope obtained by projecting the n-dimensional 1 ball to
m dimensions. The following result was proved in [6].

By comparison, the most closely related result towards
universality is by Adamczak, Litvak, Pajor, and TomczakJaegermann [1]. For a class of matrices A(n) with i.i.d.
columns, these authors prove that A(n)C n has neighborliness
scaling linearly with n. This however does not suggest that
a limit weak neighborliness exists, and is universal, as established instead in Theorem I.9.
II. K EY IDEAS OF THE PROOF
Our main results are theorems I.4, I.6, I.9. Let us brieﬂy
outline the basic steps in our proof:
(i) Show that AMP (1) is asymptotically equivalent to a
message passing algorithm with N 2 messages.

1 Although A is not symmetric here, in our proof we show that it can be
casted as a block of a larger symmetric matrix.

3

Then for any t ≥ 1 and m(1), . . . , m(q) ≥ 0, we have for
any i ∈ [N ],

(ii) Show that messages in the latter algorithm can be represented as sums over embedding of trees with bounded degree
into the complete graphs with weights Aij .
(iii) Apply the moments method to these sums over trees
to show that only the ﬁrst two moments of Aij matter to
the asymptotic distribution of the messages, and hence prove
Theorem I.4.
(v) Use the same approach (tree representation, and moments
method) to compare the iteration (1) to an analogous iteration
in which the matrix A changes and is i.i.d. across iterations.
Analyze the latter to establish state evolution, Theorem I.6.
(vi) Use the correspondence between polytope neighborliness
and compressed sensing [7] to show that the weak neighborliness ρ∗ (δ) is actually a weak threshold for compressed sensing
reconstruction under 1 minimization.
(vii) Show that a suitable AMP algorithm (introduced in [8])
asymptotically solves the 1 minimization problem. The rectangular m × n sensing matrix is embedded into a symmetric
block-structured matrix A ∈ RN ×N with N = m + n. Use
state evolution to characterize the asymptotic behavior of the
minimizer, cf. [5]. This proves Theorem I.9.

q

E

t+1
yi→j (r) =
t+1
yi (r) =

(11)

Proposition II.2. Let (A(N ), FN , x0,N )N ≥1 be a (C, d)t
regular polynomial sequence of instances. Let zt and yi be
i
the sequences of vectors obtained by iterating (8)-(9) and (10)(11) respectively. Then for any t ≥ 1 and m(1), . . . , m(q) ≥ 0,
we have for any i ∈ [N ],
q

m(r)
t
zi (r)

E
r=1

m(r)

= O N −1/2 .

t
yi (r)

−E
r=1

The proof of Proposition II.1 and II.2 relies on the tree
representation described in the next section
B. Tree representation for polynomial functions
∈ [N ] and r ∈ [q],

By assumption of Proposition II.1, for
q

z(s)is ,

ci1 ,...,iq (r, t)

fr (z, t) =
i1 +···+iq ≤d

(12)

s=1

where ci1 ,...,iq (r, t) ∈ R and |ci1 ,...,iq (r, t)| ≤ cmax (uniformly
in ∈ [N ] and t ∈ N).
We now introduce families of ﬁnite rooted labeled trees
t
that will allow us to get a simple expression for the zi→j (r)’s
t
and zi (r), see Lemma II.3 below. For a vertex v in a rooted
tree T different from the root, we denote by π(v) the parent
of v in T . We denote the root of T by ◦. We consider
that the edges of T are directed towards the root and write
(u → v) ∈ E(T ) if π(u) = v. The unlabeled trees that we
consider are such that the root and the leaves have degree
one; each other vertex has degree less than d + 1, i.e. has less
than d children. We now describe the possible labels on such
trees. The label of the root is in [N ], the label of a leaf is in
[N ] × [q] × Nq and all other vertices have a label in [N ] × [q].
For a vertex v different from the root or a leaf, we denote
its label by ( (v), r(v)) and call (v) its type and r(v) its
mark. The label (or type) of the root is also denoted by (◦);
the label of a leaf v is denoted by ( (v), r(v), v[1], . . . v[q]).

(8)

We deﬁne for i ∈ [N ] and t ≥ 0, the vector zt+1 ∈ Rq by
i
A i fr (zt →i , t).

At i fr (yt→i , t).

The proof of Theorem I.6 is based on the following result that
t
shows that the distributions of zt and yi are asymptotically
i
t
identical. This evolution of yi is much easier to analyze thanks
to the i.i.d. property of the matrices {At }t∈N .

∈[N ]\j

=

(10)

∈[N ]

In this section, we deﬁne two message passing sequences
corresponding to the instance (A, F, x0 ). For each i ∈ [N ]
we use the notation [N ] \ i to denote the set [N ] \ {i}. We
now deﬁne the sequence of vectors (zt )t∈N , where for each
i→j
q
i = j ∈ [N ], zt
i→j is a vector in R or equivalently for each
t ∈ N, we can see (zt ) as an N × N matrix with entries in
i→j
Rq (diagonal elements are never used). The initial condition is
denoted by z0 ∈ Rq for any i, j ∈ [N ] and is independent
i→j
of j, such that z0 = x0 for all j = i. The r-th entry of the
i→j
i
vector zt+1 is deﬁned by the following recursion for t ≥ 0,
i→j

t+1
zi (r)

At i fr (yt→i , t),
∈[N ]\j

q

A i fr (zt →i , t).

= O N −1/2 .

r=1

Our second message passing sequence is deﬁned
as follows: for a (C, d)-regular sequence of instances
(A(N ), FN , x0,N )N ≥1 , we deﬁne for each N , an i.i.d.
sequence of N × N random matrices {At }t∈N such that
t
0
A0 = A(N ). Then we deﬁne (yi→j ) by yi→j = x0 and for
i
t≥0

A. Message passing algorithms

=

m(r)

zi (r)
˜t

−E

r=1

In the rest of this section we provide further details regarding steps (i), (ii), (iii). We introduce a message passing algorithm that is asymptotically equivalent to the AMP iteration
(1), and describe its tree embeddings representation.

t+1
zi→j (r)

q

m(r)
t
zi (r)

(9)

∈[N ]

The key step in proving our universality Theorem I.4 is the
following result. This establishes statistical insensitivity of the
message passing algorithm moments to the entries distribution.
Proposition
II.1. Let
(A(N ), FN , x0,N )N ≥1
and
˜ ), FN , x0,N )N ≥1 be any two (C, d)-regular polynomial
(A(N
sequences of instances, that differ only in the distribution of
˜
the random matrices A(N ) and A(N ). Assume that for all
2
˜2 }. Denote by zt (resp. zt )
˜i
N and all i < j, E{Aij } = E{Aij
i
˜
deﬁned by (9) while iterating (8) with matrix A (resp. A).

4

For a vertex u ∈ T , we denote |u| its generation in the
tree, i.e. its graph-distance from the root. Also for a vertex
u ∈ T (which is not a leaf), we denote by u[r] the number
of children of u with mark r ∈ [q] (with the convention
u[0] = 0). The children of such a node are ordered with
respect to their mark: the labels of the children of u are then
( 1 , 1), . . . , ( u[1] , 1), ( u[1]+1 , 2), . . . , ( u[1]+···+u[q] , q), where
each ( u[0]+···+u[i] , . . . , u[0]+···+u[i+1]−1 ) is a u[i + 1]-tuple
with coordinates in [N ]. We denote by L(T ) the set of leaves
of a tree T , i.e. the set of vertices of T with no children. For
v ∈ L(T ), its label is such that for all i ∈ [q], v[i] ∈ N and
v[1] + · · · + v[q] ≤ d. We will distinguish between two types
of leaves: those with maximal depth t = max{|v|, v ∈ L(T )}
and the remaining ones. If v ∈ L(T ) and |v| ≤ t − 1, then
we impose v[1] = · · · = v[q] = 0. This case corresponds to
‘natural’ leaves and since they have no children, the notation
is consistent with the notation introduced for other nodes of
the tree. For all other leaves, we do no make this assumption
so that v[1] + · · · + v[q] can take any value in [d]. These leaves
are ‘artiﬁcial’ and can be thought of as leaves resulting from
cutting a larger tree after generation t so that the vector of the
v[r]’s keeps the information on the number of children with
mark r in the original tree.
We deﬁne T t to be the set of such labeled trees T with t
generations, that satisfy the following conditions:
1) If v1 = ◦, v2 , . . . , vk is a path starting from the root (i.e.
with π(vi+1 ) = vi for i ≥ 1), then the corresponding
sequence of types (vi ) is non-backtracking,i.e. for any
1 ≤ i ≤ k−2, the three labels (vi ), (vi+1 ) and (vi+2 )
are distinct.
2) If u ∈ L(T ) and |u| ≤ t − 1 (i.e. u is a ‘natural’ leaf),
then we have u[1] + · · · + u[q] = 0.
3) If u ∈ L(T ) and |u| = t (i.e. u is an ‘artiﬁcial’ leaf)
then we have u[1] + · · · + u[q] ≤ d.
For a labeled tree T ∈ T t and a set of coefﬁcients c =
(ci1 ,...,iq (r, t)), we deﬁne three weights:
A(T )

=

A

Lemma II.3. Under the same assumptions as in Proposition
II.1, we have
t
zi→j (r)

t
zi (r)

x(T )

x0(v)→

(14)

R EFERENCES
[1] R. Adamczak, A. Litvak, and A. P. N. Tomczak-Jaegermann. Restricted
isometry property of matrices with independent columns and neighborly
polytopes by random sampling. Constructive Approximation, pages 61–
88, 2011.
[2] R. Affentranger and R. Schneider. Random projections of regular
simplices. Discr. and Comput. Geometry, 7:219–226, 1992.
[3] G. W. Anderson, A. Guionnet, and O. Zeitouni. An introduction to
random matrices. Cambridge University Press, 2009.
[4] M. Bayati and A. Montanari. The dynamics of message passing on dense
graphs, with applications to compressed sensing. IEEE Transactions on
Information Theory, 57:764–785, 2011.
[5] M. Bayati and A. Montanari. The LASSO risk for gaussian matrices.
IEEE Trans. on Inform. Theory, 58:1997–2017, 2012.
[6] D. L. Donoho. High-dimensional centrally symmetric polytopes with
neighborliness proportional to dimension. Discrete Comput. Geom., page
617652, 2005.
[7] D. L. Donoho. Neighborly polytopes and sparse solution of underdetermined linear equations. Technical Report, Statistics Department,
Stanford University, 2005.
[8] D. L. Donoho, A. Maleki, and A. Montanari. Message Passing Algorithms for Compressed Sensing. Proceedings of the National Academy
of Sciences, 106:18914–18919, 2009.
[9] D. L. Donoho and J. Tanner. Neighborliness of randomly-projected
simplices in high dimensions. Proceedings of the National Academy of
Sciences, 102(27):9452–9457, 2005.
[10] D. L. Donoho and J. Tanner. Sparse nonnegative solution of underdetermined linear equations by linear programming. Proceedings of the
National Academy of Sciences, 102(27):9446–9451, 2005.
[11] D. L. Donoho and J. Tanner. Counting faces of randomly projected
polytopes when the projection radically lowers dimension. Journal of
American Mathematical Society, 22:1–53, 2009.
[12] D. L. Donoho and J. Tanner. Observed universality of phase transitions in
high-dimensional geometry, with implications for modern data analysis
and signal processing. Phil. Trans. R. Soc. A, pages 4273–4293, 2011.
[13] Y. Kabashima, T. Wadayama, and T. Tanaka. A typical reconstruction
limit for compressed sensing based on lp-norm minimization. J.Stat.
Mech., page L09003, 2009.
[14] A. Maleki, L. Anitori, A. Yang, and R. Baraniuk. Asymptotic Analysis of
Complex LASSO via Complex Approximate Message Passing (CAMP).
arXiv:1108.0477, 2011.
[15] S. Rangan. Generalized Approximate Message Passing for Estimation
with Random Linear Mixing. In IEEE Intl. Symp. on Inform. Theory,
St. Perersbourg, Aug. 2011.
[16] S. Rangan, A. K. Fletcher, and V. K. Goyal. Asymptotic analysis of
map estimation via the replica method and applications to compressed
sensing. In Neural Information Processing Systems (NIPS), Vancouver,
2009.
[17] P. Schniter. Turbo Reconstruction of Structured Sparse Signals. In
Proceedings of the Conference on Information Sciences and Systems,
Princeton, 2010.
[18] A. M. Vershik and P. V. Sporyshev. Asymptotic behavior of the number
of faces of random polyhedra and the neighborliness problem. Selecta
Math. Soviet., 11:181201, 1992.

v[s]
(π(v)) (s)

A(T )Γ(T, c, t)x(T ).

ACKNOWLEDGMENT

(u)

=

=

Partially supported by NSF CAREER award CCF- 0743978
and AFOSR grant FA9550-10-1-0360.

(u) (v) ,

(u→v)∈E(T )
q

(13)

T ∈Tit (r)

cu[1],...,u[q] (r(u), t − |u|),

=

A(T )Γ(T, c, t)x(T ),
t
T ∈Ti→j (r)

(u→v)∈E(T )

Γ(T, c, t)

=

.

v∈L(T ) s=1

We deﬁne
t
(a) Ti→j (r) ⊂ T t the family of trees such that: (i) The root
has type i; (ii) The root has only one child, call it v; (iii)
The type of v is (v) ∈ {i, j} and its mark is r(v) = r.
/
(b) Tit (r) ⊂ T t the family of trees such that: (i) The root
has type i; (ii) The root has only one child, call it v;
(iii) The type of v is (v) = i and its mark is r(v) = r.
Our ﬁnal Lemma establishes that indeed the above representation corresponds to the message passing algorithm
introduced in the previous subsection.

5

