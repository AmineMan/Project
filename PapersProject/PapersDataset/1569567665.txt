Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 13:14:14 2012
ModDate:        Tue Jun 19 12:55:27 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      415981 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569567665

Universal Rateless Coding with Finite Message
Set
Navot Blits

Meir Feder

Tel Aviv University
navot@eng.tau.ac.il

Tel Aviv University
meir@eng.tau.ac.il

transmission time implies a rate of 1 − δ, which is exactly
the BEC capacity.
This simple scheme exempliﬁes some important concepts of rateless codes. First, the transmission time is
not ﬁxed, but rather a random variable (geometricallydistributed in the above case); second, when the length of
the transmission is set dynamically, the error probability
may be controllable. In the BEC example the transmission
is only terminated once the decoder knows what message
has been transmitted, so the error probability is zero;
third, the code design is rate-independent. In fact, this
code can be used for any binary erasure channel; fourth,
the continuity of the transmission requires feedback to
the encoder. Indeed, as we shall see in the sequel, when
rateless codes are used for point-to-point communication,
some form of feedback, which can be limited to decision
feedback, must exist to enable continuity. Rateless codes,
however, are also invaluable for other settings such as
multicast or broadcast communications, in which the existence of feedback is not explicitly required. Shulman [1]
introduced the concept of Static Broadcasting, in which
the transmitter sends a message to multiple users. The
scheme does not requires feedback - each user remains
connected until it retrieved enough symbols to make a
conﬁdent decision, and its rate is determined by the time
it spent online.
In this paper we explore universal rateless coding over
unknown discrete memoryless channel (DMC), with a
ﬁnite size message set M and with an allowed small (but
ﬁxed) error probability . We investigate the dependence
between the rate, the error probability and the size of
the message set. The entire analysis is done for a ﬁnite
message set, and we show that when the size of the
message set is taken to inﬁnity, our results agree with
the classic results. Our work essentially combines two
previous works. For the case of a ﬁnite message set, but
under the assumption that the decoder knows the channel
law, Polyanskiy [5] introduced a coding scheme that
uses information density function as the decoding metric,
and analyzed its performance. That work as well as
ours utilizes Wald’s results (see [3, Ch.3]) on Sequential

Abstract—Universal rateless coding over unknown discrete memoryless channels (DMC) is considered. In rateless
codes each codeword is inﬁnitely long, and the decoding time
depends on the conﬁdence level of the decoder. This work
considers the ﬁnite message set case where a ﬁnite number of
bits are transmitted over an unknown discrete memoryless
channel, with a ﬁx allowed error probability. Using rateless
codes along with sequential universal decoding that utilizes
a mixture probability law instead of the unknown channel
law, an optimal universal scheme is obtained. The analysis
speciﬁes explicitly the attainable rate, at each message set
size and allowed error probability, which reﬂects the cost of
universality.

I. I NTRODUCTION
In traditional channel coding the code rate, which is
the ratio between the lengths of the encoder’s input and
output blocks, is an integral part of the code deﬁnition.
If one of M messages is to be encoded at rate R, then
the corresponding codeword has length n = (log M )/R.
Provided that the rate is chosen properly, the error probability decreases as M grows. The capacity of the channel
C is deﬁned as the largest value of R for which the error
probability can vanish.
An alternative approach to ﬁxed-rate channel coding
is rateless codes. In this approach, we abandon the basic
assumption of a ﬁxed coding rate, and allow the codeword
length, and hence the rate, to depend on the channel
conditions. When the encoder wants to send a certain
message, it starts transmitting symbols from an inﬁnitelength codeword. The decoder receives the symbols that
passed through the channel and when it is conﬁdent
enough about the message, it makes a decision. Perhaps
the simplest example of a rateless code (see e.g. [1, Ch.3]
or [2, Ch.7]) appears in communication over a binary
erasure channel (BEC) with erasure probability δ, and
feedback. In this case a rateless code with a message
set of size 2 (a single bit) is a simple repetition coding,
in which the binary symbol is retransmitted until the
decoder receives an unerased symbol. Since the erasure
probability is δ, the expected number of transmissions
until an unerased symbol is received is 1/(1 − δ). This

1

4) Set of decoding function gn : Y n → W ∪ {0},
n ≥ 1.

Probability Ratio Test (SPRT). For the universal case,
where the decoder is ignorant of the channel law, Shulman
[1] introduced a sequential version of the maximal mutual
information (MMI) decoder [4] for universal channel
decoding and joint-source channel coding, including the
case of side information at the decoder. However, the
results in [1] are asymptotic in the size of the message
set. We introduce here a universal decoder, which does
not require knowledge of the channel law at the receiver,
and analyzed its optimal performance.
The rest of this paper is organized as follows. In
Section II we deﬁne rateless codes and provide related
deﬁnitions and notation. The main result of this paper,
a rateless coding theorem for an unknown channel is
presented in Section III. In Section IV we extend our
scope to and show how rateless coding can be efﬁciently
used for uncompressed sources, and obtain a universal
joint source-channel coding scheme. Section V concludes
the paper.

Unlike conventional codes, for which the rate is a fundamental property, the above description does not specify
a working rate; hence the term rateless code. To encode
a message w ∈ W, the encoder starts transmitting the
codeword cw over the channel. Upon receiving each
channel output, the decoder can either decide on one of
the messages w or decide to wait for further channel
ˆ
outputs, returning ‘0’. Through feedback, the decoder’s
decision is known to the encoder, which correspondingly
decides whether to transmit further symbols from cw or
to proceed to the next message. We note that two different
forms of feedback can be assumed here: channel feedback
and decision feedback. In channel feedback, the encoder
at time instance t observes Y t−1 , the channel outputs so
far, and by imitating the decoder’s operation it becomes
aware of any decision made by the decoder. In decision
feedback, the encoder is only informed that a decision
has been made, and it can proceed to the next message.
While channel feedback requires no intervention from the
decoder in the feedback process, it essentially assumes
that the feedback channel has the same bandwidth as
the main channel. Decision feedback, in contrast, requires
only one feedback bit per symbol.
We conclude this section with a few deﬁnitions required for the next sections.

II. D EFINITIONS AND N OTATION
Throughout this paper, random variables will be denoted by capital letters and their realizations by the
corresponding lowercase letters. Vectors are denoted by
superscript that indicate their length, for instance X n =
[X1 , . . . , Xn ]. Unless otherwise stated, all logarithms are
taken to the base of 2. We focus on communication over
a discrete memoryless channel (DMC) characterized by
a transition probability p(y|x), x ∈ X , y ∈ Y, where X
and Y are the input and output alphabets of the channel,
respectively. With a slight abuse of notation, we use p(·|·)
also to denote the joint transition probabilities of the
n
channel, thus p(y n |xn ) = i=1 p(yi |xi ). The capacity
of the channel (in bits per channel use) in conventionally
deﬁned as C = maxq(x) I(X; Y ), where I(X; Y ) is the
mutual information between the input of the channel and
its output, and the maximization is over all channel input
priors q(x). If |X | = |Y|, and p(y|x) = 1 if x = y
and p(y|x) = 0 otherwise, then the channel is said to be
noiseless, and in that case C = log |X |. We also assume
that a noiseless feedback exists from the receiver to the
transmitter.
A rateless code has the following elements:
1) Message set W containing M messages. Without the loss of generality we assume that
W = {1, . . . , M }, with corresponding probabilities π(1), . . . , π(M ). Occasionally, we deﬁne K =
log M as the number of bits conveyed in a message.
2) Codebook C = {ci }M , where each codeword
i=1
ci ∈ X ∞ is generated by drawing i.i.d. symbols
according to a prior q(x), x ∈ X .
3) Set of encoding functions fn : W → X , n ≥ 1.

Deﬁnition 1. A stopping time T of a rateless code is a
random variable deﬁned as
T = min{n : gn (Y n ) = 0}

(1)

Deﬁnition 2. An effective rate R of a rateless code is
deﬁned as
log M
(2)
R=
E{T }
where E{T } = Eq {Ep {T }}, i.e. the averaging is done
over all possible codebooks and channel realizations.
Using the deﬁnition of stopping time, we can deﬁne the
error event as the case in which the decoder stops, deciding on the wrong message. The error event conditioned
on a particular message is deﬁned as
ˆ
Ew = {W = w | W = w}

(3)

ˆ
where W = gT (Y ). The average error probability for
the entire message set is therefore
T

M

π(w) · Pr{Ew }

Pe =

(4)

w=1

Deﬁnition 3. For a given DMC, an (R, M, )-code is
a rateless code with effective rate R, containing M

2

messages and error probability Pe ≤ .

independently-chosen codeword as the transmitted codeword, remains as small as in the original decoder; and
second, the excess delay in the decision on the correct
codeword is not only bounded, but also decays as the
size of the message set grows.
Suppose that we wish to communicate over a DMC
with unknown (backward) transition probabilities

III. U NIVERSAL R ATELESS C ODING
Consider a discrete memoryless source with a set of M
equiprobable messages, i.e. π(i) = 1/M , i = 1, . . . , M .
We use a rateless code as deﬁned in Section II, where
each codeword ci , i = 1, . . . , M is generated by drawing
i.i.d. symbols according to q(x), the capacity-achieving
prior of the channel. The source of randomness generating
the codewords is shared by the encoder and the decoder,
so that the codebook in known at both ends. If the
channel were known at the receiver end, one could have
used the following decision rule, originally introduced by
Burnashev [6]:
n

gn (y n ) =

w, k=1 p(cw,k |yk ) ≥ A ·
0, if no such w exists

n
k=1

θij = Pr{X = i|Y = j}, i = 1, . . . , |X | j = 1, . . . , |Y|
(9)
We use a coding scheme similar to (5) with the following
modiﬁcation. Assume, ﬁrst, a predeﬁned prior q(x). Instead of using the true transition probability pθ (xt |y t ),
which is unknown to the decoder, we use a universal
probability assignment deﬁned as

q(cw,k )
pU (xt |y t )

w, zw,1 + . . . + zw,n ≥ a
0, if no such w exists

where


Λ = θ ∈ [0, 1]|X ||Y| |


|X |

θij = 1, j = 1, . . . , |Y|

k = 1, . . . , n

(6)

C
1+

C−log
log M

dθ

B=

(7)

Λ

and we deﬁne a = log A.
The achievable rate in this scheme is [7]
R=



i=1

where
zw,k




(11)
and the weight function w(·) is chosen to be Jeffreys’
prior1 , i.e.,
1
(12)
w(θ ) =
θij
B
i,j

where
p(cw,k |yk )
,
= log
q(cw,k )

(10)

Λ

(5)
where {cw,k }∞ are the symbols in cw . If the threshold
k=1
crossing condition in (5) is satisﬁed by more than one
codeword, we randomly choose one of them and declare
an error. The decoding rule can be also written in a
logarithmic form, which is equivalent to (5) due to the
monotonicity of the logarithm function:
gn (y n ) =

w(θ )pθ (xt |y t )dθ

i,j

(13)
θij

Remark: While the unknown channel is usually characterized by a set of transition probabilities
˜
θij = Pr{Y = j|X = i}, i = 1, . . . , |X | j = 1, . . . , |Y|

(8)

the entire derivation here is done for the backward channel
parameterization given in (9). However, this does not need
to bother us since the entire analysis assumes a known
˜
input prior q(x), and therefore given {θij }, the parameters
˜
in (9) are well-deﬁned. Moreover, the region Λ, induced
˜ij } and q(x), is clearly contained in the region Λ.
by {θ
Therefore, if a coding scheme is universal with respect
to all possible realizations of the backward channel, it is
also universal with respect to all possible realizations of
the forward channel.
The universal probability assignment implies the following decoding rule, which is the universal counterpart
of (5):

A similar result has been originally introduced in [5].
It should be noted, however, that while the above
scheme does not require that the rate is predeﬁned, they
all assume that the channel law is fully known at the receiver end. An interesting question, from both theoretical
and practical points of view, is whether similar rateless
coding schemes are applicable also when the decoder is
completely ignorant of the channel law. Clearly, if the
channel law is unknown at the receiver, the decoding rule
at (8) must change, as is uses the transition probability
of the channel. To circumvent this, we introduce a novel
universal decoder, in which the unknown transition probability is replaced by a mixture probability. As we shall
see shortly, if the weight function of the mixture is chosen
properly, the universal decoder will preserve two desirable
characteristics of the decoder for the known channel. First,
the probability that the decoder will err, identifying an

gn (y n ) =
1 In

3

w, pU (cw |y n ) ≥ A · q(cw )
0, if no such w exists

(14)

this model Jeffreys’ prior is a special case of Dirichlet Distribution

The following theorem, proven in [7], is the main
result of this paper. It shows that using the rateless coding
scheme deﬁned in Section II with the sequential decoder
(14), reliable communication can be achieved, and the rate
degradation with respect to the case of a known channel
decays as O(log log M/ log M ).

(15) by

Theorem 1. For the decoder in (14) with Pe ≤ , the
following effective rate is achievable:

Hence, the penalty for lack of channel knowledge
amounts to
|X ||Y| log log M
R − RU = I(q, p) ·
2
log M
1
+O
(19)
log M

I(q, p) 1 −
RU =
1+

I(q,p)+β−log +

RU ≈ I(q, p) 1 −

|X ||Y|
2

1
(log log M −log I(q,p)− ln 2 )
log M

|X |2 |Y| |X ||Y|
+
4
2
(|X | − 1) |Y|
log(2π)
−
2

log e
(16)

|X ||Y| log log M
log
= I(q, p)−R−
·
+O
E{T }
2
log M

1
log M
(20)
As in the case of a known channel, we see that the
error exponent is a linear function of the rate, but an
additional term of order O(log log M/ log M ) is added.
Here again, we interpret this term as a penalty for the
lack of channel knowledge at the receiver. Furthermore,
by taking M → ∞, we can also see that (20) coincides
with [10, Proposition 1].
−

IV. J OINT S OURCE -C HANNEL C ODING
So far, we assumed that the messages conveyed over
the channel were equiprobable. This is the case if, for
instance, the source of information has been compressed
and the message W is the output of the source encoder.
Assume now, that the messages have arbitrary probabilities π(1), . . . , π(M ). Each message now contains a
different amount of information, which would translate
into different codeword length at the output of the source
encoder. However, in rateless codes the codeword assigned to each message is always inﬁnite, and the actual
transmission length is determined by the decoder. (The
effective length of the message depends on the decoder’s
stopping time.) It is therefore tempting to use rateless

Evidently, as the size of the massage set it taken to
inﬁnity, the rate approaches the channel capacity, in agreement with Shannon’s theory. Moreover, it is interesting
to compare the above rate to the one obtained by using
rateless coding over a known channel, and evaluate the
rate degradation due to the lack of channel knowledge at
the receiver. For the case of a known channel, the effective
rate at (8) can be approximated by
I(q, p) − log
log M

1
log M

The leading term in the latter expression behaves as
O(log log M/ log M ) = O(log K/K), where K is the
number bits in the ﬁnite message set, factorized by the
product of the cardinalities of input and output of the
channel. It is interesting to compare this result with known
results from universal source coding, where the redundancy2 is dominated by the cardinality of the alphabet of
the source, and a term that behaves as O(log n/n), where
n is the source length [9].
Theorem 1 (Eq (15)) also implies the following error
exponent:

If q(·) is known to be the capacity-achieving prior of
the channel, I(q, p) can be replaced by C, the channel
capacity. Generally, ﬁnding the capacity-achieving prior
requires knowledge of the channel, which limits the universality of this decoding scheme. However, in many practical cases the capacity-achieving prior can be found even
when the channel law is unknown. For example, if the
channel is known to be symmetric, the capacity-achieving
prior is uniform. Furthermore, using the uniform prior in
binary channels guarantees that I(q, p) ≥ 0.94C, and the
degradation from the optimum rate C − I(q, p) ≤ 0.011
bits [1]. In general, a universal approach to determining
the optimum input distribution is Prior Prediction [8].
Introduced for the case of Arbitrarily-Varying Channels
(AVC), this approach uses a sequential estimation process
for the prior, and is proved to approach the best rate
achievable using any ﬁxed prior.

R ≈ I(q, p) 1 −

|X ||Y| log log M
2
log M

(18)

|X ||Y|/2
log M ln 2

|Y|K|X |

− I(q, p) ·
+O

(15)
where I(q, p) is the mutual information induced by the
prior q(·) and the channel law p(·|·), and we deﬁne
β

I(q, p) − log
log M

(17)

2 The excess of the average codeword length above the entropy of the
source.

For the case of unknown channel, we can approximate

4

codes for uncompressed sources and try to achieve good
compression rate and reliable communication simultaneously. To simplify matters, we begin by tackling the
case of known channel and postpone the formulation for
unknown channel to the end of this section. We use the
following generalized version of the encoder (6).
gn (y n ) =

w, zw,1 + . . . + zw,n ≥ aw
0, if no such w exists

of the source or the capacity of the channel, the rate
approaches the optimum rate achievable by an informed
encoder. From practical point of view, compression algorithms can be implemented and maintained at the decoder,
while the encoder remains simple and source-independent.
V. S UMMARY
In this paper we introduced a universal coding scheme
based in rateless coding and universal sequential decoding. Throughout the analysis, the coding schemes is
allowed to have a ﬁxed error probability, while aiming to
achieve shortest mean transmission time, or equivalently,
the highest rate. This approach is different than the prevalent one, in which the communication rate is held ﬁxed
and the codebook is enlarged indeﬁnitely so that the error
probability vanishes. We demonstrated how rateless codes,
combined with universal sequential decoding, attains the
optimal performance. The decoding methods and analysis
introduced here is novel it that it enabled obtaining results
for a ﬁnite size message set and an unknown channel,
while previous studies were restricted to either a ﬁnite
message set with a known channel, or vise-versa.
The key point to be noticed is the following. To attain
the precise ﬁnite length performance for universal rateless
coding we proposed a universal decoding metric based on
a mixture probability assignment, that enabled bounding
the difference between the universal metric and the one
used by an informed decoder. That concept follows easily
to universal rateless coding for uncompressed sources
over an unknown channel, showing that even when the
encoder is uninformed on the statistics of the source or
the channel, the achievable rate approaches the optimal
rate with precise account of the ﬁnite length.

(21)

where aw is a threshold that depends on the message w,
and we deﬁne aw = log Aw . By choosing
Aw =

1
· π(w)

∀w ∈ W

(22)

we get a uniform bound on the error probability
Pr{Ew } ≤ ·

π(w ) ≤

(23)

w =w

which also implies
Pe ≤

(24)

Thus, for an appropriate choice of message-dependent
threshold values, the average probability of error for the
entire message set is bounded by . Recall, however, that
the effective rate depends on the threshold value and
therefore needs to be reexamined here. When different
thresholds are used for different messages, the stopping
time depends on which message crosses the threshold.
Suppose that we want to convey blocks of L bits from a
source with entropy rate of H(W ). It is shown in [7] that
the effective rate in this case satisﬁes
L
I(q, p)
1
(25)
R=
≥
·
I(q,p)−log
E{T }
H (W ) 1 +
H (W )·L

R EFERENCES

where we deﬁne H (W ) = H(W )/ log M as the per-bit
entropy of the source.
Interestingly, it should be noted that the encoder, as
well as the codebook, used for joint source-channel coding
are the same as these deﬁned in Section II for channel
coding only. The only difference is the decoder, which
utilizes the knowledge on the source law (or, see below,
universally learns it).
Combining the results for joint source-channel coding
in this section with the analysis of the universal case, we
obtain the following effective rate for an uncompressed
source over an unknown channel:
I(q, p) 1 −
R=
H (W ) +

I(q,p)+β−log +

[1] N. Shulman, “Communication over an unknown channel via common broadcasting,” Ph.D. dissertation, Tel Aviv University, 2003.
[2] T. Cover and J. M. Thomas, Elements of Information Theory,
2nd ed. New York: Wiley, 2006.
[3] A. Wald, Sequential Analysis. New York: Wiley, 1947.
[4] I. Csisz´ r and J. K¨ rner, Information Theory.
a
o
New York:
Academic Press, 1981.
[5] Y. Polyanskiy, H. Poor, and S. Verdu, “Feedback in the nonasymptotic regime,” IEEE Trans. Information Theory, vol. 57,
no. 8, pp. 4903–4925, Aug. 2011.
[6] M. V. Burnashev, “Data transmission over a discrete channel
with feedback. random transmission time,” Problemy Peredachi
Informatsii, vol. 12, no. 4, pp. 10–30, Jul. 1976.
[7] N. Blits, “Rateless codes for ﬁnite message set,” Master’s thesis,
Tel Aviv University, Tel Aviv, Israel, 2012.
[8] Y. Lomnitz and M. Feder, “Prediction of priors for communication
over arbitrarily varying channels,” IEEE Int. Symp. Information
Theory, p. 219 223, Jul. 2011.
[9] M. Feder and N. Merhav, “Universal prediction,” IEEE Trans.
Information Theory, vol. 44, no. 6, pp. 2126–2145, Oct. 1998.
[10] A. Tchamkerten and I. E. Telatar, “Variable length coding over
an unknown channel,” IEEE Trans. Information Theory, vol. 52,
no. 5, pp. 2126–2145, May 2006.

|X ||Y|/2
log M ln 2

|X ||Y|
2

1
(log log M −log I(q,p)− ln 2 )

L

(26)
We note the far-reaching implications of the latter
result. While the encoder is uninformed on the statistics

5

