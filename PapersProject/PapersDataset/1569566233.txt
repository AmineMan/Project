Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 20:04:39 2012
ModDate:        Tue Jun 19 12:54:28 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      423704 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566233

Directed Information on Abstract Spaces: Properties
and Extremum Problems
Charalambos D. Charalambous and Photios A. Stavrou
ECE Department, University of Cyprus, Nicosia, Cyprus
e-mail:{ chadcha, stavrou.fotios}@ucy.ac.cy

operational deﬁnition of channels with memory and feedback
is given by
1
I(X n → Y n ) (4)
C f (P ) = lim
sup
n→∞ ←
−
←
−
n+1
P
(·|·)∈ P (P )

Abstract—This paper describes a framework in which directed
information is deﬁned on abstract spaces. The framework is
employed to derive properties of directed information such as
convexity, concavity, lower semicontinuity, by using the topology
of weak convergence of probability measures on Polish spaces.
Two extremum problems of directed information related to capacity of channels with memory and feedback, and non-anticipative
and sequential rate distortion are analyzed showing existence of
maximizing and minimizing distributions, respectively.

X n |Y n−1

←
−
where P (P ) denotes the power constraint set, and
←
−
P X n |Y n−1 (dxn |dy n−1 ) = ⊗n PXi |X i−1 ,Y i−1 (dxi |xi−1 , y i−1 )
i=0

I. I NTRODUCTION

Sequential and Non-Anticipative Rate Distortion Function.
Based on a) or b) the operational deﬁnition of sequential and
non-anticipative rate distortion function is given by expression
1
Rc (D) = lim →
inf −
I(X n → Y n ) (5)
n→∞ − n n (·|·)∈→(D) n + 1
P Y |X
Q

Directed information from a sequence of Random Variables
(RV’s) X n = {X0 , X1 , . . . , Xn } ∈ X0,n = ×n Xi , to
i=0
another sequence Y n = {Y0 , Y1 , . . . , Yn } ∈ Y0,n = ×n Yi
i=0
is often deﬁned via [1], [2]1

→
−
where Q (D) is the distortion ﬁdelity constraint and
→
−
P Y n |X n (dy n |xn ) = ⊗n PYi |Y i−1 ,X i (dyi |y i−1 , xi ),
i=0

n

I(X n → Y n ) =

I(X i ; Yi |Y i−1 )

(1)

i=0
n

=

log
i=0

PYi |Y i−1 ,X i (dyi |y i−1 , xi )
PX i ,Y i (dxi , dy i )
PYi |Y i−1 (dyi |y i−1 )
(2)

PXi |X i−1 ,Y i−1 (dxi |xi−1 , y i−1 ) = PXi |X i−1 (dxi |xi−1 ) − a.s.
The complete investigation of existence, characterization, and
properties of the above extremum problems requires extensive analysis of the functional IX n →Y n (·, ·) as deﬁned
in (3). This is analogous to capacity of channels without
feedback which involves maximization of mutual information
I(X n ; Y n ) over the power constraint set, and to classical
rate distortion function which involves minimization of mutual
information I(X n ; Y n ) over the ﬁdelity constraint. However,
mutual information I(X n ; Y n ) ≡ IX n ;Y n (PX n , PY n |X n ), in-

≡ IX n →Y n (PXi |X i−1 ,Y i−1 , PYi |Y i−1 ,X i : i = 0, . . . , n) (3)
Since the joint distribution in (2) is decomposed via
PX i ,Y i (dxi , dy i ) = ⊗i PXj |X j−1 ,Y j−1 (dxj |xj−1 , y j−1 ) ⊗
j=0
PYj |Y j−1 ,X j (dyj |y j−1 , xj ), the notation IX n →Y n (·, ·)
denotes the functional dependence on two collections
of non-anticipative or causal conditional distributions
{PXi |X i−1 ,Y i−1 (·|·, ·), PYi |Y i−1 ,X i (·|·, ·) : i = 0, 1, . . . , n}.
In information theory, directed information (1)-(3) or its
variants are used to characterize capacity of channels with
memory and feedback [3], [4], [5], lossy data compression
with feedforward information at the decoder [6], lossy data
compression of sequential codes [4], lossy data compression
of non-anticipative codes [7], and capacity of networks
such as the two-way channel, multiple access channel [8],
[9], etc. The previous references derive coding theorems
based on a) stationary ergodic processes {(Xi , Yi )}∞ ,
i=0
b) Dobrushin’s stability of the information density
P

i−1

i (dyi |y

i−1

herits from its information divergence deﬁnition I(X n ; Y n ) =
D(PX n ,Y n ||PX n × PY n ), several important functional properties such as convexity, concavity, lower semicontinuity, etc.
These properties are vital both for ﬁnite alphabet spaces,
as well as abstract alphabet spaces [11], [12]. The difﬁculty associated with directed information I(X n → Y n ),
rises from the fact that this information measure (1)-(3) is
a functional IX n →Y n (·, ·) of the collection of conditional
distributions {PXi |X i−1 ,Y i−1 (·|·, ·) : i = 0, 1, . . . , n} and
{PYi |Y i−1 ,X i (·|·, ·) : i = 0, 1, . . . , n}.
The objective of this paper is to address the following questions, when X0,n and Y0,n are complete separable metric
spaces (Polish spaces).
1. Is there an equivalent directed information deﬁnition expressed via information divergence D(·||·) as a functional
of two appropriate conditional distributions P(·|y) on

,xi )

|Y
,X
log ⊗n YiP
, and c) via information
i−1 )
i=0
Yi |Y i−1 (dyi |y
spectrum methods [10].
Capacity of Channels with Feedback. Based on a) or b) the

1 Unless otherwise, integrals with respect to measures are over the spaces
on which these are deﬁned.

1

X N = ×∞ Xi for y = (y0 , y1 , . . .) ∈ Y N = ×∞ Yi
i=0
i=0
and Q(·|x) on Y N for x ∈ X N which uniquely deﬁne
{PXi |X i−1 ,Y i−1 : i = 0, 1, . . .} and {PYi |Y i−1 ,X i : i =
0, 1, . . .}, respectively, and vice-versa?
2. Is directed information convex and concave functional
with respect to the conditional distributions P(·|y) and
Q(·|x)?
3. Is directed information a lower semicontinuous functional
of the conditional distributions P(·|y) and Q(·|x)?
4. What are appropriate conditions for existence of the maximizing encoder admissible distributions and minimizing
distortion admissible distributions?
This paper answers the above questions by invoking the
topology of weak convergence of probability measures on
Polish spaces and Prohorov’s theorems.
The paper is organized as follows. Section II provides the
construction of two equivalent deﬁnitions of causal channels
on abstract spaces while in Section III the main properties
of directed information are given. Finally, in Section IV the
extremum problems (4), (5) are discussed. The derivations of
theorems are outlined since they are quite lengthy.

on (X N , B(X N )) = ×i∈N Xi ,

i∈N B(Xi )

as follows.

Let C ∈ B(X0,n ) be a cylinder set of the form C = x ∈ X N :
x0 ∈ C0 , x1 ∈ C1 , . . . , xn ∈ Cn , Ci ∈ B(Xi ), 0 ≤ i ≤ n.
Deﬁne a family of measures P(·|y) on B(X N ) by

P(C|y) =

pn (dxn |xn−1 , y n−1 )

p0 (dx0 ) . . .
C0

(6)

Cn

←
−
≡ P 0,n (C0,n |y n−1 ), C0,n = ×n Ci
i=0

(7)

←
−
The notation P 0,n (·|y n−1 ) is used to denote the restriction of
the measure P(·|y) on cylinder sets C ∈ B(X0,n ), for n ∈ N.
Thus, if conditions i) and ii) hold then for each y ∈ Y N ,
the right hand side of (6) deﬁnes a consistent family of
ﬁnite-dimensional distribution on (X N , B(X N )), and hence
there exists a unique measure on (X N , B(X N )), from which
pn (dxn |xn−1 , y n−1 ) is obtained. This leads to the ﬁrst, usual
deﬁnition of a feedback channel, as a family of functions
pn (dxn |xn−1 , y n−1 ) satisfying conditions i) and ii).
An alternative, equivalent deﬁnition of a feedback channel is
established as follows. Introduce the assumption
iii) {Xn : n ∈ N} are complete separable metric spaces
(Polish Spaces) and {B(Xn ) : n ∈ N} are the σ−algebras
of Borel sets.
Consider a family of measures P(·|y) on (X N , B(X N )) satisfying the following consistency condition.
C1: If E ∈ B(X0,n ), then P(E|y) is B(Y0,n−1 )−measurable
function of y ∈ Y N .
Then, by assumption iii), for any family of measures P(·|y)
satisfying C1 one can construct a collection of versions of
conditional distributions {pn (dxn |xn−1 , y n−1 ) : n ∈ N}
satisfying conditions i) and ii) which are connected with
P(·|y) via relation (6).
Therefore, for Polish Spaces {Xn : n ∈ N} the second
equivalent deﬁnition is given by a family of measures P(·|y)
on (X N , B(X N )) depending parametrically on y ∈ Y N and
satisfying the consistency condition C1.
The point to be made here is that the second equivalent deﬁnition of a feedback channel, together with similar deﬁnition
for the forward channel is convenient to deﬁne directed information via relative entropy, similar to the mutual information
deﬁnition, and extend well-known functional properties of
mutual information to directed information.
Forward Channel. The previous methodology is repeated for
the collection of functions {qn (dyn |y n−1 , xn ) : n ∈ N} which
satisfy the following conditions.
iv) For n ∈ N, qn (·|y n−1 , xn ) is a probability measure on
B(Yn );
v) For n ∈ N, Bn ∈ B(Yn ), qn (Bn |y n−1 , xn ) is
n−1
n
n
i=0 B(Yi ) i=0 B(Xi )−measurable function of x ∈ X0,n ,
n−1
y
∈ Y0,n−1 .
Similarly as before, given a cylinder set D = y ∈ Y N :

II. C AUSAL C HANNELS ON A BSTRACT S PACES
In this section, the aim is to establish two equivalent deﬁnitions of conditional distributions or basic processes, which
deﬁne any probabilistic channel with causal feedback, that
relates causally the input-output behavior of any channel. This
formulation is necessary to investigate questions 1.–4.
Let N = {0, 1, 2, . . .}, and Nn = {0, 1, 2, . . . , n}. Introduce two sequence of spaces {(Xn , B(Xn )) : n ∈ N} and
{(Yn , B(Yn )) : n ∈ N}, where Xn , Yn , n ∈ N are topological
spaces, and B(Xn ) and B(Yn ) are Borel σ−algebras of subsets
of Xn and Yn , respectively. Points in X N = ×n∈N Xn ,
Y N = ×n∈N Yn are denoted by x = {x0 , x1 , . . .} ∈ X N ,
y = {y0 , y1 , . . .} ∈ Y N , respectively, while their restrictions
to ﬁnite coordinates by xn = {x0 , x1 , . . . , xn } ∈ X0,n ,
y n = {y0 , y1 , . . . , yn } ∈ Y0,n , for n ∈ N.
Let B(X N ) = i∈N B(Xi ) denote the σ−algebra on X N
generated by cylinder sets {x = (x0 , x1 , . . .) ∈ X N : x0 ∈
A0 , x1 ∈ A1 , . . . , xn ∈ An }, Ai ∈ B(Xi ), 0 ≤ i ≤ n, n ≥ 1,
and similarly for B(Y N ) =
i∈N B(Yi ). Hence, B(X0,n )
and B(Y0,n ) denote the σ−algebras of cylinder sets in X N
and Y N , respectively, with bases over Ai ∈ B(Xi ), and
Bi ∈ B(Yi ), 0 ≤ i ≤ n, respectively.
Backward or Feedback Channel. Suppose for each n ∈
N, the distributions {pn (dxn |xn−1 , y n−1 ) : n ∈ N} with
p0 (dx0 |x−1 , y −1 ) = p0 (x0 ) satisfy the following conditions.
i) For n ∈ N, pn (·|xn−1 , y n−1 ) is a probability measure on
B(Xn );
ii) For n ∈ N, An ∈ B(Xn ), pn (An |xn−1 , y n−1 ) is
n−1
B(Yi )−measurable in xn−1 ∈ X0,n−1 , y n−1 ∈
i=0 B(Xi )
Y0,n−1 .
Given the collection {pn (dxn |xn−1 , y n−1 ) : n ∈ N} satisfying
conditions i), ii), one can construct a family of distributions

y0 ∈D0 , y1 ∈D1 , . . . , yn ∈Dn , Di ∈ B(Yi ), 0 ≤ i ≤ n,

2

deﬁne a family of measures on B(Y N ) by
qn (dyn |y n−1 , xn )

q0 (dy0 |x0 ) . . .

Q(D|x) =
D0

and Q(·|x), and identify its properties. Deﬁne
QC1 (X N ; Y N ) = P(·|y) ∈ M1 (X N ) : P(·|y) are regular

(8)

Dn

→
−
≡ Q 0,n (D0,n |xn ), D0,n = ×n Di
i=0

probability measures and satisfy consistency condition C1 .

(9)

QC2 (Y N ; X N ) = Q(·|x) ∈ M1 (Y N ) : Q(·|x) are regular

Similarly as before, there exists a unique measure on
(Y N , B(Y N )) for which the family of distributions
{qn (dyn |y n−1 , xn ) : n ∈ N} is obtained. Introduced
the assumption
vi) {Yn : n ∈ N} are Polish Spaces and {B(Yn ) : n ∈ N}
are the σ−algebras of Borel sets.
Consider a family of measures Q(D|x) satisfying the
following consistency condition.
C2: If F ∈ B(Y0,n ), then Q(F |x) is B(X0,n )−measurable
function of x ∈ X N .
Then, by assumption vi), for any family of measures
Q(·|x) on (Y N , B(Y N )) satisfying consistency condition
C2 one can construct a collection of functions
{qn (dyn |y n−1 , xn ) : n ∈ N} satisfying conditions iv)
and v) which are connected with Q(·|x) via relation (8).
Note that Kolmogorov’s extension theorem guarantees the
construction of countable additive probability measures for
both P(·|y) and Q(·|x).
Given the basic measures P(·|y) on X N and Q(·|x) on Y N
satisfying consistency condition C1 and C2, respectively,
construct the collections of conditional distributions as
follows.
Let A(n) = {x : xn ∈A}, A ∈ B(Xn ) and
B (n) = {y : yn ∈B}, B ∈ B(Yn ). In addition, let
P(A(n) |y|B(X0,n−1 )) denote the conditional probability
of A(n) with respect to B(X0,n−1 ) calculated on the
probability space X N , B(X N ), P(·|y) , and similarly for
Q(B (n) |x|B(Y0,n−1 )). Then

probability measures and satisfy consistency condition C2 .
Given conditional distributions P(·|·) ∈ QC1 (X N ; Y N ) and
Q(·|·) ∈ QC2 (Y N ; X N ) deﬁne the following measures.
P1: The joint distribution on X N × Y N deﬁned uniquely by
←
−
→
−
( P 0,n ⊗ Q 0,n )(×n Ai ×Bi ), Ai ∈ B(Xi ), Bi ∈ B(Yi ),
i=0
=P X0 ∈A0 , Y0 ∈ B0 , . . . , Xn ∈An , Yn ∈Bn

(10)

P2: The marginal distributions on X N deﬁned uniquely by
µ0,n (×n Ai ), Ai ∈ B(Xi ), 1 ≤ i ≤ n
i=0
= P{X0 ∈ A0 , Y0 ∈ Y0 , . . . , Xn ∈ An , Yn ∈ Yn },
←
−
→
−
= ( P 0,n ⊗ Q 0,n )(×n (Ai × Yi ))
i=0
P3: The marginal distributions on Y N deﬁned uniquely for
Bi ∈ B(Yi ), 1 ≤ i ≤ n by
←
−
→
−
ν0,n (×n Bi ) = ( P 0,n ⊗ Q 0,n )(×n (Xi × Bi ))
i=0
i=0
→
−
P4: The measure Π 0,n : B(X0,n ) B(Y0,n ) → [0, 1] deﬁned
uniquely for Ai ∈ B(Xi ), Bi ∈ B(Yi ), 1 ≤ i ≤ n by
→
−
←
−
Π 0,n (×n (Ai ×Bi )) = ( P 0,n ⊗ ν0,n )(×n (Ai ×Bi ))
i=0
i=0
←
−
P5: The measure Π 0,n : B(Y0,n ) B(X0,n ) → [0, 1] deﬁned
uniquely for Ai ∈ B(Xi ), Bi ∈ B(Yi ), 1 ≤ i ≤ n by
←
−
→
−
Π 0,n (×n (Ai ×Bi )) = (µ0,n ⊗ Q 0,n )(×n (Ai ×Bi ))
i=0
i=0

P Xn ∈A|X n−1 = xn−1 , Y n−1 = y n−1

= P {x : xn ∈A}|y|B(X0,n−1 ) = pn (An ; xn−1 , y n−1 ) − a.s. A. Directed Information
Let P(·|·) ∈ QC1 (X N ; Y N ) and Q(·|·) ∈ QC2 (X N ; Y N ).
P Yn ∈B|Y n−1 = y n−1 , X n = xn
By invoking the deﬁnition of directed information (1) or (2),
= Q {y : yn ∈B}|x|B(Y0,n−1 ) = qn (Bn ; y n−1 , xn ) − a.s.
it can be shown that
←
−
→
−
→
−
Note that pn (·; ·, ·) ∈ Q(Xn ; X0,n−1 ×Y0,n−1 ) and qn (·; ·, ·) ∈
I(X n → Y n ) = D( P 0,n ⊗ Q 0,n || Π 0,n )
(11)
Q(Yn ; Y0,n−1 × X0,n ) are stochastic kernels [13], determined
→
−
Q 0,n (dy n |xn ) ←
from P(·|·) and Q(·|·), respectively, (e.g., related via (6), (8)).
−
→
−
( P 0,n ⊗ Q 0,n )(dxn , dy n )
= log
The distribution of RV’s {(Xi , Yi ) : i ∈ N} is deﬁned by
ν0,n (dy n )
←
−
→
−
P X0 ∈A0 , Y0 ∈ B0 , . . . , Xn ∈An , Yn ∈Bn
≡ IX n →Y n ( P 0,n , Q 0,n )
(12)
=

p0 (dx0 )
A0

qn (dyn ; y n−1 , xn )

q0 (dy0 ; x0 ) . . .
B0

The right hand side of (11) follows from repeated application
of chain rule of relative entropy [13], while (12) follows from
←
−
→
−
←
−
the fact that P 0,n ⊗ Q 0,n << P 0,n ⊗ ν0,n if and only
→
−
if Q 0,n (·|xn ) << ν0,n (·) for µ0,n −almost all xn ∈ X0,n .
←
−
→
−
←
−
Further, if P 0,n ⊗ Q 0,n << P 0,n ⊗ ν0,n then the Radon-

Bn

Hence, for any P(·|·) and Q(·|·) satisfying consistency conditions there exist a probability space and a sequence of RV’s
{(Xi , Yi ) : i ∈ N} deﬁned on it, whose joint probability
distribution is deﬁned uniquely via P(·|·) and Q(·|·).

Nikodym derivative

III. D IRECTED I NFORMATION P ROPERTIES AND
C OMPACTNESS
In this section, directed information I(X n → Y n ) will be
deﬁned via relative entropy, using the basic measures P(·|y)

→
−
Q 0,n (·|xn ) n
ν0,n (·) (y ),

←
−
→
−
( P 0,n ⊗ Q 0,n )
(xn , y n )
←
−
( P 0,n ⊗ν0,n )

represents a version

of
µ0,n − a.s for all xn ∈ X0,n .
The notation IX n →Y n (·, ·) indicates the functional dependence
←
−
→
−
of I(X n → Y n ) on { P 0,n , Q 0,n }.

3

←
−
→α
−
the marginals of ( P 0,n ⊗ Q 0,n )(dxn , dy n ) α≥1 . Then
→α
−
←
−
w
α
Π 0,n (dxn , dy n ) ≡ P 0,n (dxn |dy n−1 ) ⊗ ν0,n (dy n ) =⇒
←
−
→0
−
0
P 0,n (dxn |dy n−1 )⊗ν0,n (dy n ) ≡ Π 0,n (dxn , dy n ), where
0
α
ν0,n ∈ M1 (Y0,n ) is the weak limit of ν0,n ∈ M1 (Y0,n ).

B. Convexity and Concavity of Directed Information
Let QC1 (X0,n ; Y0,n−1 ), QC2 (Y0,n ; X0,n ) be the restrictions
of QC1 (X N ; Y N ) and QC2 (Y N ; X N ), respectively, to cylinder sets with bases over Ai ∈ B(Xi ), and Bi ∈ B(Yi ),
i = 0, 1, . . . , n. These are regular conditional distributions.

Part B. Let X0,n be a compact Polish space and Y0,n a
→
−
Polish space. Assume Q 0,n (·|xn ) ∈ QC2 (Y0,n ; X0,n ) satisfy
the following condition.
CB: For all h(·)∈BC(Y0,n ), the function

Theorem 1. Let {(Xn , B(Xn )) : n ∈ N}, {(Yn , B(Yn )) : n ∈
N} be Polish spaces. Then
1) QC1 (X0,n ; Y0,n−1 ), QC2 (Y0,n ; X0,n ) are convex sets.
←
−
→
−
→
−
2) IX n →Y n ( P 0,n , Q 0,n ) is a convex functional of Q 0,n ∈
←
−
QC2 (Y0,n ; X0,n ) for a ﬁxed P 0,n ∈ QC1 (X0,n ; Y0,n−1 ).
←
−
→
−
←
−
3) IX n →Y n ( P 0,n , Q 0,n ) is a concave functional of P 0,n ∈
→
−
QC1 (X0,n ; Y0,n−1 ) for a ﬁxed Q 0,n ∈ QC2 (Y0,n ; X0,n ).

(xn , y n−1 ) −→

Proof: The proof is quite lengthy and it is based on
Prohorov’s theorem relating tightness and weak compactness
of a family of probability measures [13].
The results of Theorem 2 are sufﬁcient to establish lower
semicontinuity of directed information I(X n → Y n ) ≡
←
−
→
−
IX n →Y n ( P 0,n , Q 0,n ).

C. Lower semicontinuity-Continuity of Directed Information
This part discusses the lower-semicontinuity and continuity
←
−
of directed information as a functional of P 0,n (·|y n−1 ) ∈
→
−
QC1 (X0,n ; Y0,n−1 ) and Q 0,n (·|xn ) ∈ QC2 (Y0,n ; X0,n ). Before establishing the main results, sufﬁcient conditions for
weak compactness of the set of measures QC1 (X0,n ; Y0,n−1 ),
QC2 (Y0,n ; X0,n ), and joint and marginal measures are given.

Theorem 3. 1) Suppose the conditions in Theorem 2,
←
−
→
−
Part A hold. Then IX n →Y n ( P 0,n , Q 0,n ) is lower semi→
−
←
−
continuous on Q 0,n ∈ QC2 (Y0,n ; X0,n ) for ﬁxed P 0,n ∈
QC1 (X0,n ; Y0,n−1 ).
2) Suppose the conditions in Theorem 2, Part B hold. Then
←
−
→
−
←
−
IX n →Y n ( P 0,n , Q 0,n ) is lower semicontinuous on P 0,n ∈
→
−
QC1 (X0,n ; Y0,n−1 ) for ﬁxed Q 0,n ∈ QC2 (Y0,n ; X0,n ).

Theorem 2. Part A. Let Y0,n be a compact Polish
←
−
space and X0,n a Polish space. Assume P 0,n (·|y n−1 ) ∈
QC1 (X0,n ; Y0,n−1 ) satisfy the following condition.
CA: For all g(·)∈BC(X0,n ), where BC(X0,n ) denotes the set
of bounded continuous real-valued functions on X0,n ,
g(x)pn (dx; xn−1 , y n−1 ) ∈ R

(14)

is jointly continuous in (xn , y n−1 ) ∈ X0,n × Y0,n−1 .
←
−
The statements of Part A hold by interchanging Q 0,n with
←
−
→
−
←
−
P 0,n , ν0,n with µ0,n , Π 0,n with Π 0,n .

Proof: 1) Utilize the convexity of regular conditional
distributions, and then the consistency condition C1, C2. 2),
3), follow from log-sum formulae.

(xn−1 , y n−1 ) −→

h(y)qn (dy; y n−1 , xn ) ∈ R
Yn

Proof: Utilizes (11), Theorem 2, and lower semicontinuity of relative entropy.
For capacity problems, it is desirable to identify conditions
←
−
→
−
←
−
so that IX n →Y n ( P 0,n , Q 0,n ) as a function of P 0,n for ﬁxed
→
−
Q 0,n is either upper semicontinuous or continuous.
→
−
Theorem 4. Consider a forward channel Q 0,n (·|xn ) ∈
C2
Q (Y0,n ; X0,n ), and a closed family of feedback channels
Qc,C1 (X0,n ; Y0,n−1 ) ⊆ QC1 (X0,n ; Y0,n−1 ). Suppose there
exists a family of measures ν0,n (dy n ) on (Y0,n , B(Y0,n ))
¯
→
−
n
such that Q 0,n (·|x )
ν0,n (dy n ) with Radon-Nikodym
¯

(13)

Xn

is jointly continuous in (xn−1 , y n−1 ) ∈ X0,n−1 × Y0,n−1 .
Then the following weak convergence results hold.
←
−
A1) Let P 0,n (·|y n−1 )
∈
QC1 (X0,n ; Y0,n−1 ) and
→α
−
n
C2
Q 0,n (·|x ) α≥1 ∈ Q (Y0,n ; X0,n ). Then the
←
−
→α
−
w
joint measure ( P 0,n ⊗ Q 0,n )(dxn , dy n )
=⇒
0
0
←
−
→
−
→
−
( P 0,n ⊗ Q 0,n )(dxn , dy n ), where Q 0,n (·|xn ) ∈
C2
Q (Y0,n ; X0,n ).
←
−
A2) Let P 0,n (·|y n−1 )
∈
QC1 (X0,n ; Y0,n−1 ) and
→α
−
n
C2
Q 0,n (·|x ) α≥1 ∈ Q (Y0,n ; X0,n ) and deﬁne the
←
−
→α
−
family of joint measures ( P 0,n ⊗ Q 0,n )(dxn , dy n ) α≥1
α
having marginals {ν0,n }α≥1 on Y0,n and {µα }α≥1
0,n
w
α
0
on X0,n . Then ν0,n (dy n ) =⇒ ν0,n (dy n ) and
w
0
µα (dxn ) =⇒ µ0 (dxn ) where ν0,n ∈ M1 (Y0,n )
0,n
0,n
0
and µ0,n ∈ M1 (X0,n ) are the marginals of
←
−
→0
−
( P 0,n ⊗ Q 0,n )(dxn , dy n ).
A3) The sets of measures QC1 (X0,n ; Y0,n−1 ), and
QC2 (Y0,n ; X0,n ) are weakly compact.
←
−
A4) Let
P 0,n (·|y n−1 )
∈
QC1 (X0,n ; Y0,n−1 ),
→α
−
n
C2
α
Q 0,n (·|x ) α≥1 ∈ Q (Y0,n ; X0,n ), and {ν0,n }α≥1

→
−
Q

(·|xn )

0,n
derivatives ξν0,n (xn , y n ) = ν0,n (·) (y n ), and
¯
¯
1)
the
family
of
Radon-Nikodym
derivatives
ξν0,n (xn , y n ) is continuous on X0,n × Y0,n , and
¯
ξν0,n (xn , y n ) log ξν0,n (xn , y n ) is uniformly integrable
¯
¯
←
−
←
−
over {¯0,n ⊗ P 0,n : P 0,n ∈ Qc,C1 (X0,n ; Y0,n−1 )};
ν
2) for a ﬁxed y n
∈
Y0,n , the Radon-Nikodym
derivative ξν0,n (xn , y n ) is uniformly integrable over
¯
Qc,C1 (X0,n ; Y0,n−1 ).
←
−
→
−
Then, the directed information IX n →Y n ( P 0,n , Q 0,n ) as
←
−
→
−
a functional of { P 0,n , Q 0,n } ∈ Qc,C1 (X0,n ; Y0,n−1 ) ×
C2
Q (Y0,n ; X0,n ) is bounded and weakly continuous over
Qc,C1 (X0,n ; Y0,n−1 ).

4

Proof: Invoke (11) and generalize related results in [14].

The next theorem establishes existence of the minimizer.
Theorem 6. Let X0,n be a Polish space and Y0,n a compact
Polish space. Assume ∀ h(·)∈BC(Y0,n ), the function

IV. E XTREMUM P ROBLEMS OF D IRECTED I NFORMATION
A. Existence of Capacity Achieving Distribution
Consider a communication channel with memory and feed→
−
back Q 0,n (·|xn ) ∈ QC2 (Y0,n ; X0,n ) and power constraint
←
−
←
−
P 0,n (P ) = P 0,n (·|y n−1 ) ∈ QC1 (X0,n ; Y0,n−1 ) :
←
−
→
−
g0,n (xn , y n−1 )( Q 0,n ⊗ P 0,n )(dxn , dy n ) ≤ P

(xn , y n−1 ) ∈ X0,n × Y0,n−1 −→

h(y)qn (dy; y n−1 , xn ) ∈ R
Yn

is continuous jointly in (xn , y n−1 ) ∈ X0,n × Y0,n−1 .
Then
1) The set QC2 (Y0,n ; X0,n ) is compact.
2) Assume d0,n : X0,n × Y0,n −→ [0, ∞] is measurable and
→
−
continuous on y n ∈ Y0,n . Then Q 0,n (D) is a closed
subset of QC2 (Y0,n ; X0,n ).
→
−
c
3) R0,n (D) has a minimum in Q 0,n (D).

where for any n ∈ N, g0,n : X0,n ×Y0,n−1 −→ [0, ∞] is Borel
←
−
measurable, and P 0,n (P ) non-empty. In the absence of any
power constraints the set of input conditional distributions is
QC1 (X0,n ; Y0,n−1 ).
The ﬁnite horizon maximization of directed information over
←
−
P 0,n (P ) or QC1 (X0,n ; Y0,n−1 ) (e.g., with or without power
constraints) is deﬁned by
←
−
→
−
f
C0,n =
sup
IX n →Y n ( P 0,n , Q 0,n ) (15)

Proof: Utilize Theorem 2, Part A, and generalize the
derivation in [12] to (n + 1)−fold convolution measures.
V. C ONCLUSION
In this paper we have provided a general framework through
which the properties of mutual information are extended
to directed information on Polish spaces. The existence of
extremums to capacity problems with memory and feedback,
and to lossy non-anticipative data compression problems are
discussed.

←
−
←
−
P 0,n (·|y n−1 )∈ P 0,n (P )
C1
or Q (X0,n ;Y0,n−1 )

The next theorem establishes existence of the maximizer.
Theorem 5. Suppose the assumptions of Theorem 2, Part A
are satisﬁed.
1) The set QC1 (X0,n ; Y0,n−1 ) is compact.
2) Suppose g0,n : X0,n × Y0,n−1 −→ [0, ∞] is measurable
and continuous in (xn , y n−1 ) ∈ X0,n × Y0,n−1 . Then the
←
−
set P 0,n (P ) is a closed subset of QC1 (X0,n ; Y0,n−1 ).
3) If in addition the assumptions of Theorem 4 are satisﬁed (here the assumption on QC1 (X0,n ; Y0,n−1 ) is
f
satisﬁed by 1) and 2) ) then C0,n has a maximum in
←
−
QC1 (X0,n ; Y0,n−1 ) (without constraints) or in P 0,n (P )
(with power constraints).

R EFERENCES
[1] H. Marko, “The bidirectional communication theory–A generalization
of information theory,” IEEE Transactions on Communications, vol. 21,
no. 12, pp. 1345–1351, Dec. 1973.
[2] J. L. Massey, “Causality, feedback and directed information,” in International Symposium on Information Theory and its Applications
(ISITA ’90), Nov. 27-30 1990, pp. 303–305.
[3] S. Tatikonda and S. Mitter, “The capacity of channels with feedback,”
IEEE Transactions on Information Theory, vol. 55, no. 1, pp. 323–349,
Jan. 2009.
[4] S. C. Tatikonda, “Control over communication constraints,” Ph.D. dissertation, Mass. Inst. of Tech. (M.I.T.), Cambridge, MA, 2000.
[5] J. Chen and T. Berger, “The capacity of ﬁnite-state Markov channels
with feedback,” IEEE Transactions on Information Theory, vol. 51,
no. 3, pp. 780–798, March 2005.
[6] R. Venkataramanan, “Information-theoretic results on communication
problems with feed-forward and feedback,” Ph.D. dissertation, University of Michigan-Ann Arbor, December 2008.
[7] P. A. Stavrou, C. D. Charalambous, and C. K. Kourtellaris, “Causal
rate distortion function on abstract alphabets: Optimal reconstruction
and properties,” CoRR, vol. abs/1202.0895, 2012. [Online]. Available:
http://arxiv.org/abs/1202.0895v1
[8] G. Kramer, “Directed information for channels with feedback,” Ph.D.
dissertation, Swiss Federal Institute of Technology (ETH), December
1998.
[9] ——, “Capacity results for the discrete memoryless network,” IEEE
Transactions on Information Theory, vol. 49, no. 1, pp. 4–21, Jan. 2003.
[10] T. S. Han and S. Verdu, “Approximation theory of output statistics,”
IEEE Transactions on Information Theory, vol. 39, no. 3, pp. 752–772,
May 1993.
[11] I. Csisz´ r, “Arbitrarily varying channels with general alphabets and
a
states,” IEEE Transactions on Information Theory, vol. 38, no. 6, pp.
1725–1742, Nov. 1992.
[12] ——, “On an extremum problem of information theory,” Studia Scientiarum Mathematicarum Hungarica, vol. 9, pp. 57–71, 1974.
[13] P. Dupuis and R. S. Ellis, A Weak Convergence Approach to the Theory
of Large Deviations. John Wiley & Sons, Inc., New York, 1997.
[14] M. Fozunbal, S. McLaughlin, and R. Schafer, “Capacity analysis for
continuous-alphabet channels with side information, part I: A general
framework,” IEEE Transactions on Information Theory, vol. 51, no. 9,
pp. 3075–3085, Sept. 2005.

Proof: 1) Utilize the fact that probability measures on
compact Polish spaces are compact. 2) Utilize the fact that
closed subset of weakly compact set is compact. 3) Follows
from Weierstrass theorem.
B. Existence of Non-Anticipative Rate Distortion Achieving
Distribution
→
−
Consider a reconstruction channel Q 0,n (·|xn )
∈
QC2 (Y0,n ; X0,n ), a ﬁxed source µ0,n (dxn ) ∈ M1 (X0,n ), and
deﬁne the ﬁdelity constraint by
→
−
→
−
Q 0,n (D) = Q 0,n (·|xn ) ∈ QC2 (Y0,n ; X0,n ) :
→
−
d0,n (xn , y n )(µ0,n ⊗ Q 0,n )(dxn , dy n ) ≤ D
(16)
where D ≥ 0, and for each n ∈ N, d0,n : X0,n × Y0,n −→
→
−
[0, ∞] is Borel measurable, and Q 0,n (D) is non-empty.
The ﬁnite horizon minimization of directed information over
→
−
Q 0,n (D) is deﬁned by
→
−
c
R0,n (D) = →
inf →
IX n →Y n (µ0,n , Q 0,n ) (17)
−
−
Q 0,n (·|xn )∈ Q 0,n (D)

5

