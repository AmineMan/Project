Title:          ISIT_RSC.pdf
Author:         Sadaf
Creator:        Adobe Acrobat 9.0.0
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Mon May 14 15:10:01 2012
ModDate:        Tue Jun 19 12:55:34 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      595 x 842 pts (A4)
File size:      288855 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569557851

On Source Transmission over Some Classes of
Relay Channels
Sadaf Salehkalaibar and Mohammad Reza Aref
Information Systems and Security Lab (ISSL),
Department of Electrical Engineering,
Sharif University of Technology, Tehran, Iran
E-mails: s saleh@ee.sharif.edu, aref@sharif.edu
1

correlated side information at each receiver. A relay channel
with correlated sources was studied in some works [3], [14],
[15], [16]. Gunduz and Erkip [3] investigated the reliable
transmission of a single source over the relay channel with
correlated side information at the relay and the receiver. The
coding technique of [3] is based on source-channel separation,
where the Decode-and-Forward (DF) strategy [17] is employed
as the achievability scheme. Separation has been utilized for
the relay channel with arbitrarily correlated sources in [14]. In
[14], the block Markov coding and irregular decoding strategy
[17] was used to ﬁnd the explicit conditions under which the
sources can be reliably transmitted to the receiver. Necessary
and sufﬁcient conditions for the reliable transmission of correlated sources over linear deterministic relay networks have
been characterized in [15]. Multicasting a set of correlated
sources over cooperative relay networks was studied in [18].
In this paper, we study the reliable transmission of correlated sources (S1 , S2 ) over the relay channel for a given
source-channel rate of the joint source-channel code (Fig.
1). Sources S1 and S2 are available at the sender and the
relay, respectively. The destination wishes to reconstruct both
sources. We ﬁnd necessary and sufﬁcient conditions for the
achievability of a given source-channel rate, using separate
source-channel codes. The coding technique uses Slepian-Wolf
source coding [8] followed by the Partial Decode-and-Forward
(PDF) strategy [17] as the achievability scheme. In the PDF
strategy, the relay partially decodes the message transmitted
from the sender. We also consider the semi-deterministic relay
channel where the output at the relay is a deterministic function of inputs from the sender and the relay. We show that for
the transmission of a single source over the semi-deterministic
relay channel with correlated side information at the relay,
the proposed conditions coincide, yielding a characterization.
We also obtain necessary and sufﬁcient conditions for the
transmission of sources over the degraded relay channel where
the output at the receiver is a degraded version of the output
at the relay.
The paper is organized as follows: In Section II, we present
a mathematical framework for our work. In Section III, we
ﬁnd necessary and sufﬁcient conditions for the achievability
of a given source-channel rate, also we discuss source transmission over the semi-deterministic relay channel with side
information at the relay. Conclusions are provided in Section

Abstract—We study the reliable transmission of correlated
sources over the relay channel. One of the sources is available
at the sender while the other one is known to the relay. The
receiver wishes to reconstruct both sources. We ﬁnd necessary
and sufﬁcient conditions for optimal separation of source and
channel codes. The coding scheme is based on the combination of
the Slepian-Wolf source coding and Partial Decode-and-Forward
(PDF) strategy. In this scheme, the relay partially decodes the
message transmitted by the sender. We also consider the semideterministic relay channel where the output at the relay is
a deterministic function of inputs from the sender and the
relay. For the transmission of a single source over the semideterministic relay channel with correlated side information at
the relay, the proposed conditions coincide. We also ﬁnd necessary
and sufﬁcient conditions for the transmission of sources over the
degraded relay channel where the output at the receiver is a
degraded version of the output at the relay.

I. I NTRODUCTION
Source-channel separation theorem states that a given source
can be reliably transmitted over a channel if and only if
the minimum source coding rate is less than the channel
capacity [1]. Optimality of the separate source-channel coding
has been investigated in several works [2]-[5]. On the other
hand, suboptimality of the separation has been shown for
some multi-user systems [6], [7]. Reliable transmission of
correlated sources over Multiple Access Channels (MACs) is
considered in [7]. It is shown by an example [7] that simply
comparing Slepian-Wolf region [8] with the capacity region of
the underlying MAC does not necessarily reveal the possibility
or impossibility of reliable transmission. In [4], the general
source coding problem is redeﬁned. The problem which is
considered instead is to determine whether the sources can be
transmitted to their destinations for a given number of channel
uses per source sample (cupss), which is deﬁned to be the
source-channel rate of the joint source-channel code.
The correlation preserving technique of [7] is utilized to
obtain sufﬁcient conditions for the reliable transmission of
correlated sources over Broadcast Channels (BCs) [9], Interference Channels (ICs) [10] and relay channels [11], [12].
Tuncel [13] established the optimal source-channel rate for the
transmission of a single source over multireceiver BCs with
1 This work was partially supported by Iranian NSF under contract no.
88114/46 and by Iran Telecom Research Center (ITRC) and by Iranian NSFcryptography chair.

1

Sm
2

Yrn
S

m

X n p( y, y | x, x ) Xrn
r
r

7[

1

5HOD\

Y
Fig. 1.

n

5[

S S

m m
( ˆ1 , ˆ2 )

The relay channel with correlated sources S1 and S2

III. M AIN R ESULTS

IV.
II. P RELIMINARIES AND D EFINITIONS
We denote discrete random variables with capital letters e.g.,
j
X, Y , and their realizations with lower case letters x, y. Xi
indicates a sequence of random variables (Xi , ..., Xj ). We use
H(.) to denote the entropy of a discrete random variable and
I(.; .) to denote the mutual information between two discrete
random variables. We denote by An (X, Y ), the set of εε
strongly jointly typical sequences of length n, on p(x, y). A
random variable X takes values in a set X . Finally, we denote
the probability mass function of X over X with p(x) and the
conditional probability mass function of X given Y by p(x|y).
Deﬁnition 1: For the lossless transmission of the source
pair (sm , sm ) over the discrete memoryless relay channel (Fig.
1
2
1) deﬁned by p(y, yr |x, xr ), a length n joint source channel
code consists of an encoder mapping,

In this section, we ﬁrst ﬁnd necessary and sufﬁcient conditions for the achievability of a source-channel rate κ. Then,
we discuss a special case where the conditions coincide.
The following theorem provides sufﬁcient conditions for the
transmission of sources over a relay channel.
Theorem 1: Consider the transmission of correlated sources
S1 and S2 over a relay channel. The source-channel rate
κ is achievable if there exist auxiliary random variables V
and Vr with joint distribution of all variables factoring as
p(v, vr )p(xr |vr )p(x|v, vr ) such that

m
f (m,n) : S1 → X n ,

Proof: The proof of Theorem 1 can be obtained from the
correlation-preserving method of [11], [12]. Here, we constrain
the channel inputs to be independent of source distributions.
We provide the proof of achievability to illustrate the separation scheme that is used. We use Slepian-Wolf source coding
followed by the PDF strategy as the achievability scheme.
Consider a rate pair (R1 , R2 ) satisfying the following:

H(S1 |S2 ) ≤κ[I(X; Y |V, Vr , Xr ) + I(V ; Yr |Vr , Xr )]
H(S2 |S1 ) ≤κI(Y ; Xr |V, Vr , X)
H(S1 , S2 ) ≤κ[I(Y ; X, Xr |V, Vr ) + I(V ; Yr |Vr , Xr )]
H(S1 , S2 ) ≤κI(X, Xr ; Y ).

a set of relay encoding functions,
(m,n)

Xr,k = gk

m
(Yr,1 , ..., Yr,k−1 , S2 ), (1 ≤ k ≤ n),

and a decoder mapping,
m
m
h(m,n) : Y n → S1 × S2 .

The probability of error is deﬁned as
(m,n)
Pe

n

= Pr(h(Y ) =

(1)

H(S1 |S2 ) < R1 < κ[I(X; Y |V, Vr , Xr ) + I(V ; Yr |Vr , Xr )]
H(S2 |S1 ) < R2 < κI(Y ; Xr |V, Vr , X)

m
m
(S1 , S2 )).

H(S1 , S2 ) < R1 + R2 < κ[I(Y ; X, Xr |V, Vr ) + I(V ; Yr |Vr , Xr )]
H(S1 , S2 ) < R1 + R2 < κI(X, Xr ; Y ).

Deﬁnition 2: We say that the source-channel rate κ is
achievable if there exists a sequence of encoders f (m,n) ,
(m,n)
g (m,n) and decoder h(m,n) with κ = n/m such that Pe
→
0 as m, n → ∞.
Remark 1: For the lossless transmission of the source sn
1
over the relay channel with correlated side information sn at
2
the relay, the mapping at the decoder is deﬁned as

Also, choose the rate R0 such that 0 ≤ R0 ≤ R1 .
(a) Random partitioning:
m
mR1
• Randomly partition the source sequence s1 into 2
bins and denote the bin index for a given sm by j(sm ) ∈
1
1
{1, ..., 2mR1 }.
m
mR2
• Randomly partition the source sequence s2 into 2
m
m
bins and denote the bin index for a given s2 by l(s2 ) ∈
{1, ..., 2mR2 }.
(b) Code construction:

(m,n)
m
hs
: Y n → S1 .

Therefore, the probability of error is
(m,n)
m
= Pr(h(Y n ) = S1 ).
Pes

2

n
Generate 2mR0 sequences vr (q) according to
n
p(vr,i ), q ∈ [1 : 2mR0 ].
i=1
n
mR0
• For each vr (q), generate 2
sequences v n (t, q) acn
cording to i=1 p(vi |vr,i (q)), t ∈ [1 : 2mR0 ].
n
n
m(R1 −R0 )
• For each (vr (q), v (t, q)), generate 2
sen
quences x (j , t, q) according to
n
m(R1 −R0 )
].
i=1 p(xi |vi (t, q), vr,i (q)), j ∈ [1 : 2
n
mR2
• For each vr (q), generate 2
sequences xn (l, q) acr
n
cording to i=1 p(xr,i |vr,i (q)), l ∈ [1 : 2mR2 ].
(c) Encoding:
We use separate source and channel codes for encoding. A
total of Bm source samples are transmitted over a total of
(B + 1)n channel uses. For any (n, m), deﬁne κ = n/m.
Consider a source sequence sBm of length Bm. Partition
1
this sequence into B portions, sm , b = 1, ..., B. Similarly,
1,b
partition the source sequence sBm of length Bm into B
2
portions, sm , b = 1, ..., B. In the ﬁrst block, the transmitter
2,b
observes sm and ﬁnds its bin index j1 (sm ) . It then splits
1,1
1,1
j1 (sm ) into two parts t1 and j1 with rates R0 and (R1 − R0 ),
1,1
respectively. The transmitter sends xn (1, t1 , 1). The relay also
observes sm and ﬁnds its bin index l1 (sm ). It then sends
2,1
2,1
xn (1, 1). In block b, the relay knows tb−1 from block b − 1. It
r
then sends xn (lb−1 , qb ), where qb = tb−1 and lb−1 is the bin
r
index of sm . The transmitter sends xn (jb−1 , tb , qb ), where
2,b−1
jb−1 is the bin index of sm
1,b−1 and jb−1 = (jb−1 , tb−1 ). In
block B + 1, the transmitter and the relay send xn (jB , 1, tB )
and xn (lB , tB ), respectively.
r
(d) Decoding in block b:
At relay: The relay knowing qb ﬁnds tb such that
n
(xn (lb−1 , qb ), v n (tb , qb ), vr (qb ), yr (b)) ∈ An . This can be
r
done with an arbitrarily small probability of error if:

Finally, considering (2)-(9), we obtain the conditions in (1).

•

R0 ≤ κI(V ; Yr |Vr , Xr )

Remark 2: If there is no source sequence available at the
relay, i.e., let S2 = 0, S1 = S, Vr = Xr , p(s, v, x, xr ) =
p(s)p(v, x, xr ) and H(S) = R in the conditions of (1), then
we obtain the following achievable rate for the relay channel:
R ≤ min{I(X, Xr ; Y ), I(X; Y |V, Xr ) + I(V ; Yr |Xr )}
(10)
for the joint distribution p(v, xr , x). The achievable rate of
(10) was obtained for the relay channel in [17] using the PDF
strategy.
Corollary 1: If the relay completely decodes the message
sent from the transmitter, i.e., let V = X in the conditions of
(1), then we obtain the following conditions for the transmission of (S1 , S2 ) over the relay channel with source-channel
rate κ,
H(S1 |S2 ) ≤ κI(X; Yr |Vr , Xr ),
H(S2 |S1 ) ≤ κI(Xr ; Y |Vr , X),
H(S1 , S2 ) ≤ κI(X, Xr ; Y ),

where the joint distribution of all variables can be factored as
p(s1 , s2 , x, xr , vr ) = p(s1 , s2 )p(vr )p(x|vr )p(xr |vr ).

H(S1 |S2 ) ≤κ[I(X; Y |V, Vr , Xr ) + I(V ; Yr |Vr , Xr )] (15)
(16)
H(S1 ) ≤κI(X, Xr ; Y ),
for all joint distributions of the form p(v, vr )p(xr |vr )p(x|v,
vr ). The conditions in (15)-(16) are special cases of (1) when
S2 is not decoded at the receiver.
The following theorem provides necessary conditions for
the achievability of a source-channel rate κ.
Theorem 2: In a lossless transmission of correlated sources
(S1 , S2 ) over a relay channel, if the source-channel rate κ is
achievable then

Backward decoding at the receiver: The receiver begins
decoding after receiving the last block. Assume that tb
has been decoded accurately from block b + 1. For block
b, the receiver declares that (lb−1 , jb−1 , tb−1 ) is sent if
n
(xn (jb−1 , tb , qb ), xn (lb−1 , qb ), v n (tb , qb ), vr (qb ), y(b)) ∈ An .
r
This can be done with an arbitrarily small probability of error
if

H(S1 |S2 ) ≤ κI(X; Y, Yr |Xr )

(6)

(17)

H(S2 |S1 ) ≤ κI(Xr ; Y |X)
H(S1 , S2 ) ≤ κI(X, Xr ; Y )

(3)
(4)
(5)

R2 ≤ κI(Xr ; Y |V, Vr , X).

(18)
(19)

for some joint distribution of the form
p(s1 , s2 )p(x, xr )p(y, yr |x, xr ).

These indices are provided to the source decoder, where
jb−1 = (jb−1 , tb−1 ). The source decoder then ﬁnds sm
1,b−1 and
m
m
sm
2,b−1 such that jb−1 = jb−1 (s1,b−1 ), lb−1 = lb−1 (s2,b−1 ) and
(sm , sm ) ∈ An . From Slepian-Wolf source coding [8],
1,b−1 2,b−1
we have

m m
m
m
m m
H(S1 |S2 ) = I(S1 ; Y n , Yrn |S2 ) + H(S1 |S2 , Y n , Yrn )
(a)

m
m
≤ I(S1 ; Y n , Yrn |S2 ) + nδ

m
≤ I(X n ; Y n , Yrn |S2 ) + nδ

(7)

H(S2 |S1 ) ≤ R2
H(S1 , S2 ) ≤ R1 + R2 .

(20)

Proof: We have

(b)

H(S1 |S2 ) ≤ R1

(14)

Corollary 2: The following conditions can be obtained for
the reliable transmission of the single source S1 with correlated side information S2 at the relay and source-channel rate
κ,

(2)

(R1 − R0 ) + R2 + R0 ≤ κI(X, Xr ; Y )
(R1 − R0 ) + R2 ≤ κI(X, Xr ; Y |V, Vr )
R1 − R0 ≤ κI(X; Y |V, Vr , Xr )

(11)
(12)
(13)

(8)
(9)

(c)

n
m
I(X n ; Yi , Yr,i |S2 , Y i−1 , Yri−1 ) + nδ

=

i=1

3

n

(d)

form a Markov chain. Finally, we have

m
I(X n ; Yi , Yr,i |S2 , Y i−1 , Yri−1 , Xr,i ) + nδ

=

m
m
m
m
m
m
H(S1 , S2 ) = I(S1 , S2 ; Y n ) + H(S1 , S2 |Y n )

i=1
n

(a)

m
H(Yi , Yr,i |S2 , Y i−1 , Yri−1 , Xr,i )

=

m
m
≤ I(S1 , S2 ; Y n ) + nδ

i=1
n

(b)

H(Yi , Yr,i |X

−

n

m
, S2 , Y i−1 , Yri−1 , Xr,i )

n
≤ I(X n , Xr ; Y n ) + nδ

+ nδ

(c)

i=1
(e) n

≤

i=1
n

=

H(Yi , Yr,i |Xr,i ) −
i=1
n

=

i=1
n

≤

n

(f )

H(Yi , Yr,i |Xi , Xr,i ) + nδ
(d)

i=1

m
m
≤ I(S2 ; Y n |S1 ) + nδ

m
m
≤ I(S2 , Yrn−1 ; Y n |S1 ) + nδ
(b)

n
m
≤ I(Xr ; Y n |S1 ) + nδ
n
n
m
I(Xr ; Yi |S1 , Y i−1 ) + nδ
i=1
n

H(Yi |X

n

m
n
, S1 , Y i−1 , Xr )

i=1
n

≤
i=1
n

=

+ nδ

m
n
H(Yi |X n , S1 , Y i−1 , Xr ) + nδ
i=1
n

H(Yi |Xi ) −
i=1
n

=

I(Xi , Xr,i ; Yi ) + nδ

(21)
(22)
(23)

for some joint distribution of the form p(s1 , s2 , x, xr , y, yr ) =
p(s1 , s2 )p(x, xr )p(y, yr |x, xr ).
Remark 3: The available source correlation does not help
in creating correlated channel inputs, as Theorem 2 already
optimizes over all possible joint channel input distributions.
However, this bound is not tight in general, since, in reality
not all joint distributions can be achieved.
Remark 4: Consider the transmission of the source S1 over
a degraded relay channel with correlated side information S2
at the relay. It is shown in [3] that the source-channel rate κ
is achievable if and only if

n

H(Yi |Xi ) −

H(Yi |Xi , Xr,i ) + nδ
i=1

H(S1 , S2 ) ≤ κI(X, Xr ; Y )

m
H(Yi |S1 , Y i−1 , X n )
i=1
n

(e)

H(Yi ) −

H(S1 |S2 ) ≤ κI(X; Yr |Xr )
H(S2 |S1 ) ≤ κI(Xr ; Y |X)

n
m
I(Xr ; Yi |S1 , Y i−1 , X n ) + nδ
i=1
n

=

i=1
n

where (a) follows from Fano’s inequality. (b) follows from the
data processing inequality. (c) follows from the chain rule.
n
i−1
n
(d) follows because (X i−1 , Xi+1 , Y i−1 , Xr , Xr,i+1 ) →
(Xi , Xr,i ) → Yi form a Markov chain. From convexity
arguments and letting m, n → ∞, we obtain the conditions
in (17)-(19).
Now, we discuss the reliable transmission of the source pair
(S1 , S2 ) over the degraded relay channel where the channel is
characterized by p(y, yr |x, xr ) = p(y|yr , xr )p(yr |x, xr ). We
use the sufﬁcient conditions of Corollary 1 for this case. The
following necessary conditions can be obtained as a special
case of Theorem 2.
Corollary 3: In a lossless transmission of correlated sources
(S1 , S2 ) over a degraded relay channel, if the source-channel
rate κ is achievable then

(a)

−

i=1
n

i=1

m m
m
m
m m
H(S2 |S1 ) = I(S2 ; Y n |S1 ) + H(S2 |S1 , Y n )

=

n
H(Yi |X n , Y i−1 , Xr ) + nδ

H(Yi ) −

=

where (a) follows from Fano’s inequality. (b) follows from the
data processing inequality. (c) follows from the chain rule. (d)
m
follows because Xr,i is a function of (S2 , Yri−1 ). (e) follows
because conditioning does not increase entropy. (f ) follows
n
m
because (X i−1 , Xi+1 , S2 , Y i−1 , Yri−1 ) → (Xi , Xr,i ) →
(Yi , Yr,i ) form a Markov chain. We also have

(d)

i=1

i=1
n

I(Xi ; Yi , Yr,i |Xr,i ) + nδ

n
H(Yi |X n , Xr , Y i−1 ) + nδ

n

=

i=1

=

n

H(Yi |Y i−1 ) −

=

m
H(Yi , Yr,i |X n , S2 , Y i−1 , Yri−1 , Xr,i ) + nδ

(c)

n
I(X n , Xr ; Yi |Y i−1 ) + nδ
i=1
n

H(Yi , Yr,i |Xr,i )

i=1
n

−

n

=

H(Yi |Xi , Xr,i ) + nδ
i=1

I(Xr,i ; Yi |Xi ) + nδ
i=1

H(S1 |S2 ) ≤ κI(X; Yr |Xr )

(24)

H(S1 ) ≤ κI(X, Xr ; Y )

where (a) follows from Fano’s inequality. (b) follows from the
data processing inequality. (c) follows from the chain rule. (d)
m
follows because X n is a function of S1 . (e) follows because
m
n
i−1
n
(S1 , Y i−1 , X i−1 , Xi+1 , Xr , Xr,i+1 ) → (Xi , Xr,i ) → Yi

(25)

for some joint distribution p(s1 , s2 )p(x, xr )p(y, yr |x, xr ). The
conditions (24)-(25) are special cases of the proposed neces-

4

sary and sufﬁcient conditions. To show this, set Vr = Xr
in (15)-(16). The lower bound follows from Corollary 3 and
considering the fact that given (Yr , Xr ), Y is independent of
X.
We show that Theorems 1 and 2 coincide for a special case.
Consider the semi-deterministic relay channel where Yr is a
deterministic function of (X, Xr ), i.e., Yr = f (X, Xr ).
Theorem 3: Consider the transmission of a source S1 over
a semi-deterministic relay channel with side information S2 at
the relay. Then, the source-channel rate κ is achievable if and
only if
H(S1 |S2 ) ≤ κ[I(X; Y |Yr , Xr ) + H(Yr |Xr )]
H(S1 ) ≤ κI(X, Xr ; Y )

[11] S. Salehkalaibar and M. R. Aref, “On the transmission of correlated
sources over relay channels,” in Proc. IEEE Int. Symp. on Info. Theory
(ISIT), Saint Petersburg, Russia, Jul.-Aug. 2011, pp. 1409-1413.
[12] S. Salehkalaibar and M. R. Aref, “On the reliable transmission of
correlated sources over relay channels,” Submitted to IEEE Trans. on
Info. Theory, Oct. 2011.
[13] E. Tuncel, “Slepian-Wolf coding over broadcast channels,” IEEE Trans.
on Info. Theory, vol. 52, no. 4, pp. 1469-1482, Apr. 2006.
[14] B. Smith and S. Vishwanath, “Cooperative communication in sensor
networks: relay channels with correlated sources,” Proc. 42nd Annual
Allerton Conference on Communication, Control, and Computing, Monticello, IL, Sept.-Oct. 2004, pp. 1881-1890.
[15] S. Mohajer, C. Tian and S. N. Diggavi, “On source transmission over
deterministic relay networks,” Information Theory Workshop (ITW),
Cairo, Egypt, Jan. 2010, pp. 1-5.
[16] D. Gunduz, E. Erkip, A. Goldsmith and H. V. Poor, “Joint sourcechannel cooperative transmission over relay-broadcast networks,” Submitted to IEEE Trans. Information Theory, 2011, Available at
http://arxiv.org/abs/1202.5967.
[17] T. M. Cover and A. El Gamal, “Capacity theorems for relay channels,”
IEEE Trans. on Info. Theory, vol. 25, no. 5, pp. 572-584, Sept. 1979.
[18] M. H. Yassaee and M. R. Aref, “Slepian-Wolf coding over cooperative
relay networks,” IEEE Trans. on Info. Theory, vol. 57, no. 6, pp. 34623482, June 2011.

(26)
(27)

for some joint distribution p(s1 , s2 )p(x, xr )p(y, yr |x, xr ).
Proof: For achievability, set V = Yr and Vr = Xr in
(15)-(16). The proof of converse easily follows from Theorem
2 and considering the fact that Yr = f (X, Xr ).
IV. C ONCLUSION
We investigated the reliable transmission of correlated
sources over the relay channel. We found necessary and
sufﬁcient conditions for the achievability of the source-channel
rate κ. The coding scheme uses Slepian-Wolf source coding
followed by the PDF strategy. We also considered the semideterministic relay channel. It is shown that for the source
transmission over the semi-deterministic relay channel with
correlated side information at the relay, the proposed bounds
coincide. We also found necessary and sufﬁcient conditions
for the reliable transmission of correlated sources over the
degraded relay channel.
R EFERENCES
[1] T. M. Cover and J. A. Thomas, “Elements of information theory,” Wiley
Series in Telecommunications and Signal Processing, 2nd ed., 2006.
[2] S. Vembu, S. Verdu and Y. Steinberg, “The source-channel theorem
revisited,” IEEE Trans. on Info. Theory, vol. 41, no. 1, pp. 44-54, Jan.
1995.
[3] D. Gunduz and E. Erkip, “Reliable cooperative source transmission with
side information,” IEEE Info. Theory Workshop on Wireless Networks,
Bergen, Norway, Jul. 2007, pp. 1-5.
[4] D. Gunduz, E. Erkip, A. Goldsmith and H. V. Poor, “Source and channel
coding for correlated sources over multiuser channels,” IEEE Trans. on
Info. Theory, vol. 55, no. 9, pp. 3927-3944, Sept. 2009.
[5] M. Effros, M. Medard, T. Ho, S. Ray, D. Karger and R. Koetter, “Linear
network codes: a uniﬁed framework for source channel, and network
coding,” in DIMACS Series in Discrete Mathematics and Theoretical
Computer Science. Providence, RI: Amer. Math. Soc., 2004, vol. 66.
[6] C. E. Shannon, “Two-way communication channels,” in Proc. 4th
Berkeley Symp. Math. Satist. Probability, 1961, vol. 1, pp. 611-644.
[7] T. Cover and A. El Gamal and M. Salehi, “Multiple access channels
with arbitrarily correlated sources,” IEEE Trans. on Info. Theory, vol.
26, No. 6, pp. 648-657, Nov. 1980.
[8] D. Slepian and J. Wolf, “Noiseless coding of correlated information
sources,” IEEE Trans. on Info. Theory, vol. 19, no. 4, pp. 471-480, Jul.
1973.
[9] T. S. Han and M. H. M. Costa, “Broadcast channels with arbitrarily
correlated sources,” IEEE Trans. on Info. Theory, vol. 33, no. 5, pp.
641-650, Sep. 1987.
[10] M. Salehi and E. Kurtas, “Interference channels with correlated sources,”
Proc. IEEE Int. Symp. on Info. Theory (ISIT), San Antorio, Texas, Jan.
1993, pp. 208.

5

