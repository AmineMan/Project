Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 12:46:47 2012
ModDate:        Tue Jun 19 12:54:11 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      495685 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565669

The Capacity of the Multi-MMSE Constrained
Gaussian Channel
Ronit Bustin

Shlomo Shamai (Shitz)

Dept. Electrical Engineering Technion−IIT
Technion City, Haifa 32000 Israel
Email: bustin@tx.technion.ac.il

Dept. Electrical Engineering Technion−IIT
Technion City, Haifa 32000 Israel
Email: sshlomo@ee.technion.ac.il

The best known achievable region for the two-user interference channel is given by the Han and Kobayashi (HK) scheme
[7]. This scheme uses partial decoding of the interfering
message at the receiver. Rate splitting (that is, superposition
coding) is a special case of the HK scheme, and is also pointto-point “bad” (see [3, Appendix VIII-C]). It was shown in
[8] that these codes are close to optimal for the two-user
Gaussian interference channel, and in fact are within one bit
from capacity. On the other hand, as explained in [9], for
K-users interference channels these codes might very well be
sub-optimal. This does not contradict the result show here, that
these codes are in fact optimal MMSE-wise for any number of
unintended receivers (see concluding remarks in section IV).
A similar question has been raised in the work of Bandemer
and El Gamal [10], where they provide the rate-disturbance
region: for any given rate that can be transmitted reliably to the
intended receiver, what is the minimum possible disturbance
that can be attained at some interfered user. In [10] the authors
measure the disturbance using the mutual information between
the codeword and the output at the interfered user, rather
than the minimum possible MMSE, as done here. We further
discuss and compare the two measures in our concluding
remarks (section IV).

Abstract—We examine codes, over the additive Gaussian noise
channel, designed for reliable communication at some speciﬁc
signal-to-noise ratio (snr) and constrained by the permitted
MMSE at K lower snrs. Extending the result of the single MMSE
constrained code, we show that K-layers superposition codes
attain the constrained capacity. Moreover, we prove that given a
reliable code attaining the multi-MMSE constrained capacity,
its MMSE and mutual information, as functions of snr, are
completely deﬁned. Thus, no other multi-MMSE constrained
capacity achieving code attains better MMSE performance at
unconstrained snrs.

I. I NTRODUCTION
In this work we examine the additive Gaussian noise channel, and derive the capacity region under K minimum-mean
square error (MMSE) constraints, at lower signal-to-noise
ratios (snrs). In [1] it was shown that the mutual information
and thus also the MMSE of “good” (capacity achieving [2])
point-to-point codes is known exactly, no matter the speciﬁc
structure of the code. Constraining the MMSE may only
reduce the maximum possible rate. In [3], [4] the authors
investigate “bad” point-to-point codes (non-capacity achieving) [3], [4], heavily used in many multi-terminal wireless
networks. It was shown in [4] that “bad” codes can obtain
lower MMSE at low snrs. However, the exact tradeoff between
rate and MMSE was still an open question. In [5] this tradeoff
was exactly depicted for a single MMSE constraint at some
lower snr. This work extends the description of the tradeoff
to an arbitrary number of MMSE constraints. In both cases
it is shown that Gaussian superposition codebooks attain the
MMSE constrained capacity.
The advantage of a “bad” point-to-point code, constrained
in MMSE at some lower snrs, is meaningless in point-topoint communication, where all that matters is the performance at the receiver. However, in multi-terminal wireless
networks, such as a cellular network, the case is different.
In such networks the transmission of one user interferes with
neighboring, unintended, receivers. A lower MMSE implies
better possible interference cancelation, and thus improved
rates for the interfered user. For comparison, the performance
of optimal point-to-point codes in the interference setting was
investigated in [6].

II. P ROBLEM F ORMULATION AND P RELIMINARY R ESULTS
In this work we are looking at the transmission of codewords, of length n, through a discrete memoryless standard
Gaussian channel:
√
Y = γX + N
(1)
where N is standard additive Gaussian noise. The codewords
are constrained by the standard average power constraint:
∀x ∈ Cn

1
n

n

x2 ≤ 1
i

(2)

i=1

where Cn stands for a code of n-dimensional codewords. We
examine codebooks designed for reliable transmission at γ =
snrK (reliable decoding of the codeword from Y (γ = snrK )).
Our main interest will be in examining non-optimal codes,
alternatively known as “bad” codes [4], deﬁned using codesequences, as follows:

The work of R. Bustin and S. Shamai has been supported by the the Israel
Science Foundation (ISF). Ronit Bustin is supported by the Adams Fellowship
Program of the Israel Academy of Sciences and Humanities.

∞

Deﬁnition 1. A non-optimal code-sequence C = {Cn }n=1 , for
a channel with capacity C, is a code-sequence with vanishing

1

In [11] this result has been extended to the case of two
MMSE constraints, where we have shown that a two-layer
superposition code attains the maximum rate. In this work
we further extend the problem to arbitrary K constraints,
showing that K-layers superposition codes attain the optimal
rate. Moreover, we examine the behavior of optimal codes
complying with all K constraints, and show that their MMSE
and mutual information is completely deﬁned, and, of course,
is that of the K-layers superposition code.

error probability and rate satisfying
1
logMn < C
n
where Mn is the size of code Cn .
lim

(3)

n→∞

Their associated MMSE deﬁned as:
1
MMSEcn (γ) = Tr(EX (γ))
n
1
√
= Tr E (X − E {X | γX + N })
n
√
(X − E {X | γX + N })T
(4)
1
MMSEc (γ) = lim MMSEcn (γ) = lim Tr(EX (γ)) (5)
n→∞ n
n→∞
where EX (γ) is the MMSE matrix when estimating the
√
codeword X from the output of the channel Y = γX + N .
We further deﬁne the following for abbreviation:

A. The I-MMSE approach
The approach used in order to provide insight into the above
mentioned problem is the I-MMSE approach, this to say that
we make use of the fundamental relationship between the
mutual information and the MMSE in the Gaussian channel
and its generalizations [12], [13]. Even though we are examining a scalar Gaussian channel, the n-dimensional version
of this relationship is required since we are looking at the
transmission of n-dimensional codewords through the channel.
In our setting the relationship is as follows:

1
I (X; Y (γ))
(6)
n
1
(7)
I(γ) = lim In (γ) = lim I (X; Y (γ))
n→∞
n→∞ n
Surely, for any such codes, the error probability for any
γ > snrK is zero, when n → ∞, since reliable transmission
is guaranteed at snrK . As a result MMSEc (γ) for these snrs
is also zero. On the other hand, for γ < snrK the value of the
error probability is not guaranteed to be any speciﬁc value. For
an optimal code, it was shown in [1], that I(γ) for γ < snrK
follows that of the Gaussian i.i.d. input and thus MMSEc (γ) is
1
also known exactly and descends gradually according to 1+γ .
Our goal is to understand the tradeoff between the rate at
snrK and the MMSE at lower snrs. In other words, what
is the maximum rate, assuming we require limited MMSE
at lower snrs, or equivalently, given a reduced rate at snrK
what is the minimum possible MMSE at lower snrs. In [5]
we have shown that given a reliable code of reduced rate,
designed for some snr2 , we can obtain a lower bound on
the MMSE of that code at some snr1 < snr2 . Moreover, this
lower bound is attained by the optimal superposition codebook
for (snr1 , snr2 ). Equivalently, this result can be stated as the
maximum rate at snr2 assuming the code is limited in the
MMSE at snr1 . This result is given in the next theorem, in the
later form.
In (γ) =

I(snr) =

β
1 + βsnr1

1
1
log (1 + βsnr2 ) + log
2
2

1 + snr1
1 + βsnr1

1
2

qA (X, σ 2 , γ) =

(10)

snr

MMSEc (γ) dγ.

(11)

0

σ2
Tr (A) − Tr (AEX (γ)) (12)
1 + σ2 γ

where A is some n × n general weighting matrix. The
following theorem is proved in [14],
Theorem 2 ([14]). Let A ∈ Sn be a positive semideﬁnite
+
matrix. Then, the function γ → qA (X, σ 2 , γ), deﬁned in (12),
has no nonnegative-to-negative zero crossings and, at most,
a single negative-to-nonnegative zero crossing in the range
γ ∈ [0, ∞). Moreover, let snr0 ∈ [0, ∞) be that negative-tononnegative crossing point. Then,
1) qA (X, σ 2 , 0) ≤ 0.
2) qA (X, σ 2 , γ) is a strictly increasing function in the
range γ ∈ [0, snr0 ).
3) qA (X, σ 2 , γ) ≥ 0 for all γ ∈ [snr0 , ∞).
4) limγ→∞ qA (X, σ 2 , γ) = 0.

(8)

for some β ∈ [0, 1], is the following
I(snr2 ) =

MMSEcn (γ) dγ.
0

The main property of the I-MMSE used for these proofs is an
n-dimensional “single crossing point” property derived in [14]
given here for completeness. This property is an extension of
the scalar “single crossing point” property shown in [15]. In
[14] the following function is deﬁned for an arbitrary random
vector X:

I(snr2 )

s.t. MMSEc (snr1 ) ≤

snr

Taking the limit of n → ∞ on both sides results with:

Theorem 1 ([5]). Assume snr1 < snr2 . The solution of the
following optimization problem,
max

1
2

In (snr) =

In this work, the matrix A can be set to the identity matrix.
The above property is valid for all natural n, thus we may also
take n → ∞.

(9)

B. Superposition Coding

and is attainable when using the optimal Gaussian superposition codebook designed for (snr1 , snr2 ) with a rate-splitting
coefﬁcient β.

An important family of non-optimal codes, that is, a family
of codes that do not attain the point-to-point capacity at snrK ,

2

tion problem,

Mutual Information / MMSE

I

K=3
snr0 = 0.8, snr1 = 1.7, snr2 = 2.2, snr3 = 3
β0 = 0.6, β1 = 0.4, β2 = 0.2

1

max

opt

I(snrK )

s.t. MMSEc (snri ) ≤

0.8

Isuperposition

βi
,
1 + βi snri

∀i ∈ {0, 1, . . . , K − 1}

for some βi ∈ [0, 1], i ∈ {0, 1, . . . , K − 1}, such that

0.6

K−1

βi ≤ 1
0.4

MMSEopt

βK−1 < βK−2 < · · · < β1 < β0

MMSEsuperposition

0.2

and

i=0

is the following
0

0

0.5

1

1.5

2

2.5

3

3.5


1
1 + snr0
I(snrK ) = log 
2
1 + β0 snr0

γ
Student Version of MATLAB

Fig. 1. The mutual information and MMSEc (γ) of a 3-layers superposition
code with (snr0 , snr1 , snr2 , snr3 ) = (0.8, 1.7, 2.2, 3) and (β0 , β1 , β2 ) =
(0.6, 0.4, 0.3) and the mutual information and MMSEc (γ) of an optimal
code for rate snr3 .

K−1

j=1


1 + βj−1 snrj 
+
1 + βj snrj

1
log (1 + βK−1 snrK ) (14)
2
and is attainable when using the optimal K-layers Gaussian
superposition codebook designed for (snr0 , snr1 , · · · , snrK )
with rate-splitting coefﬁcients (β0 , · · · , βK−1 ).
Additional constraints of the following form:

is that of Gaussian superposition codes which are optimal
for a degraded Gaussian BC [16]. As will be shown in the
sequel these codes are optimal MMSE-wise. The analysis of
this family was done by Merhav et. al. in [17, section 5.3] from
a statistical physics perspective. As noted in [17], the MMSE
of this family of codebooks undergoes phase transitions, that
is, it is a discontinuous function of γ. The mutual information,
I(γ), and MMSEc (γ) of this family of codebooks is known
exactly and given in the next theorem. An example of a 3layers superposition code is depicted in Figure 1.

MMSEc (snr ) ≤

β
1 + β snr

(15)

for snri−1 ≤ snr ≤ snri when β ≥ βi−1 , do not affect the
above result.
Proof: It is simple to verify that the optimal Gaussian Klayers superposition codebook (Theorem 3) complies with the
above MMSE constraints and attains the maximum rate. Thus,
we need to derive a tight upper bound on the rate. Deriving
the upper bound begins with the usage of Theorem 1. Due to
the constraint at snr0 :

Theorem 3 (extension of [17] section V.C). A superposition
codebook designed for (snr0 , snr1 , · · · , snrK ) with the rateβ0
splitting coefﬁcients β0 > · · · > βK−1 has the following I(γ):
MMSEc (snr0 ) ≤
(16)
 1
1 + β0 snr0
log (1 + γ) , if 0 ≤ γ < snr0
 2

 1

we have the following upper bound
1+βj−1 snr
i
1+snr0

 2 log 1+β0 snr0 j=1 1+βj snrj j + 1 log (1 + βi γ) ,
2

1 + snr0
1
1
if snri ≤ γ ≤ snri+1
I(snr1 ) ≤ log (1 + β0 snr1 ) + log
. (17)
 1

K−1 1+βj−1 snrj
2
2
1 + β0 snr0
 log 1+snr0
+ 1 log (1 + βK−1 snrK ) ,

j=1
1+β0 snr0
1+βj snrj
2
 2


The other constraints, for i ∈ {1, 2, . . . , K −1}, can be written
if snrK < γ
as follows,
and the following MMSEc (γ):
βi
 1
= mmseGi (snri )
(18)
MMSEc (snri ) ≤
,
0 ≤ γ < snr0
 1+γ
1 + βi snri
βi
MMSEc (γ) =
(13) where mmse (snr ) denotes the MMSE of the estimation of a
, snri ≤ γ ≤ snri+1 .
Gi
i
 1+βi γ
0,
snrK < γ
Gaussian random variable, XGi , with zero mean and variance
√
We refer to this codebook as the optimal Gaussian K-layers βi , from Y = snri XGi + N , where N ∼ N (0, 1). Thus,
superposition codebook.
q (X, β , snr ) = mmse (snr ) − MMSEc (snr ) ≥ 0. (19)
I

i

i

Gi

i

i

According to Theorem 2 the function qI (X, βi , γ) has no
nonnegative-to-negative zero crossings, thus we may conclude
that,

III. M AIN R ESULTS
The main result of this paper:

qI (X, βi , γ) ≥ 0

Theorem 4. Assume snr0 < snr1 < · · · < snrK (K ≥ 1 is
some natural number). The solution of the following optimiza-

∀γ ≥ snri

MMSEc (γ) ≤ mmseGi (γ),

3

∀γ ≥ snri

(20)

I(snrK ) ≤

1
1
log (1 + β0 snr1 ) + log
2
2

1 + snr0
1 + β0 snr0

K−1

+
i=1

1
log
2

1 + βi snri+1
1 + βi snri

1
= log
2
1
= log
2

1 + snr0
1
1 + β1 snr2 1 + β2 snr3 1 + β3 snr4
1 + βK−2 snrK−1 1 + βK−1 snrK
+ log (1 + β0 snr1 )
···
1 + β0 snr0
2
1 + β1 snr1 1 + β2 snr2 1 + β3 snr3
1 + βK−2 snrK−2 1 + βK−1 snrK−1
1 + snr0
1
1 + β0 snr1 1 + β1 snr2 1 + β2 snr3
1 + βK−2 snrK−1
1
+ log
···
+ log (1 + βK−1 snrK )
1 + β0 snr0
2
1 + β1 snr1 1 + β2 snr2 1 + β3 snr3
1 + βK−1 snrK−1
2


K−1
1
1 + snr0
1 + βj−1 snrj  1
= log 
+ log (1 + βK−1 snrK )
(23)
2
1 + β0 snr0 j=1 1 + βj snrj
2

This allows us to provide a tight upper bound on the following
difference:
1 snri+1
MMSEc (γ)dγ
I(snri+1 ) − I(snri ) =
2 snri
1 snri+1
mmseGi (γ)dγ
≤
2 snri
1
1 + βi snri+1
= log
.
(21)
2
1 + βi snri

conclude that
MMSEc (γ) ≤ mmseGi (γ) =

(22)

i=1

1 snri+1
mmseGi (γ)dγ
2 snri
1 + βi snri+1
1
= log
.
2
1 + βi snri

I(snri+1 ) − I(snri ) ≤

Using (17) and (21) we can bound (22) as shown in (23) at
the top of this page.
Now, according to (20) we have that any additional conβ
straint, MMSEc (snr ) ≤ 1+β snr for snri−1 ≤ snr ≤ snri
when β ≥ βi−1 , is already complied with, since
βi−1
β
MMSE (snr ) ≤
≤
1 + βi−1 snr
1 + β snr

(26)

From these upper bounds we can obtain the following

(24)

I(snrK ) − I(snr0 ) =

and thus, does not affect the result. This concludes our proof.

1
2

snrK

=

c

(25)

In the proof of Theorem 4, equation (21), we have seen
that the above property can be used to construct the following
upper bounds

K−1

[I(snri+1 ) − I(snri )]

∀γ ≥ snri

for i ∈ {0, 1, 2, . . . , K − 1}, where mmseGi (snri ) denotes the
MMSE of the estimation of a Gaussian random variable, XGi ,
√
with zero mean and variance βi , from Y = snri XGi + N ,
where N ∼ N (0, 1).

Now, we can write the objective function as follows:
I(snrK ) = I(snr1 ) +

βi
,
1 + βi γ

MMSEc (γ)dγ

snr0
K−1

i=0
K−1

Theorem 4 states that K-layers superposition codes attain
the maximum possible rate at snrK under a set of K MMSE
constraints at lower snrs. However, there might be a different
codebook with this property, which also has some other
desirable properties. In the next theorem we prove that the
MMSE and mutual information as a function of the snr, for
any code attaining the maximum rate under the set of MMSE
constraints, are completely deﬁned for all snr, and are those
of K-layers superposition codes. Thus, no other code can
outperform superposition codes in this sense.

≤
i=0
K−1

=
i=0

=

1
log
2

1
2

1
2

snri+1

MMSEc (γ)dγ
snri
snri+1

mmseGi (γ)dγ
snri

1
log
2
K−1

i=0

1 + βi snri+1
1 + βi snri
1 + βi snri+1
1 + βi snri

.

(27)

Theorem 5. The MMSEc (γ) (and thus also I(γ)) of any
code attaining the maximum rate at snrK , under the MMSE
constraints, deﬁned in Theorem 4, is completely deﬁned for all
0 ≤ γ, and is that of the K-layers superposition codebook.

On the other hand, we can lower bound the above difference:
1
I(snrK ) − I(snr0 ) ≥ Rc − log (1 + snr0 )
2
K−1
1
1 + βi snri+1
= log
(28)
2
1 + βi snri
i=0

Proof: Due to the set of K constraint and following the
steps that lead to (20) in the proof of Theorem 4 we can

where we used both the assumption that the code attains the
maximum rate at snrK , under the MMSE constraints (Theorem
4), and the maximum entropy theory to obtain the maximum

4

mutual information at snr0 . From (27) and (28) we have
I(snrK ) − I(snr0 ) =

1
log
2

K−1

i=0

1 + βi snri+1
1 + βi snri

be derived directly from the I-MMSE formulation, does not
indicate a superposition coding scheme, but rather a Gaussian
code with reduced power, and does not attain the minimum
MMSE at the lower snr. On the other hand, Bandemer and El
Gamal extended their result to the two-user MIMO Gaussian
case where the result does suggest rate-splitting. Finally, extending to any number of unintended receivers, in the Gaussian
regime, using the mutual information disturbance measure, is
trivial, as only the most constraining one determines the result.

(29)

for any code attaining the maximum rate at snrK under the
MMSE constraints, given in Theorem 4. Looking at the upper
bound (27), this equality can be attained only if
1
2

snri+1

MMSEc (γ)dγ =
snri

1
2

snri+1

mmseGi (γ)dγ, (30)
snri

R EFERENCES

for all i ∈ {0, 1, . . . , K − 1}. Due to (25) this is equivalent
βi
to MMSEc (γ) = mmseGi (γ) = 1+βi γ for all snri ≤
γ < snri+1 . Thus, we deﬁned the function MMSEc (γ) for
all γ ∈ [snr0 , snrK ]. Surely since this is a reliable code
designed for snrK , we also have that MMSEc (γ) = 0 for
all γ ≥ snrK . The only region that remains to be determined
is γ ∈ [0, snr0 ]. Since the lower bound, (28), is attained with
equality we have I(snr0 ) = 1 log (1 + snr0 ) which guarantees
2
1
that MMSEc (γ) = 1+γ for all γ ∈ [0, snr0 ].

[1] M. Peleg, A. Sanderovich, and S. Shamai (Shitz), “On extrinsic information of good codes operating over Gaussian channels,” European
Transactions on Telecommunications, vol. 18, no. 2, pp. 133–139, 2007.
[2] S. Shamai (Shitz) and S. Verd´, “The empirical distiribution of good
u
codes,” IEEE Transactions on Information Theory, vol. 43, no. 3, pp.
836–846, May 1997.
[3] A. Bennatan, S. Shamai (Shitz), and A. R. Calderbank, “In prais
of bad codes for multi-terminal communications,” submitted to IEEE
Transactions on Information Theory, August 2010, available at:
arXiv:1008.1766.
[4] A. Bennatan, A. R. Calderbank, and S. Shamai (Shitz), “Bounds on
the mmse of “bad” LPDC codes at rates above capacity,” 46th Annual
Allerton Conf. Communication, Control and Computing, Monticello,
Illinois, September, 23-26 2008.
[5] R. Bustin and S. Shamai (Shitz), “Properties of MMSE of “bad” codes,”
49th Annual Allerton Conf. Communication, Control and Computing,
Monticello, Illinois, September 28-30 2011.
[6] F. Baccelli, A. El Gamal, and D. Tse, “Interference networks with pointto-point codes,” in Proc. IEEE International Symposium on Information
Theory (ISIT 2011), pp. 439–443, Saint Petersburg, Russia, July 31 August 5 2011, available at: arXiv:1102.2868v1.
[7] T. S. Han and K. Kobayashi, “A new achievable rate region for
the interference channel,” IEEE Transactions on Information Theory,
vol. 27, no. 1, pp. 49–60, January 1981.
[8] R. Etkin, D. Tse, and H. Wang, “Gaussian interference capacity to within
one bit,” IEEE Transactions on Information Theory, vol. 54, no. 12, pp.
5534–5562, December 2008.
[9] D. Tuninetti, “K-user interference channels: General outer bounds and
sum-capacity for certain Gaussian channels,” in Proc. IEEE International Symposium on Information Theory (ISIT 2011), pp. 1168–1172,
Saint Petersburg, Russia, July 31 - August 5 2011.
[10] B. Bandemer and A. El Gamal, “Communication with disurbance
constraints,” in Proc. IEEE International Symposium on Information
Theory (ISIT 2011), pp. 2089–2093, Saint Petersburg, Russia, July 31 August 5 2011, available at: arXiv:1103.0996v2.
[11] R. Bustin and S. Shamai (Shitz), “On Gaussian channels with MMSE
interference,” in proc. Internation Zurich Seminar on Communications,
February 29 - March 2 2012.
[12] D. Guo, S. Shamai (Shitz), and S. Verd´, “Mutual information and
u
minimum mean-square error in Gaussian channels,” IEEE Transactions
on Information Theory, vol. 51, no. 4, pp. 1261–1282, April 2005.
[13] D. P. Palomar and S. Verd´, “Gradient of mutual information in linear
u
vector Gaussian channels,” IEEE Transactions on Information Theory,
vol. 52, no. 1, pp. 141–154, January 2006.
[14] R. Bustin, M. Payar´ , D. P. Palomar, and S. Shamai (Shitz), “On
o
MMSE properties and I-MMSE implications in parallel MIMO Gaussian
channels,” in preparation.
[15] D. Guo, Y. Wu, S. Shamai (Shitz), and S. Verd´, “Estimation in Gaussian
u
noise: Properties of the minimum mean-square error,” IEEE Transactions
on Information Theory, vol. 57, no. 4, pp. 2371–2385, April 2011.
[16] T. M. Cover and J. A. Thomas, Elements in Information Theory. WileyInterscience, New York, 1991.
[17] N. Merhav, D. Guo, and S. Shamai (Shitz), “Statistical physics of signal
estimation in Gaussian noise: Theory and examples of phase transitions,”
IEEE Transactions on Information Theory, vol. 56, no. 3, pp. 1400–
1416, March 2010.
[18] C. Gong, A. Tajer, and X. Wang, “Interference channel with constrained
partial group decoding,” IEEE Transactions on Information Theory,
vol. 59, no. 11, pp. 3059–3071, 2011.

IV. D ISCUSSION AND C ONCLUSIONS
The model presented here is not an interference channel,
but rather a model with a single transmitter and K unintended
neighboring receivers. We have shown that the K-layers
superposition code is optimal MMSE-wise for this setting. The
relation of this model to the two-user interference channel
is clear: each receiver is interfered by a single transmitter
(K = 1 in our setting). Although the two-user interference
channel capacity is still, in general, an open problem, intuition
suggests that evaluating the affect of the interference using its
MMSE is a reasonable approach. In this sense, our results for
K = 1 support the good performance of the HK scheme [8],
as explained in [5]. On the other hand, in the K + 1-user
interference channel, each receiver is interfered by K transmitters. The total effect of the interference on each receiver,
is a function of all of these transmissions. Thus, the extension
presented here is only one of the K building blocks of the total
interference in the K +1-user interference channel. This settles
with the fact that coding schemes that deal directly with the
interference rather than with each interferer separately, such
as interference alignment and structure codes, achieve larger
number of degrees of freedom than simple HK schemes for
the Gaussian noise channel (see [9] and reference therein). The
extension shown here does provide further insight to the design
and performance of the novel decoding approach presented
in [18] for the fully connected K-user Gaussian interference
channel.
As stated in the introduction, another possible quantity
to measure the effect of the interference (or “disturbance”)
is the mutual information at the unintended receiver. This
is the approach investigated, for the general discrete memoryless channel, in [10]. In [11] we have shown that this
approach is conceptually different than the one suggested here.
More speciﬁcally, [10, Corollary 2], which derives the ratedisturbance region for the two-user Gaussian case, and can also

5

