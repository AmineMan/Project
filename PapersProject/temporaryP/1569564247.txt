Title:          MAC_Conference_ISIT2012_Final.pdf
Author:         Ziv Goldfeld
Creator:        Adobe Acrobat 9.0.0
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Sun May 20 15:47:55 2012
ModDate:        Tue Jun 19 12:55:20 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      595 x 842 pts (A4)
File size:      343997 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569564247

Capacity Region of the Finite State MAC with
Cooperative Encoders and Delayed CSI
Ziv Goldfeld

Haim H. Permuter

Benjamin M. Zaidel

Ben Gurion University of the Negev
jeniag@bgu.ac.il

Ben Gurion University of the Negev
haimp@bgu.ac.il

benjamin.zaidel@gmail.com

Abstract—In this paper, a single-letter characterization for the
capacity region of ﬁnite-state multiple access channels (MACs)
with partially cooperative encoders is derived. Partial cooperation here is in the sense that the encoders communicate with
each other through ﬁnite-capacity links. The channel states are
assumed to be governed by a Markov processes. Full channel
state information (CSI) is assumed at the receiver, while only
delayed CSI is available at transmitters. The capacity region
is derived by ﬁrst solving the case of ﬁnite-state multiple access
channels with common message, using rate splitting, multiplexing
and simultaneous decoding in order to establish the achievability.
The common message result is then used to derive the capacity
region of the partially cooperative encoders case. Finally, we
apply this result in order to obtain the capacity region for a
ﬁnite-state Gaussian MAC with partially cooperative encoders.
Index Terms—Capacity region, Common message, Cooperative
encoders, Delayed CSI, Finite-state channel, Fourier-Motzking
elimination, Gaussian multiple-access channel, Multiple-access
channel, Simultaneous decoding.
I. INTRODUCTION
In this paper we consider the communication scenario of a
time-varying multiple access channel (MAC) with partially cooperative (or conferencing) encoders. In this setting, each encoder wishes to send an independent private message through
a time-varying channel to the decoder. Delayed channel state
information (CSI) with asymmetrical delays is available at the
encoders, while full CSI is available at the decoder. Prior to
each transmission block the two encoders are allowed to hold
a conference, i.e., they can communicate with each other over
noise-free communication links of given capacities. We restrict
ourselves to the case in which the conference held between the
encoders is independent of the CSI.
The non-state-dependent MAC setting with partially cooperative encoders was ﬁrst introduced by Willems [1], who also
derived the capacity region for the discrete memoryless setting.
Special cases of encoder cooperation include the case in which
the encoders are ignorant of each other’s messages (i.e., the
capacities of the communication links between them are both
zero) and the case where the encoders are fully cooperative
(i.e., the capacities of the communication links are inﬁnite).
The ignorant encoders setting, where no conference is held,
is the basic MAC setting that was studied and solved by
Ahlswede [2]. Whereas, for the fully cooperative encoders
setting the conference is simply modeled as a common message. Slepian and Wolf solved this problem [3], as they

1

presented and derived the capacity region for the discrete
MAC with a common message. This result was later used by
Willems [1] as part of the achievability proof for the partially
cooperative encoders setting, in a manner which is similar
to the one that is presented in this paper. Willems showed
that, in order to achieve the capacity region, the encoders
should use the cooperation link to share parts of their private
messages and then use a coding scheme for the ordinary MAC
with a common message. Although Willems’s model allows
interactive communication between the encoders, both in [1]
and later in [4] he showed that the optimality is achieved even
in a single round of communication between the encoders.
However, in all the above settings the channel was not timevarying.
Wireless communication is a good example of channels
where the channel characteristics are time-varying. In a wireless setting, the user’s motion and the changes in the environment, as well as the interference, may lead to temporal changes
in the channel quality. In such communication problems,
the CSI is often transmitted back to the transmitters from
the receiver through output CSI feedback. Frequently, the
CSI feedback is not instantaneous; the transmitters only have
delayed information regarding the state of the channel. As
it seems, the future of wireless communication is heading
towards user cooperation for enhanced performance. This
motivates us to investigate the case of a time-varying channel
with partial encoders cooperation.
We model the time-varying channel as a ﬁnite-state Markov
channel (FSMC) [5]. The channel state is determined on a per
symbol basis, and governed by the underlying FSM process.
The conference between the encoders takes place prior to
transmission and is independent of the channel state.
The FSMC-MAC with delayed CSI and ignorant encoders,
i.e., where no conference between the encoders is held, has
been presented and solved by Basher et. al. [6]. The achievability scheme in [6] used successive decoding in order to
prove that the two corner points of the capacity region are
achievable and then, using time sharing, the whole region
was achieved. In our case, where conferencing takes place,
a different approach was needed. Since a straightforward
achievability proof for the conferencing model is a difﬁcult
task, we started with solving the case of perfect cooperation
(i.e. the common message case) with the same CSI properties

2

and settings, which was an open problem on it own until
this work. Later, using the achievable scheme of the common
message case, the achievability of the conferencing region was
established. In the achievability proof for the common message
case we avoided working on the corner points (in view of their
large number) and preferred to build a scheme that achieves
every possible point in the region. In order to do so, we use rate
splitting and multiplexing coding, which were also used in [6].
Thus, the structure of the encoders in both schemes is similar.
The decoding, on the other hand, is utterly different because
we present a scheme in which the decoder is able to decode all
the messages simultaneously. For this reason, error probability
analysis yields a very large number of inequalities for the
partial rates. Using the Fourier-Motzkin elimination we are
able to eliminate the partial rates and express the constraints
via a small number of inequalities, which specify the capacity
region. Such a scheme, of simultaneous decoding, is of great
importance and is one of the most signiﬁcant contributions of
our work. This is due to the fact that it holds the basis for
constructing coding schemes for multiple number of users,
which can be done by a direct and trivial extension of the
scheme we present here.
In this paper we also derive the capacity region for the
Gaussian FSMC-MAC with partially cooperative encoders and
delayed CSI. In order to do so, we use a novel tool ﬁrst
derived by Venkatesan [7] to prove that Gaussian distributions
maximize certain mutual information expressions under a
Markovity-constraint. The solution of non time-varying Gaussian MAC with conferencing encoders, which was presented
by Lapidoth, Bross and Wigger [8], also used the same method
as in [7] in order to provide an upper bound for the region.
The reason such a tool is needed is that for such maximization
problems, the traditional approach of proving the optimality
of Gaussian distributions by employing the Max-Entropy Theorem ( [9], Theorem 12.1.1.), or a conditional version of it,
fails. This is since replacing a non-Gaussian vector satisfying
the Markovity condition by a Gaussian vector of the same
covariance matrix may result in a Gaussian vector that violates
the Markovity condition. Another important result that was
presented in [8] is a sufﬁcient and necessary condition for a
Gaussian triplet of RVs to hold a Markov relation. A similar
condition is used in one of the steps of our proof of the
Gaussian model.
After solving the Gaussian case, a speciﬁc example in
which an AGN channel with two possible states (’Good’ and
’Bad’) is considered. The main difﬁculty in this example is the
fact that the optimization problems involved are non-convex.
However, by a unique deﬁnition of some new variables, we
have managed to convert them into equivalent convex problem,
which we later solved using numerical tools.
The remainder of the paper is organized as follows: in
Section II we describe both communication models of interest
- the FSM-MAC with common message and delayed CSI
and the FSM-MAC with partially cooperative encoders and
delayed CSI. In Section III, we state our main results, which

2

are the capacity regions for (i) the common message case, (ii)
the partially cooperative encoders case and (iii) the Gaussian
model of the latter. The results are followed by a brief
description of the proofs.
II. CHANNEL MODELS AND NOTATIONS
In this paper we investigate the capacity region of a ﬁnite state Markov multiple-access channel (FSM-MAC) with
CSI at the decoder (receiver), delayed CSI at the encoders
(transmitters) and partially cooperative encoders as illustrated
in Fig. 2. In order to do so, we ﬁrst consider a different
setting, which is the FSM-MAC with a common message
(fully cooperative encoders) and the same CSI properties, see
Fig. 1. The derivation of the capacity region for the common
message setting forms the basis for the achievability proof for
the FSM-MAC with cooperative encoders, as described above.
Since most deﬁnitions for both channels follow similar lines,
we start with the common message setting and then extend
the description for the partially cooperative encoders setting.

A. FSM-MAC with Common Message and Delayed CSI
Si−d1
(M0 , M1 )

Encoder1

n
X1

Channel

p(y|x1 , x2 , s)
(M0 , M2 )

Encoder2

Yn

Decoder

ˆ ˆ ˆ
(M0 , M1 , M2 )

n
X2

Si

Si−d2

Fig. 1. FSM-MAC with a common message, CSI at the decoder and delayed
CSI at the encoders with delays d1 and d2 .

We consider the communication system of FSM-MAC with
a common message, CSI at the decoder and delayed CSI at
the encoders with delays d1 and d2 , as illustrated in Fig. 1.
A ﬁnite-state Markov channel is, at each time instant, in
one of a ﬁnite number of states S = {s1 , s2 , ..., sk }. In each
state, the channel is a discrete memoryless channel (DMC).
with input alphabets X1 , X2 and output alphabet Y. Let the
random variables Si and Si−d denote the channel state at times
i and i − d, respectively. Similarly, denote by X1,i , X2,i , and
Yi the inputs and the output of the channel at time i. The
channel transition probability function at time i depends on
the state Si , and the inputs X1,i , X2,i at time i, and is given
by P (yi |x1,i , x2,i , si ). The channel output at any time i is
assumed to depend only on the channel inputs and state at
time i. Hence:
P (yi |xi , xi , si ) = P (yi |x1,i , x2,i , si ).
1
2

(1)

The state process, {Si }, is assumed to be an irreducible,
aperiodic, ﬁnite-state homogeneous Markov chain and hence

3

is ergodic. The state process is independent of the channel
inputs and output when conditioned on the previous states,
i.e.,
P (si |si−1 , xi−1 , xi−1 , y i−1 ) = P (si |si−1 ).
1
2

(2)

Furthermore, we assume that the state process is independent
of the messages M0 , M1 and M2 , hence:
n

P (sn , m0 , m1 , m2 ) =

P (si |si−1 )P (m0 )P (m1 )P (m2 ).

(3)

i=1

Now, let K be the one step state transition probability matrix
of the Markov process and let π be the steady state probability
distribution of the Markov process. The joint distribution of
(Si , Si−d ) is stationary and is given by:
πd (Si = sl , Si−d = sj ) = π(sj )K d (sl , sj ),

(4)

where K d (sl , sj ) is the (l, j)-th element of the d-step transition probability matrix K d of the Markov state process. Without loss of generality, we assume that d1 ≥ d2 . Furthermore,
for simplicity of notations, we deﬁne the joint distribution
˜ ˜
of the variables (S, S1 , S2 ) as the joint distribution of the
variables (Si , Si−d1 , Si−d2 ), i.e.,
PS S1 S2 (sl , sj , sv ) = π(sj )K

d1 −d2

d2

(sv , sj )K (sl , sv ), (5)

where sj , sl , sv ∈ S.

Code Description: An (n, 2nR0 , 2nR1 , 2nR2 , d1 , d2 ) code

for the FSM-MAC with CSI at the decoder and delayed CSI
at the encoders with delays d1 and d2 consists of:
1) Two encoding functions fj , j ∈ {1, 2}. Each fj works
through a sequence of functions fj,i that depend only on
the pair of messages (M0 , Mj ) and the channel states
up to time i − dj . For encoder j:
fj,i (M0 , Mj ),
fj,i (M0 , Mj , S i−dj ),

Xj,i =

1 ≤ i ≤ dj
dj + 1 ≤ i ≤ n

, (6)

2) A decoding function:
ψ : Y n × S n → M0 × M1 × M2 .

(7)

B. FSM-MAC with Partially Cooperative Encoders and Delayed CSI
Si−d1
M1

Encoder1
V1l

V2l

C12
M2

n
X1

Channel
Yn

ˆ ˆ
(M1 , M2 )

C21

Encoder2

p(y|x1 , x2 , s)

Decoder

n
X2

Si

Si−d2

Fig. 2. FSM-MAC partially cooperative encoders, CSI at the decoder and
delayed CSI at the encoders with delays d1 and d2 .

3

We now deﬁne the communication system of FSM-MAC
with CSI at the decoder, delayed CSI at the encoders with
delays d1 and d2 , and partially cooperative encoders with
communication link capacities between them C12 and C21 ,
as illustrated in Fig. 2. The channel deﬁnition is obtained
from the deﬁnition described in Subsection II-A, by taking
the common message set to be M0 = ∅ and deﬁning the
conference between the encoders. The conference between
both encoders takes place prior to the transmission of a
codeword through the channel and consists of consecutive
pairs of communications, simultaneously transmitted by the
encoders. The communication transmitted by each encoder
depends on the message to be transmitted by that encoder
and the previously received communications from the other
encoder. We denote the communications transmitted from
encoder j ∈ {1, 2} to the other encoder by Vj .
Note that here the state process is also independent of the
conference communications,
n

P (sn , v1 , v2 ) = P (sn )P (v1 , v2 ) =

P (si |si−1 )P (v1 , v2 ).
i=1

(8)
We now describe the conference in greater detail as part of
the code description.
Code Description: An (n, , 2nR1 , 2nR2 , d1 , d2 ) code for
FSM-MAC with CSI at the decoder, delayed CSI at the
encoders with delays d1 and d2 and encoders connected by
communication links with capacities C12 and C21 consists of:
1) Two encoders, where each encoder is completely described by an encoding function fj , and a set of
( ≥ 1) communication functions {hj,1 , hj,2 , . . . , hj, },
j ∈ {1, 2} (we use similar deﬁnitions to those in [1]).
The encoding function maps the message Mj , j ∈
{1, 2}, and what was learned from the conference with
the other encoder into channel input codewords of block
length n. Each fj works through a sequence of functions
fj,i that depend only on the message Mj , the received
communications from the other encoder and the channel
states up to time i − dj . We note that since the encoding
occurs only after the conferencing has ﬁnished, each fj,i
depends on all received communications.
The conference between both encoders consists of
consecutive pairs of communication functions. Therefore, a conference is completely described by two
sets of l communication functions {h1,1 , h1,2 , . . . , h1, }
and {h2,1 , h2,2 , . . . , h2, }. Each communication function
h1,i (or h2,i ), i ∈ {1, 2, . . . , }, maps the message
M1 (or M2 ) and the sequence of previously received
communications from the other encoder V2i−1 (or V1i−1 )
onto the ith communication V1,i (or V2,i ). Namely, the
communications are deﬁned as:
V1,i = h1,i (M1 , V2i−1 ) ; V2,i = h2,i (M1 , V1i−1 ).

(9)

4

The encoding function for encoder 1 (j = 1):
X1,i =

f1,i (M1 , V2 ),
1 ≤ i ≤ d1
f1,i (M1 , V2 , S i−d1 ), d1 + 1 ≤ i ≤ n

III. MAIN RESULTS
,

and similarly for encoder 2 (using the private message
M2 , the communications V1 and the delay d2 ).
The random variable (RV) Vj,i , for j ∈ {1, 2} and
i ∈ {1, 2, . . . , }, ranges over the ﬁnite alphabet Vj,i . In
the case of partially cooperating encoders the amount of
information exchanged during the conference is bounded
due to the ﬁniteness of the capacities C12 and C21
of the communication links between both encoders.
A conference is (C12 , C21 )-permissible if the sets of
communication functions are such that:
log |V1,i | ≤ nC12 ;
i=1

log |V2,i | ≤ nC21 .

(10)

In this section we present the main results of this paper.
Assuming without loss of generality that d1 ≥ d2 , we ﬁrst
present the capacity region for the FSM-MAC with common
message and delayed CSI.
Theorem 1: The capacity region of FSM-MAC with a common message, CSI at the decoder and asymmetrically delayed
CSI at the encoders with delays d1 and d2 , as shown in Fig.
1, is the closure of the set of rates that satisfy:
˜ ˜
R1 < I(X1 ; Y |X2 , U, S, S1 , S2 ),
˜ ˜
R2 < I(X2 ; Y |X1 , U, S, S1 , S2 ),
˜ ˜
R1 + R2 < I(X1 , X2 ; Y |U, S, S1 , S2 ),
˜ ˜
R0 + R1 + R2 < I(X1 , X2 ; Y |S, S1 , S2 ),

(13)

for some joint distribution of the form:

i=1

P (u|˜1 )P (x1 |˜1 , u)P (x2 |˜1 , s2 , u),
s
s
s ˜

2) A decoding function:
ψ : Y n × S n → M1 × M2 .

(11)

C. The Gaussian FSM-MAC with Partially Cooperative Encoders and Delayed CSI
X1,i

NSi
Yi

X2,i

Fig. 3. FSM AGN MAC with partially cooperative encoders, CSI at the
decoder and delayed CSI at the encoders with delays d1 and d2 .

In this subsection brieﬂy describe the Gaussian FSM-MAC
with partially cooperative encoders and delayed CSI. For a
ﬁnite state additive Gaussian noise (AGN) MAC with partially
cooperative encoders, the channel output, Yi , at time i, for
channel inputs X1,i , X2,i , is given by
Yi = X1,i + X2,i + NSi ,

(12)

where NSi is a zero-mean Gaussian random variable with
variance depending on the state of the channel at time i, Si ,
2
and denoted by σN (s). In addition to the channel output,
Yi , the receiver has access to the state Si . The receiver
feeds back the CSI to the transmitters through a noiseless
feedback channel. The CSI from the receiver is received at
transmitter 1 and at transmitter 2 after time delays of d1 and d2
symbol durations, respectively. The state process is assumed
to be Markov with steady state distribution π(s) and one step
transition matrix K. It is clear that the ﬁnite state AGN is an
FSMC. Moreover, the two transmitters can communicate with
each other through perfect communication lines with limited
capacities C12 and C21 as has been described in the general
channel deﬁnitions in Subsection II-B.

4

(14)

where U is an auxiliary random variable with bounded cardinality.
Due to lack of space we only give here an outline of
the achievability and converse proofs. The reader is referred
to [10] for the full details. In the achievability proof of
the theorem we use rate splitting and multiplexing coding.
The multiplexing is done using the delayed states, S1 and
(S1 , S2 ), which are known at encoders 1 and 2, respectively.
The decoding of the message triplet (M0 , M1 , M2 ) is done
simultaneously. The simultaneous decoding yields a very large
number of inequalities for the partial rates, which follow from
the error probability analysis. The number of inequalities is
then reduced using the Fourire-Motzkin elimination, ﬁnally
yielding the capacity region in (15).
The converse uses similar ideas to those presented by
Slepian and Wolf [3] regarding the common message, and
Basher et. al. [6] regarding the delayed CSI. The auxiliary
RV U is deﬁned as Ui
(M0 , S i−d1 −1 ), thus explicitly
representing the common message and the common knowledge of the state. Finally, we show that indeed the Markov
relations U − S1 − (S2 , S), X1 − (U, S1 ) − (S2 , S) and
X2 − (U, S1 , S2 ) − (X1 , S), hold.
The next theorem presents the capacity region of FSM-MAC
with partially cooperative encoders and delayed CSI, which is
the model of main interest.
Theorem 2: The capacity region of FSM-MAC with partially cooperative encoders, CSI at the decoder and asymmetrically delayed CSI at the encoders with delays d1 and d2 , as
shown in Fig. 2, is the closure of the set of rates that satisfy:
˜ ˜
R1 < I(X1 ; Y |X2 , U, S, S1 , S2 ) + C12 ,
˜ ˜
R2 < I(X2 ; Y |X1 , U, S, S1 , S2 ) + C21 ,
R1 + R2 < min

˜ ˜
I(X1 , X2 ; Y |U, S, S1 , S2 ) + C12 + C21 ,
˜ ˜
I(X1 , X2 ; Y |S, S1 , S2 )

, (15)

for some joint distribution of the form:
s
s ˜
P (u|˜1 )P (x1 |˜1 , u)P (x2 |˜1 , s2 , u)
s

(16)

5

R1 <
R2 <
R1 + R2 <

R1 + R2 <

1
2
1
2
1
2
1
2

K d2 (s, s2 ) log 1 +
˜

K d1 −d2 (˜2 , s1 )
s ˜

π(˜1 )
s

β1 (˜1 )P1 (˜1 ) + β2 (˜1 , s2 )P2 (˜1 , s2 )
s
s
s ˜
s ˜
2
σN (s)

s

s2
˜

s1
˜

˜
K d2 (s, s2 ) log 1 +

K d1 −d2 (˜2 , s1 )
s ˜

π(˜1 )
s

β2 (˜1 , s2 )P2 (˜1 , s2 )
s ˜
s ˜
2
σN (s)

s

s2
˜

s1
˜

˜
K d2 (s, s2 ) log 1 +

K d1 −d2 (˜2 , s1 )
s ˜

π(˜1 )
s

β1 (˜1 )P1 (˜1 )
s
s
2
σN (s)

s

s2
˜

s1
˜

K d2 (s, s2 ) log 1 +
˜

K d1 −d2 (˜2 , s1 )
s ˜

π(˜1 )
s
s1
˜

s2
˜

s

where U is an auxiliary random variable with bounded cardinality.
Again we describe the main ideas of the proof. The achievability is proven by using the result for the common message.
The encoders use the conference in order to share parts of
their private messages and then use the coding scheme that
was used in the common message case.
The converse presents an upper bound on an achievable rate
(V1l , V2l , S i−d1 −1 ).
by deﬁning the auxiliary RV U as Ui
Therefore it represents the communications shared during the
conference (i.e. the parts of the private messages available to
both encoders) and the common knowledge of the state. In
the ﬁnal part of the converse we show that the same Markov
relations as those of the common message case, hold.
Finally, we present the capacity region of the FSM Gaussian
MAC with partially cooperative encoders and delayed CSI, as
described in Subsection II-C.
Theorem 3: For the FSM Gaussian MAC with partially
cooperative encoders model described in Subsection II-C and
shown in Fig. 3, it is enough to consider an auxiliary RV U
that is Gaussian for the region (15), and the capacity region is
given by the union of (17), over all β1 (˜1 ), β2 (˜1 , s2 ), P1 (˜1 )
s
s ˜
s
¯ s
¯ s ˜
and P2 (˜1 , s2 ), where, β1 (˜1 ) = 1 − β1 (˜1 ) and β2 (˜1 , s2 ) =
s ˜
s
s ˜
1 − β2 (˜1 , s2 ), subject to the constraints:
s
π(˜1 )P1 (˜1 ) ≤ P1 ,
s
s1
˜

s1
˜

s2
˜

0 ≤ β1 (˜1 ) ≤ 1, ∀˜1 ∈ S,
s
s
s ˜
s ˜
0 ≤ β2 (˜1 , s2 ) ≤ 1, ∀(˜1 , s2 ) ∈ S 2 .

+ C21 ,
+ C12 + C21 ,

¯ s ¯ s ˜
P1 (˜1 ) + P2 (˜1 , s2 ) + 2 β1 (˜1 )β2 (˜1 , s2 )P1 (˜1 )P2 (˜1 , s2 )
s
s ˜
s
s ˜
2
σN (s)

1
2

.

(17)

V (˜1 ) = E[X1 |U (˜1 ), s1 ]. This is in fact the optimal estimator
s
s ˜
in the MMSE sense of X1 given U for each speciﬁed delayed
˜
CSI S1 = s1 . From the structure of the terms in (15) it is clear
that the substitution of U (˜1 ) with V (˜1 ) increases the region.
s
s
G
G
Our next step is to substitute (X1 , V, X2 ) with (X1 , V G , X2 ),
which are zero-mean jointly Gaussian RV’s with the same
covariance matrix as (X1 , V, X2 ). By doing so the region is
increased even more and due to the special structure of their
G
G
covariance matrix we get that X1 (˜1 )− V G (˜1 )− X2 (˜1 , s2 )
s
s
s ˜
˜ ˜
for any given (S, S1 , S2 ) = (s, s1 , s2 ). This result is obtained
using Lemma 5 in [8], where a similar approach is employed
in order to yield an upper bound. The upper bound obtained
is stated in (17). Finally, we achieve that upper bound by
setting (X1 , U, X2 ) as jointly Gaussian centered RV’s, which
G
G
are denoted by (X1 , U G , X2 ), for which the conditional PDF
has the structure as stated in Theorem 2, thereby characterizing
(17) as the capacity region of the Gaussian model.
ACKNOWLEDGMENT

This work was supported in part by the Marie Curie
Reintegration Fellowship within the European Community
Framework Program and by the Ministry of Defense.
R EFERENCES
[1] F. M. J. Willems. The discrete memoryless multiple channel with
partially cooperating encoders. IEEE Trans. Inf. Theory, 29(6):441445,
1983.
[2] R. Ahlswede. Multi-way communication channels. In Proceedings

of 2nd International Symposium on Information Theory (Thakadsor,
Armenian SSR, Sept. 1971), pages 23–52, Publishing House of the

s ˜
P (˜2 |˜1 )P2 (˜1 , s2 ) ≤ P2 ,
s s

π(˜1 )
s

+ C12 ,

(18)

2
First we note that P1 (˜1 )
s
E[X1 |S1 = s1 ] and
˜
2
s ˜
E[X2 |S1 = s1 , S2 = s2 ], thus P1 (˜1 ) and
˜
˜
s
P2 (˜1 , s2 )
s ˜
P2 (˜1 , s2 ) can be interpreted as the power allocations of each
user given a delayed CSI of (S1 , S2 ) = (˜1 , s2 ). β1 (˜1 )
s ˜
s
s ˜
and β2 (˜1 , s2 ), on the other hand, are associated with the
correlations between the pairs (X1 , U ) and (X2 , U ) for a given
pair (S1 , S2 ) = (˜1 , s2 ). The proof of the capacity region for
s ˜
the Gaussian case starts with upper bounding the region in
(15). This is done by substituting the auxiliary RV U , for any
˜
s
given S1 = s1 (which we denote by U (˜1 )), with a new RV
denoted by V (˜1 ), which is a function of U and deﬁned as
s

5

Hungarian Academy of Science, Budapest, 1973.
[3] D. Slepian and J. K. Wolf. A coding theorem for multiple-access channel
with correlated sources. Bell Syst. Tech. J., 51:1037-1076, 1973.
[4] F. M. J. Willems and E. C. van der Meulen. The discrete memoryless
multiple-access channel with cribbing encoders. IEEE Trans. Inf.
Theory, 31(3):313–327, 1985.
[5] R. G. Gallager. Information Theory and Reliable Communication. John
Wiley & Sons, Inc., New York, NY, USA, 1968.
[6] U. Basher A. Shirazi and H. H. Permuter. Capacity region of ﬁnite
state multiple-access channel with delayed state information at the
transmitters. CoRR, abs/1101.2389, 2011.
[7] V. Venkatesan. Optimality of Gaussian inputs for a multi-access
achievable rate region. PhD thesis, June 2007.
[8] A. Lapidoth S. I. Bross and M. A. Wigger. The Gaussian MAC with
conferencing encoders. Arxiv, abs/0805.0516, 2008.
[9] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley,
2006.
[10] Z. Goldfeld H. H. Permuter and B. M. Zaidel. Capacity region of ﬁnite
state MAC with cooperative encoders and delayed CSI. In preperation.

