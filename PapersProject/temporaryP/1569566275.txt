Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 09:55:00 2012
ModDate:        Tue Jun 19 12:54:10 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      368892 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566275

A Lifting Decoding Scheme and its Application to
Interleaved Linear Codes
Guillaume Quintin
´
Laboratoire d’informatique de l’X (LIX), Ecole polytechnique, 91128 Palaiseau, FRANCE
Email: quintin@lix.polytechnique.fr.

decode more errors than half the minimum distance of C with
a high probability over small alphabets (small ﬁnite ﬁelds).
Our approach is different from [6], which treats a priori
only the case of interleaved RS codes while our algorithm is
able to decode (further than half the minimum distance) any
interleaved linear code as soon as a decoding algorithm for the
underlying code is available. Therefore we can consider codes
over small alphabet like F2 . A lot of families of codes are
subﬁeld-subcodes of alternant codes. Thus a lot of interleaved
codes can be decoded with the approach of [6] but at a
higher cost than our approach which does not need to consider
alternant codes.

Abstract—In this paper we design a decoding algorithm based
on a lifting decoding scheme. This leads to a unique decoding
algorithm with complexity quasi linear in all the parameters
for Reed-Solomon codes over Galois rings and a list decoding
algorithm. We show that, using erasures in our algorithms, allows
one to decode more errors than half the minimum distance with a
high probability. Finally we apply these techniques to interleaved
linear codes over a ﬁnite ﬁeld and obtain a decoding algorithm
that can recover more errors than half the minimum distance.
Index Terms—Algorithm design and analysis, Decoding, Error
correction, Reed-Solomon codes, Interleaved codes.

I. I NTRODUCTION
Reed-Solomon (RS) codes form an important and wellstudied family of codes. They can be efﬁciently decoded. See
for example [10], [15]. They are widely used in practice [19].
Sudan’s 1997 breakthrough on list decoding of RS codes [18],
further improved by Guruswami and Sudan in [14], showed
that RS codes are list decodable up to the Johnson bound in
polynomial time.

B. Related work
Our approach for a lifting decoding scheme has ﬁrst been
studied in [12], then in [5], [7] RS codes over a commutative
ﬁnite ring have been studied by M. Armand in [1]–[4]. The
decoding of interleaved codes has been studied in [6], [8],
[11].

A. Our contributions
Let B be a quotient ring of a discrete valuation ring A with
uniformizing parameter π. We design a decoding scheme that
can be adapted to a wide range of linear codes over B. Let
C be a code over B, then given a black box decoding algorithm BlackBoxDec for C/πC, we can construct a decoding
algorithm for C generalizing [12, algorithm of Section 3]. The
constructed decoding algorithm has the property to correct all
error patterns that can be corrected by BlackBoxDec. We
study in detail the complexities in the case of Reed-Solomon
codes over Galois rings and truncated power series rings over
a ﬁnite ﬁeld.
We improve the construction given in [12, algorithm of
Section 3] and in [5], [7] by integrating an idea used by Marc
Armand in [2], [4]. We use erasures at suitable places within
our decoding algorithm to improve its decoding radius. This
improvement allows one to decode more error patterns than
BlackBoxDec with a high probability. We study and give
complexities when RS codes are involved. In fact, we decode
exactly the same error patterns as in Armand’s papers [2], [4]
but with a lower complexity thanks to the decoding scheme
of [12].
Finally we show that, given any linear code C over Fq , we
can view interleaved codes with respect to C as codes over
Fq [[t]]/(tr ). This allows one to apply the previous techniques
to interleaved codes to obtain a decoding algorithm that can

II. P REREQUISITES
A. Complexity model
The “soft-Oh” notation f (n) ∈ O(g(n)) means that f (n) ∈
g(n) logO(1) (3 + g(n)). It is well known [9] that the time
needed to multiply two integers of bit-size at most n in
binary representation is O(n). The cost of multiplying two
polynomials of degree at most n over a ring A is O(n) in
terms of the number of arithmetic operations in A. Thus the
bit-cost of multiplying two elements of the ﬁnite ﬁeld Fpn is
O(n log p).
B. Error correcting codes
In this section we let A be any commutative ring with
identity and n be a positive integer. Let C be a subset of
An . We call C an error correcting code over A or simply a
code over A. If C is a submodule of An we say that C is a
linear code over A. The integer n is called the blocklength of
C. If C is a linear code and C is free of rank k, then we say
that C has parameters [n, k]A .
Deﬁnition 1. Let u = (u1 , . . . , un ) ∈ An . We call the integer
w(u) := |{i ∈ {1, . . . , n} : ui = 0}|
the Hamming weight (or simply weight) of u. Let v be another
vector of An . The integer w(u − v) is called the Hamming

1

distance (or simply distance) between u and v and is denoted
by d(u, v).

III. I MPROVED π- ADIC LIFTING .
In this section we let A be a discrete valuation ring with
uniformizing parameter π and by κ = A/(π) the residue ﬁeld
of A. We also let C be a free splitting linear code over A of
parameters [n, k, d]A and with generator matrix G. We let C
denote the linear code C/πC and G a generator matrix of C
such that G = G mod π.

The integer d = minu,v∈C and u=v d(u, v) is called the
minimum distance of C. Note that when C is a linear code we
have d = minu∈C\{0} w(u), we then say that C has parameters
[n, k, d]A if C is free of rank k.
Deﬁnition 2. Suppose that C is free of rank k. A matrix whose
rows form a basis of C is called a generator matrix of C.

Algorithm 1 BlackBoxDec
Input: A positive integer τ ≤ n and a received vector y of
κn (with zero or more erasures).
Output: A nonempty set U ⊆ κk × κn satisfying

The generator matrix is used to encode a message m ∈
Ak . A generator matrix induces a one-to-one correspondence
between messages and codewords, the map m → mG is a Alinear embedding Ak → An . Under this map, we will identify
messages and codewords.
Let m be a maximal ideal of A. The vector space C/mC,
if not zero, is a linear code with parameters [n, ≤ k, ≤ d]A/m
with generator matrix G . The matrices G and G have the
same number of columns but can have a different number
of rows. However G can be deduced from G, ﬁrst compute
G = G mod m, then remove from G appropriate rows to
obtain a basis of C/mC.

(m, e) ∈ U ⇒ y = mG + e and w(e) ≤ τ

(1)

or ∅ (meaning FAILURE).
Note that BlackBoxDec can return one or more codewords in particular it can be a list decoding algorithm; but we
do not require that it return all codewords within distance τ
of y.
Algorithm 2 Decoding from valuation i up to valuation r.
Input: A positive integer τ ≤ n, two nonnegative integers i ≤
r, a received vector y of An (with zero or more erasures)
and a black box decoding algorithm BlackBoxDec for
C(π).
Output: A nonempty set U ⊆ κk × κn satisfying

Deﬁnition 3. Borrowing the terminology of [12, Section 3],
if G and G have the same number of rows and columns and
that G mod m = G then C is called a splitting code.
We will consider codes over a special kind of rings which
we deﬁne now.
Deﬁnition 4. Let A be a ring. If A is a local principal ideal
domain, we call A a discrete valuation ring (DVR). Any
element π ∈ A such that (π) is the maximal ideal of A is
called a uniformizing parameter of A.

1:
2:
3:
4:

C. Reed-Solomon codes over rings
Reed-Solomon codes over rings are deﬁned in a slightly
different way to their ﬁeld counterparts. We let A[X]<k denote
the submodule of A[X] consisting of all the polynomials of
degree at most k − 1 of A[X].

5:
6:
7:
8:

Deﬁnition 5. Let x1 , . . . , xn be elements of A such that
xi − xj ∈ A× ﬁr i = j (where A× is the group of
units of A). The submodule of An generated by the vectors
(f (x1 ), . . . , f (xn )) ∈ An where f ∈ A[X]<k is called a
Reed-Solomon code over A. The n-tuple (x1 , . . . , xn ) is called
the support of the RS code.

9:
10:
11:
12:

Proposition 6. Let C be a RS code over A. Then C has
parameters [n, k, d = n − k + 1]A .

13:
14:
15:
16:
17:
18:

Proposition 7. Let C be a RS code with parameters [n, k, d =
n−k +1]A over a discrete valuation ring A with uniformizing
parameter π. Then C/π r C is a RS code with parameters
[n, k, d]A/(πr ) over A/(π r ). Moreover of (x1 , . . . , xn ) is the
support of C then (x1 mod π r , . . . , xn mod π r ) is the support of C/π r C.

19:

2

(m, e) ∈ U ⇒ y = mG + e mod π r−i and w(e) ≤ τ
(2)
or ∅ (meaning FAILURE).
if i = r then
return {(0, 0)}.
end if
Call to BlackBoxDec with input τ and (y mod π) to
obtain the set S.
if BlackBoxDec returns ∅ (FAILURE) then
return ∅ (FAILURE).
end if
U ← ∅.
for each (m0 , e0 ) ∈ S do
y1 ← π −1 (y − m0 G − e0 ).
Put erasures in y1 at the locations indicated by
Supp(e0 ).
Call recursively Algorithm 2 with input τ , i + 1, r, y1
and BlackBoxDec to obtain the set T .
for each (m1 , e1 ) ∈ T do
if | Supp(e0 ) ∪ Supp(e1 )| ≤ τ then
U ← U ∪ {(m0 + πm1 , e0 + πe1 )}.
end if
end for
end for
return U .

Algorithm 4 Decoding algorithm for C/π r C.
Input: A positive integer τ ≤ n, a received vector y of
(A/(pr ))n (with zero or more erasures) and a black box
decoding algorithm BlackBoxDec for C(π).
Output: A nonempty set U ⊆ κk × κn satisfying

Algorithm 3 Decoding up to precision r.
Input: A positive integer τ ≤ n, a positive integer r, a
received vector y of An (with zero or more erasures) and
a black box decoding algorithm BlackBoxDec for C(π).
Output: A nonempty set U ⊆ κk × κn satisfying
(m, e) ∈ U ⇒ y = mG + e mod π r and w(e) ≤ τ
(3)
or ∅ (meaning FAILURE).
1: return the set returned by the call to Algorithm 2 with
input τ , 0, r, y and BlackBoxDec.

(m, e) ∈ U ⇒ y = mG + e and w(e) ≤ τ

(4)

or ∅ (meaning FAILURE).
Lift y ∈ (A/(pr ))n into y ∈ An .
S ← the set returned by the call to Algorithm 2 with input
τ , 0, r, y and BlackBoxDec.
3: return {c mod π r : c ∈ S}.

1:
2:

Proposition 8. Suppose that BlackBoxDec returns all the
codewords from C within distance τ of y ∈ κn . Then
Algorithm 2 can decode up to τ errors up to precision r.

RS codes are free splitting codes over B by Proposition 7
so we can apply Algorithm 4 to RS codes. Complexities
of decoding with Algorithm 4 are given by the following
proposition which is a direct consequence of Proposition 9.

Proof: The proof is done by descending induction on i.
For i = r and i = r − 1 the proposition holds.
Now let i < r − 1 and (m, e) ∈ κk × κn . Let c = mG be
such that w(e mod π r−i ) ≤ τ and y = c + e. There exists
(m0 , e0 ) ∈ S such that c0 = m0 G = c mod π, e = e0
mod π and Supp(e0 ) ⊆ Supp(e). If we count erasures as
errors, we have w(e) ≤ τ and therefore w(π −1 (e0 − e)) ≤ τ .
On the other hand we have mG = m0 G mod π and mG =
m0 G in C whence m = m0 mod π. Therefore π −1 (mG −
m0 G) = (π −1 (m − m0 ))G ∈ C.
We deduce from the above that

Example-proposition 10. Suppose that C is a RS code over
B. If B = GR(pr , s) (the unique Galois extension over Z/pZ
of degree s) then
• if BlackBoxDec is the unique decoding algorithm of
[15] (that can decode up to τ = d−1 errors) then
2
Algorithm 4 can decode up to τ errors in O(rnks log p)
bit operations,
• if BlackBoxDec is the Guruswami-Sudan list decoding algorithm of [13, Corollary 3.3, page 36] (that
can decode up to J = n − (k − 1)n − 1 errors)
then Algorithm 4 can list decode up to J errors in
O [n(|κ| − 1)]r−1 n7 k 5 s log p bit operations.
If B = κ[[t]]/(tr ) then
• if BlackBoxDec is the unique decoding algorithm of
[15] (that can decode up to τ = d−1 errors) then
2
Algorithm 4 can decode up to τ errors in O(rnk)
arithmetic operations over κ.
• if BlackBoxDec is the Guruswami-Sudan list decoding algorithm of [13, Corollary 3.3, page 36] (that
can decode up to J = n − (k − 1)n − 1 errors)
then Algorithm 4 can list decode up to J errors in
O [n(|κ| − 1)]r−1 n7 k 5 arithmetic operations over κ.

y1 = π −1 (y − (c0 + e0 )) = π −1 (c − c0 ) + π −1 (e0 − e).
By the inductive hypothesis, we can ﬁnd (m1 , e1 ) ∈ T such
that π −1 (c−c0 ) = m1 G mod π r−(i+1) and π −1 (e−e0 ) = e1
mod π r−(i+1) .
We now have the straightforward proposition which gives
the complexity of Algorithm 3 in terms of bit operations.
Proposition 9. Suppose that the number of codewords returned by BlackBoxDec is at most L > 1. Denote by
Lift(C) the complexity of lifting a codeword of C into a
codeword of C up to precision r in terms of the number of
bit operations. Denote by Dec(C) the complexity of algorithm
BlackBoxDec in terms of the number of bit operations. Then
Algorithm 3 performs at most

We show that if we choose a decoding algorithm able to
handle errors and erasures for BlackBoxDec then we can
decode, with a non negligible probability, further than half the
minimum distance and further than the Johnson bound.

Lr − 1
(Lift(C) + Dec(C)) = O(Lr−1 ) (Lift(C) + Dec(C))
L−1
bit operations. If L ≤ 1 then Algorithm 3 performs at most
r (Lift(C) + Dec(C)) bit operations.

Deﬁnition 11. Following the terminology of [16, Subsection 2.1, page 404] we say that an element of B has ﬁltration
s if it is written uπ s where u ∈ B × .

The interesting part of Algorithm 2 (and hence of all other
algorithms) resides in the BlackBoxDec argument. We have
shown that if BlackBoxDec is a classical decoding algorithm
then Algorithm 3 becomes a decoding algorithm with the same
decoding radius as BlackBoxDec.
From now we suppose that κ = A/(π) is a ﬁnite ﬁeld.
Every element of B = A/(π r ) can be uniquely written as
uπ s , where u ∈ B × and 0 ≤ s ≤ r − 1.

We let q be the cardinality of κ. s
Then the cardinality of B
A/(π )
is q r while the cardinality of A/(πs+1 ) is q.
Proposition 12. Let C be a splitting code over B
with parameters [n, k]B . Suppose that erasures occurred
and that BlackBoxErasuresDec is provided as the

3

Algorithm 5 BlackBoxErasuresDec
Input: A received vector y of κn with erasures and at most
τ ( ) errors.
Output: All the codewords within distance τ ( ) + of y or
∅ (FAILURE).

Proof: For the ﬁrst item, see for example [10, Section 4,
page 7 and 8] while for the second item see [14, Theorem 16,
page 1762]. The third item is a consequence of the ﬁrst item
and Proposition 12.
IV. A PPLICATION TO INTERLEAVED LINEAR CODES .
In this section we let A be the power series ring over the
ﬁnite ﬁeld Fq namely we let A = Fq [[t]], π = t and B =
Fq [[t]]/(tr ). We recall the construction of interleaved codes
and show that all interleaved codes over Fq are exactly codes
over B. We let C be a linear code over Fq with parameters
[n, k, d]Fq and with generator matrix G .
Let r messages m0 , . . . , mr−1 ∈ Fk and their encodings
q
c0 = m0 G , . . . , cr−1 = mr−1 G . For i = 0, . . . , r − 1 and
j = 1, . . . , n deﬁne cij to be the j-th coordinate of ci and
sj = (c0,j , . . . , cr−1,j ).

BlackBoxDec argument to Algorithm 4. The number of
error vectors of weight w that can be corrected by Algorithm 4
is at least
N ( , B, w) =
r−1

i=0

n

qr

n−
w

×
(v0 ,...,vr−1 )∈Vw

w − v0 − · · · − vi−1
(q − 1)vi q v0 +···+vi−1
vi

(5)

where

c0,1
c1,1
.
.
.

Vw = {(v0 , . . . , vr−1 ) ∈ N : v0 + · · · + vr−1 = w and
0 ≤ v0 ≤ τ ( ) and 0 ≤ vi−1 ≤ τ ( + v0 + · · · + vi−2 )
for i = 2, . . . , r − 1} ,
hence the fraction of corrigible error patterns is at least
P ( , B, w) =

w
i=0
w
i=0

N ( , B, w)
n
(q r − 1)i
i

...

cr−1,n
↓
sn

→ cr−1

Proof: Let G = G be the generator of the linear code
C over B with parameters [n, k, ≤ d]B , then C/tC = C . We
have ci = mi G for i = 0, . . . , r − 1. As a consequence we
have

Proof: This is a consequence of [17, Theorem 1.7,
page 16].
Proposition 14. Let C be a Reed-Solomon code over B with
parameters [n, k, d = n − k + 1]B then there exists

•

cr−1,2
↓
s2

→ c0
→ c1

c0,n
c1,n
.
.
.

Proposition 15. The words transmitted over the channel using
interleaved linear codes are precisely the transmitted words
using linear codes over truncated power series.

Proposition 13. Let C be a splitting code over B with
parameters [n, k, d]B . Then there exists a decoding algorithm
such that τ ( ) = d−2−1 .

•

...
...
.
.
.

The vectors transmitted over the channel are not
c1 , . . . , cr−1 ∈ Fn but s1 , . . . , sn ∈ Fr . We will make an abuse
q
q
of notation and call such an encoding scheme a interleaved
code with respect to C and of degree r. Usually the vector
sj (for j = 1, . . . , n) is seen as an element of Fqr , but we
r−1
i
can associate the element
i=0 ci,j t ∈ B to sj . In this
r n
context, if y = (y1 , . . . , yn ) ∈ (Fq ) , the weight of y is the
nonnegative integer |{i ∈ {1, . . . , n} : yi = 0}| and if y
corresponds to the received word then the weight of the error
is |{i ∈ {1, . . . , n} : yi = si }|.

(6)

Proof: Let e ∈ B n be an error vector. We let vi (e) for
i = 1, . . . , r − 1 denote the number of coordinates of e of
ﬁltration i. The number of error vectors e ∈ B n such that
(v0 (e), . . . , vr−1 (e)) ∈ Vw is given by formula (5). Let c be
a codeword of C and y = c + e with vi = vi (e) for i =
0, . . . , r − 1 and (v0 , . . . , vr−1 ) ∈ Vw . The rest of the proof is
similar to the proof of Proposition 8.

•

c0,2
c1,2
.
.
.

cr−1,1
↓
s1

r

r−1

r−1

mi ti

a unique decoding algorithm which can correct errors
and erasures with τ ( ) = n−2−k ,
a list decoding algorithm which can correct errors and
erasures with τ ( ) = (n − ) − (k − 1)(n − ) − 1
and
a unique decoding algorithm which can correct up to w
errors and erasures with w ≤ n − − k and which
does succeed for a fraction of at least P ( , B, w) error
patterns.

i=0

r−1

(mi G) ti =

G=
i=0
r−1

r−1

ci,1 ti ,

=
i=0

ci ti
i=0
r−1

ci,2 ti , . . . ,
i=0

ci,n ti
i=0

= (s1 , s2 , . . . , sn ) .
This shows that the transmitted words using interleaved linear codes correspond exactly to codewords
of C. Moreover the weight of (s1 , . . . , sn ) as deﬁned above is the same as the Hamming weight of
r−1
r−1
r−1
( i=0 ci,1 ti , i=0 ci,2 ti , . . . , i=0 ci,n ti ) ∈ C.

In addition the costs of Algorithm 4 are the same as the ones
given in Proposition 10.

4

7
8
9
10
11
12

2
1.0
0.96
0.81
0.49
0.0073
0.00012

3
1.0
0.98
0.94
0.80
0.53
0.14

4
1.0
0.99
0.96
0.88
0.70
0.38

5
1.0
0.99
0.97
0.91
0.75
0.48

6
1.0
0.99
0.98
0.91
0.78
0.53

minimum distance. Finally we applied these techniques to
decode interleaved linear codes over a ﬁnite ﬁeld and get a
decoding algorithm that can decode more errors than half the
minimum distance.
ACKNOWLEDGMENT
The author would like to thank Daniel Augot for his precious advice and readings of this article and Gr´ goire Lecerf
e
for his careful readings of this article. The author would also
like to thank the reviewers who helped improve this article.

Fig. 1. Fraction of corrigible error patterns for a Goppa code of parameters
[256, 200, 15]F2 .

22
23
25
27
28
29
30

3
1.00000
0.999997
0.999844
0.998099
0.995114
0.989079
0.978112

4
1.00000
0.999999
0.999963
0.999469
0.998531
0.996477
0.992458

5
1.00000
0.999999
0.999981
0.999715
0.999185
0.997984
0.995554

6
1.00000
0.999999
0.999987
0.999789
0.999391
0.998470
0.996581

R EFERENCES
[1] M. A. Armand, “Improved list decoding of generalized Reed-Solomon
and alternant codes over rings,” in IEEE International Symposium on
Information Theory 2004 (ISIT 2004), 2004, p. 384.
[2] ——, “Improved list decoding of generalized Reed-Solomon and alternant codes over Galois rings,” IEEE Trans. Inform. Theory, vol. 51,
no. 2, pp. 728–733, feb 2005.
[3] ——, “List decoding of generalized Reed-Solomon codes over commutative rings,” IEEE Trans. Inform. Theory, vol. 51, no. 1, pp. 411–419,
2005.
[4] M. A. Armand and O. de Taisne, “Multistage list decoding of generalized Reed-Solomon codes over Galois rings,” Communications Letters,
IEEE, vol. 9, no. 7, pp. 625–627, jul 2005.
[5] N. Babu and K.-H. Zimmermann, “Decoding of linear codes over Galois
rings,” Information Theory, IEEE Transactions on, vol. 47, no. 4, pp.
1599–1603, may 2001.
[6] D. Bleichenbacher, A. Kiayias, and M. Yung, “Decoding of Interleaved
Reed Solomon Codes over Noisy Data,” in Automata, Languages and
Programming, ser. Lecture Notes in Computer Science, J. Baeten,
J. Lenstra, J. Parrow, and G. Woeginger, Eds.
Springer Berlin /
Heidelberg, 2003, vol. 2719, pp. 188–188.
[7] E. Byrne, “Lifting Decoding Schemes over a Galois Ring,” in Applied
Algebra, Algebraic Algorithms and Error-Correcting Codes, ser. Lecture
Notes in Computer Science, S. Boztas and I. Shparlinski, Eds. Springer
Berlin / Heidelberg, 2001, vol. 2227, pp. 323–332.
[8] D. Coppersmith and M. Sudan, “Reconstructing curves in three (and
higher) dimensional space from noisy data,” in Proceedings of the thirtyﬁfth annual ACM symposium on Theory of computing, ser. STOC ’03.
New York, NY, USA: ACM, 2003, pp. 136–142.
[9] M. F¨ rer, “Faster Integer Multiplication,” in Proceedings of the Thirtyu
Ninth ACM Symposium on Theory of Computing (STOC 2007). ACM,
2007, pp. 57–66.
[10] S. Gao, “A New Algorithm for Decoding Reed-Solomon Codes,” in
Communications, Information and Network Security, V. Bhargava, H.V.
Poor, V. Tarokh, and S. Yoon. Kluwer, 2002, pp. 55–68.
[11] P. Gopalan, V. Guruswami, and P. Raghavendra, “List Decoding Tensor
Products and Interleaved Codes,” SIAM Journal of Computing, vol. 40,
no. 5, pp. 1432–1462, 2011.
[12] M. Greferath and U. Vellbinger, “Efﬁcient decoding of Zpk -linear
codes,” IEEE Trans. Inform. Theory, vol. 44, no. 3, pp. 1288–1291,
may 1998.
[13] V. Guruswami, List decoding of error-correcting codes: winning thesis
of the 2002 ACM doctoral dissertation competition, ser. Lecture Notes
in Computer Science. Springer, 2004.
[14] V. Guruswami and M. Sudan, “Improved Decoding of Reed-Solomon
and Algebraic-Geometric Codes,” IEEE Trans. Inform. Theory, vol. 45,
pp. 1757–1767, 1998.
[15] J. Justesen, “On the complexity of decoding Reed-Solomon codes
(Corresp.),” IEEE Trans. Inform. Theory, vol. 22, no. 2, pp. 237–238,
Mar. 1976.
[16] M. Lazard, “Graduations, ﬁltrations, valuations,” Publications
´
Math´ matiques de L’IHES, vol. 26, pp. 15–43, 1965.
e
[17] R. Roth, Introduction to coding theory. Cambridge University Press,
2006.
[18] M. Sudan, “Decoding Reed-Solomon codes beyond the error-correction
diameter,” in the 35th Annual Allerton Conference on Communication,
Control and Computing, 1997, pp. 215–224.
[19] S. Wicker and V. Bhargava, Reed-Solomon Codes and Their Applications. John Wiley & Sons, 1999.

Fig. 2. Fraction of corrigible error patterns for an Extended BCH code with
parameters [256, 100, 46]F2 .

Theorem 16. Given a linear code C over Fq with
parameters [n, k, d]Fq and a unique decoding algorithm
BlackBoxErasuresDec from errors and erasures that can
correct
erasures and τ ( ) errors in Dec(C ) arithmetic
operations over Fq , there exists a unique decoding algorithm
for interleaved codes with respect to C and of degree r from
errors and erasures that can correct
erasures and τ ( )
errors with at most rDec(C ) arithmetic operations over Fq .
Moreover it can correct at least a fraction of P ( , B, w) error
patterns of Hamming weight at most w > τ ( ) over B where
P is deﬁned by (6), also with at most rDec(C ) arithmetic
operations over Fq .
Proof: As G = G there is no need to lift a codeword
from C into C and the given complexities are a consequence
of Proposition 9. The existence of both algorithm is ensured
by Proposition 15 and Proposition 12.
In Tables 1 and 2, the ﬁrst row gives the degrees of
interleaving and the ﬁrst column shows the number of errors
up to which we want to decode. The second row corresponds
to half the minimum distance and, as expected, all of the
probabilities are 1.0. We can see that the fraction of corrigible
error patterns increases with the degree of interleaving and
that codes with a high minimal distance are good candidates
for interleaving.
V. C ONCLUSION
In this paper we designed a decoding algorithm based on
a lifting decoding scheme. It allowed us to obtain a unique
decoding algorithm for RS codes over Galois rings with a low
complexity. We also applied this scheme to get a list decoding
algorithm for RS codes over Galois rings. We then show
that using erasures at appropriate positions in the proposed
algorithms allows us to decode more errors than half the

5

