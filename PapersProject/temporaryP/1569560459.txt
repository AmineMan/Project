Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Sat May 19 04:21:47 2012
ModDate:        Tue Jun 19 12:56:17 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      615552 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569560459

Fundamental limits on the power consumption of encoding and decoding
Pulkit Grover† , Andrea Goldsmith† , Anant Sahai‡
† Stanford University ‡ University of California, Berkeley
{pulkit@, andrea@ee.}stanford.edu, sahai@eecs.berkeley.edu
Abstract— We provide fundamental information-theoretic bounds
on the required circuit wiring complexity and power consumption for
encoding and decoding of error-correcting codes. These bounds hold
for all codes and all encoding and decoding algorithms implemented
within the paradigm of our VLSI model. This model essentially
views computation on a 2-D VLSI circuit as a computation on
a network of connected nodes. The bounds are derived based on
analyzing information ﬂow in the circuit. They are then used to
show that there is a fundamental tradeoff between the transmit and
encoding/decoding power, and that the total (transmit + encoding +
decoding) power must diverge to inﬁnity at least as fast as cube-root
1
of log Pe , where Pe is the average block-error probability. On the
other hand, for bounded transmit-power schemes, the total power
1
must diverge to inﬁnity at least as fast as square-root of log Pe due
to the burden of encoding/decoding.

channel capacity is the limit on the achievable rate for any reliable
communication strategy.
What is the difﬁculty in obtaining fundamental limits on encoding/decoding power and complexity? Part of the challenge comes
from the observation that even for a given code and implementation technology (e.g. 45 nm CMOS), there are many possible
encoding and decoding algorithms and (even for ﬁxed algorithms)
many implementation architectures. Truly fundamental bounds
therefore need to be oblivious not only to the code construction,
but also to the chosen encoding and decoding algorithms and
circuit architectures. The goal of this paper is to provide such
bounds. In order to guide design of code-encoder-decoder triples
for a given implementation technology, our bounds necessarily
depend on the chosen technology. In order to do so, just as channel
models take into account the limitations (e.g. noise, bandwidth,
path-loss) imposed by communication channels, in Section III, we
ﬁrst provide an implementation model that captures the limitations
of information ﬂow in VLSI implementations.
Our model is essentially an adaptation of Thompson’s model
for VLSI computation [6], [7]. In his thesis [7], Thompson
provides fundamental wiring-complexity and power bounds in his
VLSI model for two problems: computing the Fourier transform,
and sorting. The bounds essentially build on fundamental network information-theoretic limits on the required communication
complexity1 of computing the desired function. To ascertain the
information bottlenecks in this “network,” a communication graph
representing the circuit is bisected into two roughly equal pieces
by cutting as few “wires” as possible. Knowing the number of
bits that need to be communicated across this cut, a simple
application of the cut-set bounding technique [9, Pg. 376] provides
the minimum run-time (the number of “clock-cycles,” denoted
here by τ ) for the computation.
The ﬁrst results on VLSI complexity of encoding/decoding were
derived by El Gamal et al [10]. However, their novel technique
does not extend to yield bounds on energy/power consumption
(this aspect is discussed in detail in Section III). In order to
establish results on energy and power consumption, in Section IV,
we build on Thompson’s original technique [7]. We analyze the
computation process by repeatedly bisecting the circuit until the
resulting sub-circuits are sufﬁciently small. We then use an errorexponent-style analysis that yields non-asymptotic bounds for
any given error probability and blocklength. Our earlier techniques [11] limited our results to just the encoding complexity.
In this paper, we are able to derive qualitatively stronger bounds

I. I NTRODUCTION
Information theory has been extremely successful in determining
fundamental capacity limits to communication over a wide range
of channels. However, these limits focus only on transmit power,
ignoring the complexity and the associated power consumption
for processing (e.g. encoding and decoding) the signals. At short
distances, available empirical evidence suggests that transmit
power does not necessarily dominate total power consumption [2],
and hence the intuition from current theory can be misleading
at short distances, at least in the context of coding. In fact, in
many cases, uncoded transmission has been proposed [3] to do
away with encoding and decoding altogether, even at the cost
of signiﬁcantly larger transmit power! What, then, is the optimal
approach? Our experimental work [4], [5] examines this issue
using circuit simulations, where we empirically observe that the
best choice of code, encoder and decoder becomes gradually
complex as the communication distance increases. Thus, neither
the traditional approach of operating close to capacity nor uncoded
transmission is optimal at short distances. This naturally raises
an important question: what is the choice of the code, encoder,
and decoder for which the total power consumed is close to the
minimum possible. Further, what is this minimum total power?
While total power minimization has received little attention
in information theory, a uniﬁed understanding of information
theory and various notions of complexity has been a long-standing
intellectual goal. The issue has been investigated for many code
families, notions of complexity, and encoding/decoding algorithms
(see [1] for a short survey). In comparison, Shannon’s capacity
results are fundamental: given the communication model, the
We thank Abbas El Gamal, Salim El Rouayheb, Reza Mirghaderi, Sudeep
Kamath, Jan Rabaey, Karthik Ganesan and Yang Wen for helpful discussions. This
research was supported by the Interconnect Focus Center of the Semiconductor
Research Corporation, the NSF Center for Science of Information (CSoI) and by
NSF grant NSF-CCF-0917212. Because of space-limitations, the full proofs of
results, and a more detailed literature survey, appear in [1].

1 The VLSI theory of computation and the theory of communication complexity [8] developed almost simultaneously, and share their origins to discussions in
decentralized computation in 1970s.

1

First
bisection

that apply to encoding as well as decoding.
We introduce the system model in Section II, and the implementation model in Section III. In Section IV, we provide
a lower bound on the energy and power consumption in encoding and decoding. Assuming communication over a Gaussian channel with average transmit power PT , this lower bound
shows that the energy consumed in encoding/decoding is at
least2 Ω k log P 1 /PT . Optimizing over transmit power, we
blk
e
conclude that the total (transmit + encoding + decoding) power
must scale at least as fast as Ω 3 log P 1 . Further, if we use
blk

Input nodes
Output nodes
Helper nodes
Second
bisection

e

Second
bisection

Fig. 1. The VLSI model of implementation and an example of a 2-step nested
bisection of output nodes. A bisection needs to divide only a speciﬁed subset of
nodes (in this case, the output nodes) into two roughly equal pieces. The above
cut bisects the set of output nodes in the communication graph of the circuit.

blk
bounded transmit power even as Pe is driven to zero, then the
lower bound is larger, and scales at least as fast as Ω
log P 1 .
blk
e
Thus, we conclude that the optimal strategy will increase transmit
blk
power as Pe is driven to zero in order to reduce the complexity
and power consumption of encoding and decoding. Section V
includes further discussion and conclusions of our main results.
Our power model here focuses on wires, ignoring the power
consumed in the computational nodes. Our complementary results
on power consumed in computational nodes appear in [12]–[14].
In an order sense, while wire-power is dominant at low error
probabilities and large gaps from capacity, node power dominates
at transmissions closer to channel capacity.
Although our results are non-asymptotic, we use asymptotic
notation (i.e. the “big-Oh” notation) to convey an intuitive understanding. Vectors are denoted in bold (e.g. Xn is a vector of
1
length n). For any set A, |A| denotes its cardinality.

computational nodes that are connected to each other using ﬁnitewidth wires. In each clock-cycle, the nodes communicate with
all the other nodes that they are connected to. The nodes can
perform simple computations (e.g. a three input NAND) on the
inputs received by the nodes. The computation terminates at a
predetermined τ number of clock-cycles.
B. Detailed model
We assume that the encoder E and the decoder D are implemented using the “VLSI model of computation.” The model,
which is detailed below, captures essentially the communication
limitations in a VLSI circuit, allowing a network-informationtheoretic analysis to be performed on the circuit itself.

II. S YSTEM MODEL

•

We consider a point-to-point communication link. An information sequence of k fair coin ﬂips bk is encoded into 2nR binary1
k
alphabet codewords Xn , hence the rate of the code is R = n
1
n
bits/channel use. The codeword X1 is modulated using BPSK
modulation and sent through an Additive White Gaussian Noise
(AWGN) channel of bandwidth W . The decoder estimates the
input sequence bk by ﬁrst performing a hard-decision on the
1
n
received channel symbols before using these hard-decisions Y1
to decode the input sequence. The overall channel is therefore a
Binary Symmetric Channel (BSC) with raw bit-error probability
x2
∞ 1
ζPT
pch := Q
, where Q(x) = x 2π e− 2 dx, ζ is the path2
σz
loss associated with the channel, PT is the transmit power of
2
the BPSK-modulated signal, and σz is the variance of the Gaussian noise in the hard-decision estimation. The encoder-channelblk
decoder system operates at an average block-error probability Pe
blk
k
k
given by Pe = Pr b1 = b1 .

•

•
•

•

III. VLSI MODEL OF ENCODING / DECODING IMPLEMENTATION
A. Overview

•

As mentioned earlier, our model is an adaptation of Thompson’s
model [7]. The model assumes that any VLSI circuit is a set of
2 We use the the “big-Oh” notation that is popular in computer science. That
is, for any two functions f (n) and g(n), asymptotically (as n → ∞), f (n) =
O(g(n)) if f (n) ≤ c2 g(n); f (n) = Ω(g(n)) if f (n) ≥ c1 g(n); and f (n) =
Θ(g(n)) if c1 g(n) ≤ f (n) ≤ c2 g(n) for some positive c1 , c2 .

The circuit to compute the encoding/decoding function must
be laid out on a grid of unit squares. It is composed
of ﬁnite-memory computational nodes and interconnecting
wires. Wires run along the edges and can cross at grid points.
A computational node is a grid point that is either a logic
element, a wire-connection, or an input/output pin.
The wires are assumed to be bi-directional. In each clockcycle, each node sends one bit of information to each of the
nodes that it is connected to over these wires.
The circuit is planar3 , and each node is connected to at most
four other nodes using the bi-directional wires.
The inputs of the computation (information bits for the
encoder, and channel outputs for the decoder) are stored
in separate source nodes, and the outputs of computation
(codeword bits for the encoder, reconstructed information bits
at the decoder) are stored in output nodes. The same node
may act as a source-node and as an output node4 . Also, each
input value may enter the circuit at only the corresponding
source node.
Each wire has a ﬁnite-width λ speciﬁed by the technology
chosen to implement the circuit. Further, the area of each
node is at least λ2 .
The processing is done in “batches,” i.e., a set of inputs is
processed and outputs are released before the next set of
inputs arrives into the source nodes.

3 Our

results extend to multi-layered chips in a manner similar to that in [7].
instance, in LDPC decoders, the variable nodes act as source nodes as well
as output nodes.
4 For

2

and R is the code rate. As noted earlier, we need lower bounds
on Awires τ in order to obtain lower bounds on energy/power5 .
The proof technique in [10] therefore does not extend to provide
bounds on Awires τ (because Awires < Achip ), which is needed
to obtain bounds on energy/power of encoding/decoding.

The last assumption rules out “pipelining” [10] and sequential
processing of inputs (e.g. by reading them from external storage
while the computation is in process). An example implementation
that lies outside our model is the decoding of a convolution code
in a streaming manner, where bit estimates are outputted before
the entire block is received.
Energy/power model: The energy consumed in computation is
assumed to be given by Eproc = ξtech Awires τ , where ξtech
is the “energy parameter” of the implementation that depends
on the capacitance per-unit length of wiring in the circuit. Our
model corresponds to a per-clock-cycle energy consumption of
1
2
2 CV [15], where C is the total wiring capacitance, and V is a
ﬁxed chosen voltage (thus we disallow “voltage scaling” [15]) .
In order to translate energy to power consumption, we assume
that the decoding throughput (the number of information bits
encoded/decoded per second) of the encoder/decoder is the same
as the data rate Rdata (information bits/sec) across the channel.
This assumption is necessitated by the requirement of avoiding
buffer overﬂow at the receiver. Because the batch of k data bits
are processed in parallel by the encoder/decoder, the amount
k
of time available for the processing is Tproc = Rdata seconds.
The required power for encoding/decoding is therefore Pproc =
Eproc
Eproc
Tproc =
k Rdata , which is simply the energy-per-informationbit multiplied by the data rate.
2
2
Deﬁnition 1 (Channel Model (ζ, σz )): Channel Model (ζ, σz )
denotes (as described in Section II) a BSC(pch ) channel that is a
result of hard-decision at the receiver across an AWGN channel
2
of average transmit power PT , path loss ζ and noise variance σz .
Deﬁnition 2 (Implementation Model (ξtech , λ)):
Implementation Model (ξtech , λ) denotes the implementation
model (as described in Section III) having minimum wire-width
λ, and energy parameter ξtech .

D. Deﬁnitions for computation on the communication graph
For analyzing a computation process in the VLSI model, we
deﬁne a communication graph for a circuit in the VLSI model.
Deﬁnition 3 (Communication graph): A
communication
graph G corresponding to a circuit implemented in the model
described in Section III, has vertices at the computational nodes,
and edges along the interconnecting wires.
The set of vertices is denoted by V , and of edges by E. The computation can be viewed as being performed on the communication
graph G. In order to analyze the bottlenecks of information ﬂow
in G, we deﬁne bisections on G. A cut is said to “bisect” G (see
Fig. 1) if roughly half of a speciﬁed subset of nodes lie on either
side of the cut. More formally:
Deﬁnition 4 (Bisection [6], [7]): Let S ⊆ V be a subset of the
vertices, and ES ⊆ E be a subset of the edges in G. Then ES
bisects S in G if removal of ES cuts V into sets V1 and V2 and
S into sets S1 ⊆ V1 and S2 ⊆ V2 such that |S1 | − |S2 | ≤ 1.
Deﬁnition 5 (Minimum bisection width, and MBW-cut):
The minimum bisection width (MBW) of S in G is deﬁned as
min{|ES | s.t. ES bisects S in G}. The corresponding cut is
called a minimum-bisection-width-cut; or an MBW cut.
Deﬁnition 6 (Nested bisections): Let S ⊆ V be a subset of the
vertices, and ES ⊆ E be a subset of the edges in G that bisects
S such that removal of ES cuts G into G1 , G2 ; of V into V1 and
V2 ; and of S into S1 ⊆ V1 and S2 ⊆ V2 . Let ES,i , i = 1, 2, be
subsets of edges in Gi that bisect Si . The resulting partitions of
sets S, V each into four disjoint subsets is called a 2-step nested
bisection of S in G.
The physical wires and computational nodes corresponding to
the disjoint and mutually disconnected subsets that result from
(nested) bisections constitute the sub-circuits. In our proof techniques, we will perform r-step nested bisections for some r ∈ Z+ ,
conceptually breaking the original circuit into 2r sub-circuits,
indexed by i = 1, 2, . . . , 2r . By assumption, bits are passed across
an edge in each direction at every clock-cycle.
Deﬁnition 7 (Bits communicated across a cut): Suppose the
communication graph V of a circuit implemented in the Implementation Model (ξtech , λ) is partitioned into two disconnected
sets on removal of edges Ecut . An ordered set of the bits
passed along the edges in Ecut in either direction during the
computation process is called the vector of bits communicated
across the cut. The length of this vector is called the number of
bits communicated across the cut.
If w is the width of a cut, the number of bits communicated across
the cut in τ processing cycles is simply 2wτ , where the factor of
2 is because of the bi-directional nature of the wires. In this paper,

C. A partial survey of existing results for the model
As noted earlier, Thompson derived fundamental complexity and
power bounds for the VLSI model for computing the Fourier
transform and sorting [7]. The “cut-set” idea described in the
introduction is used to obtain a lower bound on the minimum runtime (the number of “clock-cycles”, denoted here by τ ) for the
computation. Although this minimum run-time can be reduced by
increasing the “min-cut” (referred to as the “minimum bisection
width” in the VLSI theory literature [6]), this increase in the
“min-cut” comes at the cost of an increased wiring area. Thus
there is a fundamental tradeoff between the number of clockcycles τ , and the wiring area Awires . This is usually characterized
by lower bounds on Awires τ 2 . As discussed above, the wireenergy consumption is proportional to a closely related quantity:
the product Awires τ [7]. The results have been extended to many
other computational problems, such as multiplication, computing
boolean functions, etc. (see [1] and the references therein).
El Gamal et al. [10] use Thompson’s VLSI model, and in a single
analysis step, the authors break the entire circuit into multiple
small sub-circuits. The authors show that the product Achip τ 2 is
at least Ω nR2 log P n , where Achip is the area of the smallest
blk
e
rectangle that encloses the circuit, n is the blocklength of the code,

5 In his thesis, Thompson also places emphasis on this issue: while Theorem 1 [7,
Pg. 54] in his thesis obtains a lower bound on the area of the circuit’s minimum
enclosing rectangle (which includes unoccupied area; much like the result in [10]),
for lower bounding purposes, he uses Theorem 2 [7, Pg. 54] that obtains a lower
bound on the wiring area.

3

denominator 2r . The following lemma (Lemma 2) uses the total
[1:r]
number of communicated bits, Btotal , to obtain a lower bound on
error probability. Lemma 1 and Lemma 2 are used to obtain lower
blk
bounds on energy given the target Pe . The value of r can then
be chosen to obtain the best lower bound on energy.
blk
2
Lemma 2 (Pe lower bounds): Under Channel Model (ζ, σz )
and Implementation Model (ξtech , λ), on performing r <
log2 (k/2) steps of nested bisections on this circuit, let the total
number of bits that pass across the MBW-cuts over the r stages be
[1:r]
[1:r]
denoted by Btotal,enc at the encoder and Btotal,dec at the decoder.
[1:r]
[1:r]
If min{Btotal,enc , Btotal,dec } < k , then
4

we will mostly be interested in the bits communicated across
an MBW-cut. In particular, we will be interested in the vector
of communicated bits across all MBW-cuts in r-steps of nested
bisections of a communication graph. This vector is simply the
concatenation of vectors of bits communicated across the MBWcuts in the r steps of nested bisections. The length of this vector,
which is the total number of bits communicated across the MBW[1:r]
cuts in the r bisection steps, is denoted by Btotal .
Deﬁnition 8 (Communication bits for a sub-circuit): An ordered collection of bits communicated during the computation
process along the wires that have exactly one computational node
inside a given sub-circuit is called the vector of communica[1:r]
tion bits for the i-th sub-circuit, denoted by bcomm,i . The bits
could have been communicated in either direction along the bidirectional wires during s = 1, 2, . . . , r.
[1:r]
The length of bcomm,i , the vector of communication bits for the i[1:r]
th sub-circuit, is denoted by Bsubckt,i . Considering the sub-circuits
at the end of r steps of successive bisections, because any wire that
has exactly one computational node inside the sub-circuit must be
connected to a exactly one node outside the sub-circuit, we have
[1:r]
[1:r]
2r
that i=1 Bsubckt,i = 2Btotal .

blk
Pe ≥ pch 2
where pch = Q
0
1

1
2pch

n
2r−1

ζPT
2
σz

− log2

[1:r]
k −2 min{B [1:r]
,B
}
2
total,enc
total,dec
n

.

1 − pch

pch
pch
1 − pch

0

1

0

≡

1

1−

1−￿

￿
￿

1−￿

0

1
0.5
0.5

?
1

,

0

1

1

Fig. 2. For = 2pch , the BSC on left is equivalent to the concatenation of
erasure channel with another DMC shown on right.

Proof sketch for decoding computation: (see [1] for full proof)
Working directly with a BSC model turns out to be complicated:
the direct approach was adopted in [11]: the derived bounds there
were only for encoding, and were looser than the bounds here.
Instead, we take an indirect route: we ﬁrst ﬁnd bounds on a BEC
and use them for a BSC by observing that a BSC(pch ) can be
interpreted as a physically degraded BEC with erasure probability
2pch (see Fig. 2).
Consider the i-th decoder sub-circuit at the r-th (ﬁnal) stage,
i = 1, 2, . . . , 2r , strengthened with the knowledge of the erasureoutputs (supplied for free). Denote the bits communicated across
the boundary of the i-th decoder sub-circuit in either direction
[1:r]
by bcomm,i (summed over all the r stages, and shared across
[1:r]
MBW-cuts across different stages), and their number by Bsubckt,i .
As noted earlier, the total number of bits communicated across
[1:r]
2r
MBW-cuts (over all the r bisection stages) is i=1 Bsubckt,i =
[1:r]
[1:r]
k
2Btotal,dec . Deﬁne S := {i : Bsubckt,i < 2r }. A simple averaging
r−1
argument shows that |S| > 2 .
The i-th decoder sub-circuit can only use the communication
[1:r]
k
bits bcomm,i and the ni channel outputs in order to decode the 2r
information bits in the sub-circuit. Assuming optimistically that
k
the communication bits themselves can deliver some of the 2r
[1:r]
k
information bits in an error free manner, when Bsubckt,i < 2r ,
the decoder still has to use the ni available channel outputs to
[1:r]
k
decode the remaining 2r − Bsubckt,i information bits. However,
if the number of unerased channel outputs in the i-th sub-circuit
[1:r]
k
is smaller than 2r − Bsubckt,i , then this decoder sub-circuit likely
1
makes an error (i.e. with probability > 2 , because it has to guess
at least one bit). How many erasures should there be for this to
happen? Clearly, the number of erasures #E,i must be at least
[1:r]
k
ni − 2r + Bsubckt,i + 1. Accounting for the event when at least
the ﬁrst #E,i bits are erased, and denoting this event by Wi , the
probability of this event is

IV. M AIN RESULTS
This section presents a sequence of results that culminate in our
lower bounds on total power. The results are based on r-step
nested bisections of the communication graph of the respective
circuit, each step bisecting the respective “information nodes”
(input nodes for encoding; output nodes for decoding). For clarity
of exposition, we assume that k is a power of 2. Thus, at the end
of r steps of nested bisections, there are 2r sub-circuits, each with
k
2r “information nodes.” The results can be extended suitably to
values of k that are not powers of 2.
Lemma 1 (Energy lower bounds): Under the Channel Model
2
(ζ, σz ) and Implementation Model (ξtech , λ), let k (a power of 2)
be the number of information nodes at the encoder/decoder, and
n be the nodes corresponding to channel input/output symbols.
On performing r < log2 (k/2) steps of nested bisections on this
circuit, let the total number of bits that pass across the MBW-cuts
[1:r]
of the r stages of nested bisections be denoted by Btotal . Then,
√
( 2 − 1) n [1:r]
√
Eproc > ξtech λ2
B
,
(1)
2r total
4 2
Proof sketch: (see [1] for full proof) At each stage s = 1, 2, . . . , r,
we ﬁrst obtain a lower bound on Awires τ 2 by lower bounding the
(s)
2s
sum i=1 Awires,i τ 2 for given number of bits B (s) passed across
(s)
(s)
2s
MBW-cuts in the s-th stage, i.e., B (s) = i=1 Bi , where Bi
is the number of bits passed across the MBW-cut of the i-th subcircuit in the s-th stage. This gives r different lower bounds on
the product Awires τ 2 for the encoding/decoding process, one for
each stage, each depending on B (s) for the corresponding stage.
[1:r]
r
Then, noting that s=1 B (s) = Btotal , we obtain a lower bound
[1:r]
on the product Awires τ 2 as a function of Btotal . Using the simple
lower bound on Awires , namely, Awires ≥ nλ2 , we obtain a lower
bound on Awires τ which is proportional to the energy consumed
in our implementation model.
[1:r]
Observe in Lemma 1 that while the total number of bits Btotal
that cross MBW-cuts in r stages increases with r, so does the

Pr(Wi ) =

4

k
ni − 2r

[1:r]
+Bsubckt,i +1

ni 1−

= 2pch (2pch )

k −B [1:r]
subckt,i
2r
ni

.

Averaging this probability over i ∈ S, using the convex-∪ nature
1
of the exponential function, and using the lower bound of 2 on
the error probability under Wi yields the lower bound.
blk
The derivation for the bound on Pe given the number of bits
passed in encoding makes no assumptions on the decoding model
(the decoder is assumed to be the optimal decoder). Because of
space limitations, that derivation is relegated to [1].
Theorem 1 (Energy in encoding/decoding): Under Channel
2
Model (ζ, σz ) and Implementation Model (ξtech , λ), let the
energy consumed in encoding be denoted by Eenc , and that in
decoding be denoted by Edec . Then, for this model, for any
r < log2 (k/2), min{Eenc , Edec } satisﬁes either
√ 2r
√


8 2
k
−

blk
Pe ≥pch 2

n
2r−1

log2

1
2pch

−
1

2

− √
( 2−1)ξtech λ2

n

min{Eenc ,Edec }



n

2

√
2−1
√ √
16 2 R

,
k3
2r .

or min{Eenc , Edec } > ξtech λ
Proof: Immediate from Lemma 1 and Lemma 2.
Corollary 1 (Asymptotic lower bounds on energy): Under
2
Channel Model (ζ, σz ) and Implementation Model (ξtech , λ), in
blk
the limit of small Pe ,
√
pch
log2 P blk
2−1
e
2
Eproc ξtech λ
.
(2)
k
√
1
R
log2 2pch
24 2 1 − 3

R EFERENCES

Proof sketch: (see [1] for full proof) The proof follows by choice
of r according to the following equation:
1
pch
R
2r−1 ≈ n log2
/ log2
1−
,
(3)
blk
2pch
3
Pe

[1] P. Grover, A. J. Goldsmith, and A. Sahai, “Fundamental limits on
transmission and computation power,” Feb. 2012, extended version of paper
submitted to ISIT’12. [Online]. Available: http://tiny.cc/WireModel
[2] S Cui, AJ Goldsmith and A Bahai, “Energy Constrained Modulation Optimization,” IEEE Trans. Wireless Commun., vol. 4, no. 5, pp. 1–11, Sep.
2005.
[3] C. Marcu, D. Chowdhury, C. Thakkar, J.-D. Park, L.-K. Kong, M. Tabesh,
Y. Wang, B. Afshar, A. Gupta, A.Arbabian, S. Gambini, R. Zamani, E. Alon,
and A. Niknejad, “A 90 nm CMOS low-power 60 GHz transceiver with
integrated baseband circuitry,” IEEE Journal of Solid-State Circuits, vol. 44,
no. 12, pp. 3434 – 3447, 2009.
[4] K. Ganesan, P. Grover, and J. M. Rabaey, “The power cost of overdesigning
codes,” in Proceedings of the 2011 IEEE Workshop on Signal Processing
Systems (SiPS), Oct. 2011.
[5] K. Ganesan, Y. Wen, P. Grover, J. M. Rabaey, and A. J. Goldsmith,
“Optimizing code-decoder pairs for “green” code design,” submitted to IEEE
Globecom, March 2012.
[6] C. D. Thompson, “Area-time complexity for VLSI,” in Proceedings of the
11th annual ACM symposium on Theory of computing (STOC). New York,
NY, USA: ACM, 1979, pp. 81–88.
[7] ——, “A complexity theory for VLSI,” Ph.D. dissertation, Carnegie Mellon
University, Pittsburgh, PA, USA, 1980.
[8] A. C.-C. Yao, “Some complexity questions related to distributive computing,”
in Proceedings of the eleventh annual ACM symposium on Theory of
computing, 1979, pp. 209–213.
[9] T. M. Cover and J. A. Thomas, Elements of Information Theory, 1st ed.
New York: Wiley, 1991.
[10] A. El Gamal, J. Greene, and K. Pang, “VLSI complexity of coding,” in The
MIT Conf. on Adv. Research in VLSI, Cambridge, MA, Jan. 1984.
[11] P. Grover, A. Goldsmith, A. Sahai, and J. Rabaey, “Information theory meets
circuit design: Why capacity-approaching codes require more circuit area
and power,” in Proceedings of the Allerton Conference on Communication,
Control, and Computing, Monticello, IL, Sep. 2011.
[12] A. Sahai and P. Grover, “A general lower bound on the VLSI
decoding complexity,” in preparation, 2012. [Online]. Available:
http://www.eecs.berkeley.edu/ pulkit/papers/ComplexityITPaper.pdf
[13] P. Grover, “Bounds on the tradeoff between rate and complexity for sparsegraph codes,” in IEEE Information Theory Workshop (ITW), 2007.
[14] P. Grover, K. A. Woyach, and A. Sahai, “Towards a communication-theoretic
understanding of system-level power consumption,” IEEE Jour. Selected
Areas in Comm., Special Issue on Energy-Efﬁcient Communications. Arxiv
preprint arXiv:1010.4855, vol. 29, no. 8, Sep. 2011.
[15] J. Rabaey, A. Chandrakasan, and B. Nikolic, Digital integrated circuits.
Prentice Hall Englewood Cliffs, New Jersey, 2002.

blk
a choice which can be made in the limit of small Pe .
Corollary 2 (Asymptotic lower bounds on total power):
2
Under Channel Model (ζ, σz ) and Implementation Model
blk
(ξtech , λ), across a channel of path-loss ζ, in the limit Pe → 0,
the total power is bounded as follows
1

23 +

Ptot
√

where η =

√2−1

48

1−R/3

1
2

23

2 3

η3

ln

2
σz
2
ζ W Rλ ,

1
,
blk
Pe

(4)

and W is the channel

bandwidth.
Proof sketch: (see [1] for full proof) In our hard-decision channel
1
model, the term log 2pch scales proportionally to received power
ζPT . Because we want to minimize the total power, and because
Eproc is normalized by k in order to obtain power,
Ptotal ≥ min PT + Pproc ≈ min PT + β

log

1
blk
Pe

,
(5)
PT
blk
for some constant β. As Pe → 0, choosing PT = logx P 1 ,
blk
e
1
1
to minimize the total power, x must satisfy x = 2 − 2 x. Thus,
x = 1 , and we get that the total power, as well as the optimizing
3
transmit power, must scale at least as fast as 3 log P 1 . It is also
blk
e
clear from (5) that if we use bounded PT , then the total power
scales at least as fast as log P 1 . Plots for lower bounds on
blk
e
total power appear in [1].
PT

of error-correcting codes. The limits are derived by analyzing
the network of interconnected nodes in VLSI implementations
using simple information-theoretic tools (i.e. cut-set bounds). The
underlying intuition behind our results is simple: to utilize the
beneﬁts offered by increased blocklengths, the circuit needs to
exchange bits. Otherwise, the large code could effectively be
split into codes of smaller blocklengths which would have worse
error correction capability. Thus, in order to get the coding gains
commensurate with large codes, there is more exchange of bits
in the implementation, and hence more power consumption. This
leads to a tradeoff between transmit power and computation power.
Because of the simplicity of this intuition, we think that these
results should extend to implementations that do not ﬁt directly
into the model (e.g. 3-D circuits, soft decisions, etc.). For instance,
an extension to multi-layered circuits (such as those commonly
used today) can be easily obtained using tools developed by
Thompson in [7].
We optimistically assume that the power ampliﬁer at the transmitter has 100% efﬁciency. Incorporating RF and analog circuit power
consumption at the transmitter would require an enhancement on
our model where PT is replaced by a suitable function of PT that
accounts for power consumed by these circuit elements.

PT

V. D ISCUSSION AND CONCLUSIONS
This paper provides fundamental limits on complexity and
power consumed in VLSI implementations of encoding/decoding

5

