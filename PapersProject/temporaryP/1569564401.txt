Title:          twosamtest.dvi
Creator:        dvips(k) 5.98 Copyright 2009 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Sun Apr 22 19:33:44 2012
ModDate:        Tue Jun 19 12:55:40 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      312855 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569564401

On Optimal Two Sample Homogeneity Tests for
Finite Alphabets
Jayakrishnan Unnikrishnan
Audiovisual Communications Laboratory, School of Computer and Communication Sciences
Ecole Polytechnique F´ d´ rale de Lausanne (EPFL), Switzerland
e e
Email: jay.unnikrishnan@epﬂ.ch

exactly, two different results satisfying two different notions
of asymptotic optimality are known in the information theory
literature [1] [2]. However, the most commonly used solution
for this homogeneity testing problem is the two sample chisquare test [4] originally proposed by Pearson [5]. In this paper
we ﬁrst obtain a simpliﬁed structure for the test proposed in
[1]. We then proceed to identify the limiting behavior of the
test statistics used in the optimal tests of [1] and [2]. Such
limiting results can be used to approximate the thresholds for
these tests for a target false alarm probability, thus providing a
practical alternative to the popular two sample chi-square test
which does not have any known optimality properties.

Abstract—Suppose we are given two independent strings of
data from a known ﬁnite alphabet. We are interested in testing
the null hypothesis that both the strings were drawn from the
same distribution, assuming that the samples within each string
are mutually independent. Among statisticians, the most popular
solution for such a homogeneity test is the two sample chi-square
test, primarily due to its ease of implementation and the fact
that the limiting null hypothesis distribution of the associated test
statistic is known and easy to compute. Although tests that are
asymptotically optimal in error probability have been proposed
in the information theory literature, such optimality results are
not well-known and such tests are rarely used in practice. In this
paper we seek to bridge the gap between theory and practice.
We study two different optimal tests proposed by Shayevitz
[1] and Gutman [2]. We ﬁrst obtain a simpliﬁed structure of
Shayevitz’s test and then obtain limiting distributions of the test
statistics used in both the tests. These results provide guidelines
for choosing thresholds that guarantee an approximate false
alarm constraint for ﬁnite length observation sequences, thus
making these tests easy to use in practice. The approximation
accuracies are demonstrated using simulations. We argue that
such homogeneity tests with provable optimality properties could
potentially be better choices than the chi-square test in practice.

A. Notation
For any probability mass function π ∈ P(Z) we use π(z)
to denote the probability of symbol z ∈ Z. We sometimes
also use the notation π to denote the vector of probabilities
(π(z1 ), π(z2 ), . . . , π(zN )) and π, f to denote the inner prodN
uct
i=1 π(zi )f (zi ) for any function f deﬁned on Z. For
two distributions π, ν ∈ P(Z) the Kullback-Leibler divergence
between π and ν is given by

I. I NTRODUCTION
Suppose we are given two independent strings of data
xm := (x1 , x2 , . . . , xm ) and y n := (y1 , y2 , . . . , yn ) drawn
from the same known ﬁnite alphabet Z := {z1 , z2 , . . . , zN }.
We are interested in testing homogeneity, i.e., whether or not
both these strings are drawn i.i.d. from the same distribution
in P(Z), the collection of all multinomial probability distributions on Z. In other words this hypothesis testing problem
fundamentally aims to identify whether or not the two collections of samples are drawn from the same population. This
is a fundamental problem in statistics with various practical
applications [3].
This problem can be interpreted as a binary hypothesis
testing problem with a composite null hypothesis representing
the situation where both strings are drawn from identical
distributions and a composite alternate hypothesis where both
the strings are drawn from distinct distributions. Thus one can
deﬁne two different probabilities of error, viz., the probability
of false alarm under the null hypothesis and the probability of
missed detection under the alternate hypothesis. A reasonable
approach to solve this problem is to identify some testing
procedure that optimizes the trade-off between these two
quantities. Although it is intractable to solve this problem

D(π ν) =

π(z) log
z∈Z

π(z)
.
ν(z)

We use Γx to denote the empirical distribution of xm and Γy
m
n
to denote the empirical distribution of y n . We use Pπ1 ,π2 to
denote the probability measure when the ﬁrst string is drawn
from distribution π 1 and the second string is drawn from
distribution π 2 . When both strings are drawn from the same
distribution µ we use Pµ for the probability measure. We use
N (a, B) to denote a Gaussian random vector with mean a and
covariance matrix B and χ2 to denote a chi-square random
d
variable with d degrees of freedom.
B. Outline
In Section II we describe the mathematical problem statement and describe the known results. We provide a simpliﬁed
version of a known optimal test [1] in Section III. We then
present our new results on the weak convergence of the various
test statistics in Section IV. We discuss how these results can
be used for selecting test thresholds in Section V and conclude
in Section VI.

1

II. P ROBLEM

where δn = |Z| log n and I denotes the indicator function.
n
Gutman [2] studied this problem in the context of multihypothesis testing with training sequences. He used the following
optimality criterion

DESCRIPTION

Suppose we are given two independent strings of data
xm := (x1 , x2 , . . . , xm ) and y n := (y1 , y2 , . . . , yn ) drawn
from the same known ﬁnite alphabet Z. We think of xm as
the ﬁrst m observations from an i.i.d. sequence x and y n as
the ﬁrst n observations from another i.i.d. sequence y. We are
interested in testing whether or not both these sequences are
drawn from the same distribution in P(Z). For each m, n ∈ Z
let φm,n : Zm × Zn → {0, 1} represent a test on the ﬁrst m
observations from x and ﬁrst n observations from y. The test
outcome φm,n (xm , y n ) = 0 represents a decision in favor of
the null hypothesis that both x and y are drawn from the same
distribution and the outcome φm,n (xm , y n ) = 1 represents
a decision in favor of the alternate hypothesis that x and y
are drawn from different distributions. For any distribution
µ ∈ P(Z) of the observations under the null hypothesis, the
probability of false alarm is given by

supφ
s.t.

EMD (φ|π 1 , π 2 )
EF A (φn |µ) ≥ η for all µ ∈ P(Z)

and showed that the following sequence of likelihood ratio
tests solves problem (3):
x
y
1
2 (Γλn + Γn ))
1
+D(Γy 2 (Γx + Γy ))
λn
n
n

φB (x, y) =
n

I λD(Γx
λn

Similarly for any distinct distributions π 1 , π 2 ∈ P(Z) the
probability of missed detection is given by

χ2 (π, ν) :=
z∈Z

pMD (φm,n |π 1 , π 2 ) = Pπ1 ,π2 (φm,n (xm , y n ) = 0).

n→∞

(1)

Lemma III.1. For any distributions µ1 , µ2 ∈ P(Z), the
inﬁmum in

The following sequence of tests solves the problem (1):
φA (x, y) = I
n

inf

µ∈P(Z)

max{D(Γx µ), D(Γy µ)} ≥ δn
λn
n

(5)

III. S IMPLIFIED FORM OF T EST φA
The test statistic used in the test φA of (2) can be simpliﬁed
a great deal via the following lemma.

EMD (φ|π 1 , π 2 )
lim pF A (φn |µ) = 0 for all µ ∈ P(Z).

π, ν ∈ P(Z).

ˆ
where δn is chosen to approximately meet the false alarm
constraint based on the weak convergence of the test statistic.
The main reason for popularity of the chi-square test is
the fact that the test statistic is easy to compute and that the
limiting behavior of the test statistic is known, which makes
it possible to set an approximate threshold for a target false
alarm probability, or to compute the p-value of the test statistic
[3]. We observe that in both the optimal tests (2) and (4), the
guarantees on the false alarm probability (pF A ) hold only in
the asymptotic sense as the sequence length goes to inﬁnity.
In (2) we are guaranteed that pF A will eventually go to zero
and in (4) we are guaranteed that pF A decays as exp(−nη).
However, in practice one has access to only a ﬁnite number of
data points and is interested in guaranteeing a constant upper
bound on the false alarm probability. For this, we need to be
able to choose the thresholds of these tests to meet a given
target false alarm level for a given sequence length. In the
following sections we derive weak-convergence results for the
test statistics used in (2) and (4) and demonstrate that these
results give good approximations to the actual false alarm
probabilities in these tests. We ﬁrst obtain a simpliﬁed form
of the test of (2).

:= lim inf −

n→∞

2(π(z) − ν(z))2
,
(π(z) + ν(z))

ˆ
φC (x, y) = I χ2 (Γx , Γy ) ≥ δn
n
λn
n

1
log pF A (φn |µ)
n
1
EMD (φ|π 1 , π 2 ) := lim inf − log pMD (φn |π 1 , π 2 ).
n→∞
n
Two versions of asymptotically optimal tests are known in
literature.
Shayevitz [1] studied this problem in the context of a
two-sensor network. The null hypothesis corresponds to the
scenario in which both sensors observe noise and the alternate
hypothesis corresponds to the scenario in which some phenomenon is present which leads to both sensors making observations from distinct distributions. One of the contributions
of [1] is a solution to the following optimization problem:
s.t.

(4)

The two sample chi-square test is given by

In the classical Neyman-Pearson formulation of hypothesis
testing one seeks to minimize the probability of missed detection subject to an upper bound on the probability of false
alarm. In our problem since we do not know the values of
µ, π 1 or π 2 , it is not possible to solve this problem exactly.
Instead, we have to use an asymptotic version. For this purpose
we consider the limit as m, n → ∞. We further assume
that m scales linearly in n as m = λn for some λ ≥ 1.
In this setting we use φn (x, y) to denote the test outcome
φλn,n (xλn , y n ). We deﬁne two kinds of error exponents. The
false alarm error-exponent and the missed-detection exponent
are deﬁned respectively as

supφ

˜
≥ δn

˜
where δn = η + O( log n ). Interestingly, both the optimal
n
sequences of tests of (2) and (4) do not depend on the true
values of π 1 and π 2 .
Although these optimal solutions are known, the test usually
used by statisticians is the two sample chi-square test. The chisquare distance between two distributions is deﬁned as

pF A (φm,n |µ) = Pµ (φm,n (xm , y n ) = 1).

EF A (φ|µ)

(3)

(2)

2

inf max{D(µ1 ν), D(µ2 ν)}

ν∈P(Z)

(6)

is achieved at a point ν ∗ that satisﬁes D(µ1 ν ∗ ) = D(µ2 ν ∗ ).
Furthermore, ν ∗ can be expressed in the form ν ∗ = αµ1 +
(1 − α)µ2 for some 0 ≤ α ≤ 1.

probability as a function of the threshold, we will now show
that in the asymptotic regime, it is possible to obtain weakconvergence results on the test statistics that can be used to
approximate the false alarm probability. All our results are
based on the following basic lemma.

Proof:
Deﬁne
the
function
f12 (ν)
:=
max{D(µ1 ν), D(µ2 ν)}. Since f12 (u) is ﬁnite when
u is the uniform distribution on Z we see that the value of
the optimization problem in (6) is ﬁnite. It is also easy to see
that without loss of optimality we can restrict the inﬁmum
to P12 (Z) := {ν ∈ P(Z) : supp(ν) ⊆ supp(µ1 ) ∪ supp(µ2 )}.
This is because for any ν ∈ P(Z) its restriction ν12 onto
supp(µ1 ) ∪ supp(µ2 ) satisﬁes D(µ1 ν12 ) ≤ D(µ1 ν) and
D(µ2 ν12 ) ≤ D(µ2 ν). Now P12 (Z) is a compact set,
the function f12 (.) is bounded below by 0 on P12 (Z),
and moreover the function f12 (.) is continuous in the
relative interior of the set P12 (Z). Thus the inﬁmum in
inf ν∈P12 (Z) f12 (ν) is achieved since the optimal value is ﬁnite
by the argument above.
Now (6) can be equivalently written as a convex problem:
minτ,ν
s.t.

Lemma IV.1. Suppose we are given a string x of observations
of length λn and another independent string y of length n both
drawn i.i.d. from the same distribution µ ∈ P(Z) such that µ
has full support over Z. Let Γx denote the empirical distriλn
bution of the observations in x and Γy denote the empirical
n
distribution of the observations in y. Let h : P(Z)×P(Z) → R
be a continuous real-valued function whose gradient and
Hessian are continuous in the neighborhood of (µ, µ). If the
directional derivative satisﬁes ∇h(µ, µ)T (ν1 − µ, ν2 − µ) = 0
for all ν1 , ν2 ∈ P(Z), then
n→∞

where M = ∇2 h(µ, µ) and Wλ and W are independent random vectors distributed as Wλ ∼ N (0, Σ ) and W ∼ N (0, Σ)
λ
with Σ = diag(µ) − µµT .

τ
D(µ1 ν) ≤ τ,
ν(x) = 1,

Wλ
W

d.

T
−−
2n(h(Γx , Γy ) − h(µ, µ)) − − → [Wλ , W T ]M
λn
n

D(µ2 ν) ≤ τ,
ν(x) ≥ 0, for all x ∈ Z.

1

1

Proof: Let Gn,x := n 2 (Γx − µ) and Gn,y := n 2 (Γy −
n
λn
µ). We know that Γx and Γy can be written as empirical
n
λn
averages of i.i.d. vectors. Hence, they satisfy the central limit
theorem which says that,

x∈Z

Let ν represent the optimizer of this problem. Considering
ˆ
the ﬁrst order condition for optimality in a Lagrange-relaxed
version of this problem it follows that there exists scalars ℓ1 ,
ℓ2 , and κ such that

d.

1

−−
Gn,x = n 2 (Γx − µ) − − → Wλ
λn

(8)

n→∞
d.

ℓ1 µ1 (x) + ℓ2 µ2 (x) = κˆ(x), for all x ∈ Z
ν

−−
Gn,y = n 2 (Γy − µ) − − → W
n

which implies that the optimizer ν can be expressed as an
ˆ
afﬁne combination of µ1 and µ2 . Now by the deﬁnition of
f12 (.) it further follows that ν can be expressed as a convex
ˆ
combination of µ1 and µ2 .
From the above lemma it follows that the test (2) can
equivalently be written as

where the distributions of W and Wλ are as deﬁned in the
statement of the lemma. Considering a second-order Taylor’s
expansion and using the condition on the directional derivative,
we have, for n large enough,

φA (x, y) = I {D(Γx αn Γx + (1 − αn )Γy ) ≥ δn }
n
λn
λn
n

1

2n(h(Γx , Γy ) − h(µ, µ))
λn
n

(7)

˜n ˜n
[GT , GT ]∇2 h(Γx , Γy )
n,y
n,x

=

where αn ∈ [0, 1] satisﬁes

Gn,x
Gn,y

˜
˜
where Γx = γΓx + (1 − γ)µ and Γy = γΓy + (1 − γ)µ for
n
n
n
λn
some γ = γ(n) ∈ [0, 1]. We also know by the strong law of
˜n
˜n
large numbers that Γx and Γy and hence Γx and Γy converge
n
λn
to µ almost surely. By the continuity of the Hessian, we have

D(Γx αn Γx +(1−αn )Γy ) = D(Γy αn Γx +(1−αn )Γy ).
λn
λn
n
n
λn
n
Furthermore, it is obvious that given Γx and Γy the value of
n
λn
αn can be easily computed by binary search since the function
g(α) := D(Γx αΓx + (1 − α)Γy ) − D(Γy αΓx + (1 −
n
n
λn
λn
λn
α)Γy ) is a monotonically decreasing function of α, and αn
n
can be approximated by the value of α at which the function
g(.) is approximately zero. Now let Zn := D(Γx αn Γx +
λn
λn
(1−αn )Γy ). Thus the test of (2) is just a threshold test on Zn .
n
Although the test (2) looks complicated, the discussion above
implies that the test statistic is in fact quite easy to compute.

a.s.
˜ ˜
∇2 h(Γx , Γy ) − − → ∇2 h(µ, µ).
n
n − −

(10)

n→∞

By applying the vector-version of Slutsky’s theorem [7],
together with (8), (9) and (10), we conclude that
˜ ˜
[GT , GT ]∇2 h(Γx , Γy )
n
n
n,x
n,y
d.

IV. W EAK

(9)

n→∞

Gn,x
Gn,y

T
− − → [Wλ , W T ]∇2 h(µ, µ)
−−

CONVERGENCE RESULTS

n→∞

In the classical Neyman-Pearson hypothesis testing problem, one chooses the threshold that guarantees some bound
on the false alarm probability of the test. Although it is not
tractable to obtain an exact evaluation of the false alarm

Wλ
W

,

thus establishing the lemma.
As an immediate consequence of the above lemma we have
the following result:

3

d.

8nλ
−−
Combining with (11) we get 1+λ Wn − − → χ2 −1 . By a
N
n→∞
similar argument it also follows that Vn := max{Y1n , Y2n }
d.
8nλ
satisﬁes 1+λ Vn − − → χ2 −1 . Thus by (15) we see that nZn
−− N
n→∞
is sandwiched between two random quantities having the same
weak convergence behavior. Thus nZn should also have the
same weak convergence limit.
We now consider the test of (4) proposed in [2]. The test
statistic in this test can be expressed as:

Lemma IV.2. Suppose we are given a string x of observations
of length λn and another independent string y of length n both
drawn i.i.d. from the same distribution µ ∈ P(Z) such that µ
1
has full support over Z. Let Y1n := D(Γx 2 (Γx + Γy )) and
n
λn
λn
x
y
n
y 1
Y2 := D(Γn 2 (Γλn + Γn )). Then the following results hold:
8nλ n
Y
1+λ 1

d.

−−→
−−

χ2 −1 ,
N

(11)

8nλ
d.
(λY1n + Y2n ) − − →
−−
n→∞
(1 + λ)2

χ2 −1 ,
N

(12)

0.

(13)

n(Y1n

−

Y2n )

n→∞

d.

−−→
−−
n→∞

Yn := λD(Γx
λn

d.

1
4µ

8nλ
d.
−− N
Y − − → χ2 −1
2 n n→∞
(1 + λ)

d.

Proposition IV.5. Assume that the data strings xm and y n
are drawn i.i.d. according to some ﬁxed distribution µ ∈ P(Z)
such that µ has full support on Z. Let Xn := χ2 (Γx , Γy ).
n
λn
Then we have
nλ
d.
−− N
(17)
Xn − − → χ2 −1
n→∞
1+λ
Proof: We apply Lemma IV.1 to the function f (π, ν) =
χ2 (π, ν). It is easily veriﬁed that the gradient and Hessian
satisfy the necessary regularity conditions. Computing the
Hessian at (µ, µ) we obtain


2
2
−diag µ
diag µ
.
M =
2
2
−diag µ
diag µ

Proposition IV.3. Assume that the data strings xm and y n are
drawn i.i.d. according to some ﬁxed distribution µ ∈ P(Z)
such that µ has full support on Z. Further assume that m
grows linearly in n as m = λn. Let αn and Zn be as before
with Zn = D(Γx αn Γx + (1 − αn )Γy ). Then if m grows
n
λn
λn
linearly in n as m = λn, we have

Following the same steps as in the proof of (11) in Lemma
IV.2, the conclusion follows.
We observe from Propositions IV.3, IV.4 and IV.5 that the
limiting distribution of the test statistics of all the three tests
φA of (7), φB of (4) and φC of (5) under the null hypothesis
depend only on the support size of the true distribution µ and
not on the speciﬁc value of µ. In the following section we
discuss how these weak convergence results can be used to
select the test thresholds for a target false alarm probability.

(14)

Proof: Let Y1n := D(Γx 1 (Γx + Γy )) and Y2n :=
n
λn 2
λn
+ Γy )). From Lemma III.1 we have
n

1
D(Γy 2 (Γx
n
λn

≤ Zn ≤

max{Y1n , Y2n }.

(16)

Similarly, we can also identify the limiting behavior of the
chi-square test statistic used in (5) via the results of Lemma
IV.1. Although this result is well known in statistics literature,
we provide a simple proof for completeness.

8nλ
1
Equivalently we can write 1+λ Y1n − − → W T diag µ W.
−−
n→∞
It can be shown using the result of [8, Lemma III.7] that
1
W T diag µ W has a χ2 −1 distribution thus proving (11).
N
Similarly for proving (12) we apply Lemma IV.1 to the
function h(π, ν) := λD(π 1 (π + ν)) + D(π 1 (π + ν)).
2
2
Computing the Hessian at (µ, µ) we see that the new Hessian
is just (1 + λ) times M . Thus the result of (12) by a similar
argument as before.
Now if we apply Lemma IV.1 to the function h(π, ν) :=
D(π 1 (π + ν)) − D(ν 1 (π + ν)), we see that the Hessian at
2
2
(µ, µ) vanishes. Hence (13) follows.
We are now ready to obtain the weak convergence behavior
of the test statistic Zn used in the test of (7).

min{Y1n , Y2n }

+ Γy )).
n

Proof: This is exactly the result of (12) in Lemma IV.2.

(Wλ − W ).

8nλ
d.
−− N
Zn − − → χ2 −1
n→∞
1+λ

x
1
2 (Γλn

Proposition IV.4. Assume that the data strings xm and y n
are drawn i.i.d. according to some ﬁxed distribution µ ∈ P(Z)
1
such that µ has full support on Z. Let Yn := λD(Γx 2 (Γx +
λn
λn
x
y
y
y 1
Γn )) + D(Γn 2 (Γλn + Γn )). Then we have

1
where diag 4µ denotes a diagonal matrix with the i-th
1
diagonal entry given by 4µi . Applying the conclusion of
Lemma IV.1 we obtain

n→∞

+ Γy )) + D(Γy
n
n

In the following theorem we characterize the limiting behavior
of this test statistic.

Proof: To prove (11) we apply Lemma IV.1 to the
function h(π, ν) := D(π 1 (π + ν)). It is easily veriﬁed
2
that the gradient and Hessian satisfy the necessary regularity
conditions. Computing the Hessian at (µ, µ) we obtain


1
1
−diag 4µ
diag 4µ

M =
1
1
−diag 4µ
diag 4µ

−−
2nY1n − − → (Wλ − W )T diag

x
1
2 (Γλn

V. A PPROXIMATE

(15)

THRESHOLDS

The weak convergence behavior of the test statistics in the
three tests we have considered can be used to approximately
choose the test threshold for a target false alarm probability.
For example in the chi-square test φC of (5) if under the null

Now if Wn := min{Y1n , Y2n }, then we have |Wn − Y1n | ≤
d.
|Y1n − Y2n |. Hence by (13) we have n(Wn − Y1n ) − − → 0.
−−
n→∞

4

1
0.9
Probability of false alarm →

alphabet size is allowed to increase with the sample size. Such
a setup is relevant in problems involving data from continuous
alphabet distributions which could be ﬁrst quantized and then
tested. The literature (see, e.g., [9] and references therein) on
the simpler single sample goodness-of-ﬁt problem could be
a good starting point for such an investigation. A different
direction for future work is to extend these results on twosample tests to more general k-sample tests and to evaluate
the asymptotic efﬁciency of these tests.

χ2 approximation
Test φA

0.8

Test φB
Test φC

0.7
0.6
0.5
0.4
0.3
0.2

ACKNOWLEDGEMENTS

0.1
0
10

20

30

40

50
60
Sample size n →

70

80

The author thanks Dayu Huang for useful discussions.
This research was supported by ERC Advanced Investigators
Grant: Sparse Sampling: Theory, Algorithms and Applications
SPARSAM no 247006.

90

Fig. 1. False alarm probabilities of the various tests are shown along with
the χ2 approximation of these error probabilities obtained using the weak
convergence result.

R EFERENCES
[1] O. Shayevitz, “On R´ nyi measures and hypothesis testing,” in Proc.
e
of International Symposium on Information Theory (ISIT 2011), St.
Petersburg, 2011.
[2] M. Gutman, “Asymptotically optimal classiﬁcation for multiple tests
with empirically observed statistics,” IEEE Transactions on Information
Theory, vol. 35, no. 2, pp. 401–408, 1989.
[3] E. L. Lehmann and J. P. Romano, Testing statistical hypotheses, 3rd ed.,
ser. Springer Texts in Statistics. New York: Springer, 2005.
[4] P. Greenwood and M. Nikulin, A guide to chi-squared testing, ser.
Wiley series in probability and mathematical statistics. Probability and
mathematical statistics. New York: John Wiley & Sons, 1996.
[5] K. Pearson, “On the probability that two independent distributions
of frequency are really samples from the same population,”
Biometrika, vol. 8, no. 1/2, pp. 250–254, 1911. [Online]. Available:
http://www.jstor.org/stable/2331453
[6] I. Csisz´ r and P. C. Shields, “Information theory and statistics: A tutorial,”
a
Foundations and Trends in Communications and Information Theory,
vol. 1, no. 4, 2004.
[7] P. Billingsley, Convergence of Probability Measures. New York: John
Wiley & Sons, 1968.
[8] J. Unnikrishnan, D. Huang, S. Meyn, A. Surana, and V. Veeravalli,
“Universal and composite hypothesis testing via mismatched divergence,”
Information Theory, IEEE Transactions on, vol. 57, no. 3, pp. 1587 –1603,
March 2011.
[9] P. Harremoes and I. Vajda, “On the Bahadur-Efﬁcient Testing of Uniformity by Means of the Entropy,” IEEE Transactions on Information
Theory, vol. 54, no. 1, pp. 321 –331, Jan. 2008.

hypothesis the observations are drawn from some distribution
µ ∈ P(Z) with full support, then the test statistic Xn satisﬁes
lim Pµ {

n→∞

nλ
Xn > x} = 1 − F (x)
(1 + λ)2

where F (x) denotes the cdf of a chi-square random variable
with N − 1 degrees of freedom. This relation can be used
to approximate the threshold to be used in (5) for a target
false alarm probability, by approximating the true probability
with the limiting probability. Similarly, the thresholds for the
optimal tests φA of (7) and φB of (4) can be chosen using the
weak convergence of their respective test statistics.
In order to estimate the accuracy of the approximation
obtained from the weak convergence, we simulated the three
tests using a uniform distribution over an alphabet of size 8 for
µ. In Figure 1 we have plotted the false alarm probabilities of
the three tests as a function of the sequence length n obtained
by simulations. In the same ﬁgure we also have a plot of
the approximate value of the false alarm probability computed
using the weak convergence approximation suggested in the
previous paragraph. Clearly, that the error predictions obtained
via the weak-convergence approximations are quite accurate
for values of n greater than 45.
VI. S UMMARY

AND

F UTURE W ORK

We have studied the homogeneity testing problem for
multinomial distributions. Although optimal results have been
proposed for this problem in information theory literature, such
results are not well-known among statisticians and such tests
are rarely used in practice. In this paper, we have simpliﬁed
the structure of one of these tests and also identiﬁed the
limiting behavior of the test statistics used in both the tests.
These results can be used to approximate the thresholds for
these tests. Such homogeneity tests with provable optimality
properties could potentially be better choices than the chisquare test in practice.
In terms of future work it would be of interest to identify
the optimal tests for this problem in the setting in which the

5

