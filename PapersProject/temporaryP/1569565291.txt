Title:          isit2012_120502.dvi
Creator:        dvips(k) 5.991 Copyright 2011 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May 16 10:39:00 2012
ModDate:        Tue Jun 19 12:56:40 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      279383 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565291

Multi-stage Coding for Channels With a Rewrite
Option and Reversible Input
Kittipong Kittichokechai, Tobias J. Oechtering, and Mikael Skoglund

n
X1

Enc1

M1
M2
M1
M2
M3

Fig. 1.

Y1n

n
X2

Enc2
n
X1

Y2n

n
X3

Enc3
n
n
X1 , X2 , Y1n

PY3 |X3 ,Y2

M1

PY2 |X2 ,Y1

Abstract—We consider a problem of constrained multi-stage
coding for channels with a rewrite option. It is a natural extension
of Weissman’s channels with action-dependent states to the multistage coding case where an encoder in each stage observes its
own message as well as all previous-stage messages, inputs, and
outputs. In addition to decoding all messages at the ﬁnal stage,
the new reconstruction constraint introduced in Sumszyk and
Steinberg’s information embedding with reversible stegotext is
imposed on the problem such that the decoder is required to
be able to reconstruct all channel input sequences reliably. The
complete characterization of the channel capacity region is given
for the two-stage case, while the inner and outer bounds to
the capacity regions for the cases of three or more stages are
provided. For the two-stage case, a discussion regarding the rate
constraint of the message in the second stage is also given in which
we can draw a connection to the two-stage coding condition which
appears in our previous study on channel with action-dependent
state and reversible input.

PY1 |X1

School of Electrical Engineering and the ACCESS Linnaeus Center
KTH Royal Institute of Technology, Stockholm, Sweden

ˆ ˆ ˆ
M1 , M2 , M3

Y3n
Dec

ˆn ˆn ˆn
X1 , X2 , X3

Three-stage channel with rewrite option and reversible input.

(reversible stegotext). As with action-dependent coding, also
the framework of additional reconstruction requirements provides new useful features of simultaneous signal transmission
in the general system model.
In our recent works [10], [11], we considered the combined
setting of Weissman [3] and Sumszyk and Steinberg [9] where
the channel state is allowed to depend on the action sequence
and the decoder is interested in decoding both message and
channel input signal reliably. In [11] we provided a complete
characterization of the channel capacity in which we have
shown that the two-stage coding condition plays a role in
restricting the set of capacity achieving input distributions.
In fact, this condition arises from the causal structure of the
system in which we require to reconstruct the signal generated
in the second stage. We also showed by an example that the
condition can be active in computing the capacity.
To gain more insight into the structure of the problem
as well as the role of the two-stage coding condition, in
this work we study a natural extension of the previously
considered problem in which we add more encoding stages
(K stages) and in each stage a channel encoder observes its
own message and all previous-stage messages. The channel
output of each transmission stage will play a role of channel
state in the next-stage transmission and it is assumed to be
available noncausally to the next-stage encoder as a channel
state information. The encoder in each stage is assumed to
have memory, i.e., all previous channel inputs and outputs are
known. At the ﬁnal stage, the decoder is interested in decoding
all messages as well as all channel input sequences. See Fig. 1
for the case of K = 3. This setup can be seen as a multi-stage
coding for channels with a “rewrite option” based on the noisefree feedback, i.e., the writing output is given noncausally to
the next-stage encoder as a channel state information, and the
encoder has an option to make another pass where it may
rewrite at whichever locations it chooses [3]. Our setting is

I. I NTRODUCTION
The problem of coding for channels with random states
has received considerable attention due to its broad set of
applications, e.g., in a computer memory with defective cells,
digital watermarking, etc. In [1] Gel’fand and Pinsker solved
the problem of coding for channels with states where the states
are known noncausally at the encoder. Several extensions are
then considered such as the cases with causal, two-sided, and
partial state information [2].
In most of the previous works on coding for channels
with random states, the channel states are usually assumed to
be generated by nature, i.i.d. according to some distribution.
Recently, Weissman in [3] considered a problem in which the
channel state is allowed to depend on a message-dependent
action sequence. This novel action-dependent coding framework introduces new interesting features to the general system
model involving cost-constrained communication and interaction among nodes, and is therefore highly relevant to many
applications including sensor networking and control, and
multistage coding for memories. It is also closely related to the
problem of a MAC with common message and states at one
transmitter as mentioned in [3], and it has some connection
to the relay with unlimited lookahead channel [4]. Additional
work on coding with action includes [5], [6], [7], and [8].
Another interesting extension of the Gel’fand-Pinsker problem is studied by Sumszyk and Steinberg [9] in the context
of information embedding where apart from decoding the
embedded message, the decoder has an additional constraint
on reconstructing the channel input signal (stegotext) reliably

1

motivated by scenarios in multiple writing on a memory cell
with a rewrite option where the decoder is interested in both
decoding the embedded messages and in tracing what has been
written in the previous stages (reversible input).
We give a single-letter characterization of the capacity
region for the case of two-stage coding (K = 2). Interestingly but perhaps not surprisingly, this helps to establish a
connection to the result in our previously studied problem
on channel with action-dependent state and reversible input.
That is, the two-stage coding condition noted in [10], [11] can
simply be seen as a degenerate rate constraint derived from
the underlying rate constraint in this multi-stage coding setup.
For K ≥ 3, we provide inner and outer bounds to the capacity
region. The inner bound can be achieved by a straightforward
extension of the coding scheme for the case of K = 2. As for
proving the tightness of bounds, we ﬁnd it challenging due
to the structure of our problem formulation which prevents
us from having the desired Markov chains to account for the
dependency of the channel states on the channel outputs.
The rest of the paper is organized as follows. In Section II
we formulate the problem and present the capacity region
for the two-stage case. The discussion on how the result is
related to another variation of the problem is then given.
Section III discusses the connection between a rate constraint
in the second stage transmission and the two-stage coding
condition noted in [10], [11]. Lastly, inner and outer bounds
to the capacity region for K ≥ 3 is given in Section IV.
II. C APACITY R EGION

FOR

messages and channel inputs based on the ﬁnal-stage channel
n
output YK . The channels in all stages i = 1, . . . , K are
assumed to be memoryless and used without feedback with
transition probabilities,
n
n
n
n
n
PYin |Xi ,Yi−1 (yi |xn , yi−1 ) =
i
(n)

(n)

fi

(n)

(n)

(n)

and a channel input decoder
n
n
n
(n)
n
gx : YK → X1 × X2 × . . . × XK .

Deﬁnition 2: A rate tuple (R1 , . . . , RK ) is said to be
achievable if for any δ > 0 there exists for all sufﬁciently large
(n)
(n)
(n)
1
n an (|M1 |, . . . , |MK |, n)-code such that n log |Mi | ≥
(n)
(n)
Ri − δ, for i = 1, . . . , K, Pm,e ≤ δ, and Px,e ≤ δ,
(n)
(n)
where Pm,e and Px,e are the average error probabilities that
(n)
n
n
n
n
(M1 , M2 , . . . , MK ) = gm (YK ) and (X1 , X2 , . . . , XK ) =
(n)
n
(K)
gx (YK ), respectively. The capacity region C
is the closure
of the set of all achievable rate tuples.
We ﬁrst consider the case of K = 2 for which we can derive
the capacity region. In this case, the message M1 is given to
both encoder 1 and 2, and M2 is given only to the encoder
n
2. The ﬁrst-stage channel input sequence X1 is chosen based
on the message M1 and is the input to the ﬁrst-stage channel
which is described by a triple (X1 , PY1 |X1 , Y1 ). The output Y1n
then becomes the channel state of the second-stage channel
which is described by a quadruple (X2 , Y1 , PY2 |X2 ,Y1 , Y2 ),
and is also given to the encoder 2 noncausally. The decoder
n
n
decodes the messages (M1 , M2 ) and the input (X1 , X2 )
n
based on the ﬁnal-stage channel output Y2 .
B. Main Result
Theorem 1: The capacity region C (2) in the two-stage setup
is given by the set of all (R1 , R2 ) satisfying
R1 + R2 ≤ I(X1 , X2 ; Y2 ) − I(X2 ; Y1 |X1 ),
R2 ≤ I(X2 ; Y2 |X1 ) − I(X2 ; Y1 |X1 ),

(1)
(2)

for some joint distribution of the form
PX1 (x1 )PY1 |X1 (y1 |x1 )PX2 |X1 ,Y1 (x2 |x1 , y1 )·

Notation: We denote the discrete random variables, their corresponding
realizations or deterministic values, and their alphabets by the upper case,
j
lower case, and calligraphic letters, respectively. The term Xi denotes the
tuple (Xi , . . . , Xj ) when i ≤ j, and the empty set otherwise. When it
n
is clear from the context, we use the shorthand notation Xi or X i for
n
a length-n sequence Xi,1 . The term X j denotes the vector of lengthi
X
n sequences (X i , . . . , X j ) when i ≤ j, and the empty set otherwise,
j
n
where X i = Xi,1 = (Xi,1 , . . . , Xi,n ). The term (Xi )l denotes
n\l

n
n
× X1 × . . . × Xi−1 ×

(n)
n
gm : YK → M1 × M2 × . . . × MK ,

Let n denote the block length and Xi , Yi , i = 1, . . . , K
be ﬁnite sets. The setting consists of K encoders and one
(n)
decoder. Message Mi chosen uniformly from the set Mi =
(n)
{1, 2, . . . , |Mi |} is given to the encoders i, . . . , K, for all
i = 1, . . . , K. The encoder in each stage is assumed to observe
as well all previous-stage inputs and outputs. A set of channels
is described by a set of tuples {(Xi , Yi−1 , PYi |Xi ,Yi−1 , Yi )},
for i = 1, . . . , K, where Xi , Yi , and PYi |Xi ,Yi−1 are the input
alphabet, the output alphabet, and the transition probability
from Xi × Yi−1 to Yi in stage i, respectively. The decoder,
which might be considered as two separate decoders, i.e.,
message decoder and channel input decoder, decodes all the

, . . . , Xj

(n)

a message decoder

A. Problem Formulation

n\l

(n)

: M1 × . . . × Mi

n
n
Y1 × . . . × Yi−1 → Xin , i = 1, . . . , K

We ﬁrst provide the problem formulation for the general
K-stage setting. Then we will focus on the case of K = 2
where we are able to derive the capacity result. The discussion
on how the result is related to other problems is also given.

n\l
Xi

(n)

Deﬁnition 1: An (|M1 |, . . . , |MK |, n) code for the
channels {PYi |Xi ,Yi−1 }i=1,...,K consists of the following functions: encoders

T WO - STAGE C ASE , K = 2

j
(Xi,l , . . . , Xj,l ), and similarly (Xi )n\l denotes (Xi

PYi |Xi ,Yi−1 (yi,l |xi,l , yi−1,l ).
l=1

PY2 |X2 ,Y1 (y2 |x2 , y1 )
Proof: The achievability proof is based on a standard
random coding argument using Gel’fand-Pinsker coding, and
is a straightforward extension of that in [10]. The sketch of the
proof for a general K-stage problem is given in the appendix.
For the converse proof we refer readers to the one given for
the K-stage problem, K ≥ 3, in Section IV in which we have
to apply with some modiﬁcations at the end of the proof.

), where

= (Xi,1 , . . . , Xi,l−1 , Xi,l+1 , . . . , Xi,n ).

2

reduces to the problem of channel with action-dependent state
and reversible input. In [11], the channel capacity is given in
the form with a constraint on the set of input distributions
I(X2 ; Y2 |X1 ) − I(X2 ; Y1 |X1 ) ≥ 0 which is called the “twostage coding condition.” This condition arises essentially from
the two-stage coding structure of the problem and the extra
requirement that the decoder should be able to reconstruct
the signal generated in later stages. In addition to the rate
constraint on the message, the two-stage coding condition
can be interpreted as a necessary and sufﬁcient condition
for reliable transmission of the channel input signal over the
channel in second stage transmission.
By studying the multi-stage coding problem in this paper,
we can straightforwardly connect the two-stage coding condition to the result for K = 2 in this paper. That is, the two-stage
coding condition can be seen as a degenerate rate constraint
derived from the underlying rate constraint of the message M2
in the multi-stage coding setting. It is therefore not surprising
that this condition is present in the capacity expression in [11],
and in fact it can be active when computing the capacity as
shown in an example in [12].

C. Related Result
So far we have characterized the capacity region of the twostage problem formulated with the reversible input requirement. It is also natural to consider a new and slightly different
communication problem where the decoder is interested in
decoding the messages (M1 , M2 ) and the “channel state”
Y1n instead. Due to the deterministic encoding functions, the
n
n
channel input (X1 , X2 ) can be retrieved based on the decoded
messages and state information. We note that this communication problem has a more demanding reconstruction constraint
than our previous problem since it essentially requires that the
decoder can decode the message, the state, and the channel
input signal, all reliably. We show that if the objective is to
decode only the message and the channel input, then decoding
the message and the state ﬁrst, and then re-encoding the
channel input is suboptimal.
Proposition 1: Consider the new two-stage communication
problem in which the decoder is interested in decoding the
messages (M1 , M2 ) and the “channel state” Y1n reliably. The
(2)
capacity region CY1 of such a channel is given by the set of
all (R1 , R2 ) satisfying
R1 + R2 ≤ I(X1 , Y1 , X2 ; Y2 ) − H(Y1 |X1 ),
R2 ≤ I(Y1 , X2 ; Y2 |X1 ) − H(Y1 |X1 ),

IV. B OUNDS TO C APACITY R EGION FOR THE C ASE K ≥ 3
We now consider an extension to the multi-stage setting in
which we characterize the inner and outer bounds to the capacity region. The inner bound can be derived as a straightforward
extension of the two-stage setting, while we ﬁnd it challenging
to generalize the converse proof for K ≥ 3. Based on our
setting, the main difﬁculty in deriving tight bounds lies in the
structure of the problem which prevents us from having the
desired Markov chains to account for the dependency between
the channel outputs and the channel states.

(3)
(4)

for some joint distribution of the form
PX1 (x1 )PY1 |X1 (y1 |x1 )PX2 |X1 ,Y1 (x2 |x1 , y1 )·
PY2 |X2 ,Y1 (y2 |x2 , y1 ).

(5)

Proof: Since decoding (M1 , M2 ) and Y1n implies that
n
n
(X1 , X2 ) is also decoded from the deterministic encoding
functions, one can substitute (Y1 , X2 ) in place of X2 in
Theorem 1 and obtain the new capacity region. More speciﬁcally, the achievable scheme in this case is different from the
n
n
previous case of decoding (M1 , M2 ) and (X1 , X2 ) in that the
state information codebook is introduced and it should “cover”
all possible generated Y1n losslessly. Due to space constraint,
the detailed achievability and converse proofs are omitted.
Remark 1: We know that the channel input sequence can be
retrieved based on the decoded message, the state information,
and a known deterministic encoding function. Therefore, it is
natural to compare the capacity region C (2) in Theorem 1 with
(2)
CY1 in Proposition 1. For a given channel PY1 |X1 , PY2 |X2 ,Y1 ,
(2)
we have that C (2) ⊇ CY1 .
Proof: One can show that both terms on the right hand
side of (1) and (2) are greater than or equal to those of (3)
and (4) for all joint distributions factorized as in (5), and thus
(2)
conclude that C (2) ⊇ CY1 .
III. C ONNECTION

TO

A. Main Results
Theorem 2 (Inner Bound to the Capacity Region): The
rate tuple (R1 , . . . , RK ) is achievable if
K

K
i−1
i−1
I(Xi ; YK |X1 ) − I(Xi ; Y1i−1 |X1 ),

Ri ≤
i=j

i=j

for all j = 1, . . . , K, and some joint distribution of the form
K

PXi |X1 ,...,Xi−1 ,Y1 ,...,Yi−1 (xi |x1 , . . . , xi−1 , y1 , . . . , yi−1 )·
i=1

PYi |Xi ,Yi−1 (yi |xi , yi−1 )

(6)

Proof: The proof is a straightforward extension of the
achievability proof for the two-stage case and its sketch is
given in the appendix.
Theorem 3 (Outer Bound to the Capacity Region): For
any achievable rate tuple (R1 , . . . , RK ), K ≥ 3, it follows
that

T WO - STAGE C ODING C ONDITION

K

In this section we will relate our main result for the case
of K = 2 to our previous result on capacity of channels
with action-dependent state and reversible input considered
in [10], [11]. It is obvious from the problem formulation that
if the rate of the message M2 is zero, i.e., no new message
to be transmitted at the encoder 2, then our setting simply

j−1
K
Ri ≤ I(Xj , YK−1 ; YK |X1 , Y1j−2 )
i=j
K
i−1
I(Xi ; Yi−1 |X1 , Y1i−2 ),

−
i=j

3

i−1
Next, we replace Wi by (M1 , X i−1 , Y i−2 ) again and
1
1
continue the chain of inequalities.

for all j = 1, . . . , K − 1, and
K−1
RK ≤ I(XK ; YK |X1 , Y1K−2 )

K

K−1
− I(XK ; YK−1 |X1 , Y1K−2 ),

Ri − nǫ(j)
n

n

i=j
n

(e)

for some joint distribution of the same form as in (6).
Proof: For any achievable rate tuple (R1 , . . . , RK ), we
have that for all j = 1, . . . , K,

j−1
K
n
I(Mj , X K , Y K−2 , YK−1,l+1 ; YK,l |M1 ,
j
j−1

≤
l=1

l−1
X j−1 , Y j−2 , YK,1 )
1
1
K

K

Ri − nǫ(j)
n

n

l−1
i−1
I(Mi , X i , YK,1 ; Yi−1,l |M1 , X i−1 ,
1

−
i=j

i=j

n
Y i−2 , Yi−1,l+1 )
1

(a)

K
K
≤ H(Mj |Wj ) − H(Mj , X K |Y K , Wj )
j Y

n

(f )

K
K
Y j−1
= H(Mj , Y K−1 |Wj ) − H(Y K−1 |Mj , Wj )
j−1

j−1
K−2
K
I(ZK , (Xj )l , (Yj−1 )l ; YK,l |(X1 )l , (Y1j−2 )l )

≤

K
− H(Mj , X K |Y K , Wj )
j Y

l=1
K

(b)

K
K
Y j−1
= H(Mj , Y K−1 , X K |Wj ) − H(Y K−1 |Mj , Wj )
j
j−1

i−1
I(Zi , Xi,l ; Yi−1,l |(X1 )l , (Y1i−2 )l ) ,

−
i=j

K
− H(Mj , X K |Y K , Wj )
j Y
(c) K

≤

where (e) holds using a telescoping series to bound the
sum from i = j to i = K of the ﬁrst mutual information term, (f ) follows since conditioning reduces entropy,
i−1
and from the Markov chain Yi−1,l − ((X1 )l , (Y1i−2 )l ) −
i−1
i−1 n\l
i−2 n\l
n
(M1 , (X1 ) , (Y1 ) , Yi−1,l+1 ) and by deﬁning Zi
l−1
i
i
n
(M1 , (X1 )n\l , (Y1i−2 )n\l , Yi−1,l+1 , YK,1 ), i = 2, . . . , K.
For j = 1, . . . , K − 1, we continue the chain of inequalities.

Y
H(Mi , Y i−1 , X i |Wi ) − H(Y i−1 |Wi )

i=j

Y
− H(Mi , X i |Y K , Wi )
K

=

I(Mi , X i ; Y K |Wi ) − I(Mi , X i ; Y i−1 |Wi ),
i=j

K

K
where (a) follows from the independency Mj ⊥Wj , where
j−1
j−1
j−2
Wj
(M1 , X 1 , Y 1 ), and from Fano’s inequality,
(j)
(j)
K
i.e., H(Mj , X K |Y K , Wj ) ≤ nǫn , limn→∞ ǫn = 0,
j Y
(b) follows from the deterministic encoding function X i =
(n)
i
fi (M1 , X i−1 , Y i−1 ), i = j, . . . , K, (c) holds since X i =
1
1
(n)
i
fi (M1 , X i−1 , Y i−1 ), and Y i−1 −Wi −MiK forms a Markov
1
1
i−1
chain with Wi = (M1 , X i−1 , Y i−2 ), and conditioning
1
1
reduces entropy. Note that the term Wi is introduced for the
sake of readability and it essentially captures the previous
variables known before the encoding stage i.
Continuing the chain of inequalities, we get

Ri − nǫ(j)
n

n

i=j
(♯) n

≤

j−1
K
I((Xj )l , YK−1,l ; YK,l |(X1 )l , (Y1j−2 )l )

l=1
K
i−1
I(Xi,l ; Yi−1,l |(X1 )l , (Y1i−2 )l )

−
i=j

where (♯) holds since conditioning reduces entropy and
K
we have the Markov chains YK,l − ((X1 )l , YK−1,l ) −
K−2
((Y1
)l , ZK ). As for j = K, we have
n

nRK − nǫ(K) ≤
n

K

Ri −

n

nǫ(j)
n

l=1
K−1
− I(ZK , XK,l ; YK−1,l |(X1 )l , (Y1K−2 )l )

i=j
n

K

n

l−1
I(Mi , X i ; YK,l |Wi , YK,1 )

≤

l=1

n
− I(Mi , X i ; Yi−1,l |Wi , Yi−1,l+1 )
n

K−1
− I(ZK , XK,l ; YK−1,l |(X1 )l , (Y1K−2 )l , YK,l )

K

(∗)

l−1
n
I(Mi , X i , Yi−1,l+1 ; YK,l |Wi , YK,1 )

=

n
K−1
I(XK,l ; YK,l |(X1 )l , (Y1K−2 )l , YK−1,l )

≤

l=1 i=j

−

K−1
I(ZK , XK,l ; YK,l |(X1 )l , (Y1K−2 )l , YK−1,l )

=

l=1 i=j

(d)

K−1
I(ZK , XK,l ; YK,l |(X1 )l , (Y1K−2 )l )

l=1
K−1
− I(XK,l ; YK−1,l |(X1 )l , (Y1K−2 )l , YK,l )

l−1
n
I(Mi , X i , YK,1 ; Yi−1,l |Wi , Yi−1,l+1 ),

n
K−1
I(XK,l ; YK,l |(X1 )l , (Y1K−2 )l )

=

where (d) follows from Csisz´ r’s sum identity [13], i.e., for
a
n
l−1
n
I(Yi−1,l+1 ; YK,l |Wi , Mi , X i , YK,1 ) −
i = j, . . . , K,
l=1
l−1
n
I(YK,1 ; Yi−1,l |Wi , Mi , X i , Yi−1,l+1 ) = 0.

l=1
K−1
− I(XK,l ; YK−1,l |(X1 )l , (Y1K−2 )l ),

4

n

where (∗) follows from the Markov chain YK,l −
K
((X1 )l , (Y1K−1 )l ) − ZK . Note that when K = 2, the above
steps for j = 1 will not result in the desired expression. To ﬁx
this, we can use similar steps as in the case j = K for both
j = 1, 2 instead.
To this end, we introduce a random variable Q distributed
uniformly over {1, . . . , n} and independent of all others.
We have PYi,Q |Xi,Q ,Yi−1,Q = PYi |Xi ,Yi−1 for all Q, and
i = 1, . . . , K, and we can identify Xi,Q Xi and Yi,Q Yi .
i−1
Since Yi −(Xi , Yi−1 )−(X1 , Y1i−2 , Q) forms a Markov chain
for all i = 1, . . . , K, we have

each i.i.d. according to
l=1 PX2 |X1 (x2,l |x1,l (m1 )).
′
n
n
Next, for each pair of (X1 , X2 ), generate 2n(R3 +R3 )
n
′
′
codewords X3 (m1 , m2 , m2 , m3 , m3 ) each i.i.d. according
n
to
We continue this
l=1 PX3 |X1 ,X2 (x3,l |x1,l , x2,l ).
′
process until we have generated 2n(RK +RK ) codewords
n
n
n
XK (m1 , m2 , m′ , . . . , mK , m′ ) for all (X1 , . . . , XK−1 ).
2
K
Given the messages (m1 , . . . , mi ) and the previous inputs
n
n
n
and outputs (X1 , . . . , Xi−1 ), (Y1n , . . . , Yi−1 ), i = 1, . . . , K,
the encoder in stage i looks for and transmits the codeword
n
n
n
Xin that is jointly typical with (X1 , . . . , Xi−1 , Y1n , . . . , Yi−1 ).
By joint typicality property (covering lemma) [4], we ensure
′
the vanishing probability of encoding error as n → ∞ if Ri >
I(Xi ; Y1 , . . . , Yi−1 |X1 , . . . , Xi−1 ) for all i = 2, . . . , K.
n
Given the ﬁnal-stage output YK , the decoder looks for
n
n
(X1 , . . . , XK ) that is jointly typical with it. By joint typicality property (packing lemma) [4], we ensure the vanishing
′
probability of decoding error as n → ∞ if R1 + R2 + R2 +
K
′
′
. . . + RK + RK < I(X1 , . . . , XK ; YK ), and i=j Ri + Ri <
I(Xj , . . . , XK ; YK |X1 , . . . , Xj−1 ) for all j = 2, . . . , K.
By these results together with random coding argument,
we obtain that the rate tuple (R1 , . . . , RK ) is achievable if
K
K
i−1
i−1
i−1
|X1 ) for
i=j Ri ≤
i=j I(Xi ; YK |X1 ) − I(Xi ; Y1
all j = 1, . . . , K.

K
j−1
K
Ri − ǫ(j) ≤ I(Xj , YK−1 ; YK |X1 , Y1j−2 )
n
i=j
K
i−1
I(Xi ; Yi−1 |X1 , Y1i−2 ), for all j = 1, . . . , K − 1,

−
i=j

K−1
and RK − ǫ(K) ≤ I(XK ; YK |X1 , Y1K−2 )
n
K−1
− I(XK ; YK−1 |X1 , Y1K−2 ).

The proof is concluded by letting n → ∞.
Remark 2: In general we do not know how good the inner
and outer bounds are for the case of K ≥ 3. However, for
a degenerate case where in each stage the channel output
is conditionally independent of the state given the input, the
bounds will coincide and provide the result of the point-topoint case as expected from inspection, i.e., the optimal input
distribution turns out to be the one which is independent of
all previous states and inputs.
V.

ACKNOWLEDGEMENT

The authors wish to thank the anonymous reviewers and
Hieu Do for helpful comments and discussions. This work
was supported in part by the Swedish Research Council and
the Swedish Foundation for Strategic Research.
R EFERENCES

CONCLUSION

[1] S. I. Gel’fand and M. S. Pinsker, “Coding for channel with random
parameters,” Probl. Contr. and Inf. Theory, vol. 9, no. 1, pp. 19-31, 1980.
[2] G. Keshet, Y. Steinberg, and N. Merhav, “Channel coding in the presence
of side information,” Found. Trends Commun. Inf. Theory, vol. 4, no. 6,
pp. 445-586, 2007.
[3] T. Weissman, “Capacity of channels with action-dependent states,” IEEE
Trans. Inf. Theory, vol. 56, no. 11, pp. 5396-5411, Nov. 2010.
[4] A. E. Gamal and Y. H. Kim, Network Information Theory. Cambridge
University Press, 2012.
[5] H. Permuter and T. Weissman, “Source coding with a side information
“Vending Machine”,” IEEE Trans. Inf. Theory, vol. 57, no. 7, pp. 45304544, 2011.
[6] H. Asnani, H. Permuter, and T. Weissman, “Probing capacity,” IEEE
Trans. Inf. Theory, vol. 57, no. 11, pp. 7317-7332, 2011.
[7] Y.-K. Chia, H. Asnani, and T. Weissman, “Multi-terminal source coding
with action dependent side information,” in Proc. IEEE Int. Symp.
Information Theory, St. Petersburg, Russia, July. 2011.
[8] B. Ahmadi and O. Simeone, “Robust coding for lossy computing with
receiver-side observation costs,” in Proc. IEEE Int. Symp. Information
Theory, St. Petersburg, Russia, July. 2011.
[9] O. Sumszyk and Y. Steinberg, “Information embedding with reversible
stegotext,” in Proc. IEEE Int. Symp. Information Theory, Seoul, 2009.
[10] K. Kittichokechai, T. J. Oechtering, and M. Skoglund, “On the capacity
of a channel with action-dependent state and reversible input,” in Proc.
IEEE Int. Symp. Information Theory, St. Petersburg, Russia, July. 2011.
[11] K. Kittichokechai, T. J. Oechtering, and M. Skoglund, “Capacity of the
channel with action-dependent state and reversible input,” in Proc. IEEE
Swedish Communication Technologies Workshop (Swe-CTW), 2011.
[12] K. Kittichokechai, T. J. Oechtering, and M. Skoglund, “Coding with
action-dependent side information and additional reconstruction requirements,” submitted to IEEE Trans. Inf. Theory, 2012.
[13] I. Csisz´ r and J. K¨ rner. Information Theory: Coding Theorems for
a
o
Discrete Memoryless Systems. New York: Academic, 1981.

In this work we studied an extension of the problem of
coding with action-dependent state and reversible input to the
multistage setting. The capacity region for the two-stage case
is found, and inner and outer bounds to the capacity region
are given in the case of three stages or more. We were not
able to establish bounds that match in the case K ≥ 3 due to
the structure of our problem which prevents us from having
the desired Markov chains to account for the dependency
of the channel states on the channel outputs. For the twostage problem, it turned out that the capacity result helps us
to establish a connection to our previous result on the twostage coding condition. It shows that the two-stage coding
condition is simply the degenerate rate constraint derived from
the underlying rate constraint on the message in the second
stage. Our inner and outer bounds also suggest the presence of
similar two-stage coding conditions in the multi-stage setting,
i.e., the rate constraint on the message in later stages implicitly
includes the condition for reliable transmission of the channel
input signals over the channels in later stages.
A PPENDIX : P ROOF

OF

T HEOREM 2

Fix K PXi |X1 ,...,Xi−1 ,Y1 ,...,Yi−1 . The codebook generai=1
n
tion consists of randomly generating 2nR1 codewords X1 (m1 )
n
each i.i.d. according to l=1 PX1 (x1,l (m1 )), and for each
′
n
n
X1 (m1 ), generating 2n(R2 +R2 ) codewords X2 (m1 , m2 , m′ )
2

5

