Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Mon May  7 12:25:31 2012
ModDate:        Tue Jun 19 12:54:07 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      395286 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569564257

Data Processing Lower Bounds for Scalar Lossy
Source Codes with Side Information at the Decoder
Avraham Reani and Neri Merhav
Department of Electrical Engineering
Technion - Israel Institute of Technology
Technion City, Haifa 32000, Israel
Emails: [avire@tx,merhav@ee].technion.ac.il

related work is [19], where certain degrees of freedom of the
Ziv-Zakai generalized mutual information were exploited in
order to get better bounds.
We start by presenting the relevant random variables of the
WZ problem as a Markov chain. Then, using a data processing
theorem, we get lower bounds on the distortion in this case.
We show that replacing the logarithmic function with other
functions, may give better bounds for the distortion of delay–
limited coding (in particular, for scalar coding) in the WZ
setting. Examples of non-trivial lower bounds for scalar coding
in this setting, were obtained using the convex function Q(t) =
t−s , s > 0, which is equivalent to using the R´ nyi information
e
measure. These results will be given in detail in Section II. The
importance of such bounds stems from the fact that ﬁnding
the optimal scalar code in the WZ setting is in general an
hard problem. In fact, it is a problem of ﬁnding an optimal
partition of the source alphabet and this partition does not
necessarily correspond to intervals. A main objective will be
to use these bounds for studying the performance of concrete
coding schemes. In the next step, a possible direction will be
to extend our setting to more general scenarios, for example,
to scenarios where the output of the encoder is transmitted
over a noisy channel (instead of the noiseless one in the WZ
setting), and to scenarios of coding with memory.

Abstract—In this paper, we derive lower bounds on the
distortion of scalar ﬁxed-rate codes for lossy compression with
side information available at the receiver. These bounds are
derived by presenting the relevant random variables as a Markov
chain and applying generalized data processing inequalities a la
Ziv and Zakai.

I. I NTRODUCTION
The Wyner–Ziv (WZ) problem has received very much
attention during the last three decades. There were several
attempts to develop practical schemes for lossy coding in
the WZ setting, by using codes with certain structures that
facilitate the encoding and the decoding. Most notably, these
studies include nested structures of linear coset codes (in the
role of bins) for discrete sources, and nested lattice structures
for continuous valued sources, see e.g., [2], [3]. Other directions of introducing structure into WZ coding are associated
with trellis/turbo/LDPC designs ([4] and references therein)
and with progressive coding, i.e., successive reﬁnement with
layered code design [5], [6]. The case of scalar source codes
for the WZ problem was also handled in several papers, e.g.
[7] and [8]. Zero-delay coding strategies for the WZ problem
were also introduced in [9]. In [10] and [11] it was shown that
under high-resolution assumptions, the optimal quantization
level density is conjectured to be periodic. In addition, zerodelay schemes for speciﬁc source-side information correlation
were presented in [10], [11] and [12].
In this paper, we develop lower bounds for the distortion
in the scalar WZ setting. We generalize the results of [13]
and [14], concerning functionals satisfying a data-processing
theorem, to this setting. In [13] it was shown that the ratedistortion (RD) bound (R(d) ≤ C) remains true when
− log x, in the deﬁnition of mutual information, is replaced
by an arbitrary convex, non–increasing function satisfying
some technical conditions. For certain choices of the convex
function, the bounds obtained were better than the classical
RD bounds. These results were substantially generalized in
[14] to apply to even more general information measures. The
methods of [13] were used in [15], [16] and [17]. In these
papers, lower bounds for the distortion of delay constrained
joint source-channel coding were given. These bounds were
obtained by combining the R´ nyi information measure [18]
e
with the generalized data processing theorem of [13], and
under high-resolution and high SNR approximations. Another

II. P ROBLEM F ORMULATION AND R ESULTS
In this section, we present the relevant random variables
of the WZ problem as a Markov chain and establish a
generalized Data Processing Theorem (DPT) for this setting,
using the method of [13]. We begin with notation conventions. Capital letters represent scalar random variables (RV),
speciﬁc realization of them are denoted by the corresponding lower case letters and their alphabets - by calligraphic
letters. For k ≥ 1 (k is a positive integer), xk will denote
the vector (x1 , . . . , xk ). The logarithms are to base 2. We
consider a memoryless source producing a random sequence
X1 , X2 , . . . , Xn , Xi ∈ X , i = 1, 2, . . . , n, where X is a
ﬁnite alphabet with cardinality |X |. Without loss of generality,
we deﬁne this alphabet to be the set {1, 2, . . . , |X |}. The
probability mass function of X, PX (x), is known. The encoder
maps X n into a channel symbol Z which takes on values in
the set {1, 2, . . . , M }, M ≤ |X |n . The decoder, in addition to
Z, has access to a sequence Y n , dependent on X n via a known

1

DMC, deﬁned by the single-letter transition probability matrix
PY |X (y|x) , whose entries are the conditional probabilities
of the different channel output symbols given the channel
input symbols. Based on Z and Y n , the decoder produces the
ˆ
reconstructed sequence X n . This setting is depicted in Figure
ˆ
1. For simplicity, we assume that Xi , Yi and Xi , all take on
values in the same ﬁnite alphabet X . Let Q(t), 0 ≤ t < ∞,
be a real-valued convex function, where lim t · Q(1/t) = 0 as
t → 0. This requirement implies that Q(t) is non-increasing,
as was shown in [13]. We deﬁne 0 · Q(r/0) = 0, for all
0 ≤ r < ∞. The generalized mutual information relative to
the function Q(t) is deﬁned by:
Q

I (X; Y ) =

p(x, y)Q
x,y

p(x)
p(x|y)

Assuming the encoder is given by a deterministic function
f : X n → {1, . . . , M }, we get:
I Q (X n ; Y n , Z)

p(xn , y n )

=
xn ,y n



p(˜n |y n )
x

p(y n )

xn ∈Az
˜
p(y n |xn )


Q



(5)




where z = f (xn ) and Az ≡ {˜n : f (˜n ) = z}. Using Q(t) =
x
x
− log(t) in (5), thus turning back to the classical DPT, we get
the following result:
n (R(d) − I(X; Y )) ≤ sup {H(Z|Y n )}

(1)

(6)

where the supremum is taken over all partitions of X n into
M disjoint subsets. The proof is given in [21]. This inequality
stems from the speciﬁc Markovity of the WZ problem. We
see that given some ﬁxed-rate R = log M , we should ﬁnd
the encoder that maximizes H(Z|Y n ). In Subsection II.C, we
show some examples of scalar coding, i.e., n = 1, where
this result gives us lower bounds for the distortion, which
are better than the bounds obtained from the trivial inequality
RW Z (d) ≤ log M , where RW Z (d) is the WZ RD function.

We apply the generalized DPT [13, Theorem 3] in the

A. Generalized DPT for scalar coding

Fig. 1.

A ﬁxed-rate scalar source code with rate R = log M ,
partitions X into M disjoint subsets (A1 , A2 , . . . , AM ). The
encoder f is given by a function f : X →{1, 2, . . . , M },
that is, zi = f (xi ). The decoder g receives zi , together with
the side information yi , and generates xi , using a decoding
ˆ
function g : {1, 2, . . . , M } × X → X , i.e., xi = g(zi , yi ).
ˆ
This is the setting described in Figure 1, with block length
M
n = 1. We deﬁne the following vectors {pz }z=1 :

The WZ Setting

following way:
ˆ
I Q (X n , X n ) ≤ I Q (X n ; Y n , Z)

(2)

pz = [11∈Az , 12∈Az , . . . , 1|X |∈Az ]

ˆ
where we used the fact that X n ↔ (Y n , Z) ↔ X n is a
n
n
Markov chain. Since Z ↔ X ↔ Y is also a Markov chain,
we have:
p(xn , y n , z) = p(xn )p(y n |xn )p(z|xn )

(7)

where 1B is the indicator function for the event B. The jth
coordinate of pz is 1 if j ∈ Az and 0 else. By deﬁnition of
M
{pz }z=1 , we have the following property:
M

(3)

pz = [1, 1, . . . , 1]

(8)

z=1

and I Q (X n ; Y n , Z) is given by:
I Q (X n ; Y n , Z)

Using these vectors, we can rewrite (5) in the following way:

p(xn , y n )p(z|xn )

=

I Q (X; Y, Z)

=

xn ,y n ,z

Q

x,y

p(y n )p(z|y n )
p(y|xn )p(z|xn )
n

=

p(x, y)Q

n

=

p(y)
y

Q

n

p(x , y )p(z|x )

p(y) · [pz(x) · px|y ]
˜
p(y|x)
p(x|y)

z

x∈Az

p(y) · [pz · px|y ]
˜
p(y|x)

xn ,y n ,z



Q


p(˜n |y n )p(z|˜n )
x
x

p(y n )
xn ∈X n
˜

p(y|xn )p(z|xn )



=
=

z

p(y)
y

(4)

pz · qz,y

p(y)
y





Gy (pz )
z

(9)

2

where we have deﬁned the following |X |-vectors:

other. A uniformly distributed source is a source for which
1
PX (x) =
, ∀x ∈ X .
|X |
Lemma 2: Consider a discrete source X, uniformly distributed over a ﬁnite alphabet X , and let Q(t), 0 ≤ t < ∞
be any real-valued convex function. Then, RQ (d) w.r.t to any
weakly symmetric distortion measure is given by:

px|y = [p(x1 |y), p(x2 |y), . . .]
˜
p(y) · [pz · px|y ]
˜
p(y|x1 )
p(y) · [pz · px|y ]
˜
,...
p(y|x2 )

qz,y = p(x1 |y)Q
p(x2 |y)Q

and the set of functions

|X |
{Gy }y=1 ,

,

(10)

|X |

RQ (d) =

Gy : R|X | → R:

Gy (pz ) = pz · qz,y

k=1

Q

|X |

|X |

pk = 1,

(13)

pk dk = d

k=1

k = 1, . . . , |X |

For the Hamming distortion measure, deﬁned by:
d(x, x) =
ˆ

0
1

x=x
ˆ
x=x
ˆ

(18)

we get the following result:
Corollary 1: Consider a discrete source X, uniformly distributed over a ﬁnite alphabet X , and let Q(t), 0 ≤ t < ∞
be any real-valued convex function. Then, RQ (d) w.r.t to the
Hamming distortion measure is given by:
RQ (d) = (1 − d) · Q

1
|X |

1
1−d

+d·Q

(|X | − 1) 1
|X | d
(19)

The proof is given in [21]. The general form of RQ (d) enables
the use of any convex function Q(t). These results make the
Ziv-Zakai mechanism much more useful, at least for the case
of uniform sources.

2−α
1

µk · pk = 0,

(17)

k=1

µk ≥ 0,

z

p(x ) · p(y|x ) 2−α
y

1
+λ1 +λ2 dk −µk = 0 (16)
|X |pk

where {dk }k=1 are the elements of each row of the matrix
{d(x, x)} (the same elements appear in each row) and λ1 , λ2 ,
ˆ
|X |
{µk }k=1 are constants that are chosen such that:

The inﬁmum is taken over all conditional distributions P (ˆ|x)
x
ˆ
that satisfy the distortion constraint Ed(X, X) ≤ d, where
d(x, x) is a distortion measure. The supremum should be
ˆ
taken over all scalar encoders with a ﬁxed-rate R = log M .
Alternatively, we can carry out a continuous optimization by
M
taking the supremum over all sets of positive vectors {pz }z=1
that satisfy (8), i.e., over all conditional distributions P (z|x).
The result will be of course greater than or equal to C Q . We
end this subsection with an upper bound on C Q for a speciﬁc
convex function Q(t). Using Q(t) is equivalent to using the
R´ nyi information measure, for which the logarithmic measure
e
is a special case (α → 1). In addition, Q(t) is relatively
convenient to work with.
Lemma 1: For the convex function Q(t) = t1−α , 1 < α <
2, we have the following upper bound:
C Q ≤ M α−1 ·

1
1
−
Q
|X |pk
|X |pk
|X |

(12)

where:

y

(15)

where {pk }k=1 is a probability measure which is given by the
following equations (k = 1, . . . , |X |):

Notice that the vector px|y depends only on y and that the dot
˜
product [pz · px|y ] is a function of z and y. Applying the RD
˜
bound [13, Theorem 4], we get:

ˆ
RQ (d) = inf I Q (X, X)
CQ
= sup I Q (X; Y, Z)
= sup
p(y)
Gy (pz )

1
|X |pk

|X |

(11)

ˆ
RQ (d) ≤ I Q (X, X) ≤ I Q (X; Y, Z) ≤ C Q

pk · Q

(14)

x

C. Applications
In this subsection, we use the results of the previous
subsections to get lower bounds on the distortion in several
cases. Non-trivial bounds were obtained using the convex
function Q(t) = t1−α , α > 1, which was mentioned above.
We assume that the source is uniformly distributed, and that
the DMC is symmetric. A channel is said to be symmetric if
the rows of the channel transition matrix, PY |X (y|x) , are
permutations of each other, and the columns are permutations
of each other. Since the source is uniformly distributed and the
channel is symmetric, Y is also uniformly distributed. Under
these conditions, Eq. (11) becomes:

The proof is given in [21]. The usefulness of this result stems
from its generality. It holds for any source distribution and any
transition probability matrix {PY |X (y|x)}. This result is used
in Subsection II.C, along with tighter bounds on the capacity
that can be achieved in several special cases.
B. The generalized rate-distortion function for uniform source
distribution
In this subsection, we introduce the generalized ratedistortion function of uniformly distributed sources w.r.t general weakly symmetric distortion measures. We refer to a
distortion measure d(x, x) as weakly symmetric if the rows
ˆ
of the distortion matrix, {d(x, x)}, are permutations of each
ˆ

Gy (pz ) = |X |α−1

3

pz · pα x
y|˜
(pz · py|˜ )α−1
x

(20)

Obviously, z Mz = |X |. Notice that the supremum is taken
over all feasible encoders, where each encoder is represented
by a speciﬁc set {pz }M as deﬁned in (7). The second equality
z=1
is explained in [21]. The function qα (Mz ) is concave for 1 <
α ≤ 2, as shown in [21], and may be concave also out of
this range, with dependence on the channel parameters. When
qα (Mz ) is concave, we can bound the supremum by taking
equal Mz ’s, i.e., Mz = |X |/M , ∀z, and we get:

where we have deﬁned the following |X |-vectors:
py|˜
x

=

[p(y|x1 ), p(y|x2 ), . . .]

pα x
y|˜

=

[p(y|x1 )α , p(y|x2 )α , . . .]

(21)
Applying (19) and (20) in (12), we get:
|X |

α−1

(1 − d)α +

dα
(|X | − 1)

α−1

≤
(22)

|X |

α−2

sup

pz · pα x
y|˜
(pz · py|˜ )α−1
x
y,z

=C

CQ ≤

Q

where the supremum is taken over all sets of positive vectors
M
{pz }z=1 that satisfy (8). Notice that optimizing over α will
produce the best lower bound for the distortion. Generally, the
functions {Gy (pz )} in this case are neither convex nor concave. However, at least for some cases, C Q can be calculated
directly, as shown in the following examples. We can also
upper bound C Q as was done in (14). This upper bound may
give us non-trivial bounds, as shown in example 2. Additional
examples, with more general distortion measures, will be given
in [21].
1) Example 1: The symmetric DMC is deﬁned by:
µ

p(y|x) =

y=x
else

|X |α−1

(|X |/M + µα /

α

− 1)

(|X |/M + µ/ − 1)
2−α
|X |
(M − 1)
M

α−1

+
(26)

If M divides |X |, this bound is achieved by any feasible
encoder that partitions the source alphabet into equally-sized
subsets, thus the optimization is exact. An example for speciﬁc
values of µ and is presented in Figure 2. The bound is compared with the bound obtained from the classical DPT (6), the
bound obtained from the trivial inequality RW Z (d) ≤ log M ,
the bound obtained by using (14) and the exact solution of
Lemma 3. RW Z (d) was calculated using the Blahut-Arimototype algorithm presented in [20]. Eq. (22) was optimized over
α, for each M ≤ |X |, as to get the best lower bound on
the distortion. We see that even the classical DPT gives us

(23)

where µ, ∈ [0, 1], µ > , and µ + (|X | − 1) = 1. The
distortion measure we use, is the Hamming distortion, deﬁned
in (18).
Lemma 3: Consider the WZ setting with uniformly distributed source and the DMC deﬁned in (23). Then, the
minimal achievable distortion w.r.t to the Hamming distortion
measure, of a scalar source code with a ﬁxed-rate R = log M ,
is:
d(M ) = (|X | − M )
(24)
This result, fully given in [21], follows directly from simple
calculation of the distortion. Having the exact solution in this
case, we can compare it to our bound to examine its quality.
The generalized capacity (13) for this channel is given by:
CQ

pz · pα x
y|˜

= |X |α−2 · sup
y,z

(pz · py|˜ )α−1
x
α

=

Fig. 2. |X | = 4, µ = 0.7. Plus - the lower bound obtained from (26),
Circle - the lower bound obtained from the classical DPT (6), Star - the exact
solution, Solid line - the lower bound obtained from RW Z (d), Square - the
lower bound obtained from (14).

|X |α−2 · sup

Mz ·
z

(Mz + µ /

α

non-trivial lower bounds and that the lower bound obtained
from (26) is much better than the trivial bound obtained from
RW Z (d). The lower bound obtained from (14) is not useful
in this case. There is a gap between the exact solution and the
best bound, even for M = 2, where the optimization (13) is
exact.
2) Example 2: The symmetric DMC is deﬁned by:

− 1)

(Mz + µ/ − 1)

α−1

2−α
+(|X | − Mz )Mz

= |X |α−2 · sup

y ∈ {x mod |X |, . . . , (x + l − 1) mod |X |}
else
(27)
where l is an integer, 0 < l < |X |, and |X | mod |X | is deﬁned
to be |X |. Given an input x, the channel produces one of l

qα (Mz )

p(y|x) =

z

(25)
where Mz , Mz ∈ {1, . . . , |X | − M + 1}, is the cardinality of
Az , i.e., the number of source symbols that are encoded to z.

4

1/l
0

values with equal probability. The generalized capacity (13)
for this channel is given by:
CQ

=

pz · pα x
y|˜

|X |α−2 · sup
y,z

=

My,z (1/l)α

|X |α−2 · sup
y,z

=

(pz · py|˜ )α−1
x
(My,z (1/l))

α−1

(28)

2−α
My,z

|X |α−2 · l−1 · sup
y,z

where My,z = l · [pz · py|˜ ]. It is easy to see that z My,z = l.
x
2−α
For 1 < α < 2, the function My,z is concave in Mz . Thus,
the supremum is achieved by setting My,z = l/M , ∀{y, z}:

Fig. 3. |X | = 4, l = 3. Square - the lower bound obtained from (26) and
(14), Circle - the lower bound obtained from the classical DPT (6), Solid line
- the lower bound obtained from RW Z (d), Star - the exact distortion of the
encoder (30).

C Q = |X |α−2 · l−1 · |X | · M · (l/M )2−α = |X |α−1 · (M/l)α−1
(29)
If M divides l, equal My,z ’s can be obtained by the following
feasible encoder:
z = f (x) = 1 + x mod M

[5] Y. Steinberg and N. Merhav, “On successive reﬁnement for the Wyner–
Ziv problem,” IEEE Trans. Inform. Theory, vol. 50, no. 8, pp. 1636–1654,
August 2004.
[6] S. Cheng and Z. Xiong, “Successive reﬁnement for the Wyner–Ziv
problem and layered code design,” in Proc. DCC 2004, Snowbird, UT,
2004.
[7] J. Kusuma, L. Doherty and K. Ramchandran, “Distributed compression
for sensor networks,” in Proc. ICIP 2001, vol. 1, pp. 82–85, Thessaloniki,
Greece, Oct. 2001.
[8] D. Muresan and M Effros, “Quantization as histogram segmentation:
Optimal scalar quantizer design in network systems,” IEEE Trans. Inform.
Theory, vol. 54, pp. 344–366, Jan. 2008.
[9] D. Teneketzis, “On the structure of optimal real-time encoders and
decoders in noisy communication,” IEEE Trans. Inform. Theory, vol. 52,
pp. 4017–4035, Sept. 2006.
[10] J. Nayak, E. Tuncel, “Low-delay quantization for source coding with
side information,” Proc. ISIT 2008, pp. 2732–2736, Toronto, Canada,
Aug. 2008.
[11] X. Chen, E. Tuncel, “Low-delay prediction and transform-based WynerZiv Coding,” IEEE Trans. Signal Processing, vol. 59, no. 2, pp. 653–666
, Nov. 2010.
[12] X. Chen, E. Tuncel, “High-resolution predictive Wyner-Ziv coding of
Gaussian sources,” Proc. ISIT 2009, pp. 1204–1208, Seoul, Korea, Aug.
2009.
[13] J. Ziv and M. Zakai, “On functionals satisfying a data-processing
theorem,” IEEE Trans. Inform. Theory, vol. 19, no. 3, pp. 275–283, May
1973.
[14] M. Zakai and J. Ziv, “A generalization of the rate-distortion theory and
applications,” in: Information Theory New Trends and Open Problems,
edited by G. Longo, Springer-Verlag, pp. 87-123, 1975.
[15] I. Leibowitz, R. Zamir, “A Ziv-Zakai-R´ nyi lower bound on distortion
e
at high resolution,” Proc. ITW 2008, pp. 174–178, Porto, Portugal, May.
2008.
[16] S. Tridenski, R. Zamir, “Bounds for joint source-channel coding at high
SNR,” Proc. ISIT 2011, pp. 771–775, St. Petersburg, Russia, Aug. 2011.
[17] A. Ingber, I. Leibowitz, R. Zamir, M. Feder, “Distortion lower bounds
for ﬁnite dimensional joint source-channel coding,” Proc. ISIT 2008,
pp. 1183–1187, Toronto, Canada, Aug. 2008.
[18] A. R´ nyi, “On measures of entropy and information,” Proc. 4th Berk.
e
Symp. Math., Stat. and Prob., pp. 547-561, Univ. of Calif. Press, 1961.
[19] N. Merhav, “Data Processing Theorems and the Second Law of Thermodynamics,” IEEE Trans. Inform. Theory, vol. 57, no. 8, pp. 4926–4939,
Aug. 2011.
[20] F. Dupuis, W. Yu, F.M.J. Willems, “Blahut-Arimoto algorithms for
computing channel capacity and rate-distortion with side information,”
Proc. ISIT 2004, pp. 179, Chicago, USA, June 2004.
[21] A. Reani, N. Merhav, “Data processing lower bounds for scalar lossy
source codes with side information at the decoder,” in preparation.

(30)

Therefore, in this case, the optimization is exact. For α > 2,
C Q is inﬁnite, because we can always set some of the My,z ’s
to 0 by appropriate choice of encoder. Thus, this range of α
does not lead to a useful bound. An example for speciﬁc values
of |X | and l is presented in Figure 3. The lower bound for the
distortion, which coincides with the bound obtained from (14)
(the upper bound on C Q is tight for this channel), is compared
with the bound obtained from the classical DPT (6) and the
bound obtained by the trivial inequality RW Z (d) ≤ log M . Eq.
(22) was optimized over α, for each M ≤ |X |, as to get the
best lower bound on the distortion. We see that in this case, the
generalized DPT leads to bounds that are better than the trivial
bound, whereas the classical DPT does not lead to a useful
bound. We also present the exact distortion of the encoder
deﬁned in (30), which is of course an upper bound on the
distortion. Thus, the distortion of the optimal encoder must be
in the range between this upper bound and our highest lower
bound. For M = l, zero distortion can indeed be achieved
using the encoder deﬁned in (30), thus our lower bound in
this point is tight.
Acknowledgments
This research is supported by the Israeli Science Foundation
(ISF), grant no. 208/08.
R EFERENCES
[1] C. E. Shannon, “A mathematical theory of communication,” Bell Syst.
Tech. J., vol. 27, pt. I, pp. 379–423, 1948; pt. II, pp. 623–656, 1948.
[2] J. Kusuma, “Slepian–Wolf coding and related problems,” preprint 2001.
Available at:
www.mit.edu/∼6.454/www fall 2001/kusuma/summary.pdf
[3] S. D. Servetto, “Quantization with side information: lattice codes, asymptotics, and applications to sensor networks,” vol. 53, no. 2, pp. 714–731,
February 2007.
[4] Z. Xiong, A. D. Liveris, and S. Cheng, “Distributed source coding
for sensor networks,” IEEE Signal Processing Magazine, pp. 80–94,
September 2004.

5

