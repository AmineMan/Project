Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Mon May 14 14:48:13 2012
ModDate:        Tue Jun 19 12:56:22 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      581798 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569561021

The Information Content
in Sorting Algorithms
Ludwig M. Busse, Morteza Haghir Chehreghani, and Joachim M. Buhmann
Department of Computer Science, ETH Zurich
ludwig.busse@inf.ethz.ch, morteza.chehreghani@inf.ethz.ch, jbuhmann@inf.ethz.ch

Besides computational complexity, however, robustness can
be a key concern as well in the analysis of algorithms. Two
possible sources of uncertainties in sorting problems are: (i.)
Comparing items might suffer from noise in the input data, i.e.
queries i < j might return the wrong result (ii.) Algorithmic
operations might be unreliable, here: the operation i < j is
subject to possibly erroneous execution.

Abstract—Sorting algorithms like MergeSort or BubbleSort
order items according to some criterion. Whereas the computational complexities of the various sorting algorithms are well
understood, their behavior with noisy input data or unreliable
algorithm operations is less known.
In this work, we present an information-theoretic approach to
quantifying the information content of algorithms. We exemplify
the signiﬁcance of this approach by comparing different algorithms w.r.t to both informativeness and stability. For the ﬁrst
time, the amount of order information that a sorting algorithm
can extract in uncertain settings is measured quantitatively.
Such measurements not only render a principled comparison of
algorithms possible, but also guide the design and construction
of algorithms that provide the maximum information.
Results for ﬁve popular sorting algorithms are illustrated,
giving new insights about the amount of ordering information
achievable for them. For example, in noisy settings, BubbleSort
can outperform MergeSort in the number of bits that can be
effectively extracted per comparison made.

In Fig. 1, we observe the performance of ﬁve standard
sorting algorithms in terms of both the number of comparisons (efﬁciency) and the deviation from the correct ordering
(robustness). Items are to be ordered, but when the algorithms
query the comparison i < j between items, the item’s true
ranks are perturbed by Gaussian noise (centered around the
true rank with a given standard deviation; more details on
the experimental setting later in this paper). The deviation
from the true ordering is measured with the Spearman’s
rho distance (quadratic distance of ranks) [3]. As we can
see, the sorting algorithms suffer differently from the noisy
comparisons depending on how errors can affect the overall
result. Since algorithms with computational complexity higher
than the lower bound Ω(n log n) obviously make a number
of redundant pairwise comparisons, one would in general
expect a tradeoff between the number of comparisons and the
accuracy of the result. Very efﬁcient O(n log n) algorithms
might be more prone to errors because they exploit each
comparison to a larger extent. However, we observe that some
algorithms are Pareto-dominant over others: MergeSort, and
also the berated BubbleSort.

I. I NTRODUCTION
Keeping items in order is at the very heart of organising
information. Computers spend a massive amount of their time
sorting data.
General-purpose sorting routines have been well studied over
the past 100 years. An in-depth overview on sorting algorithms
is provided in [7]. There is a whole zoo of sorting algorithms
available today. Sorting approaches can be schematically classiﬁed into sorting by insertion, exchanging, selection, and
merging [7]. In this work, we focus on comparison-based
sorting algorithms, where the sorting is produced by comparisons between pairs of items i < j. Mathematically, the
problem rephrases as: determine the total order on a set of n
items via questions of the form “is i < j?”. Technically, the
different algorithms then differ in the order that the pairwise
comparisons are queried from the oracle, and in the way they
use each comparison for generating the ﬁnal order.
The computational complexities of sorting algorithms
have been well studied and typically lie between O(n2 ) and
O(n log n). On contrary, the behavior of sorting algorithms
in uncertain settings are rarely tackled. In [6], a lower
bound for the number of comparisons that any randomized
algorithms needs to approximate any given ranking within
an expected Spearman’s footrule distance is shown. The
robustness of sorting and tournament algorithms against
faulty comparisons is analyzed in [5]. A lower bound on the
query complexity with erroneous comparisons is shown in [8].

Fig. 1.

1

Efﬁciency vs. robustness of sorting algorithms

This experimental observation raises suspicion that – in
noisy conditions – the amount of attainable sorting information
is different for different sorting algorithms. We now proceed
in analyzing sorting algorithms with the goal of measuring
their information content. That is, we wish to quantify how
many bits a noise-exposed algorithm can effectively extract for
the order relation. Thereby, we can consolidate the snapshot
result in Fig. 1, and also select or build an algorithm that
will operate with the maximum information gain possible. The
information-theoretic approach that we present here is actually
applicable to all kind of algorithms, and we demonstrate it for
sorting algorithms. The approach generalizes Approximation
Set Coding [2], which was initially proposed for model validation in clustering [1].
Section II introduces the theory of informativeness of algorithms, and in section III the application to sorting algorithms
is presented. Results are given in section IV, before this paper
ends with a conclusion (Section V).

B. Representer Sets of Algorithms
In the optimization setting described above, introducing
approximation sets pursues the goal to ultimately determine
the informativeness of cost functions exposed to noisy data.
Indeed, the deﬁnition of sets Cγ (X) by means of a cost
function and a closeness-concept parametrized by γ constitutes
a special case for a set-deﬁning mechanism. We generalize to
mechanisms A, also called algorithms, i.e. A is a mapping
into the powerset P(H) of the hypothesis class
A:X ×R

(X, γ) →

P(H)
A
Cγ (X) ∈ P(H)

(2)

A
and we call the sets Cγ (X) the representer sets of algorithm
A. Fig. 2 visualizes representer sets as characteristic output of
an algorithmic procedure with input data X and parameter γ.

II. T HE I NFORMATION C ONTENT OF A LGORITHMS
A. Introductory Example
Let us begin with an example that is helpful for understanding the main concept of the following work. The
example is optimization under uncertainty. Assume there is
data X ⊂ X given, for example measurements in a data
space X . Such measurements can be d-dimensional vectors or
relations X = (xij ) ∈ Bn×n (these will become the pairwise
comparisons in sorting). Let h be a solution or hypothesis of
an optimization problem. In sorting, h is a ranking encoding
the order information of the items. The hypothesis class is the
set of hypotheses H(X) = {h(X) : X ∈ X }. An optimization
problem involves a cost function R : H × X → R≥0
that assigns each hypothesis h(X) a real value R(h, X).
h⊥ (X) = arg minh R(h, X) denotes the min-cost solution,
the hypothesis that empirically minimizes costs.
The key concept brought up in Approximation Set Coding
(ASC) [1] is the notion of an approximation set. This set
contains all solutions Cγ (X) of the optimization problem that
are γ-close in costs to h⊥ (X):
Cγ (X) := {h(X) : R(h, X) ≤ R(h⊥ , X) + γ}

→

Fig. 2.

Representer set of a set deﬁning mechanism A

We now wish to quantify the amount of information –
measured w.r.t. the hypothesis class H(X) – that A is able to
extract from the possibly noisy data X.
Example: Imagine the hypotheses {h(X)} being the set of
all rankings (which order items). Given pairwise data in X,
a procedure that selects just a few compatible rankings (and
excludes many others; ultimately, just one ﬁnal ranking is
selected) is more informative compared to a procedure that
does not commit or dare to restricting the set of solutions to
the same extent. At the same time, a high overlap of the representer sets for two instances X (1) and X (2) ensures robustness
against the noise actually present in the data. Mathematically,
this rephrases as: How small can the representer sets be chosen
A
(size adjustable by γ) to still ensure identiﬁability of Cγ (X)
under variation of the data X?

(1)

These sets serve the purpose to stabilize optimization results.
The input data of the problem, the measurements X, can
be affected by perturbations due to random noise. For the
same signal in X but a different noise instantiation, h⊥ (X)
might considerably differ. However, for sufﬁciently large γ,
the approximation set Cγ (X) is almost invariant under noise.
ASC develops a criterion to minimize the approximation
parameter γ under the constraint that Cγ (X) is stable
under noise perturbation. Shrinkage of the approximation
set (governed by the parameter γ) is of advantage to gain
information about the solution, but it should happen under
the constraint of robustness. In summary, provided are both
informative and robust solutions when choosing hypotheses
guided by a cost function.

C. The Capacity of Algorithms
With the representer sets of an algorithm A at hand, we
now have the ingredient to compute the algorithm’s capacity
as the maximum information it can make use from the data X
under noise effects. In analogy to Approximation Set Coding
(ASC) [1], [2], we refer to a generic set-based coding and
A
communication scenario and use the representer sets Cγ (X) in
a ﬁctitious communication scenario to derive a criterion for the
optimal information gain. There exists a conceptual analogy
between learning and communication: For communication,
one demands a high rate (a large amount of information
transferred per channel use) together with a decoding rule that
is stable under the perturbations of the messages by the noise
in the channel.

2

ASC adopts the two sample set scenario, which is widely
used in statistical learning theory [9] and is analogous to
two-terminal systems in information theory [4]. Two datasets
X (1) and X (2) are generated with the same underlying signal
structure (e.g. pairwise comparisons as induced by the same
ordering), but different noise realizations (e.g. some pairwise
comparisons ﬂip randomly). This two-dataset situation corresponds to a communication scenario where the code on
both sides of the channel differs due to a noise process. As
derived in [1], an asymptotically vanishing error probability is
achievable for rates bounded by1
1
ˆA
log
Iγ =
m

A
|{σ}||∆Cγ (X (1) , X (2) )|
A
A
|Cγ (X (1) )||Cγ (X (2) )|

A. A Group-theoretic Interpretation of Sorting Algorithms
We now give a viewpoint on sorting algorithms that will
help us tackling them in a set-based fashion.
In fact, sorting technologies can be understood as mechanisms
that operate on the symmetric group Sn (the space of all
permutations of n items=rankings/orderings). At the beginning
of a sorting procedure, every element of Sn (each ordering)
is equally admissible. During the execution of the algorithm
– in the light of the acquired pairwise comparisons – the
set of possible orders is further reduced until the algorithms
ﬁnally ﬁnd the one and only ordering that is compatible with
the pairwise comparisons that were obtained. In other words,
sorting algorithms aim at constructing a series of partial orders,
reducing their cardinality so as to ultimately narrow down to
a single element in Sn which is the sorting result.

.

Here, |{σ}| denotes the cardinality of the hypothesis class
A
and is the term accounting for the problem complexity. ∆Cγ
is the overlap, how many γ-solutions of the ﬁrst dataset
are also in the γ-representer set of the second dataset, i.e.
A
A
A
∆Cγ (X (1) , X (2) ) := |Cγ (X (1) ) ∩ Cγ (X (2) )|. For a ﬁxed γ,
a large overlap means that the algorithm’s evaluation of the
ﬁrst dataset generalizes to the second dataset, whereas a small
or empty intersection indicates lack of generalization. The
A
A
A
fraction |∆Cγ (X (1) , X (2) )|/|Cγ (X (1) )||Cγ (X (2) )| measures
the stability of the solutions under noise ﬂuctuations. It is in
[0, 1] and thereby controls the effective useable size of the
hypothesis class. Normalizing with m ensures a bit rate per
ˆA
measurement made. Iγ is an estimate for the mutual information and, in analogy to information theory, the capacity
ˆA
is deﬁned as C A := maxγ E[Iγ ]. This information capability
of an algorithm A is context-sensitive, because it is measured
w.r.t. to the hypothesis class.

B. Information-theoretic Validation of Sorting Algorithms
The relation to the representer sets is now immediate: The
hypothesis class is the set of all permutations H = {π ∈ Sn },
and the data are the pairwise comparisons xij ∈ {0, 1}.
Steps in an algorithm’s execution path are related to the γparametrization. An algorithm that has just started, at time
t = tStart , has not yet condensed any information (0 Bits):
A
the representer set Cγ=tStart is the full hypothesis class with
cardinality n!. Now, suppose we look at the algorithm at time
t = t + 1 after it has established some order. The cardinality
of the representer set has been reduced, thereby providing
more information about the order of the items. Finally, at
t = tEnd , the algorithm has narrowed down the hypothesis
class to one ﬁxed ordering π ∗ ∈ H, yielding log2 (n!) bits
of information. Such a trajectory of a sorting algorithm is
visualized in Fig. 3. In analogy with statistical physics, an
algorithm at the beginning is at high temperature smoothing
over the solution space and containing no information, then
runs and cools down committing more and more to speciﬁc
solutions (and thus exhibiting information).
With noise involved, an algorithm might end up with the
wrong result (Fig. 4 left). In another run, the result might differ
again; Fig. 4 (right) shows both the representer sets for the two
samples and their overlap at step t. Their cardinalities can be
used to compute the mutual information, whose maximum is
the capacity of the algorithm.

III. T HE I NFORMATION C ONCENTRATION OF S ORTING
A LGORITHMS
Now, we study the use of the new framework to compute
the capacity of sorting algorithms. Thereby, we can quantify
how much information (in terms of bits, here: for ordering
items) an algorithm can actually provide in the presence of
noise, and then compare the algorithms against each other.
Pairwise comparison-based sorting algorithms sequentially
query pairs of items i < j until they determine a complete
order as their result. Sorting algorithms differ in the number
and order in which they compare pairs of items. The lower
bound on the number of necessary pairwise comparisons to
sort n items is known to be Ω(n log n) [7]. The computational complexity of sorting algorithms typically lies between
O(n log n) and O(n2 ). Hence, algorithms of the latter complexity class obviously do redundant comparisons. Confronted
with noisy data, the question then is whether these algorithms
can exploit the redundancy to their advantage when it comes
to improving the accuracy of the sorting result.

C. Computations of Capacity
The following sections detail the computation of representer
sets for ﬁve standard algorithms.
a) SelectionSort: We begin with SelectionSort for didactical reasons, because of simplicity in understanding the role
of the representer sets and the ease of their computation.
Basically, as with all algorithms, we need to identify time
points in the execution path of the algorithm where the algorithm has achieved “some” partial sorting. In SelectionSort,
after one iteration of the outer loop, the ﬁrst position of the
ordering is ﬁlled with an item (the “selected” item). The item
in the ﬁrst position is ﬁxed from now on, and will never be
moved again. On contrary, comparisons made in the inner

1 We omit here in-depth details concerning the derivation and show a
simpliﬁed version. Please see [1] for a rigor description.

3

Fig. 3.

Fig. 4.
(right)

d) QuickSort (random pivot): In an intermediate partitioning step of QuickSort, assume a items have already been
decided to be before the pivot element, b items have already
been placed after the pivot element, and c is the number of
items that still need to be placed. As always, the representer
set is the set of linear extensions, and its cardinality is
c
QuickSort
c!
|Ct=1,... | = k=0 k!(c−k)! (a + k)!(b + (c − k))! · f , where
f is the factorial of all other divisions not yet partitioned by
QuickSort (can be computed recursively). The capacity is then
QuickSort
n!
computed via It∗
= log2 |Ct∗ |
e) BubbleSort: There is one peculiarity with noisy BubbleSort. The standard implementation of the algorithm continues bubbling until no adjacent items have been exchanged
in a complete pass over the ordering. Thus, with noisy
comparisons, BubbleSort is not guaranteed to halt, rather
it will continue resorting items. We decided to parametrize
BubbleSort by the number of outer loop iterations (more on
this in the results section).
After one iteration, the largest item should have found its
place on the last position, then the second largest on the
second to last position and so on. Thus, BubbleSort works
in the opposite direction as SelectionSort. The parameter of
the number of outer iterations determines the number of steps,
e.g. t = 1, . . . , n − 1. Since the BubbleSort dynamical system
has the potential to “forget” wrong pairwise order information,
the time point t∗ can be choosen, at which the two trajectories
were closest together (overlapped in the last t positions).
n!
BubbleSort
= log2 (n−t)! .
There, the maximal capacity is It∗

Trajectory of a sorting algorithm

Representer sets of a noisy run (left) and their overlap in two runs

IV. R ESULTS
loop are not necessarily binding for the ﬁnal ordering output.
In total, the SelectionSort procedure decomposes into n time
steps, after each of them the next position will be ﬁlled.
Thus, a series of top-t orderings constitute the representer
SelectionSort
sets Ct=1,...,n . Their cardinality can be straightforwardly
SelectionSort
| = (n − t)!. The overlap is 100%
computed as |Ct
SelectionSort
SelectionSort
(|∆Ct
| = |Ct
|) as long as the ﬁrst t positions
are equal in the two runs of the algorithm (resulting in
n!
SelectionSort
I t∗
= log2 ( (n−t∗ )! ) bits of information), otherwise the
overlap immediately collapses to 0%. The largest t∗ achievable
on average by the algorithm will determine its capacity.
b) InsertionSort: InsertionSort produces a series of n−1
transitive chains of lengths t = 2, . . . , n. The average last t∗
before two samples of the algorithm diverge determines this
n!
InsertionSort
algorithm’s capacity by It∗
= log2 ( n!/t∗ ! ).
c) MergeSort: Because MergeSort compares items from
disjoint subsets organized as a balanced tree, it roughly splits
the hypothesis class of orderings into two halfs after each
of t = 1, . . . , log2 (n!) pairwise comparisons. Intuitively,
MergeSort has a strong tendency to respect all transitive
consequences of the comparisons done. The overlap of noisy
MergeSort trajectories immediately drops down to 0 after any
inconsistent comparison. The capacity of MergeSort can be
MergeSort
n!
approximated via It∗
= log2 n!/2t∗ . For a given noise
model or assumption, this time point t∗ should be amenable
to analytical calculation.

Results are provided for a comparison of ﬁve standard
sorting algorithms. n = 16 items have been sorted2 . A true
underlying ranking π ∈ Sn is assumed. When the rank π(i)
of item i is smaller than the rank π(j) of item j, the pairwise
comparison between the items yields i < j. When the algorithms put a query for a pairwise comparison, the true ranks
are infected with Gaussian noise, i.e. π (i) ∼ N (π(i), σ 2 ) is
˜
used for evaluating the comparison. The respective standard
deviation is denoted in the plots.
Fig. 5 shows the algorithms’ absolute capacity versus the
noise present in the data. We observe consistency between the
ranking of the algorithms as induced by their capacity and the
initial experimental ﬁndings in Fig. 1. Algorithms closer to the
Pareto-front receive higher capacities. In the second plot, the
capacity is normalized on the actual number of comparisons
made, yielding the effective number of bits extractable by an
algorithm per comparison. As expected, MergeSort reaches a
value of 1 Bit here. Interestingly, passing some noise threshold,
BubbleSort turns out to dominate other sorting procedures.
Plots in Fig. 6 visualize the capacity of different MergeSortand BubbleSort-variants. For MergeSort, we implemented
2 For the presentation of the results, we chose a low number n = 24 of
items. The phenomena that are portrayed are better visible in this resolution of
the plots. Since the effects depend on the noise level relative to the number of
items, the same results hold for larger n and we did perform these experiments
as well.

4

Fig. 5.

Fig. 6.

which have to cope with uncertainties, e.g. for new transistor
technologies that are energy-saving but prone to unreliable
operation execution.
The impact of the new analysis was demonstrated for sorting
algorithms. The method also will allow for an analytic calculation of the maximal information capacity of an algorithm and
thus can guide to optimally robust algorithm design. Moreover,
the approach presented in this article relates data-dependent
algorithmic complexities and statistical complexities.

multiple comparisons (3, 5 and 7) and grounded each decision
on their majority vote. This parametrization of the algorithm
lets expect a higher capacity despite increasing noise, and the
results conﬁrm this. The same strategy for BubbleSort can be
implemented by increasing the time amount for bubbling (we
used n/2, n − 1, 2n and 3n outer loop iterations). Also here
we recognize that the capacity tends toward an upper bound.
Knowledge of this upper bound will enable the selection of
the sorting algorithm optimal in a practical noise situation.
The plots with the capacity normalized by the number of
comparisons again attest on BubbleSort performing better in
terms of information extraction than MergeSort when the noise
passes some threshold. This coincides with the experimental
observation that, with increasing noise, the MergeSort Paretofront moves behind the front spanned by BubbleSort.

R EFERENCES
[1] J. M. Buhmann, Information theoretic model validation for clustering,
ISIT’10. IEEE, 2010.
[2] J. M. Buhmann, Context Sensitive Information: Model Validation by
Information Theory, Pattern Recognition, Lecture Notes in Computer
Science, Volume 6718, Springer, 2011.
[3] D. E. Critchlow, Metric Methods for Analyzing Partially Ranked Data,
Springer, 1985.
[4] I. Csiczar and J. Koerner, Information Theory: Coding theorems for
discrete memoryless systems, Academic Press, New York, 1981.
[5] W. Elmenreich and T. Ibounig and I. Fehervari, Robustness versus
Performance in Sorting and Tournament Algorithms, Acta Polytechnica
Hungarica, 2009.
[6] J. Giesen and E. Schuberth and M. Stojakovic, Approximate Sorting,
Lecture Notes in Computer Science 3887, 2006.
[7] D. E. Knuth, The Art of Computer Programming, VOLUME 3, Sorting
and Searching, 2nd ed., Addison-Wesley, 1998.
[8] K. B. Lakshmanan and B. Ravikumar and K. Ganesan, Coping with
Erroneous Information While Sorting, IEEE Transactions on Computers,
Volume 40 Issue 9, 1991.
[9] V. N. Vapnik, Estimation of Dependences Based on Empirical Data,
Springer, 1982.

V. C ONCLUSION
Algorithm design has been mostly concerned with resource
issues like time and space requirements. In this work, we
have presented a new analysis technique for algorithms: The
trajectories of algorithm runs are tracked in the relevant solution space, thereby counting the number of bits an algorithm
can extract before the overlap (and thus stability) between
runs vanishes. This information-theoretic approach sheds new
light on the informativeness of algorithms in noisy settings,
and thereby will enable a new dimension of analysis of
algorithms. It fosters the development of new stable algorithms

5

