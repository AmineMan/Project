Title:          jscc_isit.dvi
Creator:        dvips(k) 5.991 Copyright 2011 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Sat May 19 01:24:37 2012
ModDate:        Tue Jun 19 12:55:43 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      469872 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569565571

The Adversarial Joint Source-Channel Problem
Yuval Kochman, Arya Mazumdar and Yury Polyanskiy
ˆ
d : S × S → R+ . The distortion between a source string sk
and a reproduction sk is given by:
ˆ

Abstract—This paper introduces the problem of joint sourcechannel coding in the setup where channel errors are adversarial
and the distortion is worst case. Unlike the situation in the
case of stochastic source-channel model, the separation principle
does not hold in adversarial setup. This surprising observation
demonstrates that designing good distortion-correcting codes
cannot be done by serially concatenating good covering codes
with good error-correcting codes. The problem of the joint code
design is addressed and some initial results are offered.

d(sk , sk ) =
ˆ

1
k

k

d(sj , sj ) .
ˆ

(1)

j=1

In the stochastic setting, an (k, Mk , D)-source code is
ˆ
speciﬁed by a surjective map φ : S k → C for some C ⊆ S k
such that |C| = Mk and the expected distortion is at most D,
where the mean is taken with S k ∼ P k (memoryless source).
The rate of the source code is deﬁned by 1/k · log Mk and
asymptotically, the best possible rate for the distortion D is
given by [3]:

I. I NTRODUCTION
One of the great contributions of Shannon [1] was creation
of tractable and highly descriptive stochastic models for the
signal sources and communication systems. Shortly after, his
work was followed up by Hamming [2], who proposed a
combinatorial variation of the channel coding part. This combinatorial formulation has become universally accepted in the
coding-theoretic community. Similarly, for the case of lossy
compression Shannon [3] proposed a stochastic model and the
rate-distortion formula, while shortly after Kolmogorov followed up with a non-stochastic deﬁnition of the -entropy [4].
The research that followed demonstrated how both ways of
thinking, stochastic and combinatorial, naturally complement
each other, reinforcing intuition and yielding new results.
To the best of our knowledge, in the setup of joint sourcechannel coding, however, only the stochastic approach has
been investigated so far, starting with [1], [3]. This paper aims
to ﬁll in this omission.
In Section II we deﬁne the adversarial separate source and
channel coding problems and present known results about
them. Then, we build on these deﬁnitions to deﬁne the
adversarial joint source channel coding (JSCC) problem. Next,
in Section III we prove asymptotic bounds on the performance
limits of adversarial JSSC codes. It turns out that the celebrated
separation principle [1], [3] does not hold in the adversarial
model. Therefore, the problem of constructing asymptotically
optimal adversarial JSSC codes requires a joint approach
and cannot be solved by combining good compressors with
good error-correcting codes. In Section IV we focus on the
binary case and propose methods for designing such codes
and analyzing their performance.

R(P, D) =

min

ˆ
PS|S :E [d(S,S)]≤D
ˆ

ˆ
I(S; S) .

In the adversarial setting, a source set F ⊆ S k is selected
and then the smallest cardinality of a covering of F upto
distortion D is sought; cf. [4]. Here we restrict ourselves to
the case of F being the set of all source sequences that are
strongly typical1 with respect to the source distribution P .
The adversarial (k, Mk , D) source code is deﬁned by a
ˆ
collection of Mk points C ⊂ S k such that for any P typical source sequence sk there exists a point sk in C such
ˆ
that d(sk , sk ) ≤ D. The asymptotic fundamental limit of
ˆ
adversarial source coding is deﬁned to be
1
log max{Mk : ∃(k, Mk , D)
k→∞ k
-adversarial source code} .

Rad (P, D) = lim

Not only does this limit exist, but remarkably it coincides with
R(P, D):
Theorem 1 (Berger’s type covering [6]):
Rad (P, D) = R(P, D).
ˆ
As an example, take S = S = F2 and P is the uniform
distribution, with the Hamming distortion measure. It is known
that

II. P RELIMINARIES
A. Source coding

Rad (P, D) = R(P, D) = 1 − h2 (D),

A source problem is speciﬁed by a source and reproduction
ˆ
alphabets S, S, a distribution P on S and a distortion metric

where h2 (x) = −x log x − (1 − x) log(1 − x) is the binary
entropy function. Indeed the same rate is achievable even if
the source set is entire Fk [7].
2

Yuval Kochman is with the School of Computer Science and
Engineering, the Hebrew University of Jerusalem, Jerusalem, Israel.
e-mail: yuvalko@cs.huji.ac.il. Arya Mazumdar and Yury Polyanskiy
are with the Department of Electrical Engineering and Computer Science,
MIT, Cambridge, MA 02139, USA. e-mail: {aryam,yp}@mit.edu.

1 Here

1

and in the sequel, strong typicality is in the sense of [5, Chapter 2].

√
with ˆ
h(x) = h2 (1/2 − 1/2 1 − x), and the GilbertVarshamov bound [9] is

B. Channel coding
A channel problem is speciﬁed by input2 and output alphabets X , Y, and a conditional distribution W : X → Y.
In the stochastic setting, an (n, M, )-channel code is
speciﬁed by a pair of maps f : {1, . . . , M } → X n and
g : Y n → {1, . . . , M } such that
P[g(Y n ) = i|X n = f (i)] ≥ 1 − ,

RGV (δ) = 1 − h2 (2δ).
C. Adversarial JSCC

A JSCC problem is speciﬁed by:
ˆ
• Adversarial source: S, S, PS , d(·, ·)
• Adversarial channel: X , Y, WY |X .
At source and channel blocklengths (k, n), a JSCC scheme is
speciﬁed by:
k
n
• an encoder map S → X
from the source to channel
n
k
input: x = f (s ).
n
ˆ
• a decoder map Y
→ S k from the channel output to
k
n
reconstruction: s = g(y ).
ˆ
We say that a JSSC scheme is (k, n, D) adversarial if for
all P -typical source sequence sk and corresponding channel
outputs y n ∈ A(f (sk )), d(sk , g(y n )) ≤ D.
The asymptotically optimal tradeoff between the achievable
distortion and the bandwidth expansion factor ρ = n is given
k
by

i = 1, . . . , M ,

where PY n |X n = W n (a memoryless channel). The rate of the
code is deﬁned as 1/n · log Mn and asymptotically the largest
achievable rate is given by Shannon capacity [1]:
C(W ) = max I(X; Y ) .
PX

In the adversarial setting, for each input sequence xn ∈ X n
the channel output may be arbitrary within a subset of Y n . We
choose this set to be A(xn ) ⊆ S n , the set of strongly typical
sequences y n given xn with respect to W. The adversarial
(n, Mn ) channel code is deﬁned as a collection of Mn points
C ⊂ X n such that for any pair of different points xn , z n ∈ C,
A(xn ) ∩ A(z n ) = ∅. The asymptotic fundamental limits of
adversarial channel coding are deﬁned to be
1
log max{Mn : ∃(n, Mn )
n
-adversarial channel code}
1
C ad (W ) = lim inf log max{Mn : ∃(n, Mn )
n→∞ n
-adversarial channel code} .

∗

Dad (ρ) = lim sup inf{D : ∃(k, ρk , D)
k→∞

C ad (W ) = lim sup

adversarial JSCC} ,

n→∞

D∗ (ρ)
ad

(2)

(3)

min

(8)

In the asymptotic limit this can be approached, yielding the
asymptotic fundamental limit:
D∗ (ρ) = inf{D : R(P, D) ≤ ρC(W )} .
D. The separation principle
We say that an (k, n) JSCC scheme is separation-based if
for some space M (“the message space”) the encoder consists
of a source encoder fS : S k → M and a channel encoder
fC : M → X n . The decoder consists of a channel decoder
ˆ
gC : Y n → M and a source decoder M → S k . Furthermore,
following e.g. [10] we introduce a bijection σ : M → M that
is applied at the encoder and reversed in the decoder, which
is meant to ensure that there the mapping of source messages
to channel ones is arbitrary. The encoder and decoder are thus
given by

(4)

where the MRRW II bound [8] is
0<α≤1−4δ

k→∞

As in the source and channel cases, we use the stochastic
setting performance as a benchmark. In this setting, the source
and channel are i.i.d. according to P = PS and W = WY |X ,
and the requirement is for expected distortion to be at most D.
It is well known that any k-to-n stochastic JSCC must satisfy
[3],
k · R(P, D) ≤ n · C(W ).
(9)

and similarly for C ad . Therefore, by the classical results on
A(n, d):

RMRRW (δ) =

(7)

= lim inf inf{D : ∃(k, ρk , D)
adversarial JSCC} .

Note that because the limits are not known to coincide for
most channels of interest, we have to deﬁne both upper and
lower limits.
It is known that C ad (W ) ≤ C(W ). Furthermore, in contrast
to source coding, this inequality is known to be strict in the
next example.
The most studied case of the adversarial channel coding is
that of a binary symmetric channel with crossover probability
δ, BSC(δ). Let A(n, d) be the cardinality of a largest set in Fn
2
with minimal Hamming distance between any pair of elements
not smaller than d. We have3 :
1
C ad = lim sup log A(n, 2nδ + 1) ,
n→∞ n

RGV (δ) ≤ C ad (δ) ≤ C ad (δ) ≤ RMRRW (δ) < C(δ),

(6)

1 + ˆ 2 ) − ˆ 2 + 4δα + 4δ) , (5)
h(α
h(α

2A

f = fS ◦ σ ◦ fC ; g = gC ◦ σ −1 ◦ gS

cost function on X may also be present. We omit it to save space.
one considers the case when the adversary is free to choose
noise vectors e satisfying wt(e) ≤ δn, whereas in our setting the typicality
constrains wt(e) = δn ± o(n). This is asymptotically immaterial, since in
Hamming space two spheres of the same radius are disjoint if and only if the
corresponding balls are.
3 Traditionally,

(10)

where performance is required to hold for any bijection σ.
The asymptotic performance limits of the separation
∗
schemes are denoted as Dad,sep (ρ) and D∗
ad,sep (ρ) and deﬁned

2

in complete analogy with (7) and (8). In the stochastic setting,
the asymptotic performance of the optimal separation scheme
coincides with D∗ (ρ) and thus does not need a special
notation.
III. B OUNDS

ON ADVERSARIAL

C. Binary example
We now combine the binary examples presented in sections
II-A and II-B: the source is binary symmetric with Hamming distortion, and the channel is BSC(δ). The informationtheoretic optimum D∗ (ρ) is given by the solution D to:

JSCC

1 − h2 (D) = ρ(1 − h2 (δ))

We start this section with an immediate lower bound on the
fundamental limit of adversarial asymptotic distortion.
Theorem 2 (Converse):

whenever the r.h.s. is lower than one, zero otherwise. Bounds
on the performance of separation-based schemes are given by
the solutions to:

D∗ (ρ) ≥ D∗ (ρ) .
ad

1 − h2 (D) = ρ · RMRRW (δ)
1 − h2 (D) = ρ · RGV (δ),

Proof: Any adversarial JSCC can be used as a usual
(probabilistic) JSCC, in which case by typicality arguments
it will achieve (maximal) distortion D with vanishing excess
probability (namely, we assume excess distortion whenever the
source or channel behavior are not strongly typical). Thus D
must not be smaller than D∗ (ρ).

where again the bounds are zero for r.h.s. above one. Since
RMRRW < 1 − h2 (δ) for all δ > 0, it follows that
∗
D∗
ad,sep (ρ) > D (ρ) strictly whenever ρRMRRW (δ) < 1.
For ρ = 1 the optimum D∗ (1) is achievable by a trivial
single-letter scheme (namely, the identity encoder and decoder). Therefore, for ρ = 1 and any δ > 0,
∗
D∗ (1) = Dad (1) < D∗
ad,sep (ρ).

A. Separated schemes
Theorem 3 (Separated schemes): If R(P, D) > ρC ad (W )
then
D∗
ad,sep (ρ) ≥ D .

For other values of ρ, separation may also be suboptimal:
Proposition 6: For any positive integer ρ, repetition coding
(i.e., xn is constructed by ρ repetitions of sk ) achieves
asymptotically:
2ρδ
Drep (ρ) =
(14)
1+ρ

(11)

If R(P, D) ≤ ρC ad (W ) then
∗

Dad,sep (ρ) ≤ D .

(12)

∗

By (4) and Theorem 3, it is easy to see that Dad,sep (ρ) =
= 1/2 whenever δ = 1/4. Thus, comparing
with (14) and by continuity for any positive integer ρ there is
an interval of δ for which simple repetition coding outperforms
any separation-based scheme.

D∗
ad,sep (ρ)

We will show shortly, that (11) demonstrates (in special
∗
∗
cases) that Dad,sep > Dad .
B. Single-letter schemes

IV. B INARY SYMMETRIC SOURCE - CHANNEL (BSSC)
In this section we slightly change the problem deﬁnition, in
order to make it closer in spirit to that of traditional approach
taken in the coding-theoretic literature for the Hamming space.
Namely, we drop the strong typicality constraints on the source
and the channel. Instead, we let the source outputs be any
binary sequences in Fk , while the (adversarial) channel is
2
allowed to ﬂip up to δn bits.
Deﬁnition 1: A (k, n, D) adversarial JSSC code for the
BSSC(δ) is a pair of maps f : Fk → Fn , g : Fn → Fk
2
2
2
2
such that
wt(x + g(f (x) + e)) ≤ kD ,

Another special class of JSCC schemes is single-letter
codes. In that case, the mappings f (·) and g(·) are scalar,
and when applied to a block they are computed in parallel
for each entry. Some examples where single-letter schemes
yield the optimum D∗ have been known for a long time, and
Gastpar et al. [11] give the sufﬁcient and necessary conditions
for that to hold.
Theorem 4: If in the stochastic setting a single-letter
scheme achieves some Dsl , then
∗

D ad (1) ≤ Dsl .

(13)

We omit a simple proof of this result, but its essence will
be clear from the example in the next section.
Corollary 5: Whenever single-letter codes are optimal in
the stochastic setting, i.e., Dsl = D∗ (1) we have

for all x ∈ Fk and all wt(e) ≤ δn. The asymptotic fundamen∗2
tal limits Dad (ρ) and D∗ (ρ) are deﬁned as in (7)-(8).
ad
Note that while in channel coding the two deﬁnitions lead
to similar results (recall Footnote 3), it is not clear whether the
same holds for JSCC. For example, in Proposition 6, for even
ρ the decoding relies on the fact that the adversary must ﬂip
approximately δn bits, and if this assumption does not hold,
repetition with even expansion ρ is equivalent to repetition
with expansion ρ − 1 followed by channel uses that can be
ignored.

∗

Dad (1) = D∗ (1) = D∗ (1).
ad
Using Theorems 3 and 4, one may ﬁnd examples in
which single-letter schemes achieve D∗ while separationbased scheme do not, leading to the surprising conclusion that
separation is not optimal in the adversarial setting.

3

A. Information theoretic converse

D. The optimal decoder for BSSC

Note that by Theorem 2, we have that any asymptotically
achievable distortion D over BSSC(δ) satisﬁes

Let Bn (x, r) denote a ball of radius r centered at x in Fn .
2
For any set S ∈ Fn , the radius of the set rad(S) is deﬁned
2
to be the smallest r such that S ⊆ Bn (x, r) for some x ∈ Fn ,
2
with the optimal x’s called the Chebyshev center(s) of S.
Consider some JSSC encoder f : Fk → Fn for the BSSC(δ).
2
2
There exists a decoder achieving distortion D for this if and
only if
∀y ∈ Fn : rad(f −1 Bn (y, δn)) ≤ Dk .
2

1 − h2 (D) ≤ ρ(1 − h2 (δ)).

(15)

In fact, if there exists a JSCC that achieves distortion D,
then any ball of radius δn in Fn must not contain more than
2
k
m
TDk codewords, where Tr is the volume of a ball of radius
r in Fm . However there exists a ball of radius δn in Fn that
2
2
n
contains at least 2k−n Tδn codewords. Hence D must satisfy
n
k
2k Tδn ≤ 2n TDk .

The optimal decoder is then:
g(y) = Chebyshev center of f −1 Bn (y, δn) .

(16)

Asymptotically (16) coincides with (15), but otherwise is
tighter.

In other words, the distortion achievable by the encoder f
is given by
D(f, δ) =

B. New coding converse
The above lower bound on achievable distortion D can be
improved for a region of δ if we consider the fact that any
JSCC also gives rise to an error-correcting code. Recalling
the cardinality A(n, δ) deﬁned in Section II-B, we have the
following.
Theorem 7: If a k-to-n JSCC achieves the distortion D over
BSSC(δ), then

In contrast to channel coding, repetition of a single code
of small block length leads to a non-trivial asymptotic performance.
Fix an arbitrary encoder given by the mapping f : Fu → Fv .
2
2
If there are t errors in the block of length v, t = 0, . . . , v the
performance of the optimal decoder (knowing t) is given by
r0 (t) = max rad(f −1 Bv (y, t)).
v
y∈F2

Fk
2

Proof: Suppose there is a code D ⊂
that corrects up to
ˆ
any Dk errors. Let D be the image of this code in Fn under
2
ˆ
the JSCC encoding. We claim that D is a code in Fn that
2
corrects any up to δn errors. Indeed, up to δn errors can be
reduced to at most Dk errors in Fk with the JSCC decoding.
2
These errors are then correctable with the decoding of D.
Asymptotically, applying (4) to Theorem 7 we obtain:
Corollary 8: For the BSSC(δ) the distortion D∗ (ρ) satisad
ﬁes:
(17)
RGV (D ∗ (ρ)) ≤ ρRMRRW (δ) .
ad

rg (t) = max max wt(g(f (x) + e) + x).
u
wt(e)≤t x∈F2

Clearly
rg (t) ≥ r0 (t)
and the decoder g achieving this bound with equality is called
a universal decoder. Some trivial properties: r0 (0) = 0 if and
only if f is injective, rg (0) = 0 if and only if g is a left inverse
of f , r0 (v) = rg (v) = u.
Example: Any repetition code F2 → Fv is universally
2
decodable with a majority-vote decoder g (resolving ties
arbitrarily):

As explained in Footnote 3, the limits for channel coding
are the same for strongly typical channel and for maximum
number of ﬂips. Thus, by Theorem 3, the asymptotic performance of the separation schemes must satisfy
≤ ρRMRRW (δ).

(20)

Consider also an arbitrary decoder g : Fv → Fu and its
2
2
performance curve:

C. Achievability and converse for separation scheme

ρRGV (δ) ≤ 1 −

1
max rad(f −1 Bn (y, δn)) .
k y∈Fn
2

E. Repetition of a small code

A(k, 2Dk + 1) ≤ A(n, 2nδ + 1).

∗
h2 (Dad,sep (ρ))

(19)

rg (t) = r0 (t) =

(18)

0, t <
1, t ≥

v
2
v
2

,
.

From a given code f we may construct a longer code by
repetition to obtain an Fk → Fn code as follows, where Lu =
2
2
k, Lv = n:

Remark: Note that, although the exact value of C ad or C ad
is unknown, the argument in Theorem 7 demonstrates that in
the regime of distortion D → 0, separation yields an optimal
(but unknown) performance.
Just as in Section III-C it is clear that in the case ρ = 1
separation is strictly suboptimal for all δ > 0. Comparison of
the different bounds for this case is shown in Fig. 1. Next, we
show examples of codes that beat separation for other ρ = 1.

fL (x1 , . . . , xL ) = (f (x1 ), . . . , f (xL )) .
This yields a sequence of codes with ρ = n/k = v/u. We want
to ﬁnd out the achieved distortion D(δ) as a function of the
maximum crossover portion δ of the adversarial channel.

4

ρ=1

3-repetition code is contrasted with that of the separation
schemes. In the same plot the converse bounds (17) and (15)
are plotted. For δ > 0.23 it is clear that 3-repetition achieves
better performance than any separation scheme.
Example: [5,2,3] linear code for ρ = 5/2: Consider the
linear map f : F2 → F5 given by the generator matrix
2
2

0.5

0.45

0.4

0.35

D

0.3

0 0
1 1

0.25

0.2

0.1

0

Eqn. (15)
Lower bound of Eqn. (18)
Upper bound of Eqn. (18)
0

0.05

0.1

0.15

0.2

0.25
δ

0.3

0.35

0.4

0.45

0.5

Fig. 1. Trade-off between δ and D in a BSSC(δ) for ρ = 1. An identity
map (single-letter scheme) is everywhere optimal.
ρ =3
0.5
0.45
0.4
0.35

D

0.3
0.25
0.2

.

R EFERENCES

0.15

[1] C. E. Shannon. A mathematical theory of communication. Bell Syst.
Tech. J., 27:379–423 and 623–656, July/October 1948.
[2] R.W. Hamming. Error detecting and error correcting codes. Bell System
Technical Journal, 29(2):147–160, 1950.
[3] C. E. Shannon. Coding theorems for a discrete source with a ﬁdelity
criterion. In 1959 IRE Nat. Conv. Rec., volume pt. 4, pages 142–163,
1959.
[4] A. N. Kolmogorov and V. M. Tikhomirov. Theory of transmission of
information. Amer. Math. Soc. Trans., 17:277–364, 1961.
[5] I. Csisz´ r and J. K¨ rner. Information Theory: Coding Theorems for
a
o
Discrete Memoryless Systems. Academic, New York, 1981.
[6] T. Berger. Rate-Distortion Theory: A Mathematical Basis for Data
Compression. Prentice-Hall, Englewood Cliffs, NJ, 1971.
[7] G. Cohen, I. Honkala, S. Litsyn, and A. Lobstein. Covering Codes.
North-Holland, 1997.
[8] R. McEliece, E. Rodemich, H. Rumsey, and L. Welch. New upper
bounds on the rate of a code via the Delsarte-MacWilliams inequalities.
IEEE Trans. Inf. Theory, 23(2):157–166, 1977.
[9] F. J. MacWilliams and N. J. A. Sloane. The Theory of Error-Correcting
Codes. North-Holland, 1997.
[10] D. Wang, A. Ingber, and Y. Kochman. The dispersion of joint sourcechannel coding. In Proc. 2011 49th Allerton Conference, volume 49,
Allerton Retreat Center, Monticello, IL, USA, September 2011.
[11] M. Gastpar, B. Rimoldi, and M. Vetterli. To code, or not to code:
Lossy source-channel communication revisited. IEEE Trans. Inf. Theory,
49(5):1147–1158, May 2003.
[12] P. Elias. Error-correcting codes for list decoding. IEEE Trans. Inf.
Theory, 37(1):5–12, 1991.
[13] V. M. Blinovsky. Bounds for codes in the case of list decoding of ﬁnite
volume. Prob. Peredachi Inform., 22(1):7–19, 1986.
[14] V. Guruswami and S. Vadhan. A lower bound on list size for list
decoding. IEEE Trans. Inf. Theory, 56(11):5681–5688, November 2010.

Eqn. (15)
Lower bound of Eqn. (18)
Upper bound on Eqn. (18)
Eqn. (17)
Eqn. (23) with v = 3

0.1
0.05
0

1
1

It can be shown that r0 (t) = {0, 0, 1, 2, 2, 2} for t =
{0, 1, 2, 3, 4, 5} and there exists a universal decoder g. Thus
by Theorem 9 this code achieves D = 5δ/3. For δ > 0.22, this
is better than what any separation scheme can achieve. This
example demonstrates that in the JSSC setup one should not
always use a simple decoder that maps to the closest codeword.
In fact, further analysis demonstrates that perfect codes, Golay
and Hamming, are among the worst in terms of distortion
tradeoff.
Remark: Note that there exist [12] linear codes of rate ρ−1
decodable with ﬁnite list size and capable of correcting all
errors up to the information theoretic limit n h2 −1 (1 − ρ−1 ).
However, by the converse bound (17) it follows that the
radius of the list in Fk must be Ω(k) regardless of the map
2
between Fk and the codewords. This provides some interesting
2
complement to the study of the properties of lists of codes
achieving the information theoretic limit [13], [14].

0.15

0.05

1 1
0 0

0

0.05

0.1

0.15

0.2

0.25
δ

0.3

0.35

0.4

0.45

0.5

Fig. 2. Trade-off between δ and D in a BSSC(δ) for ρ = 3. 3-repetition
code achieves better distortion than any separation scheme for δ > 0.22

Theorem 9: The asymptotic distortion achievable by fL
(repetition construction) satisﬁes
1 ∗∗
r (δv) .
u 0
A block-by-block decoder g achieves
lim inf D(fL , δ) ≥
L→∞

(21)

1 ∗∗
r (δv) ,
(22)
u g
are upper convex envelopes of r0 and rg

lim inf Dg (fL , δ) =
L→∞

∗∗
∗∗
where r0 and rg
respectively.
Example: Repetition code: Consider using a [v, 1, v] repetition code. Since for such a code rg (t) = r0 (t), the upper and
lower bounds of Theorem 9 coincide. For odd v we have:
2δv
D=
.
(23)
v+1
(Compare this with Proposition 6 for the strong-typicality
model of Section II-C.) In Fig. 2 the performance of the

5

