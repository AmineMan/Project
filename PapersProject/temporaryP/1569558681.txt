Title:          DI_full conf.dvi
Creator:        www.freepdfconvert.com         
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May 16 19:09:50 2012
ModDate:        Tue Jun 19 12:54:29 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      406627 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569558681

Universal Estimation of Directed Information via
Sequential Probability Assignments
Jiantao Jiao

Haim H. Permuter

Lei Zhao

Young-Han Kim

Tsachy Weissman

Tsinghua University
xajjt1990@gmail.com

Ben Gurion University
haimp@bgu.ac.il

Jump Operations
zhaolei122@gmail.com

UCSD
yhk@ucsd.edu

Stanford University
tsachy@stanford.edu

directed information to test the direction of inﬂuence in gene
networks.
Since directed information has signiﬁcance in various ﬁelds,
it is of both theoretical and practical importance to develop
efﬁcient ways for estimating it. The problem of estimating
information measures, such as entropy, relative entropy and
mutual information, has been extensively studied in the literature. Verd´ [11] gave an overview of universal estimation of
u
information measures. Wyner and Ziv [12] applied the idea
of Lempel–Ziv parsing to estimate the entropy rate, which
converges in probability for all stationary ergodic processes.
Ziv and Merhav [13] used Lempel–Ziv parsing to estimate
relative entropy (Kullback–Leibler divergence) and established
consistency under the assumption that the observations are
generated by independent Markov sources. Cai, Kulkarni, and
Verd´ [14] proposed two universal divergence estimators for
u
ﬁnite-alphabet sources, one based on the Burrows–Wheeler
transform (BWT) [15] and the other based on the context tree
weighting method (CTW) [16]. The BWT-based estimator was
applied in universal entropy estimation in Cai, Kulkarni, and
Verd´ [17], while the CTW-based one was applied in universal
u
erasure entropy estimation in Yu and Verd´ [18].
u
For the problem of estimating directed information, Quinn,
Coleman, Kiyavashi, and Hatspoulous [19] developed an
estimator to infer causality in ensemble neural spike train
recordings. Based on parametric generalized linear model
(GLM) assumption and stationary ergodic Markov assumption
[19], they showed strong consistency results. Compared to
[19], Zhao, Kim, Permuter, and Weissman [20] focused on
universal methods and showed L1 consistency for all jointly
stationary ergodic process pairs with ﬁnite alphabet.
As an improvement and further development of [20], and a
reﬂection of Jiao, Permuter, Zhao, Kim, and Weissman [21],
the main contribution of this paper is a general framework
for estimating information measures of stationary ergodic processes, using “single-letter” information-theoretic functionals.
Although our methods can be applied in estimating a number
of information measures, we focus—for concreteness and
relevance to emerging applications—on estimating the directed
information rate between a pair of jointly stationary ergodic
processes. The ﬁrst proposed estimator is adapted from the
universal divergence estimator in [14] using the CTW method,
and we give a reﬁned analysis yielding strong consistency

Abstract—We propose four approaches to estimating the directed information rate between a pair of jointly stationary
ergodic processes with the help of universal probability assignments. The four approaches yield estimators with different merits
such as nonnegativity and boundedness. We establish consistency
of these estimators in various senses and derive near-optimal rates
of convergence in the minimax sense under mild conditions. The
estimators carry over directly to estimating other information
measures of stationary ergodic processes, such as entropy rate
and mutual information rate, and provide alternatives to classical
approaches in the existing literature. Guided by the theoretical
results, we use context tree weighting as the vehicle for the
implementations of the proposed estimators. Experiments on
synthetic and real data are presented, demonstrating the potential
of the proposed schemes in practice and the efﬁcacy of directed
information estimation as a tool for detecting and measuring
causality and delay.
Index Terms—Causal inﬂuence, context tree weighting, directed information, rate of convergence, universal probability
assignment

I. I NTRODUCTION
First introduced by Marko [1] and Massey [2], directed information arises as a natural counterpart of mutual information
for channel capacity when causal feedback from the receiver
to the sender is present. In [3] and [4], Kramer extended the
use of directed information to discrete memoryless networks
with feedback, including the two-way channel and the multiple access channel. Tatikonda and Mitter [5] used directed
information spectrum to establish a general feedback channel
coding theorem for channels with memory. Kim [6] established
the feedback capacity for a class of stationary channels using
directed information. In [7], Permuter, Weissman, and Goldsmith considered the capacity of discrete-time channels with
feedback where the feedback is a time-invariant deterministic
function of the output, and used directed information to describe the capacity under mild conditions. Recently, Permuter,
Kim, and Weissman [8] showed that directed information plays
an important role in portfolio theory, data compression, and
hypothesis testing, in the presence of causality constraints.
Beyond information theory, directed information is a valuable tool in biology, for it provides an alternative to identify
causal inferences between two processes. In Mathai, Martins,
and Shapiro [9], directed information was used to identify
pairwise inﬂuence. Rao, Hero, States, and Engel [10] used

1

directed information has the causally conditional entropy in
place of the conditional entropy. Unlike mutual information,
directed information is not symmetric, i.e., I(Y n → X n ) =
I(X n → Y n ) in general.
We have the conservation law for directed information

results. We further propose three additional estimators in a
uniﬁed framework to estimate the directed information rate,
present both weak and strong consistency results, and establish
near-optimal rates of convergence under mild conditions. We
then employ our estimators on both simulated and real data,
showing their effectiveness in measuring channel delays and
causal inﬂuences between processes. In particular, we use
these estimators to establish signiﬁcant causal inﬂuence from
the Dow Jones Industrial Average to the Hang Seng Index, but
relatively low causal inﬂuence in the reverse direction, based
on the daily market data in the period from 1990 to 2011.
The rest of the paper is organized as follows. Section
II reviews some preliminaries and Section III presents our
proposed estimators and some of their basic properties. Section
IV is dedicated to performance guarantees for the proposed
estimators, rates of convergence results under mild conditions,
and minimax optimality. Section V shows experimental results
applying the proposed estimators, both on simulated and real
data, and demonstrates the effectiveness of these estimators
in inferring delay of channels and causal inﬂuences between
processes. For proofs of stated results please see [21].

I(X n ; Y n ) = I(X n → Y n ) + I(Y n−1 → X n ),

(5)

where
I(Y n−1 → X n ) = I((∅, Y n−1 ) → X n )

(6)

n

= H(X n ) −

H(Xi |X i−1 , Y i−1 )

(7)

i=1

denotes the reverse directed information. Other interesting
properties of directed information can be found in [3], [22].
The directed information rate [3] between a pair of jointly
stationary random processes X and Y is deﬁned as
I(X → Y)

lim

n→∞

1
I(X n → Y n ),
n

(8)

and we can easily check that
I(X → Y) = H(Y) − H(Y X),

II. P RELIMINARIES

(9)

We begin with deﬁnitions of directed information, universal
and pointwise universal probability assignments. We then
introduce the context tree weighting (CTW) method used in
our implementations.
We use uppercase letters X, Y, . . . to denote random variables, and lowercase letters x, y, . . . to denote values they
assume. We denote the n-tuple (X1 , X2 , . . . , Xn ) as X n and
(x1 , x2 , . . . , xn ) as xn . Calligraphic letters X , Y, . . . denote
alphabets of X, Y, . . ., and |X | denotes the cardinality of X .
Given a probability law P , P (xi |X i−1 ) denotes the conditional pmf P (xi |xi−1 ) evaluated for the random sequence
X i−1 , while P (Xi |X i−1 ) is the random variable denoting the
Xi th component of P (xi |X i−1 ). Throughout this paper, log(·)
means log2 (·).

where H(Y) =
is the entropy rate of
process Y, H(Y X) is the causally conditional entropy
rate deﬁned as H(Y X)
limn→∞ (1/n)H(Y n X n ) =
−1
0
H(Y0 |X−∞ , Y−∞ ).
Equation (9) shows that we can estimate H(Y) and
H(Y X) separately to estimate the directed information rate.

A. Directed Information

1
D(P (xn )||Q(xn )) = 0
(10)
n→∞ n
for every probability measure P in P. A probability assignment Q is said to be universal (without a qualiﬁer) if it is
universal for the class of stationary probability measures.
Deﬁnition 2 (Pointwise universal probability assignment):
A probability assignment Q is said to be pointwise universal
for a class P if

−1
H(Y0 |Y−∞ )

B. Universal Probability Assignment
A probability assignment Q consists of a set of conditional
pmfs Q(xi |xi−1 ) for every xi−1 ∈ X i−1 . Note that Q induces
a probability measure on a random process X.
Deﬁnition 1 (Universal probability assignment): A probability assignment Q is said to be universal for a class P
if the normalized Kullback–Leibler divergence satisﬁes
lim

Introduce the notation of causal conditionsl pmf as follows:
n

p(xn y n ) =

p(xi |xi−1 , y i ),

(1)

i=1

the directed information from X n to Y n is deﬁned as
n

I(X n → Y n )

I(X i ; Yi |Y i−1 ) = H(Y n )−H(Y n X n ),
i=1

(2)
where H(Y n X n ) is the causally conditional entropy [3],
concretely,

lim sup
n→∞

H(Yi |Y i−1 , X i ).

(3)

i=1

Compared with the deﬁnition of mutual information,
I(X n ; Y n ) = H(Y n ) − H(Y n |X n ),

≤0

P -a.s.

(11)
for every probability measure P in P. A probability assignment Q is said to be pointwise universal (without a qualiﬁer)
if it is pointwise universal for the class of stationary ergodic
probability measures.
It is well known that there exist universal and pointwise
universal probability assignments, see, for example, [23].

n

H(Y n X n )

1
1
1
1
log
− log
n
Q(X n ) n
P (X n )

(4)

2

One particularly celebrated universal probability assignment
is the context tree weighting (CTW) algorithm by Willems,
Shtarkov, and Tjalken [16]. The computational complexity of
the CTW is linear in the block length n, and the algorithm
provides the probability assignments Q directly, which is the
weighted probability at the root node. For details, see [16],
[24] and [21]. Note that here we use the extended version of
CTW for non-binary alphabets, which is discussed in [25].
The probability assignment Q in CTW is both universal and
pointwise universal for the class of stationary ergodic Markov
processes. For the proof of universality, see, [16], and for the
pointwise universality, please see [21].

again on dataset Y i−1 . Of course, Q(yi |xi , X i−1 , Y i−1 ) is
computed from Q(xi , yi |X i−1 , Y i−1 ).
ˆ
The estimator I1 is adapted from one universal divergence
ˆ
estimator in [14]. One disadvantage of I1 (X n → Y n ) is that
it has a nonzero probability of being very large, which is
ˆ
overcome by I2 , the estimator introduced in [20], by using
information-theoretic functionals to “smooth” the estimate.
ˆ
Evidently we can show |I2 | ≤ log |Y|.
ˆ
ˆ
The common disadvantage of I1 and I2 is that they are
computed by subtraction of two quantities, and have a nonzero
ˆ
ˆ
probability of being negative. I3 and I4 are introduced to
overcome this, since they take the form of a Kullback–Leibler
divergence and are always nonnegative.

III. F OUR E STIMATION A LGORITHMS

IV. P ERFORMANCE G UARANTEES

In this section, we introduce four algorithms to estimate
the directed information rate I(X → Y) of a pair of jointly
stationary ergodic processes X and Y. Let M(X , Y) be the
set of all probability distributions on X × Y. Deﬁne f as
the function that maps a joint pmf P (x, y) of a random pair
(X, Y ) to the corresponding conditional entropy H(Y |X), i.e.,

In this section, we present consistency results for the
proposed estimators. Under some mild conditions, we derive
near-optimal rates of convergence in the minimax sense. For
proofs, please see [21].
Theorem 1: Let Q be a universal probability assignment
and (X, Y) be jointly stationary ergodic. Then

C. Context Tree Weighting Method

−

f (P )

P (x, y) log P (y|x),

ˆ
lim I1 (X n → Y n ) = I(X → Y)

(12)

n→∞

x,y

ˆ
I3 (X n → Y n )

ˆ
ˆ
H1 (Y n ) − H1 (Y n X n ),
n
ˆ
ˆ
H2 (Y ) − H2 (Y n X n ),
1
n

(13)
(14)

n

D(Q(yi |X i , Y i−1 ) Q(yi |Y i−1 )),
i=1

(15)

ˆ
E I1 (X n → Y n ) − I(X → Y) ≤ C1 n−1/2 log n,

ˆ
I4 (X → Y )
n

1
n

(19)

Furthermore, if Q is also a pointwise universal probability
assignment, then the limit in (19) holds almost surely as well.
If (X, Y) is a stationary ergodic aperiodic Markov process,
ˆ
we can say more about the performance of I1 using the
probability assignment in CTW method.
Proposition 1: Let Q be the probability assignment in
CTW. If (X, Y) is a jointly stationary ergodic aperiodic
Markov process whose order does not exceed the prescribed
maximum depth in CTW, then there exists a constant C1 such
that

where P (y|x) is the conditional pmf induced by P (x, y). Take
Q as a universal probability assignment.
Deﬁne four estimators as follows:
ˆ
I1 (X n → Y n )
ˆ
I2 (X n → Y n )

in L1 .

n

(20)

and ∀ > 0,

n

D(Q(xi+1 , yi+1 |X i , Y i ) Q(yi+1 |Y i )Q(xi+1 |X i , Y i )),
i=1

ˆ
I1 (X n → Y n ) − I(X → Y) = o(n−1/2 (log n)5/2+ ) P -a.s.
(21)
We can establish similar consistency results for the second
ˆ
estimator I2 in (14).
Theorem 2: Let Q be a universal probability assignment,
and (X, Y) be jointly stationary ergodic. Then

(16)

where
ˆ
H1 (Y n X n )

−

ˆ
H2 (Y n X n )

1
n

1
log Q(Y n X n ),
n

(17)

n

f (Q(xi+1 , yi+1 |X i , Y i )),

(18)

ˆ
lim I2 (X n → Y n ) = I(X → Y) in L1 .

i=1

n→∞

ˆ
ˆ
ˆ
ˆ
and H1 (Y n ) = H1 (Y n ∅), H2 (Y n ) = H2 (Y n ∅). Note that
ˆ
an entropy estimate such as H1 (Y n X n ) is a random variable
(since it is a function of (X n , Y n )), as opposed to entropy
terms such as H(Y n X n ), which are deterministic and depend
on the distribution of (X n , Y n ).
Note that the universal probability assignments conditioned
on different data are calculated separately. For example,
Q(yi |Y i−1 ) is not computed from Q(xi , yi |X i−1 , Y i−1 ), but
by running the universal probability assignment algorithm

(22)

ˆ
As was the case for I1 , if the process (X, Y) is a jointly
stationary ergodic aperiodic Markov process, we can say more
ˆ
about the performance of I2 as follows:
Proposition 2: Let Q be the probability assignment in
CTW. If (X, Y) is a jointly stationary ergodic Markov process
whose order does not exceed the prescribed maximum depth
in CTW, then
ˆ
lim I2 (X n → Y n ) = I(X → Y)

n→∞

3

P -a.s. and in L1 . (23)

Furthermore, if (X, Y) is also aperiodic, there exists a constant C2 such that

To illustrate the idea, suppose that the binary processes X
and Y are related as

ˆ
E I2 (X n → Y n ) − I(X → Y) ≤ C2 n−1/2 (log n)3/2 .
(24)
The rates of convergence for the ﬁrst two estimators are
optimal within a logarithmic factor in the minimax sense.
Proposition 3: Let P(X, Y) be any class of processes that
includes the class of i.i.d. processes. Then, there exists a
positive constant C3 such that

Yi = Xi−D + Xi−D−1 + Wi ,

ˆ
inf sup E|I − I(X → Y)| ≥ C3 n−1/2 ,
ˆ
I P(X ,Y)

(29)

where Wi ∼ Bernouli( ) and addition in (29) is modulo
1
2. Note that the mutual information rate lim n I(Y n ; X n )
is not inﬂuenced by D. However, the shifted directed in1
formation rate lim n I(Y n+d → X n ) is highly inﬂuenced
by D. Assuming that there is no feedback, for d < D
we have the Markov chain Y i+d → X i−1 → Xi due to
(29), and therefore I(Y n+d → X n ) = 0. However, for
d ≥ D, I(Y n+d → X n ) > 0. For instance, in the channel
example (29), if Wi = 0 almost surely, then for d ≥ D,
I(Y n+d → X n ) = H(X n ). Therefore, we can use the shifted
directed information I(Y n+d → X n ) to estimate D.
For the sake of simplicity, we only show the estimation
results using I2 , other estimators have similar outputs. Fig. 2
ˆ
depicts I2 (Y n+d → X n ) where n = 106 for the setting in
Fig. 1, where the input is a binary stationary Markov process
of order one and the channel is given by (29). The delay of
the channel is D = 2. One can note clearly that for d <
ˆ
D, I2 (Y n+d → X n ) is very close to zero and for d ≥ D,
ˆ
I2 (Y n+d → X n ) is signiﬁcantly larger than zero.

(25)

ˆ
where the inﬁmum is over all estimators I of the directed
n
n
information rate based on (X , Y ).
Evidently, convergence rate better than O(n−1/2 ) is not attainable even with respect to the class of i.i.d. sources and thus,
a fortiori, in our setting of a much larger uncertainty set.
For the third and fourth estimators, we establish the following results.
Theorem 3: Let Q be the probability assignment in CTW.
If (X, Y) is a stationary ergodic Markov process whose order
does not exceed the prescribed maximum depth in CTW, then
ˆ
lim I3 (X n → Y n ) = I(X → Y) P -a.s. and in L1 . (26)

n→∞

Theorem 4: Let Q be the probability assignment in CTW.
If (X, Y) is a stationary ergodic Markov process whose order
does not exceed the prescribed maximum depth in CTW, then

0.5

ˆ
I2 (Y n+d → X n )

0.4

ˆ
lim I4 (X n → Y n ) = I(X → Y) P -a.s. and in L1 . (27)

0.3

n→∞

V. E XPERIMENTAL R ESULTS

0.2

In this section, we show how one can use the directed
information estimator to detect delay of a channel, and to
measure the “causal inﬂuence” of one sequence on another.
We generate simulated data to detect the channel delay and
use real stock market data to detect and measure the causal
inﬂuence that exists between the Chinese and the US stock
markets.

0.1
0
−4

−2

0

2

4

d
ˆ
Fig. 2. The value of I2 (Y n+d → X n ) where n = 106 for the setting
depicted in Fig. 1 with D = 2. One can observe that when d < D,
ˆ
ˆ
I2 (Y n+d → X n ) = 0 and for d ≥ D, I2 (Y n+d → X n ) > 0.

A. Channel Delay Estimation via Shifted Directed Information
Assume a setting depicted as follows: in Fig. 1.

B. Causal Inﬂuence Measurement
· · · X−1 , X0 , X1 , · · ·

Fig. 1.
D.

D units delay

A channel · · · Y−1 , Y0 , Y1 , · · ·
with memory

There is extensive literature on detecting and measuring
causal inﬂuence. See, for example, [26] for a recent survey
of some of the common tools and approaches in biomedical
informatics. One particularly celebrated tool - in both the life
and economics sciences - for assessing whether and to what
extent one time series inﬂuences another is the Granger causality test [27]. It is a simple exercise to verify that under jointly
Gauss-Markov assumptions, the Granger causality coincides
with the directed information (up to a multiplicative constant).
Assuming for every pair (Xi , Yi ), Xi happens earlier than
Yi . It can be easily veriﬁed that I(X n → Y n ) = 0 if and only
if P (yi |xi , y i−1 ) = P (yi |y i−1 ) for i ≥ 1, and I(Y n−1 →

Using the shifted directed information estimation to ﬁnd the delay

Our goal is to ﬁnd the delay D. We use the shifted directed
information I(Y n+d → X n ) to estimate D, where I(Y n+d →
X n ) is deﬁned as
n

I(Y

n+d

n

H(Xi |X i−1 ) − H(Xi |X i−1 , Y i+d ).

→X )
i=1

(28)

4

X n ) = 0 if and only if P (xi |xi−1 , y i−1 ) = P (xi |xi−1 ) for
i ≥ 1. More generally, the directed information I(X n → Y n )
quantiﬁes how much X causally inﬂuences Y, while the
directed information in the reverse direction I(Y n−1 → X n )
quantiﬁes how much Y inﬂuences X.
To illustrate this idea, we compute the directed information
rate between the Hang Seng Index (HSI) and the Dow Jones
Index (DJIA) using data from 1990 and 2011 on a daily scale.
Since everyday the HSI changes before DJIA, HSI should play
the role of process X in the estimation. We discretize the value
of stock market into three values: −1, 1, and 0, by going down
by more than 0.8%, going up by more than 0.8%, and changes
between them, respectively.
We denote by Xi and Yi the (quantized ternary valued)
change in the HSI and the DJIA in day i, respectively, and es1
1
1
timate n I(X n ; Y n ), n I(X n → Y n ), and n I(Y n−1 → X n ),
using all four algorithms. Fig. 3 plots our estimates of these
information-theoretic measures.
Evidently, the reverse directed information is much higher
than the directed information; hence we can say that between
1990 and 2011, it was the Chinese market that was inﬂuenced
more by the US market rather than the other way around.
Alg. 2

Alg. 1
1
I(X n ; Y n ) n
1
I(X n → Y n ) n
n−1 → X n ) 1
I(Y
n

0.2
0.15

[4] ——, “Capacity results for the discrete memoryless network,” IEEE
Trans. Inf. Theory, vol. 49, no. 1, pp. 4–21, 2003.
[5] S. Tatikonda and S. Mitter, “The capacity of channels with feedback,”
IEEE Trans. Inf. Theory, vol. 55, no. 1, pp. 323–349, 2009. [Online].
Available: http://dx.doi.org/10.1109/TIT.2008.2008147
[6] Y.-H. Kim, “A coding theorem for a class of stationary channels with
feedback,” IEEE Trans. Inf. Theory, vol. 54, no. 4, pp. 1488–1499, 2008.
[7] H. H. Permuter, T. Weissman, and A. J. Goldsmith, “Finite state channels
with time-invariant deterministic feedback,” IEEE Trans. Inf. Theory,
vol. 55, no. 2, pp. 644–662, 2009.
[8] H. H. Permuter, Y.-H. Kim, and T. Weissman, “Interpretations of directed information in portfolio theory, data compression, and hypothesis
testing,” IEEE Trans. Inf. Theory, vol. 57, no. 3, pp. 3248–3259, Jun.
2011.
[9] P. Mathai, N. C. Martins, and B. Shapiro, “On the detection of gene
network interconnections using directed mutual information,” in Proc.
UCSD Inf. Theory Appl. Workshop, 2007.
[10] A. Rao, A. O. Hero, D. J. States, and J. D. Engel, “Using directed
information to build biologically relevant inﬂuence networks,” Journal
on Bioinformatics and Computational Biology, vol. 6, no. 3, pp. 493–
519, 2008.
[11] S. Verd´ , “Universal estimation of information measures,” in Proc. of
u
IEEE ISOC ITW2005 on Coding and Complexity, 2005.
[12] A. D. Wyner and J. Ziv, “Some asymptotic properties of the entropy of
a stationary ergodic data source with applications to data compression,”
IEEE Trans. Inf. Theory, vol. 35, no. 6, pp. 1250–1258, 1989.
[13] J. Ziv and N. Merhav, “A measure of relative entropy between individual
sequences with application to universal classiﬁcation,” IEEE Trans. Inf.
Theory, vol. 39, no. 4, pp. 1270–1279, 1993.
[14] H. Cai, S. R. Kulkarni, and S. Verd´ , “Universal divergence estimation
u
for ﬁnite-alphabet sources,” IEEE Trans. Inf. Theory, vol. 52, no. 8, pp.
3456–3475, 2006.
[15] M. Burrows and D. J. Wheeler, A block-sorting lossless data compression algorithm. Digital Systems Research Center, Tech. Rep. 124,
1994.
[16] F. M. J. Willems, Y. M. Shtarkov, and T. J. Tjalkens, “The context-tree
weighting method: Basic properties,” IEEE Trans. Inf. Theory, vol. 41,
no. 3, pp. 653–664, 1995.
[17] H. Cai, S. R. Kulkarni, and S. Verd´ , “Universal entropy estimation via
u
block sorting,” IEEE Trans. Inf. Theory, vol. 50, no. 7, pp. 1551–1561,
2004.
[18] J. Yu and S. Verd´ , “Universal erasure entropy estimation,” in Proc.
u
IEEE Int. Symp. Inf. Theory, 2006.
[19] C. J. Quinn, T. P. Coleman, N. Kiyavash, and N. G.
Hatsopoulos, “Estimating the directed information to infer causal
relationships in ensemble neural spike train recordings,” Journal of
Computational Neuroscience: Special Issue on Methods of Information
Theory in Computational Neuroscience, 2011. [Online]. Available:
http://dx.doi.org/10.1007/s10827-010-0247-2
[20] L. Zhao, Y.-H. Kim, H. H. Permuter, and T. Weissman, “Universal
estimation of directed information,” in Proc. IEEE Int. Symp. Inf. Theory,
2010, pp. 230–234.
[21] J. Jiao, H. H. Permuter, L. Zhao, Y.-H. Kim, and T. Weissman,
“Universal estimation of directed information,” IEEE Trans. Inf. Theory,
submitted. [Online]. Available: http://arxiv.org/abs/1201.2334
[22] J. L. Massey and P. C. Massey, “Conservation of mutual and directed
information,” in Proc. IEEE Int. Symp. Inf. Theory, 2005, pp. 157–158.
[23] N. Merhav and M. Feder, “Universal prediction,” IEEE Trans. Inf.
Theory, vol. 44, no. 6, pp. 2124–2147, 1998.
[24] F. Willems and T. Tjalkens, Complexity Reduction of the Context-Tree
Weighting Algorithm: A Study for KPN Research. Tech. Rep. Univ.
Eindhoven, Eindhoven, The Netherlands, EIDMA Rep. RS.97.01, 1997.
[25] T. J. Tjalkens, Y. M. Shtarkov, and F. M. J. Willems, “Sequential
weighting algorithms for multi-alphabet sources,” in 6th Joint SwedishRussian International Workshop on Information Theory, 1993, pp. 230–
234.
[26] S. Kleinberg and G. Hripcsak, “A review of causal inference for
biomedical informatics,” Journal of Biomedical Informatics, vol. 44,
no. 6, pp. 1102–1112, 2011.
[27] C. Granger, “Investigating causal relations by econometric models and
cross-spectral methods,” Econometrica, vol. 37, no. 3, pp. 424–438,
1969.

1
I(X n ; Y n ) n
1
I(X n → Y n ) n
n−1 → X n ) 1
I(Y
n

0.2
0.15

0.1

0.1

0.05

0.05

0

0
2000

2005

2010

2000

year

1
I(X n ; Y n ) n
1
I(X n → Y n ) n
n−1 → X n ) 1
I(Y
n

0.15

2010

Alg. 4

Alg. 3
0.2

2005
year

1
I(X n ; Y n ) n
1
I(X n → Y n ) n
n−1 → X n ) 1
I(Y
n

0.2
0.15

0.1

0.1

0.05

0.05

0

0
2000

2005

2010

year

2000

2005

2010

year

Fig. 3. Estimates of information-theoretic measures between HSI denoted
by X, and DJI denoted by Y. It is clear that the reverse directed information
is much higher than the directed information, hence it is DJI that causally
inﬂuences HSI rather than the other way around.

R EFERENCES
[1] H. Marko, “The bidirectional communication theory–a generalization of
information theory,” IEEE Trans. Commum., vol. COM-21, pp. 1345–
1351, 1973.
[2] J. L. Massey, “Causality, feedback, and directed information,” in Proc.
IEEE Int. Symp. Inf. Theory Appl., Honolulu, HI, Nov. 1990, pp. 303–
305.
[3] G. Kramer, Directed Information for Channels with Feedback. Konstanz: Hartung-Gorre Verlag, 1998, Dr. sc. thchn. Dissertation, Swiss
Federal Institute of Technology (ETH) Zurich.

5

