Creator:         TeX output 2012.05.07:1531
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Mon May  7 15:31:48 2012
ModDate:        Tue Jun 19 12:55:03 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      321304 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569564123

On a Construction of Universal Network Code
using LDPC Matrices
Shigeki Miyake

Jun Muramatsu

Network Innovation Laboratories, NTT
1-1 Hikarinooka, Yokosuka-shi, Kanagawa, Japan.
Email: miyake.shigeki@lab.ntt.co.jp

Communication Science Laboratories, NTT
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan
Email: muramatsu.jun@lab.ntt.co.jp

Abstract—An LDPC matrix is used as a local encoding kernel
at each link to construct a universal code to address network
coding problems. It is also shown that at each terminal node
the global encoding kernel that constructs a decoder becomes
an LDPC matrix. This provides the perspective that decoding
complexity can be reduced to a linear order of a block length by
using an efﬁcient decoding algorithm such as the sum-product
algorithm.

source. Since it is also shown that the global encoding kernel
at each terminal node becomes an LDPC matrix, it can be
expected that using an efﬁcient decoding algorithm such as
the sum-product algorithm will enable decoding complexity
to be reduced to O(n).

I. I NTRODUCTION

G = (V, E) denotes a directed acyclic graph, where V is
a vertex set and E is a link set whose element is described
by an ordered pair of vertices (i, j) i, j ∈ V. At each link
(i, j) ∈ E, a rate constraint Rij is assigned. Network coding
handles the problem of multicasting from the source node
s ∈ V to the set of terminal nodes T ⊂ V (Figure 1). The

II. P ROBLEM S ETTING

The theory of network coding [1] provides the bound of
transmission rate (“max-ﬂow bound”) with which a source
node multicasts a message to terminal nodes. A signiﬁcant
property of network coding is the transmission of information
after being encoded at each intermediate node, which is
different from the ordinary store and forward scheme. The
problem of how to construct an encoder at each node or
decoder at each terminal node has been investigated by many
researchers. Li et al. [2] showed that linear codes that assign
matrices to each node attain the max-ﬂow bound. Using the
random coding technique, Ho et al. [3] showed that a linear
code constructed by randomly selected matrices also attains
the max-ﬂow bound and established the theory of random
linear network coding.
Regarding channel coding problems, on the other hand,
since the 1990’s it has been known that using linear codes
constructed from LDPC matrices in combination with an efﬁcient decoding algorithm such as the sum-product algorithm
enables decoding complexity to be reduced to O(n) of the
block length n. As for the lossless source coding problem,
Caire et al. [4] proposed an efﬁcient variable-length universal
code using multiple LDPC matrices. It is quite natural that
many researchers have tried to construct network codes using
LDPC or sparse matrices to reduce encoding and decoding
complexity. By constructing each local encoding kernel [10]
using sparse matrices, Jaggi et al. [5] reduced encoding
complexity to O(n). Li et al. [6] derived a condition with
which a square sparse matrix is not non-singular. Note that
in [5], since the decoding process adopts an ordinary inverse
matrix computation, computational complexity of the process
is not reduced compared with ordinary linear network coding.
In this paper, by constructing local encoding kernels using
LDPC matrices, constructed code is shown to have a universal
property that does not depend on the statistical property of the

P

ωn X

ωn

x

s
3=

3
1=

R

R

3s

3=

1=

3t

2R

R

2

1
3=

3 t3

1=

3t
ωn

x

3 = 1sR

2s

1=

R

2 t3

2t

ωn

x

2t

R 2 = 1R

1t 2

1t

2R

1t

ωn

x
Fig. 1.

An example of a directed acyclic graph with rate constraints

network code consists of an encoder fij and a decoder gt that
are assigned at each link (i, j) ∈ E or at a terminal node t ∈ T .
Messages and codewords are assumed to be binary sequences
of 0 and 1, which are elements of binary ﬁeld F2 . Let a
message transmitted from the source node s be xnω ∈ Fnω ,
2
which is generated from the discrete memoryless probability
distribution PX . Parameters n and ω are the number of
link usage and transmission rate, respectively. The nω-length
binary sequence xnω can be interpreted as ω-juxtapositions
of n-length binary sequences. When we deﬁne encoder and

1

k

decoder as mappings fij : Fki → F2ij , and gt : Fkt → Fnω ,
2
2
2
the rate constraints at each link (i, j) ∈ E can be denoted as
kij
≤ Rij ,
n
where
∑
def
ki =
kli . (When i = s, ks = nω.)

where z i is ki -dimensional row vector, and Aij is a ki × kij
LDPC matrix.
[Construction of decoders]
Let an information (or row) vector that is received by a
terminal node t be z t . Then

(1)

gt (z t ) = arg

(2)

l:(l,i)∈E

where

Note that if we let z i be an information (or row) vector
entering into node i, then decoding error can be described
as an event xnω ̸= gt (z t ) for a terminal node t ∈ T . The
multicasting problem treated here is the following:
[Multicasting problem]
Assume that message xnω is generated according to the
discrete memoryless probability distribution PX and sent out
from the source node s. Then construct a network code using
LDPC matrices that attains the maxﬂow bound and at the same
time makes decoding error at each terminal node arbitrarily
small asymptotically with the block length n.

min

˜
xnω :xnω At =z t

(
def

H(xnω ) = Hb

w(xnω )
nω

H(xnω ),

(6)

)
.

(7)

˜
At in the above deﬁnition (6) represents the global encoding
kernel [10] at the terminal node t: When the transmitted
message is xnω , z t , the information received by the terminal
node t is represented by
˜
xnω At = z t .

(8)

˜
For the network conﬁguration shown in Figure 1, At1 is an
nω × kt1 matrix, and represented by

III. M AIN T HEOREM

˜
At1 = [As1 A1t1 , As2 A2t1 ] .

A universal network code is constructed using LDPC matrices.
After showing the construction of LDPC matrices, encoders
and decoders are described. For integers i < j, [i : j] is deﬁned
def
as [i : j] = {i, i + 1, ..., j}, and let Hb (·) be a binary entropy
function: For 0 < γ < 1,

Remark 1: Since in the binary alphabet case, the sequence
that attains minimum entropy in (6) corresponds to the sequence that has the least number of 1’s or 0’s under the given
linear constraints, which is the maximum likelihood estimator
with an appropriate probability parameter, the decoder can
be approximately implemented using, e. g. , the sum-product
algorithm.
Before stating Theorem 1, three lemmas are shown.
Lemma 1 (Modiﬁed version of Theorem 3 in [7]): Let A
be an n × k LDPC matrix, and 0 < γ < 1/2 be a weight
parameter satisfying
)
−12−K
k(
Hb (γ) +
log(1 + e−4e
) − 1 < 0,
(9)
n

def

Hb (γ) = −γ log γ − (1 − γ) log(1 − γ).

(3)

Weight function w is deﬁned for xn ∈ {0, 1}n ,
n

def

w(x ) =

n
∑

1[xi = 1],

(4)

i=1

where 1[(Proposition)] takes a value 1 if the proposition is
true and 0 otherwise.
[Construction of n × k LDPC matrices A ][7]
Let c be an even number which is greater than 6, and d be an
odd number, and nc = kd is satisﬁed [7]. A bipartite graph
constructed by a variable node set labeled [1 : n] and a check
node set labeled [1 : k] is considered.
Step 1. From each node in variable node set c hands are
stretched, and from each node in check node set d hands
are stretched.
Step 2. With the condition nc = kd, since the number of hands
stretched from both sides of the bipartite graph is equal,
the hands from each side can be connected 1 : 1 at
random.
Step 3. If the number of connections between variable node i ∈
[1 : n] and check node j ∈ [1 : k] is even, set aij = 0,
otherwise set aij = 1.
[Construction of encoders]
For (i, j) ∈ E,
fij (z i ) = z i Aij ,

ln(d)
where K = 6n kd . If w(xn ) ≥ nγ and n is sufﬁciently large,
then
[
] αk
PA xn A = 0k ≤ k
(10)
2
holds, where
d
def
αk = 22k log(1+(1−2γ) ) .

(11)

Lemma 2 (Modiﬁed version of arguments at Section V-B [7]):
Let A be an n × k LDPC matrix with k = nR for a constant
def
R, and dmin (A) = minxn ̸=0n :xn A=0k w(xn ). Then for any
0 < ξ < 1,
PA [dmin (A) < nξ] ≤ βn + 2αk 2−n(R−Hb (ξ))

(12)

holds, where αk is deﬁned in (11), and
def

βn =

(5)

2

4e−12−K 1−c/2 (c/2)c 2−c/2
R
n
.
c
(c/2)!

(13)

Note that in the above lemma, from the assumption c ≥ 6,
βn → 0 (n → ∞) holds [7].
The next lemma refers to the universality of a code. Let Pn
be a type set [8] constructed by n-length sequences, and for
each e ∈ E,
}
def {
De = Ae dmin (Ae ) < kIni(e) γ ,
(14)

∑
nω
xnω ∈TQ

≤

and for t ∈ T , Q ∈ Pnω ,


∑ 1[Err A,t (xnω )]
def
Ft (Q) = A
nω

|TQ |
nω
nω
x

(Proof of Theorem 1) If Lemma 3 holds, then with the
probability of more than 1 − β(n) − ξ, for ∀t ∈ T and
∀Q ∈ Pnω , LDPC matrices {Ae }e∈E can be taken that satisfy

And for ∀t ∈ T , it holds that

∈TQ

}

PX nω [X nω ̸= gt (Zt )] =
=
≤

(

)

(

max Rij log 1 + (1 − 2γ)

δ(d) = 2

d

)

+ ε,

(16)

(a)

≤

def

β(n) =

∑

(
−kIni(e)

βkIni(e) + 2αke 2

ke
kIni(e)

nω
xnω ∈TQ

2−nD(Q||PX )

1[Err A,t (xnω )]
nω
|TQ |

|T |(nω + 1)2
ξ

(20)
where at (a), (19) is used.
(End of Proof of Theorem 1)
From Theorem 1 and its proof, it can be easily seen that if
d is sufﬁciently large and

. (17)

ωH(PX ) < min maxﬂow(s, t)
t∈T

e∈E

is satisﬁed, then
{
}
+
min D(Q||PX ) + |maxﬂow(s, t) − ωH(Q) − 2|E|δ(d)|
>0

Th proof of Lemma 3 is deferred to Appendix.
Theorem 1: Suppose that a message xnω is generated according to a discrete memoryless distribution PX . When a directed acyclic graph G and rate constraints R = {Rij }(i,j)∈E
are given, let a weight parameter γ be taken which satisﬁes
(
)
−12−Kij
Rij
min 1 − log(1 + e−4e
) ∑
> Hb (γ) ,
(i,j)∈E
l:(l,i)∈E Rli

def 6

∑

+

)}

−Hb (γ)

2−n(D(Q||PX )+H(Q)) 1[Err A,t (xnω )]

2−nD(Q||PX )

∑

PX nω (xnω )1[Err A,t (xnω )]

·2−n|maxﬂow(s,t)−ωH(Q)−2|E|δ(d)|
|T |(nω + 1)4
≤
ξ
{
}
+
−n minQ D(Q||PX )+|maxﬂow(s,t)−ωH(Q)−2|E|δ(d)|
·2
,

t∈T Q∈Pnω

{

∑

Q∈Pnω

def

Ini(e) = (origin of a directed link e), Err A,t (xnω ) denotes
an event of decoding error at a terminal node t for a message
xnω , and H(Q) is an entropy function of a type Q.
Lemma 3: For any 0 < ξ < 1,

{
} 

∪
∨∪ ∪
PA 
De
Ft (Q)  ≤ β(n) + ξ


where

∑

Q∈Pnω

(i,j)∈E

e∈E

∑

∑
xnω

nω
Q∈Pnω xnω ∈TQ

(15)

def

|T |(nω + 1)2 −n|maxﬂow(s,t)−ωH(Q)−2|E|δ(d)|+
2
.
ξ
(19)

|T |(nω + 1)2 −n|maxﬂow(s,t)−ωH(Q)−2|E|δ(d)|+
,
2
>
ξ

where

1[Err A,t (xnω )]
nω
|TQ |

∑

Q

holds. And when the weight parameter γ satisﬁes the condition
given in the theorem, LDPC matrices with which the decoding
error can be arbitrarily small asymptotically with the block
length n can be taken with high probability.
Remark 2: The code constructed here is also a static code
[10]. When some links are broken and disconnected, at unbroken links the same LDPC matrices can be used for encoding.
The proof can be shown applying the union bound to (19),
using the fact that the number of disorder patterns is upperbounded by 2|E| .
While local encoding kernels Ae (e ∈ E) are LDPC
matrices, in which the number of 1’s of any row or column
is constant with respect to the block length n, it is not
˜
obvious whether the global encoding kernel At (t ∈ T )
˜
is also an LDPC matrix or not. If At is an LDPC matrix,
to approximately implement the decoder gt the sum-product
˜
algorithm can be applied. The next lemma states that At
becomes an LDPC matrix:

(18)
Rli ln(d)

l:(l,i)∈E
.
where Kij =
Rij d
If ωH(PX ) < mint∈T maxﬂow(s, t), then for sufﬁciently
large d there exist LDPC matrices {Ae }e∈E with high probability and decoding error at each terminal node can be
arbitrarily small asymptotically with the block length n, where
H(PX ) is an entropy of the source PX , and maxﬂow(s, t)
denotes the max-ﬂow [1] between the source node s and the
terminal node t.
Note that (18) denotes a condition with which Lemma 1 and
2 hold on a given graph.

3

˜
Lemma 4: For each global encoding kernel At (t ∈ T ),
the number of 1’s of any row or column is upper-bounded by
a constant that does not depend on the block length n
(Proof of Lemma 4) Assume that n × l matrix A and l × k
matrix B are LDPC matrices with parameters (c1 , d1 ) and
(c2 , d2 ), respectively. Let row vectors of B be b1 , ..., bl . Then
the i-th row of the product matrix AB can be described as

C
C at transmission rate ω the data is divided into large nω
packets, while in the deterministic case it is divided into
small nCω packets. If we assume that decoding complexity
0
in the former case is O(nω) per packet, the total decoding
C
time is about nω × O(nω) ≃ C. In the latter case if we
adopt FFT for multiplication of the elements of extension
ﬁeld GF(2n0 ) whose computation complexity is O(n0 log n0 ),
since the computation complexity of multiplication of the ωvector and ω × ( matrix is O(ω 2 ), the total decoding time is
ω
)
about nCω × O ω 2 n0 log n0 ≃ ω log n0 × C. This argument
0
shows that the network code constructed by LDPC matrices
can be expected to be smaller decoding complexity than that
of deterministically constructed network code. It should be
noted that deterministic algorithms can decode with decoding
error of exactly 0, while algorithms constructed with a random
selection manner (such as the code constructed in this paper)
can decode only with arbitrarily small error. The type of
algorithm adopted should be determined according to the
situation in which it is applied.

ai1 b1 + ... + ail bl .
From the construction of matrices, since the number of nonzero elements among ai1 , ..., ail is at most c1 and the number
of non-zero elements in k-dimensional row vectors bj (j ∈
[1 : l]) is at most c2 , it can be seen that the number of nonzero elements of the i-th row of n × k matrix AB is at most
c1 c2 . From a similar argument, it can be seen that the number
of non-zero elements of each column of the matrix AB is at
most d1 d2 .
If we suppose that the length of the longest path from the
source node s to the terminal node t is h, since the global
˜
encoding kernel At is constructed by multiplying matrices
that are at most |V| juxtapositions of LDPC matrices, whose
parameters are (c, d), at most h times, it can be seen that the
number of 1’s of each column is upper-bounded by dh and
the number of 1’s of each row is upper-bounded by (c|V|)h .
These upper-bounds are constants with respect to the block
length n.
(End of Proof of Lemma 4)
Remark 3: If di (i = 1, 2) is O(log n) with respect to
(
)
the block length n, since d1 d2 ≃ O (log n)2 and 2d1 d2
can not be upper-bounded by a polynomial of n, a known
decoding algorithm such as the sum-product algorithm can
not be applied efﬁciently in this case.
Summing up the above arguments, the following should be
noted: If the parameters of LDPC matrices are chosen appropriately, the computational complexity of encoding at each
node becomes O(n). Furthermore, if we apply a known efﬁcient decoding algorithm such as the sum-product algorithm,
the decoding complexity at each terminal node can be expected
to become O(n).

R EFERENCES
[1] R. Ahlswede, N. Cai, S.-Y.R. Li, and R.W. Yeung, “Network information
ﬂow,” IEEE Trans. Inform. Theory ,vol. 46, no. 4, pp. 1204–1216, 2000.
[2] S. Y. R. Li, R. W. Yeung, and N. Cai, “Linear network coding,” IEEE
Trans. Inform. Theory, vol. 49, no. 2, pp. 371–381, 2003.
[3] T. Ho, M. M´ dard, R. Koetter, D. R. Karger, M. Effros, Jun Shi, and
e
Ben Leong, “A random linear network coding approach to multicast,”
IEEE Trans. Inform. Theory, vol. 52, no. 10, pp. 4413–4430, 2006.
[4] G. Caire, S. Shamai, and S. Verd´ , “Noiseless data compression with low
u
density parity check cods,” in DIMACS Series in Discrete Mathematics
and Theoretical Computer Science: Advances in Network Information
Theory, vol. 66, pp. 263–284, American Mathematical Society, 2004.
[5] S. Jaggi, Y. Cassuto, and M. Effros, “Low complexity encoding for
network codes,” in Proc. ISIT 2006, pp. 40–44, 2006.
[6] X. Li, W. H. Mow, and F. L. Tsang, “Singularity probability analysis
for sparse random linear network coding,” in Proc. ICC 2011, 2011.
[7] G. Miller and D. Burshtein, ”Bounds on the maximum-likelihood
decoding error probability of low-density parity-check codes,” IEEE
Trans. Information. Theory, vol. IT-47, no. 7, pp. 2696–2710, 2001.
[8] I. Csisz´ r and J. K¨ rner, Information Theory: Coding Theorems for
a
o
Discrete Memoryless Systems, 2nd ed., Cambridge, 2011.
[9] S. Jaggi, P. Sanders, P. A. Chou, M. Effros, S. Egner, K. Jain, and
L. Tolhuizen, “Polynomial time algorithms for multicast network code
construction,” IEEE Trans. Inform. Theory, vol. 51, no. 6, pp. 1973–
1982, 2005.
[10] R. W. Yeung, Information Theory and Network Coding, Springer, New
York, 2008.

IV. C ONCLUDING R EMARKS
In this paper, it was shown that by using an LDPC matrix as
a local encoding kernel at each link, the global encoding kernel
also becomes an LDPC matrix at each terminal node. With this
fact and by using an efﬁcient decoding algorithm such as the
sum-product algorithm, we can expect that the decoding can
be approximately implemented with computational complexity
of O(n).
While the code shown here is constructed using random
selection, there exist deterministically constructed network
codes such as the Jaggi-Sanders algorithm [9]. In the latter
case, n does not denote block length but the extension degree
of the ﬁeld that comprises the alphabet. When in the case
of a deterministic algorithm we denote the extension degree
as n0 , the magnitude of n0 is ordinary about 2n0 ≤ 100.
Since in the random selection case the magnitude of n can be
about nω ≃ 104 , when we want to send data of magnitude

A PPENDIX
(Proof of Lemma 3) From Lemma 2,
∪
PA [
De ] ≤ β(n)

(21)

e∈E

holds, where β(n) is deﬁned at (17), and the condition in
Theorem 1 is necessary to make β(n) → 0 (n → ∞).
The ﬁrst half of the following proof is similar to that in
[1]. Let the transmitted message be xnω ∈ {0, 1}nω . The
probability that at a ﬁxed terminal node t there exists another
message xnω ∈ {0, 1}nω that provides the same codeword
˜
as that of transmitted message xnω is evaluated. Note that
from Lemma 2, with high probability we can select Ae that
provides only the case w(xnω + xnω ) ≥ nγ, and the case
˜

4

w(xnω + xnω ) < nγ can be omitted.
˜
A random variable U0 that depends on {Ae }e∈E is deﬁned as
{
}
U0 = i ∈ V z i (xnω ) ̸= z i (˜nω ) .
x
(22)

then it holds that
[
PA {U0 = U } ∧

def

=2

In the following, we sometimes use the notations z i =
def
z i (xnω ) and z i = z i (˜nω ) as appropriate. Let a random
˜
x
def
variable A be A = {Ae }e∈E . Since each Ae is selected
independently, from the deﬁnition of De in (14) and Lemma
1, for any (i, j) ∈ E it holds that
[
] αkij
PA {z i Aij = z i Aij } ∧ Dij z i ̸= z i ≤ kij ,
˜
˜
(23)
2

= PA {U0 = U } ∧

∪

≤ PA {U0 = U } ∧

(a)

≤ PA 

∪

∩

(i,j)∈EU
(b)

≤

∏

≤

∏
(i,j)∈EU

where a cutset of U is deﬁned as
{
}
EU = (i, j) ∈ E i ∈ U, j ∈ U .
/

Rij

(i,j)∈EU

(27)

(28)

At (d), the max-ﬂow min-cut theorem [1] is used.
Considering U ⊂ V, since
[
]
[
]
∪
∪
˜
PA {z t = z t } ∧
De = PA {U0 ⊂ V} ∧
De
e∈E

e∈E

|V| −nmaxﬂow(s,t)

≤ αE 2

2

holds, it can be seen that
[
]
∪
∑
EA 1
De
nω
xnω ∈TQ

e∈E

= EA

[

∑

1

nω
xnω ∈TQ

·

{z l ̸= z l }
˜

≤

∪

1[Err A,t (xnω )]
nω
|TQ |
]

De

1 [∃˜nω ̸= xnω s.t. z t = z t and H(˜nω ) ≤ H(xnω )]
x
˜
x
nω
|TQ |
[∪
]
z
∑
∑ EA 1 e∈E De 1 [˜t = z t ]
nω
|TQ |

nω ˜
xnω ∈TQ xnω ̸=xnω

(e)

≤

(29)

e∈E

l∈U

[
]
PA {z i Aij = z i Aij } ∧ Dij z i ̸= z i
˜
˜
αkij
,
2kij

∑

def

(i,j)∈EU
(c)

αkij

(i,j)∈EU

αE = 2n|E|δ(d) .

De U 0 ⊃ U

{z i Aij = z i Aij } ∧ Dij
˜

(i,j)∈EU

where δ(d) is deﬁned in (16), and

]

∩

∏

(Rij −ε)

αkij 2−n(Rij −ε)

≤ αE 2−nmaxﬂow(s,t) ,

De U0 ⊃ U PA [U0 ⊃ U ]

e∈E

∏

De ≤

(d)

]

e∈E

[

(i,j)∈EU

≤ αE 2

holds. From the above observation, since U0 = U holds, it
can be seen that
]
[
∪
De
PA {U0 = U } ∧
e∈E

]

e∈E

∑

−n

−n

where Dij denotes a complement of Dij . Note that the left
hand side of (23) becomes 0 for w(zi − zi ) < ki γ. At the
˜
terminal node t, if z t (xnω ) = z t (˜nω ) occurs, there exists
x
U ⊂ V such that for any i ∈ U zi ̸= zi holds, and at the
˜
same time, between node i in U and node j outside U with
(i, j) ∈ E
z i Aij = z i Aij
˜

[

∪

∑

∑

·1 [H(˜nω ) ≤ H(xnω )]
x
−n(maxﬂow(s,t)−|V|/n−|E|δ(d))
2
nω
|TQ |

nω ˜
xnω ∈TQ xnω ̸=xnω

·1 [H(˜nω ) ≤ H(xnω )]
x

(24)

≤ 2−n(maxﬂow(s,t)−ωH(Q)−|V|/n−2 log(n+1)/n−|E|δ(d))
(f)

≤ 2−n(maxﬂow(s,t)−ωH(Q)−2|E|δ(d)) ,

(25)

(30)

At (a), we use the fact that when U0 ⊃ U , for any l ∈ U
z l ̸= z l holds, and the fact that when U0 = U , for (i, j) ∈ EU
˜
z i Aij = z i Aij holds. At (b), we use the fact that {Ae }
˜
∩
that is associated with the event l∈U {z l ̸= z l } is selected
˜
independently from Aij that satisﬁes z i Aij = z i Aij , and the
˜
fact that for e ̸= e Ae is selected independently from Ae . At
˜
˜
(c), (23) is used.
To consider a code whose transmission rate is near the rate
constraint, for a given ε > 0 let kij be satisﬁed by

where at (e), (29) is used and at (f), n is assumed to be
sufﬁciently large to satisfy

Rij − ε ≤

kij
≤ Rij ,
n

|V| + 2 log(n + 1)
≤ |E|δ(d).
n

(31)
(End of Proof of Lemma 3)

(26)

5

