Title:          isit_12_mismatch_final.dvi
Creator:        dvips(k) 5.98 Copyright 2009 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 22:38:51 2012
ModDate:        Tue Jun 19 12:54:36 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      651869 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569559597

Mismatched MMSE Estimation of Multivariate
Gaussian Sources
I˜ aki Esnaola
n

Antonia M. Tulino

H. Vincent Poor

Dept. of Electrical Engineering
Princeton University
Princeton, NJ 08544, USA
Email: jesnaola@princeton.edu

Bell Laboratories, Alcatel-Lucent
Wireless Communication Theory Research
Holmdel, NJ 07733, USA
Email: a.tulino@alcatel-lucent.com

Dept. of Electrical Engineering
Princeton University
Princeton, NJ 08544, USA
Email: poor@princeton.edu

x + z where z ∼ N (0, In ). Based in this channel model,
we deﬁne the signal to noise ratio as snr = tr(Σ)/n. The
minimum mean-square error (MMSE) estimator of the input
x given the channel output y is

Abstract—The distortion increase in minimum mean-square
error (MMSE) estimation of multivariate Gaussian sources is
analyzed for the situation in which the statistics are mismatched,
i.e., the covariance matrix is not perfectly known during the
estimation process. First a deterministic mismatch model with
an additive perturbation matrix is considered, for which we
provide closed form expressions for the distortion excess caused
by the mismatch. The mismatch study is then generalized by
using random matrix theory tools which allow an asymptotic
result for a broad class of perturbation matrices to be proved.

ˆ
x = E[x|y, Σ].

(1)

When the covariance matrix is known, this estimation method
minimizes the reconstruction error measured by the persymbol mean square error (MSE) distortion given by

I. I NTRODUCTION
A. Source Model
We model a source signal as a Gaussian random vector with
mean µ ∈ Rn and covariance matrix Σ ∈ Rn×n . Multivariate
Gaussian distributions describe a wide range of real signals
and provide a ﬂexible way of modeling prior knowledge by
means of the covariance matrix. However, in general, perfect
knowledge of second order statistics is not available, due to the
need for estimating them during a training period, or because
source statistics are nonstationary. For this reason, a practical
system operates with a postulated covariance matrix, Σ∗ ,
which for the case of perfect prior knowledge is Σ∗ = Σ. The
degree of mismatch between postulated and real covariance
matrices characterizes the amount of prior knowledge. We
assume µ is known, and thus without loss of generality,
throughout the rest of the paper we assume µ = 0.
Although the theoretical results presented in this paper apply
to any positive deﬁnite covariance matrix Σ, for the sake of
discussion and in order to illustrate the results, we introduce a
particular covariance matrix model. Since covariance matrices
of weakly stationary random processes are Toeplitz [1], we
consider an exponentially decaying Toeplitz model that resembles a physical temporal correlation where the correlation
decreases as the temporal distance increases and the strength
of the correlation is set by a parameter ρ. This covariance
matrix is given by Σ = sij = ρ|i−j| ; i, j = 1, 2, . . . , n .

D=

1
E
n

ˆ
x−x

2

.

(2)

II. M ISMATCHED E STIMATION
In practical scenarios, access to perfect source statistics is
not a realistic assumption. Instead, postulated statistics are
available which do not match real statistics, and the resulting
distortion of the mismatched estimator is D∗ = D+DΓ , where
DΓ is the excess distortion incurred by the system due to
imperfect knowledge of the distribution. Since we consider
multivariate Gaussian sources, we analyze the excess distortion
for the case in which the mismatch between real and postulated
distributions is modeled through the mismatch between the
true and the postulated covariance matrices.
A. Deterministic Mismatch
For a given true covariance matrix, Σ, and a given postulated covariance matrix, Σ∗ , we quantify the distortion excess
as a function of the additive perturbation matrix γΓ = Σ∗ −Σ.
Without loss of generality, we assume tr[Γ]/n = 1, and thus
γ = tr[Σ − Σ∗ ]/n quantiﬁes the mismatch strength.
Theorem 1 Let Σ∗ = Σ + γΓ be the postulated covariance
matrix available in the MMSE estimation recovery process,
where Σ is the real covariance matrix, Γ ∈ Rn×n is an
additive mismatch which ensures Σ∗ is positive deﬁnite, and
γ is a parameter governing the strength of the mismatch. The
excess distortion incurred by the mismatched estimator is

B. System Model
The source sequence, x ∈ Rn , is transmitted through an
additive white Gaussian noise (AWGN) channel such that y =
This research was supported in part by the Air Force Ofﬁce of Scientiﬁc
Research under MURI Grant FA9550-09-1-0643 and in part by the Army
Research Ofﬁce under MURI Grant W911NF-11-1-0036.

DΓ =

1

1
tr Υe Σ† Υ†
yy e
n

(3)

where

and the average total excess distortion (ATED) as
Υe

=

γΓ† Σyy + γΓ†

−1

−γΣ† Im + γΣ−1 Γ†
yy

¯
∆ = E[∆],
−1

Σ−1 Γ† Σ−1
yy
yy

where the expectations are with respect to the distribution
induced by the perturbation statistics.

(4)

and the covariance matrix for the received signal is Σyy =
Σ + In .

III. A SYMPTOTIC M ISMATCHED E STIMATION
The average quantities given in (10) and (11) are in general
difﬁcult to express in an explicit form and even for classical
Gaussian Wigner perturbation statistics [4] the resulting expression can be very involved. For this reason, we use random
matrix theory tools and analyze the asymptotic case when
n → ∞. In the following, we obtain closed form expressions
for the ATED for a large class of random matrices in the
asymptotic regime.

Proof: The estimate with the mismatched covariance
matrix can be written as
ˆ
x∗

=

Σ† Σ† + In + γΓ†

−1

+γΓ† Σ† + In + γΓ†

y
−1

y

(5)

and by applying the inversion lemma it follows that
Σyy + γΓ†

−1

= Σ−1 − γ In + γΣ−1 Γ†
yy
yy

−1

(11)

Σ−1 Γ† Σ−1
yy
yy

A. Auxiliary Results in Random Matrix Theory

which combined with (5) and after some manipulation allows
us to isolate the effect of the mismatch in the linear estimation
such that
ˆ
ˆ
x∗ = (Υ + Υe ) y = x + Υe y.
(6)

In this section we collect several useful deﬁnitions and some
results in random matrix theory. We also present new results
for a broader class of matrices of the Wishart counterparts in
[4]. Speciﬁcally, Theorems 3, 4 and 5 are new results which
allow us to prove Theorem 6.

The result follows by performing simple algebraic operations
on the error covariance matrix.
Based in the previous theorem we characterize the total
excess distortion deﬁned as

Deﬁnition 1 [4] The η-transform of a nonnegative random
variable X is
ηX (γ) = E

∞

∆=

DΓ dsnr

(7)

0

1
1 + γX

(12)

and the Shannon transform of a nonnegative random variable
X is deﬁned as

which can be seen as an integral value of the distortion excess
averaged over all possible values of the signal to noise ratio.

VX (γ) = E[log(1 + γX)]
Lemma 1 The total excess distortion, ∆, induced by the
mismatched MMSE estimator is given by
∆=

1
2n

log

|Σ + γΓ|
+ tr Σ(Σ + γΓ)−1 − n .
|Σ|

(13)

with γ ≥ 0.
Deﬁnition 2 [4] The asymptotic eigenvalue distribution,
FA (·), of an n × n Hermitian random matrix A is deﬁned
as
n
1
FA (x) = lim
1{λi (A) ≤ x}
(14)
n→∞ n
i=1

(8)

Proof: The proof follows by using the link between
mismatch estimation and relative entropy given in [2] which
states
∞
f (x, Σ)
DΓ dsnr =
f (x, Σ) log
dx. (9)
f (x, Σ + γΓ)
0
Rn

where λ1 (A), . . . , λn (A) are the eigenvalues of A and 1{·}
is the indicator function.

The result follows by integrating the right hand side term of
(9) which was done in [3].

We recall now the deﬁnition of variance proﬁle given in [4].

B. Random Mismatch
In real systems, the mismatch between prior and postulated
statistics arises from an estimation process in which the
system learns the statistics of the source. For that reason, it is
reasonable to assume that the perturbation matrix, Γ, admits a
statistical model and γ is linked to the average of the variances
of the entries of the perturbation matrix. That being the case,
the results given in Theorem 1 and Lemma 1 can be averaged
over an ensemble of random matrices which allows us to deﬁne
the average excess distortion (AED) as
¯
D = E[DΓ ],

Deﬁnition 3 [4] Consider an n× n random Hermitian matrix
Vi,j
Γ whose entries have variances Var[Γi,j ] = n such that
Vi,j = Vj,i with V an n × n deterministic symmetric matrix
whose entries are uniformly bounded. For each n, let
v n : [0, 1) × [0, 1) → R

(15)

be the variance proﬁle function given by the 1-exchangable1
function
v n (x, y) = Vi,j with

(10)

1 I.e.,

2

i−1
n

≤x<

i
n,

and

j−1
n

≤y<

j
n

the function is invariant to the permutations of its arguments.

(16)

Whenever v n (x, y) converges uniformly to a limiting bounded
measurable function, v(x, y), we deﬁne this limit as the
asymptotic variance proﬁle of Γ.

where (24) follows from Theorem 2 and A(x, γ) satisﬁes (22).
We deﬁne G(Y, γ) = E[v(X, Y)A(X, γ)|Y] and notice also that
∆

φ(γ) =

Theorem 2 [5] Let Γ be an n × n Hermitian complex random matrix whose upper-triangular entries are independent
zero-mean complex random variables (arbitrarily distributed)
satisfying a Lindeberg-type condition
1
k

2

E |Γi,j | 1 {|Γi,j | ≥ δ} → 0

d
loge (1 + γ G(Y, γ))
dγ
˙
γ E[v(X, Y) A(X, γ)|Y]

E

=

Based on this deﬁnition we can claim the following theorem
which is an immediate consequence of [5].

E

1 + γ E[v(X, Y) A(X, γ)|Y]

+E

E[v(X, Y) A(X, γ)|Y]
1 + γ E[v(X, Y) A(X, γ)|Y]

(26)

which, in conjunction with (24) and (21), yields

(17)
˙
VΓ (γ) − φ(γ) log e

i,j

for any δ > 0 and with variances

= −E

˙
γ E[v(X, Y) A(X, γ)|Y]
log e
1 + γ E[v(X, Y) A(X, γ)|Y]

˙
˜
= −E G(Y, γ) A(X, γ) log e

Vi,j
,
(18)
n
where V is an n × n deterministic symmetric matrix whose
entries are uniformly bounded and from which the asymptotic
variance proﬁle of Γ, denoted by v(x, y), can be obtained as
per Deﬁnition 3. As n → ∞, the empirical eigenvalue distribution of Γ converges almost surely to a limiting distribution
whose η-transform is
E |Γi,j |2 =

ηΓ (γ) = E [ A(X, γ) ]

(25)

(27)

where we have further used (22), which yields
˜
A(y, γ) =
=

γ
1 + γ E[v(X, y) A(X, γ)]
γ A(y, γ)

(28)

where (28) follows from the 1-exchangability of the variance
proﬁle v(x, y). Note that

(19)

d
˜
E G(Y, γ)A(Y, γ)
dγ
˙
˜
˙
˜
= E G(Y, γ)A(Y, γ) + E G(Y, γ)A(Y, γ)
∆

δ(γ) =

with
A(x, γ) = (1 + γE[v(x, Y)A(Y, γ)])−1

(20)

where X and Y are independent random variables uniform on
[0, 1].

=E

The zero-mean hypothesis in Theorem 2 can be relaxed using
[4, Lemma 2.23]. Speciﬁcally, if the rank of E[Γ] is o(n), then
Theorem 2 still holds.
Some new results follow which represent a Wigner matrix
counterpart of the results in [4].

˙
˜
A(Y, γ) v(X, Y)
˙
˜
+ E G(Y, γ) A(Y, γ)
˜
1 + E[v(X, Y)A(Y, γ)|X]


˙
˜
E A(Y, γ) v(X, Y)|X
˙
˜
 + E G(Y, γ) A(Y, γ)
= E
˜
1 + E[v(X, Y)A(Y, γ)|X]

=E

Theorem 3 Let Γ be an n×n complex random matrix deﬁned
as in Theorem 2. Its Shannon transform is

=

VΓ (γ) = 2 E [log(1 + γ G(Y, γ))] −γ E [G(Y, γ)A(Y, γ)] log e
with
G(Y, γ) = E[v(X, Y)A(X, γ)|Y]

=

(21)

and
−1

A(x, γ) = (1 + γE[v(x, Y)A(Y, γ)])

(22)

=

where X and Y are independent random variables uniform on
[0, 1].
Proof: Based on its deﬁnition, the derivative of VΓ (γ)
˙
with respect to γ, denoted by (·), is
˙
VΓ (γ) =
=

1
log e
1−
dF (λ)
γ
1+γλ
log e
(1 − E [A(X, γ)])
γ

˙
˜
A(Y, γ) v(X, Y)
˙
˜
+ E G(Y, γ) A(Y, γ)
1 + γE[v(X, Y)A(Y, γ)|X]

=

d
˜
E loge (1 + E[v(X, Y) A(Y, γ)|X])
dγ
˙
˜
+ E G(Y, γ) A(Y, γ)
d
˜
E loge (1 + E[v(X, Y) A(X, γ)|Y])
dγ
˙
˜
+ E G(Y, γ) A(Y, γ)

(29)

d
E [loge (1 + γE[v(X, Y) A(X, γ)|Y])]
dγ
˙
˜
+ E G(Y, γ) A(Y, γ)
d
˙
˜
E [loge (1 + γG(Y, γ))] + E G(Y, γ) A(Y, γ)
dγ

where (29) follows from the 1-exchangability of the variance
proﬁle v(x, y). Integrating over γ and using VΓ (0) = 0 the
claimed result is found.

(23)
(24)

3

Theorem 4 Let Γ be an n×n complex random matrix deﬁned
as
Γ = AΓw A
(30)

and
ηΓ (γ) = 1 − γ (γ).
Moreover,

with A = diag A2 , . . . , A2 , and Γw denoting an n × n
1
n
Hermitian complex random matrix whose upper-triangular
entries are independent identically distributed zero-mean complex random variables. Its Shannon transform is
VΓ (γ) = 2 E [log(1 + γ a(X) (γ))] − γ
with

(γ)2 log e

E [G(Y, γ)A(Y, γ)]

(32)

˜
Γ = Λ1/2 Γw Λ1/2

(33)

Proof: The proof of Theorem 5 is an immediate consequence of free probability theory [4]. From [7] and [8] it
follows that Γw is almost surely free with any deterministic
matrix. Hence, using [4, Theorem 2.67], we have that
ΣΓ (x)

(36)

Theorem 6 Let Γw be an n × n Hermitian complex random
matrix whose upper-triangular entries are independent zero1
mean complex random variables with variances n , and let Σ
a n × n deterministic matrix. Then as n → ∞

= E [ A(X, γ) ]

(37)
a(X)
. (38)
= E 1−γ
1 + γa(X)E[a(Y)A(Y, γ)]

∆ =

Let us further denote
(39)

log

|Σ + γΓw |
+ tr Σ(Σ + γΓw )−1 − n
|Σ|
(49)

∆∞ =

1
{V(γ) + η(γ) − 1}
2

(50)

where

(40)

V(γ) = 2 E [log(1 + γ ΛΣ (γ))] − γ

Then, (35) and (37) become
G(y, γ) = a(y) (γ)

1
2n

converges almost surely to ∆∞ , with ∆∞ given by

(γ) satisﬁes the following ﬁxeda(X)
1 + γa(X) (γ)

(47)
(48)

B. Main Result

where X and Y are independent random variables uniform on
[0, 1]. Using (19), we have that

(γ) = E

(46)

˜
from which it follows that Γ and Γw admit the same asymptotic eigenvalue distribution.

and

From (36), we have that
point equation

= ΣΣ (x)ΣΓw (x)
= ΣΛ (x)ΣΓw (x)
= ΣΓ (x)
˜

(35)

A(x, γ) = (1 + γa(x)E[a(Y)A(Y, γ)])−1

(45)

with Γw deﬁned as before and Λ denoting the diagonal matrix
whose diagonal elements are the eigenvalues of Σ. Then, Γ
˜
and Γ admit the same asymptotic eigenvalue distribution.

(34)

G(y, γ) = a(y)E[a(X)A(X, γ)]

(γ) = E[a(X)A(X, γ)].

(43)

with Σ an n×n Hermitian semi-positive deﬁnite deterministic
matrix whose empirical eigenvalue distribution converges to
a compactly supported measure, and Γw denoting an n × n
Hermitian complex random matrix whose upper-triangular
entries are independent identically distributed zero-mean com˜
plex random variables. Furthermore, let Γ be an n×n complex
random matrix deﬁned as

with a(·) such that the distribution of a(Z) (with Z uniform
on [0, 1]) equals the asymptotic empirical distribution of
A2 = diag A2 , . . . , A2 . Hence, Theorem 4 can be proved as
1
n
a special case of Theorem 3 when the function v(x, y) can be
factored. In this case, the expressions for A(x, γ) and G(y, γ)
given by Equations (21) and (22) in Theorem 3 become

ηΓ (γ)

(γ)2 .

Theorem 5 Let Γ be an n×n complex random matrix deﬁned
as
Γ = Σ1/2 Γw Σ1/2
(44)

Proof: In order to prove Theorem 4, we need to evaluate
the η and the Shannon transforms of the Hermitian matrix
Γ = AΓw A whose entries are independent zero-mean random
variables with variance

v(x, y) = a(x)a(y)

E [a(Y)E[a(X)A(X, γ)]A(Y, γ)]
E [a(Y)A(Y, γ)] E[a(X)A(X, γ)]

(31)

where Z is a uniform random variable on [0, 1], and a(·) such
that the distribution of a(Z) equals the asymptotic empirical
distribution of A2 .

A2 A2
i j
˜
E[|Γi,j |2 ] =
n
and whose variance proﬁle is

=
=
=

(γ) satisfying the ﬁxed-point equation
a(Z)
(γ) = E
1 + γa(Z) (γ)

(42)

(γ)2 log e,

(51)

and
η(γ) = 1 − γ (γ)

(41)

4

(52)

1

5
4.5
4

10

ρ=0.1
ρ=0.5
ρ=0.9
ρ=0.99

γ=0.2
γ=0.4
γ=0.6
γ=0.8
γ=1

3.5

0

10

∞

2.5

Δ

Δ∞

3

2
−1

10

1.5
1
0.5
0
0

−2

2

4

6

8

10

10

γ

0

0.2

0.4

0.6

0.8

1

ρ

Fig. 1. ∆∞ as a function of the mismatch strength parameter, γ, for different
values of the correlation parameter, ρ.

Fig. 2. ∆∞ as a function of the correlation parameter, ρ, for different
mismatch values of the strength parameter, γ.

with

of the correlation parameter ρ for different values of γ. In fact,
Figure 2 shows that the distortion growth increases for highly
correlated sources, reaching a maximum in the limit when
ρ = 1. Note that although the case for which ρ = 1 makes no
physical sense, the vicinity of it is of practical interest.

(γ) = E

ΛΣ
1 + γΛΣ (γ)

(53)

where ΛΣ is a random variable whose distribution is the
asymptotic spectrum of Σ−1 .

IV. C ONCLUSION

Proof: Using the fact that Σ is a full rank matrix, after
some simple algebraic manipulation, we can write ∆ as

We have studied the effect of incomplete statistical information on MMSE estimation of multivariate Gaussian
sources corrupted by additive white Gaussian noise. For an
additive perturbation matrix modeling the mismatch, closed
form expressions for the excess distortion and the total excess
distortion have been obtained. By using random matrix theory
tools we have analytically derived the asymptotic average
total excess distortion of an MMSE estimation process. In
order to do so, we have generalized for a broader class
of matrices some results which were previously known for
Wishart matrices.

1
log |I + γΣ−1/2 Γw Σ−1/2 |
(54)
2n
1
tr (I + γΣ−1/2 Γw Σ−1/2 )−1 − n (55)
+
2n
from which, via Theorem 3, it is immediate to prove that it
converges to almost surely to ∆∞ , with ∆∞ given by the
following expression
∆ =

1
{VΨ (γ) + ηΨ (γ) − 1} ,
(56)
2
where VΨ (γ) and ηΨ (γ) denote, respectively, the Shannonand the η-transform of the Hermitian matrix given by
∆∞ =

Ψ = Σ−1/2 Γw Σ−1/2

R EFERENCES
[1] R. M. Gray, “Toeplitz and Circulant Matrices: A Review,” Foundations
and Trends in Communications and Information Theory, vol. 2, no. 3, pp.
155-239, 2006.
[2] S. Verd´ , “Mismatched Estimation and Relative Entropy,” IEEE Trans.
u
Inf. Theory, vol. 56, no. 8, pp. 3712-3720, August 2010.
[3] R. Kleeman, “Measuring Dynamical Prediction Utility Using Relative
Entropy,” J. of Atmospheric Sciences, vol. 59, pp. 2057-2072, 2002.
[4] A. Tulino and S. Verd´ , “Random Matrix Theory and Wireless Commuu
nications,” Foundations and Trends in Communications and Information
Theory, vol. 1, no. 1, pp. 1-182, 2004.
[5] D. Shlyankhtenko, “Random Gaussian Band Matrices and Freeness with
Amalgamation,” Int. Math Res. Note, vol. 20, pp. 1013-1025, 1996.
[6] Y. Q. Yin, “Limiting Spectral Distributions for a Class of Random
Matrices,” J. of Multivariate Analysis, vol. 20, pp. 50-68, 1986.
[7] M. Capitaine and C. Donati-Martin,“Strong Asymptotic Freeness for
Wigner and Wishart Matrices,” Indiana University Mathematics Journal,
vol. 56, no. 2, pp. 767-803, 2007.
[8] K. Dykema, “On Certain Free Product Factors via an Extended Matrix
Model,” Journal of Functional Analysis, vol. 112, no. 1, pp. 31-60, 1993.

(57)

from which using Theorems 5 and 4, the claim follows.
In order to illustrate numerically the effect of the average
mismatch, we assume a perturbation model where Γ = HH†
with H ∈ Rn×n a real random matrix with entries distributed
as N (0, n−1 ). Figure 1 shows how asymptotic ATED increases
with the mismatch strength parameter γ. The curves show
that the distortion increases rapidly for small values of γ and
saturate as the mismatch increases. As expected, the case of
ρ = 0.99 is the most sensitive one to variations of γ. This
can be intuitively interpreted due to the fact that the higher
parameter ρ is, the stronger correlation the source presents,
and as a result reliable prior knowledge plays an important
role. Figure 2 depicts total mismatch distortion as a function

5

