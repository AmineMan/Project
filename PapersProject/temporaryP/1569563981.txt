Title:          ISIT12_SPC_final.dvi
Creator:        www.freepdfconvert.com         
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Sat May 12 21:41:28 2012
ModDate:        Tue Jun 19 12:55:05 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      288035 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569563981

Gaussian Robust Sequential and Predictive Coding
Lin Song, Jun Chen

Jia Wang

Tie Liu

McMaster University
Hamilton, ON L8S 4K1, Canada

Shanghai Jiao Tong University
Shanghai 200240, China

Texas A&M University
College Station, TX 77843, USA

1, where X1 , ∆1 , · · · , ∆L−1 are mutually independent zero2
2
mean Gaussian random variables with σX1 > 0 and σ∆i > 0,
∞
i = 1, · · · , L−1. Let {(X1 (t), · · · , XL (t))}t=1 be i.i.d. copies
of (X1 , · · · , XL ).

Abstract—We introduce two new source coding problems:
robust sequential coding and robust predictive coding. For the
Gauss-Markov source model, we characterize certain supporting
hyperplanes of the rate region of these two coding problems.
Our investigation also reveals a class of extremal inequalities
and minimax theorems.

Deﬁnition 1: A rate vector R (R1 , · · · , RL ) is said to be
achievable with a sequential coding system subject to individual distortion constraint d (d{1} , · · · , d{L} ) and hierarchical
distortion constraint δ (δ{1,2} , · · · , δ{1,··· ,L} ) if there exist
(n)
encoding functions fi : Ri×n → Ci , i = 1, · · · , L, such that

I. I NTRODUCTION
The sequential coding problem was ﬁrst introduced by
Viswanathan and Berger in [1]. Due to its potential relevance to video coding applications, this problem has received
renewed interests in recent years [2], [3]. In a sequential
coding system, L sources X1 , · · · , XL , each representing a
video frame, are encoded and decoded in a causal manner,
where Encoder i has access to X1 , · · · , Xi , i = 1, · · · , L,
and Decoder i reconstructs Xi based on the outputs from the
ﬁrst i encoders, i = 1, · · · , L. If Encoder i is only allowed to
have access to Xi as well as the outputs from the ﬁrst i − 1
encoders (if i ≥ 2), then the resulting problem is known as
predictive coding. It is shown in [4] that the rate regions of
these two coding problems are identical if X1 −X2 −· · ·−XL
form a Markov chain. Note that this Markov chain condition
is trivially satisﬁed when L = 2.
The existing coding schemes for sequential coding and predictive coding rely critically on the assumption that Decoder i
has access to the ﬁrst i encoded frames (i.e., the outputs from
the ﬁrst i encoders) when reconstructing the ith frame (i.e.,
Xi ). As a consequence, these schemes are vulnerable to the
loss of encoded frames at the decoder end. Motivated by this
observation, we introduce a robust version of these two coding
problems. Speciﬁcally, we require that Decoder i has to meet
a certain ﬁdelity constraint even when it only has access to
the ith encoded frame (i.e., the output from the ith encoder).
The remainder of this paper is organized as follows. We
state our main results in Section II. Section III contains a class
of extremal inequalities, which will play an important role in
subsequent analysis. The proofs of some main results are given
in Sections IV and V. We conclude the paper with a discussion
of certain minimax theorems inspired by our analysis.
Throughout this paper, for any random object W and 1 ×
1
2
n random vector X n we deﬁne σX n = n E[X n (X n )T ] and
2
2
σX n |W = σX n −E[X n |W ] .

1
log |Ci | ≤ Ri , i = 1, · · · , L,
n
2
σX n |Ci ≤ d{i} , i = 1, · · · , L,
i

2
σX n |C1 ,··· ,Ci ≤ δ{1,··· ,i} ,
i

i = 2, · · · , L,

(n)

n
where Ci = fi (X1 , · · · , Xin ), i = 1, · · · , L. The rate
region RS (d, δ) is the closure of the set of all the rate
vectors achievable with a sequential coding system subject
to individual distortion constraint d and hierarchical distortion
constraint δ.

Deﬁnition 2: A rate vector R
(R1 , · · · , RL ) is said
to be achievable with a predictive coding system subject to
individual distortion constraint d
(d{1} , · · · , d{L} ) and
hierarchical distortion constraint δ
(δ{1,2} , · · · , δ{1,··· ,L} )
(n)
if there exist encoding functions f1
: Rn → C1 and
(n)
n
fi : C1 × · · · × Ci−1 × R → Ci , i = 2, · · · , L, such that
1
log |Ci | ≤ Ri , i = 1, · · · , L,
n
2
σX n |Ci ≤ d{i} , i = 1, · · · , L,
i

2
σX n |C1 ,··· ,Ci ≤ δ{1,··· ,i} ,
i

(n)

i = 2, · · · , L,
(n)

n
where C1 = f1 (X1 ) and Ci = fi (C1 , · · · , Ci−1 , Xin ),
i = 2, · · · , L. The rate region RP (d, δ) is the closure of the
set of all the rate vectors achievable with a predictive system
subject to individual distortion constraint d and hierarchical
distortion constraint δ.
2
Without loss of generality, we assume 0 < d{i} ≤ σXi ,
2
i = 1, · · · , L, and 0 < δ{1,··· ,i} ≤ σXi , i = 2, · · · , L. Since
both RS (d, δ) and RP (d, δ) are closed convex sets, it sufﬁces
to characterize their supporting hyperplanes, i.e., to solve the
following optimization problems

II. M AIN R ESULTS
In this work we focus on the special case where X1 − X2 −
· · · − XL form a Gauss-Markov chain. With no essential loss
of generality, we assume Xi+1 = Xi + ∆i , i = 1, · · · , L −

inf
R∈RS (d,δ)

1

T

αR

and

inf
R∈RP (d,δ)

T

αR ,

where α = (α1 , · · · , αL ) with αi ≥ 0, i = 1, · · · , L. In view
of the fact that RP (d, δ) ⊆ RS (d, δ), we must have
T

inf

αR ≤

inf

−

ˆ
(d{1} − δ{1,2} )(d{2} − δ{1,2} ).

III. E XTREMAL I NEQUALITIES

T

αR .

The extremal inequality in the following theorem is a
generalization of the one in [5, Lemma 1]. The proof is
To state the main results of this paper, we need to deﬁne omitted.
the following function:
Theorem 4: Let Nin be a zero-mean Gaussian random vec2
2
σ
tor with i.i.d. entries of variance σNi , i = 1, 2, 3; it is assumed
α1
ψ(α, d, d{1,2} , · · · , d{1,··· ,L} , θ) =
log X1
2
2
that σN2 ≥ σN3 . Let µ1 , µ2 , β1 , β2 , and δ be arbitrary real
2
d{1}
numbers satisfying µ1 ≥ µ2 ≥ 0 and δ > 0. Then for any
L
4
2
σXi (d{1,··· ,i−1} + σ∆i−1 )
αi
random vector S n and random object W , jointly independent
+
log
2
2
2
n
n
n
2
((σXi − θi−1 )(d{1,··· ,i−1} + σ∆i−1 ) + σXi θi−1 ) of (N1 , N2 , N3 ), such that σ 2 n ≤ δ, we have
i=2
S |W
R∈RS (d,δ)

L

+
i=2

R∈RP (d,δ)

2
2
(σXi − θi−1 )d{1,··· ,i} + σXi θi−1
αi
log
,
2
2
2
((σXi − θi−1 )d{i} + σXi θi−1 )d{1,··· ,i}

n
µ1 (h(β1 S n + N1 |W ) − h(S n |W ))
n
n
n
− µ2 (h(β2 S + N2 |W ) − h(β2 S n + N3 |W ))
2
2
2
2
β1 d + σN1
µ1 n
µ2 n β2 d + σN2
≥ min
log
−
.
2
2
d
2 β2 d + σN3
d∈[0,δ] 2

where θ = (θ1 , · · · , θL−1 ). Furthermore, let
κl (α, d, δ) =

sup
2
θi ∈(0,σX

i+1

min

),i=1,··· ,L−1 d{1,··· ,i} ∈[0,δ{1,··· ,i} ],i=2,··· ,L

ψ(α, d, d{1,2} , · · · , d{1,··· ,L} , θ),
κu (α, d, δ) =

inf

2
d{1,··· ,i} ∈(0,δ{1,··· ,i} ),i=2,··· ,L θi ∈[0,σX

max

i+1

],i=1,··· ,L−1

ψ(α, d, d{1,2} , · · · , d{1,··· ,L} , θ).
Theorem 1: For α with α1 ≥ · · · ≥ αL ≥ 0,
T

inf

αR ≥ κl (α, d, δ).

T

αR ≤ κu (α, d, δ).

R∈RP (d,δ)

Theorem 3: For α with α1 ≥ · · · ≥ αL ≥ 0,
κl (α, d, δ) = κu (α, d, δ).

n
h(S n + N1 |W ) − h(S n |W )
2
d + σN1
n
≥ min
log
,
d∈[0,δ] 2
d

The proofs of Theorem 1 and Theorem 2 are given in Section IV and Section V, respectively. The proof of Theorem 3 is
omitted. These theorems together provide a characterization of
certain supporting hyperplanes of RS (d, δ) and RP (d, δ). In
particular, setting α1 = · · · = αL = 1 gives the minimum sum
rate of these two rate regions. For the special case L = 2, one
can verify that κl (α, d, δ) and κu (α, d, δ) have the following
explicit expression:

2
ˆ
ˆ
 α1 log σX1 + α2 log d{1} , δ{1,2} ≤ d{1} + d{2} − σ 2
 2
X2

d{1}
2
δ{1,2}

2
2
σX1
σX2
−1
−1
−2 −1
α1
α2
ˆ
log d{1} + 2 log d{2} ,δ{1,2} ≥ (d{1} + d{2} − σX2 )
 2

2
ˆ
 α1
σX1
d{1} γ

log
+ α2 log
, otherwise
d{1}

2

δ{1,2}

2
(σX2 − δ{1,2} )2
2
(σX2 − δ{1,2} )2 − θ2

n
n
n
h(S n + N1 |W ) − h(S n + N2 |W ) − h(S n + N3 |W )
n
n
2
2
≤ max log(2πe(d + σN1 )) − log(2πe(d + σN2 ))
d∈[0,δ] 2
2
n
2
− log(2πe(d + σN3 )).
2

with
θ=

(1)

(3)

where the minimum is achieved at d = δ. This is a
variant of the well-known worst additive noise lemma
by Ihara [6] as well as Diggavi and Cover [7, Lemma
II.2].
The next result can be viewed as a variant of [8, Theorem
5]. The proof is omitted.
Theorem 5: Let Nin be a zero-mean Gaussian random vec2
tor with i.i.d. entries of variance σNi , i = 1, 2, 3. Let δ be an
arbitrary positive real number. Then for any random vector S n
n
n
n
and random object W , jointly independent of (N1 , N2 , N3 ),
2
such that σS n |W ≤ δ, we have

2
ˆ
where d{1} = d{1} + σ∆1 and

γ=

2
µ2 n δ + σN2
µ1 n
log(2πeδ) −
.
2
2
2 δ + σN3

Furthermore, choosing µ1 = 1 and µ2 = 0 yields the
following elementary result
n
h(S n |W ) ≤ log(2πeδ).
(2)
2
2) It can be shown by setting µ1 = β1 = 1 and µ2 = 0
that

Theorem 2: For α with αi ≥ 0, i = 1, · · · , L,

2

n
n
− µ1 h(S n |W ) − µ2 (h(S n + N2 |W ) − h(S n + N3 |W ))
2
µ1 n
µ2 n d + σN2
≥ min −
log(2πed) −
2
d∈[0,δ]
2
2 d + σN3

=−

R∈RS (d,δ)

inf

Remark:
1) One can readily show by setting β1 = 0 and β2 = 1
that

2
2
ˆ
(σX2 − d{1} )(σX2 − d{2} )

2

L−1

Theorem 4 will play an instrumental role in the proof of
Theorem 1. Although Theorem 5 is not needed for proving our
main results (i.e., Theorems 1, 2, and 3), it provides valuable
insight into the choice of auxiliary random vectors in Section
IV and will be used to establish the strengthened version of
the minimax theorem in Section VI.

IV. P ROOF

OF

+
i=2

αi+1
n
n
n
−
(I(Xi+1 ; C1 , · · · , Ci ) − I(Xi+1 + Zi ; C1 , · · · , Ci ))
n
αL
n
n
n
+
(I(XL ; C1 , · · · , CL ) − I(XL + ZL−1 ; C1 , · · · , CL ))
n
L
αi
n
+
I(Xin + Zi−1 ; Ci )
n
i=2

T HEOREM 1

σ2
α1
α2
2
log(2πeσX1 ) −
log 2 X2 2
2
2
σX2 + σZ1
α1
α2
n
n
n
n
−
h(X1 |C1 ) −
(h(X2 + Z1 |C1 ) − h(X2 |C1 ))
n
n
L−1
2
σXi+1
σ2
αi+1
αi
log 2 Xi 2
−
log 2
+
2
2
σXi + σZi−1
2
σXi+1 + σZi
i=2
αi
n
+ (h(Xin + Zi−1 |C1 , · · · , Ci ) − h(Xin |C1 , · · · , Ci ))
n
αi+1
n
n
n
−
(h(Xi+1 + Zi |C1 , · · · , Ci ) − h(Xi+1 |C1 , · · · , Ci ))
n
σ2
αL
αL
n
n
+
log 2 XL 2
+
(h(XL + ZL−1 |C1 , · · · , CL )
2
σXL + σZL−1
n
=

Our proof relies on Theorem 4 as well as the techniques
developed in [5], [9], [10].
(n)
Let fi
: Ri×n → Ci , i = 1, · · · , L, be L encoding
functions such that
2
σX n |Ci ≤ d{i} ,

i = 1, · · · , L,

i

2
σX n |C1 ,··· ,Ci
i

≤ δ{1,··· ,i} ,

i = 2, · · · , L,

(n)

n
where Ci = fi (X1 , · · · , Xin ), i = 1, · · · , L. We have
L

i=1

αi
log |Ci |
n
L

≥
i=1

L
n
− h(XL |C1 , · · · , CL )) +

αi
H(Ci )
n

i=2

α1
αi
n
I(X1 ; C1 ) +
I(C1 , · · · , Ci−1 , Xin ; Ci )
n
n
i=2

=

α1
αi
n
I(X1 ; C1 ) +
[I(C1 , · · · , Ci−1 ; Ci )
n
n
i=2

L

n
Let Zi be a zero-mean Gaussian random vector with i.i.d.
2
entries of variance σZi , i = 1, · · · , L−1; moreover, we assume
n
n
Zi is independent of (Xi+1 , C1 , · · · , Ci+1 ), i = 1, · · · , L−1.
Note that

Moreover, we have

I(C1 , · · · , Ci−1 ; Ci )
n
Zi−1 ; C1 , · · ·

=
+
, Ci−1 ) +
n
n
+ I(C1 , · · · , Ci−1 ; Ci |Xi + Zi−1 )

I(Xin

+

n
Zi−1 ; Ci )

n
n
n
n
− α1 h(X1 |C1 ) − α2 (h(X2 + Z1 |C1 ) − h(X2 |C1 ))
2
2
d{1} + σ∆1 + σZ1
−α1 n
α2 n
≥
log(2πed{1} ) −
log
,
2
2
2
d{1} + σ∆1
(6)

n
− I(Xin + Zi−1 ; C1 , · · · , Ci )
n
n
≥ I(Xin + Zi−1 ; C1 , · · · , Ci−1 ) + I(Xin + Zi−1 ; Ci )
n
− I(Xin + Zi−1 ; C1 , · · · , Ci ), i = 2, · · · , L.

n
n
n
h(XL + ZL−1 |C1 , · · · , CL ) − h(XL |C1 , · · · , CL )
2
d{1,··· ,L} + σZL−1
n
log
,
≥
min
d{1,··· ,L} ∈[0,δ{1,··· ,L} ] 2
d{1,··· ,L}

Therefore, we have

i=1

αi
log |Ci |
n

(7)

n
− h(Xin + Zi−1 |Ci )
n
2
≥ − log(2πe(d{i} + σZi−1 )), i = 2, · · · , L,
(8)
2
where (6), (7), and (8) are due to (1), (3), and (2), respectively.
Substituting (5), (6), (7), (8) into (4) yields

L

≥

(4)

n
αi (h(Xin + Zi−1 |C1 , · · · , Ci ) − h(Xin |C1 , · · · , Ci ))
n
n
n
− αi+1 (h(Xi+1 + Zi |C1 , · · · , Ci ) − h(Xi+1 |C1 , · · · , Ci ))
2
d{1,··· ,i} + σZi−1
αi n
≥
min
log
d{1,··· ,i}
d{1,··· ,i} ∈[0,δ{1,··· ,i} ] 2
2
2
d{1,··· ,i} + σ∆i + σZi−1
αi+1 n
−
log
, i = 2, · · · , L − 1.
2
2
d{1,··· ,i} + σ∆i
(5)

+ I(Xin ; C1 , · · · , Ci ) − I(Xin ; C1 , · · · , Ci−1 )].

I(Xin

αi
2
2
log(2πe(σXi + σZi−1 ))
2

αi
n
− h(Xin + Zi−1 |Ci ) .
n
In view of Theorem 4,

L

≥

L

αi
n
(I(Xin ; C1 , · · · , Ci ) − I(Xin + Zi−1 ; C1 , · · · , Ci ))
n

α1
αi
n
n
I(X1 ; C1 ) +
(I(Xin + Zi−1 ; C1 , · · · , Ci−1 )
n
n
i=2

n
n
+ I(Xin + Zi−1 ; Ci ) − I(Xin + Zi−1 ; C1 , · · · , Ci )

1
n

+ I(Xin ; C1 , · · · , Ci ) − I(Xin ; C1 , · · · , Ci−1 ))
α1
α2
n
n
n
n
=
I(X1 , C1 ) −
(I(X2 ; C1 ) − I(X2 + Z1 ; C1 ))
n
n

3

L

αi log |Ci |
i=1

L
2
σX1
α1
αi
≥
min
log
+
d{1,··· ,i} ∈[0,δ{1,··· ,i} ],i=2,··· ,L 2
d{1} i=2 2
2
2
2
2
(σXi + σZi−1 )(d{1,··· ,i−1} + σ∆i−1 )(d{1,··· ,i} + σZi−1 )

× log

2
2
2
(d{1,··· ,i−1} + σ∆i−1 + σZi−1 )(d{i} + σZi−1 )d{1,··· ,i}
(9)

2
Replacing σZi with

2
σX

i+1

2
σX

i+1

θi

−θi

let
R1 = I(X1 ; U{1} ),
.

αi log |Ci | ≥
i=1

Now we proceed to construct (U{1} , · · · , U{L} ) with certain
desired properties. We assume E[U{i} ] = 0, i = 1, · · · , L. For
d{i} , i = 1, · · · , L, and d{1,··· ,i} , i = 2, · · · , L, speciﬁed in
Lemma 1, deﬁne

, i = 1, · · · , L − 1, in (9) gives
min

d{1,··· ,i} ∈[0,δ{1,··· ,i} ],i=2,··· ,L

The proof is complete in view of the fact that this lower bound
2
holds for arbitrary θi ∈ (0, σXi+1 ), i = 1, · · · , L − 1.
OF

T

αR ≤

R∈RP (d,δ)

max

E[U{1,··· ,i−1} U{i} ] =
+ λ{1,··· ,i} −

i = 1, · · · , L,

i = 1, · · · , L,

4
σXi
2
2
ˆ
(σXi + λ{1,··· ,i−1} )(σXi + λ{i} )

2
σXi

ˆ
(λ{1,··· ,i−1} − λ{1,··· ,i} )(λ{i} − λ{1,··· ,i} ) ,

and ((Xj )j=i , U{1} , · · · , U{i−1} ) − (Xi , U{1,··· ,i−1} ) − U{i}
form a Markov chain, where we deﬁne U{1,··· ,k} =
E[Xk+1 |U{1,··· ,k−1} , U{k} ], k = 2, · · · , L−1. It is easy to see
that the constructed (U{1} , · · · , U{L} ) satisﬁes the conditions
in Lemma 2; as a consequence,

i = 2, · · · , L,

inf

such that

αR

T

R∈RP (d,δ)

max
2

θi ∈[0,σX ],i=1,··· ,L−1

ψ(α, d , d{1,2} , · · · , d{1,··· ,L} , θ)

L

≤ α1 I(X1 ; U{1} ) +

i

max
2

θi ∈[0,σX ],i=1,··· ,L−1

ψ(α, d, d{1,2} , · · · , d{1,··· ,L} , θ).

αi I(Xi , U{1,··· ,i−1} ; U{i} ).
i=2

Moreover, it can be veriﬁed that

i

An inner bound of RP (d, δ) is stated in the following
lemma, which can be proved via the standard random coding
argument.
Lemma 2: For any (U{1} , · · · , U{L} ) jointly Gaussian with
(X1 , · · · , XL ) such that

•

,

2
2
E[U{i} ] = E[Xi U{i} ] = σXi − d{i} ,

2
2
d{1,··· ,i−1} + σ∆i−1 + d{i} − σXi ≤ d{1,··· ,i}
1
1
1 −1
≤
+
− 2
, i = 2, · · · , L,
2
d{1,··· ,i−1} + σ∆i−1
d{i}
σXi

•

2
σXi − d{1,··· ,i}

and (X2 , · · · , XL ) − X1 − U{1} form a Markov chain. Now
successively from i = 2 to L, let U{i} be jointly Gaussian
with (X1 , · · · , XL , U{1} , · · · , U{i−1} ) such that

2
For simplicity, we assume that d{i} < σXi , i = 1, · · · , L, and
2
δ{1,··· ,i} < σXi , i = 2, · · · , L. The general case where some
2
d{i} and/or δ{1,··· ,i} are equal to σXi can be handled via a
continuity argument.
The following lemma is a variant of [5, Lemma 3] and its
proof is omitted.
Lemma 1: There exist d
(d{1} , · · · , d{L} ) and d{1,··· ,i} ,
i = 2, · · · , L, with

•

2
σXi d{1,··· ,i}

2
2
E[U{1} ] = E[X1 U{1} ] = σX1 − d{1}

ψ(α, d, d{1,2} , · · · , d{1,··· ,L} , θ).

≤

i = 2, · · · , L,

Let U{1} be jointly Gaussian with (X1 , · · · , XL ) such that

2
θi ∈[0,σX ],i=1,··· ,L−1

0 < d{1,··· ,i} ≤ d{1,··· ,i} ,

,

2
ˆ
d{1,··· ,i} = d{1,··· ,i} + σ∆i , i = 1, · · · , L − 1,
2
ˆ
σXi+1 d{1,··· ,i}
ˆ
, i = 1, · · · , L − 1.
λ{1,··· ,i} =
2
ˆ
σXi+1 − d{1,··· ,i}

T HEOREM 2

i

0 < d{i} ≤ d{i} ,

2
σXi − d{i}

λ{1,··· ,i} =

It sufﬁces to prove that for any d{1,··· ,i} ∈ (0, δ{1,··· ,i} ),
i = 2, · · · , L,
inf

2
σXi d{i}

λ{i} =

ψ(α, d, d{1,2} , · · · , d{1,··· ,L} , θ).

V. P ROOF

i = 2, · · · , L.

We have (R1 , · · · , RL ) ∈ RP (d, δ).

L

1
n

Ri = I(Xi , U{1} , · · · , U{i−1} ; U{i} ),

I(X1 ; U{1} ) =

σ2
1
log X1 ,
2
d{1}

I(Xi , U{1,··· ,i−1} ; U{i} )
=
+

(X2 , · · · , XL ) − X1 − U{1} form a Markov chain,
(Xj )j=i − (Xi , U{1} , · · · , U{i−1} ) − U{i} form a Markov
chain, i = 2, · · · , L,
2
2
σXi |U{i} ≤ d{i} , i = 1, · · · , L, and σXi |U{1} ,··· ,U{i} ≤
δ{1,··· ,i} , i = 2, · · · , L,

4 ˆ
σXi d{1,··· ,i−1}
1
log
2
2
ˆ
2
((σXi − θi−1 )d{1,··· ,i−1} + σX θi−1 )
2
2
(σXi − θi−1 )d{1,··· ,i} + σXi θi−1
1
log
,
2 −θ
2 θ
2
((σXi
i−1 )d{i} + σXi i−1 )d{1,··· ,i}

i = 2, · · · , L,

4

(10)

n
n
sian auxiliary random vectors Z1 , · · · , Z{L−1} in the proof of
Theorem 1 leads to a tight lower bound.
Theorem 7: Let V
(V1 , · · · , VL ) be jointly distributed
n
with (X n , U) such that Vi − Xi+1 − (U1 , · · · , Ui+1 ) form a
Markov chain, i = 1, · · · , L − 1. Deﬁne

where
ˆ
(λ{1,··· ,i−1} − λ{1,··· ,i} )(λ{i} − λ{1,··· ,i} )

θi−1 =

− λ{1,··· ,i} .
Therefore, we have

n
n
f (pU|X n , pV1 |X2 , · · · , pVL−1 |XL )

L

α1 I(X1 ; U{1} ) +

αi I(Xi , U{1,··· ,i−1} ; U{i} )

L
n
= α1 I(X1 ; U1 ) +

i=2

= ψ(α, d , d{1,2} , · · · , d{1,··· ,L} , θ ),

(11)

+ I(Vi−1 ; Ui ) − I(Vi−1 ; U1 , · · · , Ui )
+ I(Xin ; U1 , · · · , Ui ) − I(Xin ; U1 , · · · , Ui−1 )).

where θ = (θ1 , · · · , θL−1 ). Note that
ψ(α, d , d{1,2} , · · · , d{1,··· ,L} , θ )
≤

max
2

θi ∈[0,σX ],i=1,··· ,L−1

Then

ψ(α, d , d{1,2} , · · · , d{1,··· ,L} , θ)

sup

i

≤

max

2
θi ∈[0,σX ],i=1,··· ,L−1

pVi |X n

i+1

ψ(α, d, d{1,2} , · · · , d{1,··· ,L} , θ),

=

i

(12)

[1] H. Viswanathan and T. Berger, “Sequential coding of correlated sources,”
IEEE Trans. Inf. Theory, vol. 46, no. 1, pp. 236-246, Jan. 2000.
[2] N. Ma and P. Ishwar, “On delayed sequential coding of correlated
sources,” IEEE Trans. Inf. Theory, vol. 57, no. 6, pp. 3763-3782, Jun.
2011.
[3] E.-H. Yang, L. Zheng, D.-K. He, and Z. Zhang, “Rate distortion theory
for causal video coding: characterization, computation algorithm, and
comparison,” IEEE Trans. Inf. Theory, vol. 57, no. 8, pp. 5258-5280,
Aug. 2011.
[4] J. Wang and X. Wu, “Information ﬂows in video coding,” in Proc. IEEE
Data Comp. Conf., Snowbird, UT, USA, Mar. 24-26, 2010, pp. 149-158.
[5] J. Chen, “Rate region of Gaussian multiple description coding with
individual and central distortion constraints,” IEEE Trans. Inf. Theory,
vol. 55, no. 9, pp. 3991-4005, Sep. 2009.
[6] S. Ihara, “On the capacity of channels with additive non-Gaussian noise,”
Inform. Contr., vol. 37, no. 1, pp. 34-39, Apr. 1978.
[7] S. N. Diggavi and T. M. Cover, “The worst additive noise under a
covariance constraint,” IEEE Trans. Inf. Theory, vol. 47, no. 7, pp. 30723081, Nov. 2001.
[8] J. Chen and J. Wang, “Vector Gaussian multiple description coding
with individual and central distortion constraints,” in Proc. IEEE Symp.
Information Theory (ISIT), St. Petersburg, Russia, Jul. 31-Aug. 5, 2011,
pp. 1693-1697.
[9] L. Ozarow, “On a source coding problem with two channels and three
receivers,” Bell Syst. Tech. J., vol. 59, no. 10, pp. 1909-1921, Dec. 1980.
[10] H. Wang and P. Viswanath, “Vector Gaussian multiple description with
individual and central receivers,” IEEE Trans. Inf. Theory, vol. 53, no. 6,
pp. 2133-2153, Jun. 2007.

n
αi (I(Xin + Zi−1 ; U1 , · · · , Ui−1 )
i=2

n
n
+ I(Xin + Zi−1 ; Ui ) − I(Xin + Zi−1 ; U1 , · · · , Ui )

+ I(Xin ; U1 , · · · , Ui ) − I(Xin ; U1 , · · · , Ui−1 )).
Then

inf

sup

2
f (pU|X n , σZ )

i

PU ∈pU|X n σ2 >0,i=1,··· ,L−1
Z

n
n
f (pU|X n , pV1 |X2 , · · · , pVL−1 |XL ).

R EFERENCES

L

=

,i=1,··· ,L−1

The work of Lin Song and Jun Chen was supported in
part by an Early Research Award from the Province of
Ontario and in part by the Natural Science and Engineering
Research Council (NSERC) of Canada under a Discovery
Grant. The work of Jia Wang was supported in part by the
NSFC under Grant 60802020 and in part by the 973 Program
(2010CB731401, 2010CB731406). The work of Tie Liu was
supported by the National Science Foundation under Grant
CCF-09-16867.

2
f (pU|X n , σZ )

inf

sup

ACKNOWLEDGMENT

We have characterized certain supporting hyperplanes of the
rate region of robust sequential coding and robust predictive
coding for the Gauss-Markov source model. Note that our
main results can be viewed as a manifestation of certain
information-theoretic minimax theorems. Indeed, the proofs
of Theorems 1, 2, and 3 can be leveraged to establish the
n
n
following result. We assume that X n
(X1 , · · · , XL ),
(d{1} , · · · , d{L} ), (δ{1,2} , · · · , δ{1,··· ,L} ) are deﬁned as in
Section I and α1 ≥ · · · ≥ αL ≥ 0.
Theorem 6: Let PU denote the set of conditional distrin
butions pU |X n with U
(U1 , · · · , UL ) such that Xi+1 −
n
Xi − (U1 , · · · , Ui ) form a Markov chain, i = 1, · · · , L − 1,
2
2
σX n |Ui ≤d{i} , i = 1, · · · , L, and σX n |U1 ,··· ,Ui ≤ δ{1,··· ,i} ,
i
i
n
i = 2, · · · , L. Let Zi be a zero-mean Gaussian random vector
2
with i.i.d. entries of variance σZi , i = 1, · · · , L − 1; moren
n
over, we assume Zi is independent of (Xi+1 , U1 , · · · , Ui+1 ),
2
2
2
i = 1, · · · , L − 1. Deﬁne σZ = (σZ1 , · · · , σZL−1 ) and

sup

inf

PU ∈pU |X n pV |X n
i

n
n
f (pU|X n , pV1 |X2 , · · · , pVL−1 |XL )

n
n
Remark: We write f (pU|X n , pV1 |X2 , · · · , pVL−1 |XL ) instead
of f (pU|X n , pV |X n ,U ) because f depends on pV |X n ,U only
n
through pVi |Xi+1 , i = 1, · · · , L − 1.

VI. C ONCLUSION

2
σZ >0,i=1,··· ,L−1 PU ∈pU|X n

inf

,i=1,··· ,L−1 PU ∈pU|X n

i+1

where (12) follows from Lemma 1. Combining (10), (11), and
(12) completes the proof.

n
= α1 I(X1 ; U1 ) +

αi (I(Vi−1 ; U1 , · · · , Ui−1 )
i=2

2
f (pU|X n , σZ ).

i

In fact, Theorem 5 implies that the following strengthened
version is also true, which explains why our choice of Gaus-

5

