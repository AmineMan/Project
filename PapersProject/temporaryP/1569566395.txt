Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Sat May 19 01:19:06 2012
ModDate:        Tue Jun 19 12:56:31 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      448083 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566395

Extremality Properties for Gallager’s Random
Coding Exponent
Mine Alsan
LTHI, I&C, EPFL
Lausanne, Switzerland
Email: mine.alsan@epﬂ.ch

Abstract—We describe certain extremality properties for Gallager’s reliability function E0 for binary input symmetric DMCs.
In particular, we show that amongst such DMC’s whose E0 (ρ1 )
has a given value for a given ρ1 , the BEC and BSC have the
largest and smallest value of the derivative of E0 (ρ2 ) for any
ρ2 ≥ ρ1 . As the random coding exponent is obtained by tracing
the map ρ → (E0 (ρ), E0 (ρ) − ρE0 (ρ)) this conclusion includes
as a special case the results of [1]. Furthermore, we show that
amongst channels W with a given value of E0 (ρ) for a given ρ the
BEC and BSC are the most and least polarizing under Arıkan’s
polar transformations in the sense that their polar transforms
W + and W − has the largest and smallest difference in their E0
values.
Index Terms—Channel reliability function, random coding
exponent, channel polarization.

The BEC and BSC also exhibit themselves as being extremal for Arıkan’s polarization transforms. In his award
winning paper [3], Arıkan describes two synthetic channels
W + , and W − which can be obtained from two independent
copies of W . It is well known (proved as a corollary to
extremes of information combining) that among all channels
W with a given symmetric capacity I(W ), the BEC and BSC
polarize most and least in the sense of having the largest and
smallest difference between I(W + ) and I(W − ). We report
a more general conclusion: amongst all channels W with a
given value of E0 (ρ, W ), the BEC and BSC polarize most
and least in the sense of having the largest difference between
E0 (ρ, W + ) and E0 (ρ, W − ).

I. I NTRODUCTION

II. E XTREMALITY OF THE R ANDOM C ODING E XPONENT
A. Deﬁnitions and Properties

While the capacity of a memoryless channel W gives the
largest rate that may be communicated reliably across it, the
reliability function E(R, W ) provides a ﬁner measure on the
quality of the channel: for any rate R less than channel
capacity, it is possible to ﬁnd a sequence of codes of increasing
blocklength, each of which of rate at least R, and whose
block error probability decays exponentially to zero in the
blocklength — E(R, W ) is the largest possible rate of this
decay.
Gallager classical treatise [2] gives a lower bound to
E(R, W ), the random coding exponent Er (R, W ) in the form
Er (R, W ) = maxρ∈[0,1] E0 (ρ, W ) − ρR. Remarkably, this
lower bound is tight for rates above the critical rate E0 (1, W ).
The function E0 (ρ, W ) that appears as an auxiliary function on
the road to deriving Er (R, W ) turns out to be of independent
interest in its own right. In particular, E0 (ρ, W )/ρ is the
largest rate for which a sequential decoder can operate while
keeping the ρ-th moment of the decoder’s computation effort
per symbol bounded.
In this paper we investigate the extremal properties of
E0 (ρ, W ) for the class of binary input symmetric channels.
We show that among all such channels with a given value of
E0 (ρ1 , W ) the binary erasure channel (BEC) and the binary
symmetric channel (BSC) distinguish themselves in certain
ways: they have, respectively, the largest and smallest value
of E0 (ρ2 , W ) for any ρ2 ≥ ρ1 . Among the simple corollaries
of this is the conclusion that of all the channels with the same
capacity, the BEC and BSC have the largest and smallest value
of Er (R, W ), a result reported in [1] last year.

Deﬁnition 1: [2] Given a discrete memoryless channel W
with input alphabet X and output alphabet Y, ﬁx a distribution
Q on its input alphabet.
Consider the function Er (R, Q, W ) deﬁned as
Er (R, Q, W ) = max {E0 (ρ, Q, W ) − ρR}

(1)

ρ∈[0,1]

for R ≥ 0, with
1+ρ

E0 (ρ, Q, W ) = − log

Q(x)W (y | x)
y∈Y

1
1+ρ

.

x∈X

(2)
The random coding exponent is deﬁned as
Er (R, W ) = max Er (R, Q, W ).
Q

(3)

We restrict our attention to symmetric binary input channels
(B-DMCs). In this case the uniform input distribution maximizes Equation (3). Accordingly, we drop the variable Q from
the notations. The expression in (2) becomes
E0 (ρ, W ) = − log
y∈Y

1
1
1
1
W (y | 0) 1+ρ + W (y | 1) 1+ρ
2
2

1+ρ

.
(4)

Theorem 5.6.3 in [2] summarizes the properties of
E0 (ρ, W ) with respect to the variable ρ. For ρ ≥ 0, E0 (ρ, W )
is a positive, concave increasing function in ρ. Moreover, the
symmetric capacity I(W ) and the Bhattacharyya parameter
Z(W ) of the channel can be derived from E0 (ρ, W ) by

1

E0 (ρ, W )
= I(W )
ρ
ρ=0
2
• E0 (1, W ) = log
.
1 + Z(W )
The maximization in the right hand side of Equation (1)
over ρ ∈ [0, 1] can be described in terms of the following
parametric equations:
•

∂ E (ρ, W )
∂ρ 0

R(ρ, W ) =

∂
E0 (ρ, W )
∂ρ

Er (ρ, W ) = E0 (ρ, W ) − ρ
for R in the range

The inequality for the binary symmetric channel can be
obtained similarly.
Another particular case of Theorem 1 when ρ1 → 0
recovers the result in [1]: among all symmetric B-DMCs of
the same capacity, the binary erasure channel and the binary
symmetric channel are extremal with respect to the random
coding exponent. This is shown in the next corollary.
Corollary 2 (Theorem 2.3 [1]): Given a symmetric BDMC W of capacity I(W ), we deﬁne a binary symmetric
channel WBSC , and a binary erasure channel WBEC of the same
capacity through the equality

=

ρ=0

∂E0 (ρ, W )
∂ρ

∂
E0 (ρ, W )
∂ρ

≤R≤
ρ=1

∂E0 (ρ, W )
∂ρ

(5)

ρ=0

I(W ) = I(WBEC ) = I(WBSC ).

.

Then, the random coding error exponent of the channels satisfy

B. Extremality of the Random Coding Exponent

Er (ρ, WBSC ) ≤ Er (ρ, W ) ≤ Er (ρ, WBEC ).

We show in this section that the binary erasure channel and
the binary symmetric channel are extremal among all symmetric B-DMCs with respect to the random coding exponent.
In particular, we show in Theorem 1 a certain extremality
property holds even when the quantities appearing in the
parametric form of the error exponent are allowed to be
evaluated at different values of the parameter.
Theorem 1: Given a symmetric B-DMC W , for any ﬁxed
value of ρ1 ∈ [0, 1], we deﬁne a binary symmetric channel
WBSC , and a binary erasure channel WBEC through the equality
E0 (ρ1 , W ) = E0 (ρ1 , WBSC ) = E0 (ρ1 , WBEC ).

Proof: Same capacity is equivalent to
E0 (ρ1 , W )
ρ1

(6)

E0 (ρ1 , WBEC )
ρ1

=
ρ1 =0

E0 (ρ1 , WBSC )
ρ1

.
ρ1 =0

R(ρ2 , WBSC ) ≤ R(ρ2 , W ) ≤ R(ρ2 , WBEC ).
(7)

As

The proof of the theorem is given in Appendix VI-A.
Remark 1: The erasure probability of WBEC and the
crossover probability of WBSC depend both on the channel
W and the parameter ρ1 .
For the special case when ρ1 = ρ2 , we recover in the next
Corollary a result obtained in [4].
Corollary 1 ([4]): Given a symmetric B-DMC W , for any
ﬁxed value of ρ ∈ [0, 1], we deﬁne a binary symmetric channel
˜
˜
WBSC , and a binary erasure channel WBEC through the equality
˜
˜
R(ρ, W ) = R(ρ, WBEC ) = R(ρ, WBSC ).

=
ρ1 =0

Moreover, we can reformulate Theorem 1 by replacing the
equality condition imposed for the functions E0 (ρ, W ) in
E0 (ρ, W )
Equation (6), by equality among the functions
.
ρ
Since this latter is a continuous function of ρ, we deduce
that the result of the theorem is still valid for ρ1 → 0. As
a consequence, for any ρ2 ∈ [0, 1], we have

Then, for any ρ2 ∈ [ρ1 , 1], we have
R(ρ2 , WBSC ) ≤ R(ρ2 , W ) ≤ R(ρ2 , WBEC ).

(9)

∂
E0 (ρ, W ) = R(ρ, W ), this last inequality implies that
∂ρ
E0 (ρ, WBSC ) ≤ E0 (ρ, W ) ≤ E0 (ρ, WBEC )

which, in turn, implies the inequality for the random coding
exponent.
III. E XTREMALITY FOR THE P OLARIZATION
T RANSFORMATIONS
In this section, we study the behavior of E0 (ρ, W ) from the
aspect of channel polarization. In Theorem 2, we show that
the binary erasure channel and the binary symmetric channel
are also extremal in the evolution of E0 (ρ, W ) under the
polarization transformations.

(8)

Then,

A. Basic Polarization Transformations

˜
˜
E0 (ρ, WBEC ) ≤ E0 (ρ, W ) ≤ E0 (ρ, WBSC )
˜
˜
Er (ρ, WBEC ) ≤ Er (ρ, W ) ≤ Er (ρ, WBSC ).

In [3], a low complexity code construction that achieves
the capacity of symmetric B-DMCs is given based on the
recursive application of two basic channel transformations.
These transforms synthesize two new channels W − : X → Y 2
and W + : X → Y 2 × X by combining two copies of the
channel W . The channels are deﬁned by

Proof: Since Er (ρ, W ) = E0 (ρ, W ) − ρR(ρ, W ), it
sufﬁces to prove the ﬁrst set of inequalities in view of (8). For
a ﬁxed value of ρ, we deﬁne another binary erasure channel
WBEC through the equality
E0 (ρ, W ) = E0 (ρ, WBEC ).

W − (y1 y2 | u1 ) =
u2 ∈X

By Theorem 1, we know that R(ρ, W ) ≤ R(ρ, WBEC ). Via
˜
(8), we thus get R(ρ, WBEC ) ≤ R(ρ, WBEC ). As a result,

+

W (y1 y2 u1 | u2 ) =

˜
E0 (ρ, WBEC ) ≤ E0 (ρ, WBEC ) = E0 (ρ, W ).

2

1
W (y1 | u1 ⊕ u2 )W (y2 | u2 ) (10)
2

1
W (y1 | u1 ⊕ u2 )W (y2 | u2 ).
2

(11)

Arıkan shows that the recursive application of these transforms polarize any W in the sense that almost all synthesized
channels are either almost perfect or almost very noisy. Intuitively, one expects that E0 (ρ)/ρ similarly polarizes. The next
proposition provides a simple proof for this argument.
Proposition 1: After the recursive application of the polarization transformations, the E0 (ρ)/ρ parameters of the
synthesized channels polarize to the extremal values {0, 1}
when a long sequence of steps is applied to the channel.
Proof: In [5], it is shown that E0 (ρ, W )/ρ is a decreasing
function in ρ . Therefore, we can squeeze the function between
− log

1 + Z(W )
2

≤

of a given symmetric capacity the BEC and BSC are extremal
with respect to the polarization transformations, in the sense
that
+
−
I(WBSC ) − I(WBSC ) ≤ I(W + ) − I(W − )
+
−
≤ I(WBEC ) − I(WBEC ).

These inequalities can also be obtained by the results on the
extremes of information combining [7], together with the fact
that symmetric capacity is preserved under the polarization
transformations [3]. Note also that, when combined with the
preservation property, only one of the Equations (13) and (14)
would be sufﬁcient to get the result.

E0 (ρ, W )
≤ I(W ).
ρ

(12)

IV. C ONCLUSIONS
We have described some extremality properties for binary
input channels when the information measure is Gallager’s
E0 . These properties yield in straightforward fashion already
known extremality results.
The extremality of BEC and BSC for polar transforms
can be interpreted in the context of information combining.
Theorem 2 shows that even if we change the measure of
information from the customary mutual information to E0 the
channels BEC and BSC still remain extremal.
The assumed symmetry of the channel W is not a major
limitation: as long as we are interested only in the case when
the input distribution Q is uniform, the E0 of a channel W
˜
is the same as an associated symmetric channel W as deﬁned
in Lemma 1.4 of [8]. Since the symmetrizing operator W →
˜
W commutes with the polar transforms, all the conclusions
of the paper are valid for arbitrary binary input channels as
long as one evaluates all quantities under the uniform input
distribution.

−
−
E0 (ρ, WBEC ) ≤ E0 (ρ, W − ) ≤ E0 (ρ, WBSC )

(13)

V. ACKNOWLEDGMENT
The author would like to thank Emre Telatar for helpful
discussions.

+
+
E0 (ρ, WBSC ) ≤ E0 (ρ, W + ) ≤ E0 (ρ, WBEC ).

(14)

The proof follows based on the fact that the quantities I(W ),
and Z(W ) both polarize, i.e., I(W ) → 0 ⇐⇒ Z(W ) → 1,
and I(W ) → 1 ⇐⇒ Z(W ) → 0 [3].
Remark 2: Based on numerical experiments, we conjecture
the channels W , W − , and W + satisfy the next relationship:
E0 (ρ, W − ) + E0 (ρ, W + ) ≥ 2E0 (ρ, W ).
Remark 3: [6] For a symmetric W the channels W − , W ,
and W + are ordered by degradation. Consequently,
E0 (ρ, W − ) ≤ E0 (ρ, W ) ≤ E0 (ρ, W + ).
B. Extremality of the Basic Channel Transformations
Theorem 2: Given a symmetric B-DMC W , for any ﬁxed
value of ρ ∈ [0, 1], we deﬁne (as in Theorem 1) a binary
symmetric channel WBSC , and a binary erasure channel WBEC
through the equality
E0 (ρ, W ) = E0 (ρ, WBEC ) = E0 (ρ, WBSC ).
Then

R EFERENCES
[1] A. Guillen i Fabregas, I. Land, and A. Martinez. Extremes of random
coding error exponents. In Information Theory Proceedings (ISIT), 2011
IEEE International Symposium on, pages 2896 –2898, 31 2011-aug. 5
2011.
[2] R. G. Gallager. Information Theory and Reliable Communication. John
Wiley & Sons, Inc., New York, NY, USA, 1968.
[3] E. Arıkan. Channel polarization: a method for constructing capacityachieving codes for symmetric binary-input memoryless channels. IEEE
Trans. Inf. Theor., 55(7):3051–3073, 2009.
[4] E. Arıkan and E. Telatar. BEC and BSC are E0 extremal. Unpublished
manuscript. July 2008.
[5] S. Arimoto. Information measures and capacity of order α for discrete
memoryless channels. In Topics in information theory, I.Csiszar and
P. Elias, editors, Amsterdam, The Netherlands, 1977. North-Holland
Publishing Co.
[6] E. Telatar, Private Communications.
[7] I. Sutskover, S. Shamai, and J. Ziv. Extremes of information combining.
Information Theory, IEEE Transactions on, 51(4):1313 – 1325, April
2005.
[8] S.B. Korada, ”Polar codes for channel and source coding,” PhD Thesis,
EPFL, Switzerland, 2009.
[9] M. Alsan. Channel polarization and polar codes. Technical Report. EPFL,
Switzerland, 2012. http://infoscience.epﬂ.ch/record/176515
[10] I. S. Gradshteyn and I. M. Ryzhik. Table of Integrals, Series and
Products. Academic Press Inc, 1994.

The proof is given in Appendix VI-B.
In Theorem 2, we have shown that among all B-DMC’s W
of ﬁxed E0 (ρ, W ), the binary erasure channel W − transformation results in a lower bound to any E0 (ρ, W − ) and the binary
symmetric channel’s one in an upper bound to any E0 (ρ, W − ).
For the W + transformation, a similar extremality property
holds except the difference that the binary erasure channel
W + transformation provides an upper bound and the binary
symmetric channel’s one a lower bound to any E0 (ρ, W + ).
This shows that the binary erasure and binary symmetric
channels appear on reversed sides of the inequalities for
E0 (ρ, W − ) and E0 (ρ, W + ).
Corollary 3: Under the same assumptions as Theorem 2
+
−
E0 (ρ, WBSC ) − E0 (ρ, WBSC ) ≤ E0 (ρ, W + ) − E0 (ρ, W − )
+
−
≤ E0 (ρ, WBEC ) − E0 (ρ, WBEC ).

Remark 4: Dividing all sides of the inequality above by ρ
and taking the limit as ρ → 0, we see that among channels

3

VI. A PPENDICES

Corollary 5: Under the same assumptions of Theorem 1,
we have for ρ2 ≥ ρ1

A. Appendix A: Proof of Theorem 1

E0 (ρ2 , WBSC ) ≤ E0 (ρ2 , W ) ≤ E0 (ρ2 , WBEC ).

We start with a lemma proved in [4] that expresses the
function E0 (ρ, W ) in a more suitable form for the proof.
Lemma 1: [4] Given a symmetric B-DMC W , and a ﬁxed
ρ ∈ [0, 1], there exist a random variable Z taking values in
the [0, 1] interval such that
E0 (ρ, W ) = − log E [g(ρ, Z)]

(19)

Proof: By the continuity of E0 (ρ, W ) in the channel, it
sufﬁces to show that
E0 (ρ1 , WBSC ) < E0 (ρ1 , W ) < E0 (ρ1 , WBEC )

(15)

implies

where

E0 (ρ2 , WBSC ) < E0 (ρ2 , W ) < E0 (ρ2 , WBEC ).

g(ρ, z) =

1
1
1
1
(1 + z) 1+ρ + (1 − z) 1+ρ
2
2

1+ρ

.

From Theorem 1,

(16)

E0 (ρ, WBSC ) < E0 (ρ, W ) < E0 (ρ, WBEC )

Moreover, the random variable ZBEC of a binary erasure
channel is {0, 1} valued. The random variable ZBSC of a binary
symmetric channel is a constant zBSC .
The proof of the lemma can be found in Lemma 4.1 [9].
Based on this representation, the function R(ρ, W ) equals
E [−∂g(ρ, Z)/∂ρ]
R(ρ, W ) =
.
E [g(ρ, Z)]

implies
R(ρ, WBSC ) < R(ρ, W ) < R(ρ, WBEC ).
We deﬁne F (ρ) = E0 (ρ, W ) − E0 (ρ, WBEC ), or F (ρ) =
∂
E0 (ρ, WBSC ) − E0 (ρ, W ). Noting that R(ρ) =
E0 (ρ), the
∂ρ
corollary is implied by the following statement:

(17)

To prove the theorem we need to introduce a set of lemmas,
and corollaries to them.
Lemma 2: For a ﬁxed value of ρ, the function g(ρ, z)
deﬁned in Equation (16) is a concave decreasing function in
the variable z.
The proof of the lemma is carried in Appendix A of [9].
Lemma 3: For ﬁxed values of ρ1 , ρ2 ∈ [0, 1] such that ρ1 ≤
˜
ρ2 , the function fρ1 ,ρ2 (t) deﬁned as

F (ρ1 ) < 0 and (F (ρ) < 0 ⇒ F (ρ) < 0) ⇒ F (ρ2 ) < 0.
But this is true by elementary considerations on differential
equations.
Now, we proceed with the proof of the theorem for the case
˜
ρ2 > ρ1 . By Lemma 3, the function fρ1 ,ρ2 (t) is concave in
t. So, we can apply the two sides of Jensen’s inequality to
obtain

∂
˜
fρ1 ,ρ2 (t) =
g(ρ2 , g −1 (ρ1 , t))
∂ρ2

˜
˜
E −fρ1 ,ρ2 (g(ρ1 , ZBSC )) ≤ E −fρ1 ,ρ2 (g(ρ1 , Z))
˜
≤ E −fρ1 ,ρ2 (g(ρ1 , ZBEC )) .

is a concave function in the variable t.
The convexity analysis is tedious, and we refer the readers to
Appendix D of [9] for a proof. We ﬁrst prove Theorem 1 for
the case ρ1 = ρ2 in the next corollary.
Corollary 4: Under the same assumptions of Theorem 1,
the following extremality property holds:
R(ρ1 , WBSC ) ≤ R(ρ1 , W ) ≤ R(ρ1 , WBEC ).

On the other hand, as E0 (ρ, W ) = − log E [g(ρ, Z)] by
Lemma 1, we know by Corollary 5 that
E [g(ρ2 , ZBEC )] ≤ E [g(ρ2 , Z)] ≤ E [g(ρ2 , ZBSC )] .
Taking the ratios of the last two set of inequalities proves the
extremality property stated in Equation (7).

(18)

Proof: By the equality conditions in Equation (6), the
denominator in Equation (17) is the same for all the channels.
Then, the proof follows directly by the concavity of the
˜
function fρ1 (t) in t, and the special structure of the Z random
variable corresponding to the channels WBSC , and WBEC . We
deﬁne the random variable T = g(ρ, Z). By the two sides of
the Jensen’s inequality for concave functions, i.e.

B. Appendix B: Proof of Theorem 2
As we did in Appendix A, we start with two lemmas to
express the functions E0 (ρ, W + ), and E0 (ρ, W − ) in a similar
form as in Lemma 1.
Lemma 4: Given a B-DMC W and a ﬁxed ρ ∈ [0, 1], there
exist i.i.d. random variables Z1 and Z2 taking values in the
[0, 1] interval such that

˜
˜
fρ (1) − fρ1 (2−ρ1 )
˜
˜
(E [T ] − 1) ≤ E fρ1 (T ) .
fρ1 (1) + 1
1 − 2−ρ1
˜
≤ fρ1 (E [T ])

E0 (ρ, W ) = − log E [g(ρ, Z)]
and
E0 (ρ, W − ) = − log E [g(ρ, Z1 Z2 )]

˜
Equation (18) follows (−f is convex).

where g(ρ, z) is given by (16).

4

Lemma 5: Given a B-DMC W and a ﬁxed ρ ∈ [0, 1], there
exist i.i.d. random variables Z1 and Z2 taking values in the
[0, 1] interval such that

where (1) follows by symmetry of the variables Z1 and zBSC .
Similarly, given E0 (ρ, W ) = E0 (ρ, WBEC ), we have
E [g(ρ, Z)] = E [g(ρ, ZBEC )] = P (ZBEC = 0)(1 − 2−ρ ) + 2−ρ .

E0 (ρ, W ) = − log E [g(ρ, Z)]

Due to convexity, we also know the following inequality holds:

and
Fz,ρ (t) ≤ 1 +

Z1 + Z2
1
(1 + Z1 Z2 ) g(ρ,
)
E0 (ρ, W + ) = − log E
2
1 + Z1 Z2
1
Z1 − Z2
+ (1 − Z1 Z2 ) g(ρ,
)
2
1 − Z1 Z2

Therefore,
exp{−E0 (ρ, W − )}

(20)

−
= exp{−E0 (ρ, WBEC )}.

where both z, ρ ∈ [0, 1]. For ﬁxed values of ρ and z, the
function Fz,ρ (t) is convex with respect to the variable t.
The convexity analysis of this function is tedious. We refer
the readers to Appendix B of [9] for a proof. By Lemmas 1
and 4, we have

This concludes the proof for the W − transformation.
Now, we sketch the proof of the extremality property for
the W + transformation, given in Equation (14).
We deﬁne the function hρ (z1 , z2 ) as

exp{−E0 (ρ, W )} = E[g(ρ, Z)]

hρ (z1 , z2 ) =

exp{−E0 (ρ, W − )} = E[g(ρ, Z1 Z2 )]
where Z1 and Z2 are independent random variables. Moreover
by Lemma 1, we know ZBSC = zBSC and ZBEC ∈ {0, 1}.
Hence,

Let be the erasure probability of WBEC . Then, we have
P (ZBEC = 0) = , and
exp{−E0 (ρ, WBEC )} = P (ZBEC = 0)(1 − 2
Since, it is known that the channel W
probability 2 − 2 , we get

−

)+2

−ρ

1
z1 + z2
(1 + z1 z2 ) g(ρ,
)
2
1 + z1 z2
1
z1 − z2
+ (1 − z1 z2 ) g(ρ,
) (22)
2
1 − z1 z2

where ρ, z1 , z2 ∈ [0, 1]. Note that hρ (z1 , z2 ) is symmetric in
the variables z1 , and z2 .
Lemma 7: Let the function Hz,ρ (t) : [2−ρ , 1] →
−ρ
[2 , g(ρ, z)] be deﬁned as

−
exp{−E0 (ρ, WBSC )} = g(ρ, zBSC zBSC ).

−ρ

(21)

= EZ1 [EZ2 [Fz1 ,ρ (g(ρ, Z2 )) | Z1 = z1 ]]
g(ρ, Z1 ) − 1
≤ EZ1 1 +
(EZ2 [g(ρ, Z2 )] − 1)
2−ρ − 1
EZ1 [g(ρ, Z1 )] − 1
=1+
(EZ2 [g(ρ, Z2 )] − 1)
2−ρ − 1
2
2−ρ − 1 + [P (ZBEC = 0)(1 − 2−ρ ) + 2−ρ − 1]
=
2−ρ − 1
= 2P (ZBEC = 0) − P (ZBEC = 0)2 (1 − 2−ρ ) + 2−ρ

where g(ρ, z) is given by (16).
The proofs can be found in Lemmas 4.2, 4.3 in [9].
The proof of the extremality property for the W − transformation, given in Equation (13), relies on the convexity of the
function deﬁned in the next lemma.
Lemma 6: We deﬁne the function Fz,ρ (t) : [2−ρ , 1] →
[g(ρ, z), 1] as
Fz,ρ (t) = g(ρ, zg −1 (ρ, t))

g(ρ, z) − 1
(t − 1).
2−ρ − 1

Hz,ρ (t) = hρ (g −1 (ρ, t), z)

.

where both z, ρ ∈ [0, 1]. For ﬁxed values of ρ and z, the
function Hz,ρ (t) is concave with respect to the variable t.
We refer the readers to Appendix F of [9] for a proof.
The proof of the theorem for the W + transformation can be
completed following similar steps to the W − case. By Lemma
5, we have E [hρ (Z1 , Z2 )] = exp{−E0 (ρ, W + )}. We deﬁne
T1 = g(ρ, Z1 ), and T2 = g(ρ, Z2 ). Then, using the concavity
of the function Hz,ρ (t) with respect to t, and symmetry of Z1
and Z2 , we obtain

is a BEC with erasure

−
exp{−E0 (ρ, WBEC )}

= 2P (ZBEC = 0) − P (ZBEC = 0)2 (1 − 2−ρ ) + 2−ρ .
Moreover, given E0 (ρ, W ) = E0 (ρ, WBSC ), we also have
E [g(ρ, Z)] = g(ρ, zBSC ).

exp{−E0 (ρ, W + )} = E Hg−1 (ρ,T2 ),ρ (T1 )

Therefore, using Jensen’s inequality we obtain

+
≤ hρ (zBSC , zBSC ) = exp{−E0 (ρ, WBSC )}

exp{−E0 (ρ, W − )} = EZ1 [EZ2 [Fz1 ,ρ (g(ρ, Z2 )) | Z1 = z1 ]]
and

≥ EZ1 [FZ1 ,ρ (EZ2 [g(ρ, Z2 )])]
= EZ1 [FZ1 ,ρ (g(ρ, zBSC ))]

exp{−E0 (ρ, W + )} = E Hg−1 (ρ,T2 ),ρ (T1 )

(1)

+
≥ 2−ρ +P (ZBEC = 0)2 1 − 2−ρ = exp{−E0 (ρ, WBEC )}.

= EZ1 [FzBSC ,ρ (g(ρ, Z1 ))]

≥ FzBSC ,ρ (EZ1 [g(ρ, Z1 )])
−
= exp{−E0 (ρ, WBSC )}

5

