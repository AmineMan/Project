Title:          hoChanGrant20120516.pdf
Author:         hosw
Creator:        Adobe Acrobat 9.5.1
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   5/16/2012 11:39:43
ModDate:        Tue Jun 19 12:55:01 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      595 x 842 pts (A4)
File size:      321305 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569567045

Non-entropic Inequalities
from Information Constraints
Siu-Wai Ho, Terence Chan and Alex Grant
Institute for Telecommunications Research
University of South Australia
Email: {siuwai.ho, terence.chan, alex.grant}@unisa.edu.au

The results in this paper can also contribute to information
inequalities. An information inequality is called Shannon-type
if it can be proved by the nonnegativity of Shannon information measures. Otherwise, it is called non-Shannon-type [3]
[4]. So far, it was proved that all unconstrained information
inequalities involving three random variables are Shannontype inequalities [3, Ch. 15]. For four or more random variables, there are inﬁnite number of unconstrained information
inequalities [5] [6]. While it seems that a characterization
of information inequalities involving three random variables
is complete, [7] proved an interesting result about piecewise linear constrained information inequality involving three
random variables. This result suggested that characterization
of constrained information inequalities involving three random
variables has not been completely solved.
In this paper, our tighter bounds on H(R) and H(X) are
new constrained information inequalities and can be reduced to
the existing results. More importantly, it presents the relations
among random variables by a simpler form.
In addition to the above case, we will consider some special
cases in Section II. While in Case (a), {U, X, R} satisﬁes only
(1)–(3), the random variables are also required to satisfy

Abstract—This paper investigates a new method in proving
converses in secure communication problems. The method gives
a converse result in terms of the logarithm of support size instead
of entropy. The results are connected to constrained information
inequalities involving three random variables. A new constrained
non-Shannon type inequality is shown.

I. I NTRODUCTION
The capacity of a system is usually written in terms of
Shannon’s information measures (e.g., entropy and mutual
information). However, the system constraints may involve
parameters other than entropy, such as error probability. In the
typical approach for proving converses, we ﬁrst rephrase these
constraints in terms of entropy. For example, Fano’s inequality
is usually used to transform a decoding constraint in terms of
error probability into a constraint on entropy. Then information
inequalities can be applied to show the desired results. Here,
information inequalities refer to inequalities involving only
functions of entropy on both sides.
Recently, we have investigated a security system, in which
the capacity of a system cannot be expressed in terms of
entropy while the system requirements are in terms of only
entropy or mutual information [1]. Therefore, typical approach
cannot give our desired results. The security system involves
a set of three random variables {U, X, R} which satisﬁes

H(X|U R) = H(R|U X) = 0

(4)

I(U ; R) = 0,

(1)

in Case (b). In Case (c), {U, X, R} is further required to satisfy
(1)–(4) and

I(U ; X) = 0,

(2)

I(X; R) = 0.

These cases allow us to rederive the non-Shannon-type inequalities obtained in [7] and [3, Ch. 15]. Finally, Case (d) is
considered in Section III where {U, X, R} satisﬁes only (1)–
(3) and (5). We will show that the support sizes of the key R
and the ciphertext X greatly depend on whether the probability
distribution of secret message U has only rational probability
masses. Information diagrams of {U, X, R} in the four cases
are shown in Fig. 1. These diagrams pictorially illustrate the
constraints {U, X, R} required in each case.
Notations: Random variables will be denoted by a capital
letter such as A, U, V . For random variables A and B, the
probability of the event “A = a” and the conditional probability of the event “A = a” given that B = b will be speciﬁed
by PA (a) and PA|B (a|b) respectively. The support of a random
variable A is the set of outcomes a such that PA (a) > 0. We
will denote the support by the calligraphic letter A.

and
H(U |XR) = 0.

(5)

(3)

Here, U is a secret message, R is a secret key shared by
the transmitter and receiver, and X is the ciphertext. The
constraint (1) is due to the independence between the secret
message and the secret key. We assume that the ciphertext is
transmitted over a public and insecure channel. The constraint
(2) thus ensures that an eavesdropper who observes only X
can infer no information about the secret message U . Finally,
the constraint (3) guarantees that the receiver can reconstruct
the secret message with zero error. Intuitively, one may expect
that H(U ) ≤ H(R) and H(U ) ≤ H(X). In this paper, we
will show that both H(R) and H(X) are lower bounded by
log |U|, where U is the support of U . Our proving technique
has been recently applied in other problems [2].

1

;

8

;

8

D


F

D

G


D

D

D



D
G
5

5

5

G

F

E

Information diagrams of {U, X, R} in the four cases.

R

R
1

F

D


5

D

Fig. 1.


D

E

respectively. The area of the cell with coordinate (u, r) is equal
to PU (u)PR (r) = PU R (u, r) due to (1). Now, we partition the
cell (u, r) proportional to PX|U =u,R=r and assign values for
each partition. For example, 1 and 2 are assigned the cell (1, 1)
in Fig. 2(b) because PX|U R (x|1, 1) > 0 for x = 1 or 2. Since
the same ciphertext x can be generated from the same message
U with different key R, the same number x can appear more
than once in the same row but in different columns. However,
the same number cannot appear more than once in the same
column but in different rows due to (3). Otherwise, the decoder
side will be confused. Therefore, if we sum the widths of
the partitions with same label, say x, the summation is less
than the width of the unit square which is equal to 1 (i.e.,
r PR (r)). This can be seen in Fig. 2(c). Finally, the total
widths of the partitions with label x is the same for different
rows due to (2). Therefore, PX (x) ≤ |U|−1 .
Since a uniform distribution with support U always majorizes PX from (6), (8 ) can be easily veriﬁed by the theory
of majorization [8 ] even if X may be deﬁned on a countably
inﬁnite alphabet [9 ]. Finally, (7) and (9 ) follow from the
symmetric roles of X and R in (2)–(3).
R emark : The inequalities (6) and (7) can be restated as
information inequalities involving R´ nyi entropies [10 ]. L et
e
A be a discrete random variable whose support is A. Its αth
order R´ nyi entropy, denoted by Hα (A), is deﬁned as follows:
e

D




D

E

;

8

D



D
D

;

8

D

2 3

1

5 6 …

4

2 3

4

5 6 …

1 1 2…

1
U

U

2 3 4 …

2
…

…

M

M
()
a

()
b
R
1

1

2 3

1

5 6 …

4

1

1

U
2

1

1

…

H0 (A) = log |A|,

M

H1 (A) = −

1

PA (a) log PA (a),
a∈ A

H∞ (A) = − log sup PA (a),

()
c

a∈ A

Fig. 2.

PA (a)α
if 0 < α < 1 or 1 < α < ∞.
α−1
Note that H1 (A) is simply the Shannon entropy of A. It is
easy to see that (6) and (7) are equivalent to

P roof of Theorem 1.

Hα (A) =

II. T HE FIRST THREE CASES
We ﬁrst give an alternative proof of Theorem 1 in [1], i.e.,
Case (a) and then we will compare the results with Cases (b)
and (c). Instead of giving a formal proof, we will see more
insights from this alternative proof.

x∈ X

m ax PR (r) ≤ |U|

−1

r∈ R

.

a∈ A

H0 (U ) ≤ H∞ (X)
H0 (U ) ≤ H∞ (R).

(10 )
(11)

Furthermore, as Hα (A) is a monotonic decreasing function of
α for any given random variable A (i.e., Hα (A) ≥ Hβ (A) if
α ≤ β [11]), (8 ) and (9 ) follow accordingly.

Theorem 1 (C ase (a): H o et al. [1 ]): L et X , R and U be
the supports of random variables X, R, and U , respectively.
Suppose |U| is ﬁnite. If {U, X, R} satisﬁes (1)–(3), then
m ax PX (x) ≤ |U|−1 ,

− log

C orollary 2 (E x tension of Theorem 1 in [7 ]): If the set
of random variables {U, R, X} satisﬁes (1)–(3), then

(6)

aH(U ) ≤ aH(X) ,

(7)

(12)

where logarithms are with respect to base a.

Consequently,

P roof: If {U, R, X} satisﬁes (1)–(3), then by (8 ),
log |U| ≤ H(X),

(8 )

log |U| ≤ H(R).

(9 )

H(U ) ≤ log |U| ≤ H(X).

(13)

aH(U ) ≤ |U| ≤ aH(X) .

(14)

Therefore,

P roof:
Consider a unit square as shown in Fig. 2(a). We divide
it into |U| rows and |R| columns such that the width of
rows and columns are proportional to PU (u) and PR (r),

Since |U| is an integer, we have
aH(U ) ≤ aH(X) .

2

(15)

R
1

R emark : Corollary 2 extends [7, Theorem 1] in two aspects:
First, {U, R, X} in [7, Theorem 1] is required to satisfy (1)–
(4) in order to prove (12). Corollary 2 extends [7, Theorem 1]
by showing that only the constraints (1)–(3) are needed.
Furthermore, the inequality (12) (which is also the same as
that used in [7, Theorem 1]) in Corollary 2 can in fact be
strengthened to
a

Hα (U )

≤a

Hβ (X)

1

4

U 2

2

3

3

1

3

2

4

5

6

2

4

6

1

3

5

5

1

6

4

3

1

3

6

2

4

1

1

4

5

(a)

1

2

R
3

4

5

6

7

(16)

1

1

2

3

4

5

7

6

for any α, β ≥ 0. If α = β = 1, then (16) is equivalent to
(12). On the other hand, if α = 0 and β = ∞, then (16) is
equivalent to (6).
C orollary 3 (N on-ex istence): If U takes values from a
countably inﬁnite alphabet (i.e., |U| = ∞), then one cannot
construct any auxiliary random variables {X, R} such that (1)–
(3) hold simultaneously.
Now, we will study the second case where {U, R, X}
satisﬁes not only (1)–(3) but also (4).

U 2

4

1

2

3

6

5

7

3

3

4

1

2

7

6

5

(b)
20
Fig. 3.
In Example 1, X is randomly generated according to U 30 R 25
and
as shown in (a). For example, if (U, R) = (1, 1), X can only be 1 0.4 4. In
or 0.333333 0.266667
Example 2, X is determined by U and R as shown in (b). Here, the width of
75
rows and columns are proportional to PU (u) and PR (r), respectively. These
tables can help us to visualize the results in Theorems 1 and 4.
U

L emma 1 : Suppose U , X, and R satisfy (1)–(4). For any
r ∈ R and x ∈ X such that PRX (r, x) > 0,

P roof: If PRX (r, x) > 0, then (3) implies the existence
of a unique u ∈ U such that PXRU (x, r, u) > 0. B y (4),
PXRU (x, r, u) = PRU (r, u) = PXU (x, u).
Also, by (1) and (2)
PRU (r, u) = PR (r)PU (u)
PXU (x, u) = PX (x)PU (u).

Theorem 4 (C ase (b)): If U , X, and R satisfy (1)–(4),
then there exists a random variable Y such that
1) H(Y |X) = H(Y |R) = 0, and
2) for any Y = y, the conditional distributions of X and R
are uniform over their supports. In particular, for all y ∈
Y, there exists an integer θy ≥ |U| such that PX|Y (x|y)
−1
and PR|Y (r|y) are either equal to zero or θy .

5
6
4

3
1
2

6
4
5

P roof: B y Theorem 4 and symmetry, we have
PX (x) = PU (u) = PR (r)

(18 )

for all x ∈ X , u ∈ U and r ∈ R.
III. T HE FOURTH CASE

P roof: Due to the limited space, please refer to [12].
R emark : a) The random variable Y constructed in Theorem
4 is the common information between X and R [13]. b) The
second statement in Theorem 4 can be rewritten as

In this section, we will consider the fourth case where the
set of random variables {U, X, R} satisﬁes (1)–(3) and (5). In
[1], we have characterized the expected key consumption is
I(R; U X). Under the conditions (1)–(3), I(R; U X) ≥ H(U ).
Furthermore, the minimum expected key consumption, i.e.,
I(R; U X) = H(U ), can be achieved if (5) is satisﬁed. Therefore, the fourth case is when the security system satisfying
(1)–(3) achieves minimum key consumption as well.

(17)

Here, Hα (X|Y = y) is deﬁned as the αth order R´ nyi entropy
e
of a random variable A is deﬁned on the set X such that
PX|Y (x|y),

2
3
1

C orollary 5 (C ase (c): C hapter 1 5 in [3 ]): If the random variables U, X, R satisfy (1)–(5), then PU , PX and PR
are all uniform distributions with the same support size.

Hence, the lemma follows.

PA (x)

4
5
6

Corollary 5 shows that Theorem 4 subsumes the results in [3,
R
1 1 2 2 3 3
p. 364].
E x ample 1 (C ase (a)): Consider {U, R, X} such that:
1) probability distributions of U and R are respectively
1
3
{ 1 , 1 , 1 } and 1 11 , 11 , 11 , 113, 11 ,5 11 }, 2) 4 6 4 R are inde{ 3 1 2 11 1 6 1 2 3 1 3
U and
2 4 4
2
4
U 2
5
pendent, and 3) X is randomly generated 1 4 5 to U and
according
3
2
1
3
6
4
1
/
A
2
2
/ /
R as deﬁned in Fig.1 3(a). Then {U, X, R}/ satisﬁes (1)–(3).
3
2
4 5 6
E x ampleR2 (C ase 1
(b)): Suppose the probability distribu3
2
2
2
1
1
tion of R is changed to { 11 , 11 , 11 , 11 , 11 , 11 }, and X =
(a)
g(U, R) where the function g is deﬁned in Fig. 3(b). Then
{U, X, R} satisﬁes (1)–(4).

PR (r) = PX (x).

H0 (X|Y = y) = H∞ (X|Y = y)
= H0 (R|Y = y) = H∞ (R|Y = y).

1
2
3

Theorem 6 (C ase (d)): L et X , R and U be the supports
of random variables X, R, and U , respectively. If the random
variables satisfy (1)–(3) and (5), then

∀x ∈ X .

In the following example, random variables satisfying the
constraints in Theorems 1 and 4 are compared. After that,

m ax PX (x) ≤ i PU (u)
nf
x∈ X

3

u∈ U

(19 )

and {U, X, R} satisﬁes (1)–(3) and (5).

and
m ax PR (r) ≤ i PU (u).
nf
r∈ R

u∈ U

P roof: Assume that there exists an integer θ such that
θ · PU (u) is an integer for all u. We can construct a random
variable U as described in L emma 2. Assume without loss of
generality that U takes values from the set {0, 1, . . . , θ − 1}.
L et R be uniformly distributed over {0, 1, . . . , θ −1} and X =
U + R m od θ. Then

(20 )

P roof: Consider any u ∈ U and x ∈ X . As U and X are
supports, PU (u) > 0 and PX (x) > 0. From (2), we have
PU X (u, x) = PU (u)PX (x) > 0.

(21)

I(U ; R) = I(U ; X) = I(X; R) = H(U |X, R) = 0. (29 )

Consequently, there exists r ∈ R such that

Furthermore, |X | = |R| = θ and H(X) = H(R) = log θ.
Since U is a function of U , {U, X, R} still satisﬁes (1)–(3)
and (5) so that the theorem follows.

PU XR (u, x, r) > 0.
Notice that
PU XR (u, x, r)

=

PXR (x, r)

(22)

=

PX (x)PR (r),

(23)

Theorem 8 shows that for any PU having only rational
probability masses, it is always possible to construct X and R
which are deﬁned on ﬁnite alphabets and satisfy (1)–(3) and
(5). In the following theorem, we will prove that the rationality
condition on PU (u) is an important condition. In other words,
it is impractical to construct a security system satisfying (1)–
(3) with minimum key consumption when PU has irrational
probability masses.

where (22) is due to (3) and (25) is due to (5). On the other
hand, we have
PU XR (u, x, r)

≤

PU R (u, r)

(24)

=

PU (u)PR (r),

(25)

where (25) is due to (1). Finally, as PR (r) > 0, we have

Theorem 9 : Suppose U , X, and R satisfy (1)–(3) and (5).
If PU (u) is irrational for any u ∈ U, then

PX (x) ≤ PU (u)
and the result follows.

|X | = ∞,

(30 )

R emark s: a) Comparing (19 ) with (6) in Theorem 1, the
requirement on PX is more difﬁcult to achieve if we want to
achieve minimum key consumption. b) Instead of expressing
everything in terms of entropy, the bounds in Theorems 1 and
6 give simpler descriptions of the consequences due to the
constraints in (1)–(3) or (5).

|R| = ∞.

(31)

Or equivalently, both H0 (X) and H0 (R) are unbounded.
P roof: Suppose there exists uo ∈ U such that PU (uo ) is
irrational. Deﬁne a new random variable U ∗ such that
U∗ =

C orollary 7 : Suppose {Xn , Rn , Un } satisfy (1)–(3), (5)
m
and H(Un ) > 0 for all n. If li n→ ∞ H(Un ) = 0, then
li H(Xn ) = li H(Rn ) = ∞.
m
m

n→ ∞

n→ ∞

0
1

if U = uo
otherwise.

Then PU ∗ (0) and PU ∗ (1) are irrational. As U ∗ is a function
of U , by (1)–(3) and (5),

(26)

I(U ∗ ; R) = I(U ∗ ; X) = I(X; R) = H(U ∗ |XR) = 0. (32)

Roughly speaking, Corollary 7 illustrates that if U has a
small positive entropy, then the entropies of X and R must
be large in order to satisfy the constraints (1)–(3) and (5).

Therefore, it sufﬁces to consider U deﬁned on the set {0, 1}
to prove the theorem.
Suppose to the contrary ﬁrst that |X | and |R| are both ﬁnite.
We can assume without loss of generality that

In the following, we show that the supports of X and R
depend on whether PU (u) is rational for all u ∈ U. Note that
PU (u) is rational for all u if and only if there exists an integer
θ such that θ · PU (u) is an integer for all u.

X = {1, . . . , n}
R = {1, . . . , m}.

L emma 2 : There exists an integer θ such that θ · PU (u)
is an integer for all u if and only if there exists a random
variable U such that
1) U is a function of U (i.e., H(U |U ) = 0), and
2) U is uniformly distributed over its support with size θ.

(33)
(34)

L et
xi = PX (i),

i = 1, . . . , n

(35)

rj = PR (j),

j = 1, . . . , m.

(36)

|X | = |R| = θ

(27)

L et x be a n-row vector such that its ith entry is xi . Similarly,
we can deﬁne the column vector r.
As X and R are independent and H(U |XR) = 0, there
exists a function f such that U = f (X, R). Hence, we can
deﬁne a n × m matrix G such that for any i = 1, . . . , n and
j = 1, . . . , m, the (i, j)th entry of G is given by

H(X) = H(R) = log θ,

(28 )

Gi,j = f (i, j).

Theorem 8 (E x istence): If there exists an integer θ such
that θ · PU (u) is an integer for all u, then X and R can be
constructed such that

4

We call G the decoding m atrix induced by X and R.
L et Gi be the ith row of G. Then
m
1) Gi r = j=1 Gi,j rj = PU (1) for all i = 1, . . . , n
n
m
2)
i=1 xi =
j=1 rj = 1;
3) xi and rj are non-negative for all i, j;
4) for all j = 1, . . . , m,

satisfying (1)–(3) and (5), then there exists another set of
auxiliary random variables {X ∗ , R∗ } such that (32) is satisﬁed
and the rows and columns of the decoding matrix induced by
X ∗ and R∗ are all linearly independent. Hence, the decoding
matrix G induced by X ∗ and R∗ must be a square matrix (and
thus m = n). Consequently,
n

n

xi Gi,j = PU (1).

xi Gi,j = PU (1),

(37)

I(U ; R) = I(U ; X) = I(X; R) = H(U |XR) = 0

n

zi Gi,j = 1,

(38 )

IV . C ONCL USION
B y using an alternative approach to prove converse, bounds
on entropy in terms of support size or Renyi entropy have been
given, leading to new constrained information inequalities
involving three random variables. Our inequalities can present
the relations among random variables by a simple form. These
inequalities can be applied to coding theorems in secure
transmission problems (as studied in [1]). In particular, we
have seen that a message cannot be transmitted securely if it
has inﬁnite support. Minimum key consumption is impractical
if the source distribution has irrational numbers.

and that |X ∗ | < |X |, where X ∗ is the support of X ∗ .
To prove the claim, let A and B be two disjoint subsets of
{1, . . . , n} and αi , i ∈ A ∪ B be positive numbers such that
α k Gk .

(40 )

k∈ B

Now, we claim that
αi =

αk .

R EFERENCES

k∈ B

[1] S.-W. Ho, T. Chan and C. Uduwerelle, “Tradeoff between K ey Rate and
Number of Channel Use,” in P roc. 2 0 1 1 IE E E Int. Sym posium Inform .
T heory (ISIT 2 0 1 1 ), pp. 1613–1617, Aug. 20 11.
[2] S.-W. Ho, L . L ai and A. Grant, “Multiuser P roblems in B iometric Security
Systems,” preprint.
[3] R. W. Y eung, Inform ation T heory and Netw ork C oding , Springer, 20 0 8 .
[4] T. Chan, “Recent P rogresses in Characterising Information Inequalities,”
E ntropy J ., vol. 13, pp. 379 –40 1, 20 11.
[5] Matus, F. Inﬁnitely Many Information Inequalities. In P roceedings ISIT
20 0 7, Nice, France, J une 20 0 7.
[6] T. Chan and A. Grant, “Non-linear Information Inequalities,” E ntropy J .,
vol. 10 , pp. 765–775, 20 0 8 .
[7] F. Mat´ s, “P iecewise L inear Conditional Information Inequality.” IE E E
uˇ
T rans. Inform . T heory, vol. 52, pp. 236–238 , J an. 20 0 6.
[8 ] A. W. Marshall and I. Olkin, Ineq ualities: T heory of M ajoriz ation and
Its A pplications, Academic P ress, New Y ork, 19 79 .
[9 ] S.-W. Ho and S. V erd´ , “On the Interplay B etween Conditional Entropy
u
and Error P robability.” IE E E T rans. Inform . T heory, vol. 56, pp. 59 30 –
59 42, Dec. 20 10 .
[10 ] A. R´ nyi, “On measures of information and entropy.” P roc. of the 4 th
e
B erk eley Sym posium on M aths., Stats. and P rob ., pp. 547–561, 19 61.
[11] T. S. Han and S. V erd´ , “Generalizing the Fano Inequality,” IE E E T rans.
u
Inform . T heory, vol. 40 , pp. 1247-1250 , J ul. 19 9 4.
[12] S.-W. Ho, T. Chan, and A. Grant, “Inequalities from Information
Constraints,” preprint. Available online at: http://www.itr.unisa.edu.au/
∼hosw/Ine.pdf.
[13] P . G´cs and J . K ¨rner, “Common information is far less than mutual
a
o
information,” P rob lem s of C ontrol and Inform ation T heory, vol. 2,
pp. 149 –162, 19 73.

Multiplying both sides of (40 ) by r,
α i Gi r =
i∈ A

α k Gk r

(41)

αk PU (1)

(42)

αk .

(43)

k∈ B

αi PU (1) =
i∈ A

k∈ B

αi =
i∈ A

(45)

Clearly, zi = xi /PU (1). As all the entries in G are either
0 or 1, all the zi are rational numbers. Therefore, 1 =
n
n
i=1 xi = PU (1)
i=1 zi . Hence, PU (1) must be rational
and a contradiction occurs. We have proved that X and R
cannot be both ﬁnite. The case when only X or R is ﬁnite
can be proved similarly.

I(U ; R) = I(U ; X ∗ ) = I(X ∗ ; R) = H(U |X ∗ R) = 0 (39 )

α i Gi =

j = 1, . . . , n.

i=1

where U = f (X, R) and the probability distributions of X
and R are speciﬁed by the vectors x and r respectively.
In the following, we will prove that if the rows of G are not
independent, then we can construct another random variable
X ∗ such that

i∈ A

(44)

There exists a unique solution (z1 , . . . , zn ) such that

Here, 1) is due to that I(U ; X) = 0, 2) and 3) are due to that
PX and PR are probability distributions, and 4) is due to that
I(U ; R) = 0.
In fact, for any x, r and binary matrix G satisfying the above
four conditions, one can construct random variables {U, R, X}
such that

i∈ A

j = 1, . . . , n.

i=1

i=1

k∈ B

L et
m i i∈ A ∪ B xi /αi . Assume without loss of generality
n
that n ∈ A and that = xn /αn . Deﬁne

xi − αi if i ∈ A

x∗ = xi + αi if i ∈ B
i


xi
otherwise.

Note that x∗ = 0. Suppose that X’s probability distribution
n
is changed such that PX (i) = x∗ . Then it can be checked
i
easily that the set of random variables {U, X, R} still satisﬁes
the constraint (1)–(3) and (5). Furthermore, the size of the
support of PX is |X | ≤ n − 1.
Repeating this procedure, we can prove that for any random
variable U , if there exists auxiliary random variables {X, R}

5

