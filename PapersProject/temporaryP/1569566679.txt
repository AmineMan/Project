Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May 16 17:08:36 2012
ModDate:        Tue Jun 19 12:55:26 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      930832 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566679

Repairable Fountain Codes
Megasthenis Asteris and Alexandros G. Dimakis
Department of Electrical Engineering
University of Southern California
Los Angeles, CA 90089
Email: {asteris, dimakis}@usc.edu
the encoded sequence. This enables the reading of symbols
without decoding and is a practical requirement for most
storage applications. Raptor codes, a class of Fountain codes,
can be transformed into a systematic form [3] by introducing
a change of variables. Unfortunately, due to this two layer
encoding, the parity symbols are no longer sparse in the input
symbols.
Our Contribution: We introduce a new family of Fountain
codes that are systematic and also have parity symbols with
logarithmic sparsity. We show that this is impossible if we
require the code to be MDS, but is possible if we allow a nearMDS property similar to the probabilistic guarantees provided
by LT and Raptor codes.
More concretely, for any > 0 we construct codes that
guarantee that a random subset of (1 + )k symbols sufﬁces
to recover the original symbols w.h.p. Our codes produce an
unbounded number of output symbols, creating each parity
independently by linearly combining a logarithmic number of
input symbols.
This structure provides also logarithmic locality: each symbol in our codes is repairable by accessing only O(log k) other
coded symbols. The disadvantage of our codes is less efﬁcient
decoding complexity, since the peeling decoder cannot be used
for our construction.
Technically, we rely on a novel random matrix result:
we show that systematic matrices with independent parity
columns and logarithmic density have full rank submatrices of
near optimal size. Our analysis builds on the connections of
matrix determinants to ﬂows on random bipartite graphs, using
techniques from [9], [10]. Our key technical result is showing
that a new family of sparse random graphs have matchings
w.h.p.

Abstract—We introduce a new family of Fountain codes that
are systematic and also have sparse parities. Although this is
impossible if we require the code to be MDS, we show it can be
achieved if we relax our requirement into a near-MDS property.
More concretely, for any we construct codes that guarantee
that a random subset of (1 + )k symbols sufﬁces to recover
the original symbols with high probability. Our codes produce
an unbounded number of output symbols, creating each parity
independently by linearly combining a logarithmic number of
input symbols.
This structure has the additional beneﬁt of logarithmic locality:
a single symbol loss can be repaired by accessing only O(log k)
other coded symbols. This is a desired property for distributed
storage systems where symbols are spread over a network of
storage nodes. Our mathematical contribution involves analyzing
the rank of sparse random matrices over ﬁnite ﬁelds. We rely on
establishing that a new family of sparse random bipartite graphs
have large matchings with high probability.

I. I NTRODUCTION
Fountain codes [1], [2], [3] form a new family of linear
erasure codes with several attractive properties. For a given
set of k input symbols, a Fountain code produces a potentially
limitless stream of output symbols, each created independently
of others as a random combination of input symbols according
to a given distribution. Ideally, given a randomly selected
subset of (1 + )k encoded symbols, a decoder should be able
to efﬁciently recover the original k input symbols with high
probability (w.h.p.) for some small overhead . Independent
and random construction of encoded symbols allows decentralized encoding and dynamic adjusting in the number of coded
symbols. Further, Fountain codes can be used with efﬁcient
encoding and decoding algorithms.
Current cloud storage systems are starting to use erasure
coding techniques, typically Reed-Solomon codes [4]. In this
paper we investigate and design Fountain codes for such
distributed storage applications.
One important property of distributed storage codes is
efﬁcient repair [5]: when a single encoded symbol is lost it
should be possible to reconstruct it without communicating
too much information from other coded symbols. A related
property is that of locality of each symbol: the number of
other code symbols that need to be accessed to reconstruct
that symbol [4], [6], [7], [8].
In addition, it is highly desired that distributed storage codes
are systematic, i.e., the original information symbols appear in

II. P ROBLEM D ESCRIPTION
Given k input symbols, elements of a ﬁnite ﬁeld Fq , we
want to encode them into n symbols using a linear code.
Linear codes are described by a k × n generator matrix G
over Fq , which when multiplied by an input vector u ∈ F1×k
q
·
produces a codeword v = uG ∈ F1×n . We want G to have
q
the following properties:
• Systematic form, i.e., a subset of the columns of G
forms the identity matrix, I, which implies that the input
symbols are reproduced in the encoded sequence.
• Rateless property, i.e., each column is created independently. The number n of columns does not have to be

This research was supported in part by NSF Career Grant CCF-1055099
and research gifts by Intel and Microsoft Research.

1

speciﬁed for the encoder a priori.
MDS property, i.e., any k columns of G have rank k,
implying that any subset of k encoded symbols sufﬁces
to retrieve the input.
• Good locality. G has locality l if each column can
be written as a linear combination of at most l other
columns. If the code is systematic, then sparse parities
sufﬁce to obtain good locality [8].
Any sufﬁciently large subset of encoded symbols should
allow recovery of the original data. In the case of MDS codes,
an information theoretically minimum subset of k encoded
symbols sufﬁces to decode. It can be easily shown, however,
that the generator matrix of a systematic MDS code affords
no zero coefﬁcient in the parity columns. Consider a parity
column with a zero coefﬁcient in the i-th position: this parity
column along with any k−1 systematic columns excluding the
one corresponding to the i-th input symbol, form a singular
matrix. Therefore, if parities are deliberately sparse in the input
symbols, seeking to improve the code’s locality, the “any k”
property has to be relaxed.

It is useful to describe our randomized construction through
a correspondence to a bipartite graph G(U, V, E), depicted in
Fig. 1. The set of nodes U on the left side corresponds to
the input symbols and the set V on the right corresponds to
the encoded symbols. An edge (i, j) ∈ E if the input symbol
i ∈ U is one of the symbols combined into the encoded symbol
j ∈ V . Evidently, each of the k ﬁrst encoded symbols is
connected to a single, distinct input symbol (systematic part).
The remaining nodes in V correspond to parity symbols. Each
parity node j ∈ V randomly and independently selects (with
repetition) a subset N (j) of the input symbols as follows: an
input symbol is selected uniformly and independently from U
and added in N (j) and this procedure is repeated d(k) times.
Therefore, |N (j)| will be smaller than d(k) if the same data
node is selected twice. In fact, the size of the set N (j) is
exactly the number of coupons a coupon collector would have
after purchasing d(k) coupons from a set of k coupons. It is not
hard to see that when d(k)
k, |N (j)| will be approximately
equal to d(k) w.h.p. Each parity symbol is a random linear
combination of the input symbols it is connected to:

•

∀i ∈ N (j),

vj = fij ui

III. P RIOR W ORK

(1)

where the coefﬁcients fij are selected uniformly and independently over Fq .

…

In LT codes, the ﬁrst practical realizations of Fountain
codes invented by Luby [2], the average degree of the output
symbols, i.e., the number of input symbols combined into an
output symbol, is O (log k). Note, however, that sparsity in
this case does not imply good locality, since LT codes lack
systematic form.
Shokrollahi in [3] introduced Raptor codes, a different class
of Fountain codes. Building on LT, Raptor codes decreased the
per symbol encoding and decoding cost (which corresponds
to average degree of encoded symbols) to a constant. A
systematic ﬂavour of Raptor codes is provided in [3], but the
parity symbols are no longer sparse in the input symbols.
Gummadi in his thesis [11] also considers the use of
Fountain codes for storage applications and suggests sparse
systematic variants of LT and Raptor codes. His constructions
have the disadvantage of requiring an overhead that cannot be
made arbitrarily small but rather is bounded by 1 + δ, (δ > 0)
and 0.25 respectively. On the other hand, the advantage of this
construction is that efﬁcient decoding is still possible.

…

…

k

i

d(k)
Fig. 1.

￿

…

…

…

j

V

n−k

·fij

…

U k

Bipartite graph corresponding to our randomized code construction.

In matrix notation, the construction can be summarized into
·

IV. R EPAIRABLE F OUNTAIN C ODES

v = uG,

We introduce a new family of Fountain codes that are
systematic and also have sparse parities. Each parity symbol
is a random linear combination of up to d(k) randomly
chosen input symbols. Due to their randomized nature, our
codes provide a probabilistic guarantee on successful decoding
of k = (1 + )k randomly selected encoded symbols, for
arbitrarily small > 0 (near-MDS). Requiring decoding of a
random set of k encoded symbols to be successful w.h.p. (that
is with vanishingly small probability of failure as k grows), we
show that a sparsity level of O (log k) is achievable. Our main
result, which is asymptotic in k, is established in Theorem 1,
at the end of this section.

where v is a 1 × n vector of encoded symbols, u is a 1 × k
input symbol vector and G is a k × n matrix of the form G =
[ I | P ], i.e., the k ﬁrst columns correspond to the identity
matrix (systematic part). For j ∈ {k + 1, . . . , n}, the j-th
column of G (i.e., each column of the parity matrix, P) has
|N (j)| ≤ d(k) nonzero entries. To retrieve the k input symbols
based on k encoded symbols, we need k out of the latter to be
linearly independent combinations of the former. Therefore,
the key property required for successful decoding is that a
randomly selected matrix GS , consisting of k columns of G
(including any combination of systematic and parity parts) has
full rank w.h.p.

2

(2)

graph GSk :

So far, we have seen that our construction is systematic and
has the rateless property: parity symbols can be created randomly and independently and hence any number of symbols
can be produced dynamically. Its locality is directly related
to d(k), which is clearly a measure of the sparsity of G: the
repair of any erased parity symbol involves as few as d(k)
systematic symbols, while similarly, the repair of a systematic
symbol involves a parity symbol covering the erased symbol
and up to d(k) − 1 other systematic symbols contributing to
the same parity.
Our main contribution is identifying how small d(k) can be
to ensure that a randomly selected submatrix GS is full rank
w.h.p.

GSk (i, j) =

Lemma 1. The determinant of GSk is nonzero if and only if
there exists a perfect matching in GSk , i.e.
det(GSk ) = 0

⇔

∃ perfect matching M in GSk .

(5)

Proof: We use the following expression for the determinant:
n

det(GSk ) =

GSk (i, π(i)),

sgn(π)

(6)

i=1

π∈Sn

where Sn is the set of all permutations on {1, . . . , n} and
sgn(π) is the sign of permutation π. There is a one to
one correspondence between a permutation π ∈ Sn and a
candidate P.M. (u1 , vπ(1) ), . . . , (un , vπ(n) ) in GSk . Note
that if the candidate P.M. does not exist in GSk , i.e. some
edge (ui , vπ(i) ) ∈ ESk then the term corresponding to π in
/
the summation is 0. Therefore, we have:

Theorem 2. (Converse) If each parity column of G is generated independently, then d(k) = Ω(log k) is necessary for a
random k × k submatrix GS of G to be full rank w.h.p.

n

det(GSk ) =

ai,π(i) ,

sgn(π)
π∈P

From the two theorems, it follows that our codes achieve
optimal locality with a logarithmic degree for every parity
symbol. Original data is reconstructed in O(k 3 ) using Maximum Likelihood (ML) decoding, which corresponds to solving
a linear system of k equations in GF(q). Note, however,
that the Wiedemann algorithm [12] can reduce complexity to
O k 2 log k on average, exploiting the sparsity of the linear
equations, with negligible extra memory requirement. Finally,
note that in order to achieve vanishingly small probability of
failure as k grows, the size of the ﬁeld must grow accordingly.

(7)

i=1

where P is the set of perfect matchings in GSk . This is clearly
zero if P = ∅, i.e., if GSk has no P.M. If GSk has a P.M.,
there exists a π ∈ P and the term corresponding to π is
n
i=1 ai,π(i) = 0. Additionally, there is no other term in the
summation containing the exact same set of variables and this
term cannot be cancelled out. In this case det(GSk ) = 0,
which concludes the proof of the lemma.
However, GSk is not an actual Edmond’s matrix; its entries
are (randomly selected) elements of a ﬁnite ﬁeld Fq , not
indeterminates. There are two substantially different cases in
which det(GSk ) = 0:
• The determinant polynomial is identically zero which
occurs if and only if GS has no P.M., or
• it is not identically zero (i.e., GSk has a P.M.), but
the selected coefﬁcients correspond to a root of the
polynomial.
In other words, in contrast to the use of indeterminates, an
unfortunate selection of the random coefﬁcients of GSk can
lead to zero determinant even when GSk has a P.M.
Taking into account that a P.M. in some subgraph GSk of
GS is a P.M. M in GS and vice versa, the probability we are
interested in can be written as

V. A NALYSIS AND P ROOFS
Proof of Theorem 1. The main body of this section is
dedicated to the proof of Theorem 1, stating that when G is
constructed as described in section IV, a randomly selected
k × k submatrix GS is full rank w.h.p. More formally,
k
+ o(1) .
q

(4)

where ui ∈ U, vj ∈ VSk and ai,j ’s are indeterminates. Then:

Theorem 1. Let G = [ I | P ] be a random matrix with
independent columns constructed as described. Then, d(k) =
c log k is sufﬁcient for a random k × k submatrix GS of G to
be full rank, i.e. contain an invertible k × k submatrix GSk ,
with probability 1 − k − o(1), for some positive constant c.
q

Pr (∃ GSk : det(GSk ) = 0) = 1 −

ai,j , if (ui , vj ) ∈ ESk
,
0,
if (ui , vj ) ∈ ESk
/

(3)

In the following, we exploit a connection between determinants and perfect matchings (P.M.’s) in bipartite graphs.
In section IV, we showed the correspondence of the randomly constructed matrix G to an unbalanced bipartite graph
G = (U, V, E). The submatrix GS corresponds to a subgraph
GS = (US = U, VS , ES ), depicted in Fig. 2, containing all
k nodes of U on the left side, k out of the n nodes of V
on the right side, and the subset ES of the edges incident
only to nodes in these sets. Similarly, any k × k submatrix
GSk of GS corresponds to a smaller, balanced bipartite graph,
GSk = (U, VSk , ESk ), with k nodes on each side. GSk can be
regarded as the Edmond’s matrix of the corresponding bipartite

Pr(∃GSk : det(GSk ) = 0)
=1 − Pr( GSk : det(GSk ) = 0)

=1 − Pr( GSk : det(GSk ) = 0|∃M ) ·Pr(∃M )
α



+ Pr( GSk : det(GSk ) = 0| M ) ·Pr( M ) .
β

3

(8)

exist. (Recall that, since |U | < |VS |, a P.M. is a matching
that saturates all nodes in U ). In particular, we require the
probability to vanish asymptotically with a rate 1/poly(k) and
show that the value d(k) = O(log k) used in constructing the
bipartite graph sufﬁces to achieve that.
Let Vs denote the subset of VS corresponding to systematic
encoded symbols (0 ≤ |Vs | ≤ k). We can assume that all nodes
in Vs are used in the P.M.: each saturates a distinct node in
U , leaving us with more options for saturating the remaining
nodes in U . Since k = (1 + )k > k, VS will deﬁnitely have
a nonempty subset corresponding to parity (nonsystematic)
symbols, denoted by Vns = VS \Vs . Let Us denote the subset
of U that is saturated by Vs and Uns = U \Us . Since Us and
Vs are matched, a P.M. between U and VS exists if and only
if a P.M. exists between Uns and Vns .
The probability that a P.M. does not exist equals the
probability that there exists a contracting set of nodes in Uns .
For simplicity, let s = |Vs | and consequently, |Vns | = k − s.
Let Ei be the event that there exists a set of i nodes in Uns
that contracts, i.e., has at most i−1 neighbours in Vns . This is
equivalent to at least k − s − (i − 1) nodes in Vns being only
adjacent to nodes in Uns other than the i nodes of interest.
Then,

Nonexistence of a P.M. in GS , implies that we cannot ﬁnd
k nodes in VS that can be perfectly matched with the k
nodes of U . In that case, no submatrix GSk can have nonzero
determinant, i.e. β = 1. On the other hand, existence of a
P.M. M in GS , implies the existence of a submatrix GSk that
will most probably have a nonzero determinant, depending on
the randomly selected coefﬁcients of GSk . The determinant of
GSk corresponding to M , is a polynomial of degree exactly
k and the probability that it equals zero can be bounded
using the Schwartz-Zippel Theorem [13], by k , where q is
q
the number of elements in the ﬁnite ﬁeld from which the
coefﬁcients of GSk are drawn. A step further, the probability
Pr( GSk : det(GSk ) = 0 | ∃M ), i.e., that no invertible
matrix GSk exists despite the existence of a P.M., can be upper
bounded by the same quantity. Hence α ≤ k . Continuing from
q
(8), we have:
k
Pr (∃M ) + Pr ( M )
q
k
k
= 1 − (1 − Pr ( M )) − Pr ( M ) = 1 − − o(1),
(9)
q
q

Pr (∃GSk : det (GSk ) = 0) ≥ 1 −

where the fact that Pr ( M ) = o(1) is based on Lemma 2,
provided in section V-A. This completes the proof of theorem
1.
Proof of Theorem 2 An input symbol is covered by an
encoded symbol, if it participates with a nonzero coefﬁcient
in the formation of the latter. In order to be able to retrieve
the original k symbols from a subset of k encoded symbols,
it is imperative that the subset covers all input symbols. It
is a standard result in balls and bins analysis that covering
k bins w.h.p. requires throwing Ω (k log k) balls. Here, each
of the k encoded symbol “throws d(k) balls”. Therefore, we
need k · d(k) = (1 + )k · d(k) = Ω (k log k) from which the
theorem becomes obvious.

|Uns |

|Uns |

Pr ( P.M. in GS ) = Pr(
i=1
|Uns |

≤

i=1
k−s

=
i=1
k−s

=

A. Existence of Perfect Matching in the random subgraph

|Uns |
i

i=1

Ei ) ≤

Pr(Ei )
i=1

|Vns |
|Vns | − (i − 1)

k−s
i

k −s
k −s−i+1

k−s
k−s−i

d(k)(|Vns |−(i−1))

k−i
k
k−i
k

k −s
k −s−i+1

d(k) k −s−i+1

d(k) k −s−i+1

k−i
k

.

A

(10)

Vs

Fig. 2.

…

…

…

…

￿

k−s
Uns

n
k

Using the fact that

k = (1 + ￿)k

s

…

…

Us

k
≤ 2[nH ( n )+log2 (n)] , where H(·) is

the binary entropy function, we have:
k −s
k −s−i+1

≥ ￿k

Vns

since there are at most
•
k systematic nodes that
can be used in the col•
lection of (1+￿)k nodes.

(2)

k−s
k−s−i

≤ 2Bk,s (i) where

(1)

Bk,s (i) = (k − s)H

(2)
Bk,s (i)

= (k − s)H

k−s−i
k−s

(1)

≤ 2Bk,s (i) and

+ log2 (k − s)

k −s−i+1
k −s

+ log2 (k − s)

The entire sum can be bounded by k − s times the largest
among these bounds. In other words,

Bipartite Graph GS = (U, VS , ES ) corresponding to GS .

(1)

A ≤ (k − s) max 2

Lemma 2. The bipartite graph GS = (U, VS , ES ) corresponding to the submatrix GS of G has a perfect matching with
probability 1 − o(1) as k → ∞.

i

(3)

(2)

(3)

Bk,s (i)+Bk,s (i)+Bk,s (i)

(11)

where Bk,s (i) = d(k) k − s − i + 1 log2 k−i . We rek
quire Pr ( P.M. in GS ) to vanish asymptotically faster than
1
kρ , ρ > 0, for each value of s ∈ {0, . . . , k}. It sufﬁces to

Proof: We establish an upper bound on the probability
that a perfect matching (P.M.) between U and VS does not

4

(iii) For the third and ﬁnal part, since for i = 1 the entropy
in this term is zero, we need only consider i ≥ 2:

require
3
j=1

(k − s)2

(j)

Bk,s (i)

≤ k −ρ ,

s ∈ {0, . . . , k}
, (12)
i ∈ {1, . . . , k − s}

∀

(1 + )kH

and equivalently,

k log2

3

log2 (k − s) +

(j)

j=1

Bk,s (i) ≤ −ρ log2 (k)

≤

which by expanding and rearranging terms leads to



d(k) ≥

ρ log2 (k) + 2 log2 (k − s) + log2 k − s
+(k − s)H

k−s−i
k−s

+ (k − s)H

− (k − s − i + 1) log2

k −s−i+1
k −s

k−i
k

≤



. (13)

Let N and D denote the numerator and denumerator of the
right hand side of inequality (13). In the following, we show
that N = O (ln(k)) and hence choosing d(k) = c ln(k)
D
for some constant c sufﬁces to satisfy (13) and achieve the
vanishing probability.
• For the numerator, N , we have the following upper
bound:

•

,

≥0
(γ)

≥ k

where (γ) is due to ln(1 + x) >
i
(here, x = k−i > 0).

ln(2)

x
x+1

i,

(14)

for x > −1, x = 0

We examine the ratio N in parts:
D
(i) For the ﬁrst part:
(3 + ρ) log2 (k) + log2 (1 + )
k log2
(14)

≤

ln(2)

k
k−i

(4 + ρ) log2 (k),

(15)

where the last inequality holds for k > (1 + ).
(ii) For the second part, expanding the entropy we obtain:
k−i
k
k
k log2 k−i

kH

≤

1

+

i log2
i
kk

=
k
i

i
i
log2 k
k−i
− k
k
k
log2 k−i

≤

2

log2 (k) ,

2 ln(2)(1 + )

i − 1 log2 i−1
i
(1 + )k ln(2)(1+ )k
log2 (k) ,

(17)

[1] J. W. Byers, M. Luby, M. Mitzenmacher, and A. Rege, “A digital
fountain approach to reliable distribution of bulk data,” SIGCOMM
Comput. Commun. Rev., vol. 28, pp. 56–67, October 1998. [Online].
Available: http://doi.acm.org/10.1145/285243.285258
[2] M. Luby, “Lt codes,” in Proceedings of the 43rd Symposium on
Foundations of Computer Science, ser. FOCS ’02. Washington, DC,
USA: IEEE Computer Society, 2002, pp. 271–. [Online]. Available:
http://dl.acm.org/citation.cfm?id=645413.652135
[3] A. Shokrollahi, “Raptor codes,” IEEE/ACM Trans. Netw.,
vol. 14, pp. 2551–2567, June 2006. [Online]. Available:
http://dx.doi.org/10.1109/TIT.2006.874390
[4] O. Khan, R. Burns, J. Plank, W. Pierce, and C. Huang, “Rethinking
erasure codes for cloud ﬁle systems: Minimizing i/o for recovery and
degraded reads,” in Usenix Conference on File and Storage Technologies
(FAST), 2012.
[5] A. Dimakis, P. Godfrey, Y. Wu, M. Wainwright, and K. Ramchandran,
“Network coding for distributed storage systems,” Information Theory,
IEEE Transactions on, vol. 56, no. 9, pp. 4539 –4551, sept. 2010.
[6] F. Oggier and A. Datta, “Self-repairing homomorphic codes for distributed storage systems,” in INFOCOM, 2011 Proceedings IEEE, april
2011, pp. 1215 –1223.
[7] D. Papailiopoulos, J. Luo, A. Dimakis, C. Huang, and J. Li, “Simple
regenerating codes: Network coding for cloud storage,” Arxiv preprint
arXiv:1109.0264, 2011.
[8] P. Gopalan, C. Huang, H. Simitci, and S. Yekhanin, “On the locality of
codeword symbols,” Arxiv preprint arXiv:1106.3625, 2011.
[9] P. Erd˝ s and A. R´ nyi, “On random matrices,” Publ. Math. Inst. Hungar.
o
e
Acad. of Sciences. 8, 1964.
[10] A. G. Dimakis, V. Prabhakaran, and K. Ramchandran, “Decentralized
erasure codes for distributed networked storage,” IEEE/ACM Trans.
Netw., vol. 14, pp. 2809–2816, June 2006. [Online]. Available:
http://dx.doi.org/10.1109/TIT.2006.874535
[11] R. Gummadi, “Coding and scheduling in networks for erasures
and broadcast,” Ph.D. dissertation, University of Illinois at UrbanaChampaign, 2011.
[12] D. Wiedemann, “Solving sparse linear equations over ﬁnite ﬁelds,”
Information Theory, IEEE Transactions on, vol. 32, no. 1, pp. 54 –
62, jan 1986.
[13] R. Motwani and P. Raghavan, “Algorithms and theory of computation
handbook,” M. J. Atallah and M. Blanton, Eds. Chapman & Hall/CRC,
2010, ch. Randomized algorithms, pp. 12–12. [Online]. Available:
http://dl.acm.org/citation.cfm?id=1882757.1882769

≥0

≥ k log2

(1+ )k

1+

R EFERENCES

where the inequality is due to the monotonicity of the
logarithm and the fact that f (x) = xH x−y is increasx
ing with respect to x when 0 ≤ y ≤ x.
For the denumerator, D, we have:


k
D = k − s − i + k + 1 log2
k−i
k
k−i

k
k−i

where the last inequality holds when log2 (k) ≥ 1 +
ln(2) log2 (1 + ).
Combining (15), (16) and (17), it is evident that using d(k) =
c log(k), where c ∝ 1/ , sufﬁces to make Pr( P.M. in GS ) =
o(1) which completes the proof.



N ≤ (3 + ρ) log2 (k) + log2 (1 + )
k−i
(1 + )k − i + 1
+ kH
+ (1 + )kH
k
(1 + )k

(1 + )

(1+ )k−i+1
(1+ )k

(16)

where the last inequality holds for k ≥ 2.

5

