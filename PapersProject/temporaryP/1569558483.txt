Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 23:00:23 2012
ModDate:        Tue Jun 19 12:55:01 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      449835 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569558483

Entropy Functions and Determinant Inequalities
Terence Chan

Dongning Guo

Raymond Yeung

Institute of Telecom. Research,
University of South Australia

Dept. of Electrical Engineering
and Computer Science,
Northwestern University

Institute of Network Coding, and
Dept. of Information Engineering,
The Chinese University of Hong Kong

Characterisation of determinantal inequalities is closely related to characterisation of jointly Gaussian random variables.
In the relevant work [4], properties of (normalised) entropy
function of jointly Gaussian random variables were studied.
The characterisation problem was directly solved for no more
than four random variables. This paper offers an alternative
proof for the result in [4] by showing that all representable
functions are Gaussian.

Abstract—In this paper, we show that the characterisation of
all determinant inequalities for n × n positive deﬁnite matrices
is equivalent to determining the smallest closed and convex cone
containing all entropy functions induced by n scalar jointly
Gaussian random variables. We have obtained inner and outer
bounds on the cone by using representable functions and entropic
functions. In particular, these bounds are tight and explicit for
n ≤ 3, implying that determinant inequalities for 3 × 3 positive
deﬁnite matrices are completely characterized by Shannon-type
information inequalities.
Index Terms—Entropy, Gaussian distribution, rank functions

II. I NFORMATION INEQUALITY FRAMEWORK
The work [3] provides a geometric approach to understanding information inequalities. See also [5, Ch. 13-16] for a
comprehensive treatment. Its idea will be illustrated shortly.

I. I NTRODUCTION
Let n be a positive integer and denote the ground set by
N = {1, ..., n}. Suppose K is an n×n positive deﬁnite matrix.
For any subset α ⊆ N , let Kα be the sub-matrix of K obtained
by removing those rows and columns of K indexed by N \ α
and its determinant be denoted by |Kα |. When α is the empty
set, we will simply deﬁne Kα as the scalar of value 1. There
are many determinant inequalities in the existing literature that
involve only the principle minors of the matrix. These include

Deﬁnition 1 (Rank functions): A rank function over the
ground set N is a real-valued function deﬁned on all subsets
of N . The rank function space over the ground set N , denoted
n
by R2 , is the set of all rank functions over N .
n

As usual, R2 is treated as an Euclidean space. Hence,
concepts such as metric and limits can be deﬁned accordingly.

1) Hadamard inequality

i=1

Deﬁnition 2 (Entropic functions): Let g be a rank function.
Then g is called entropic if there exists a set of discrete random
variables {Xi , i ∈ N } such that g(α) is the Shannon entropy1
H(Xi , i ∈ α), or H(Xα ) for short, for all α ⊆ N .



On the other hand, if {Xi , i ∈ N } is a set of continuous
scalar random variables such that g(α) is the differential
entropy h(Xα ) for all α ⊆ N , then g is called s-entropic.

n

|K| ≤

|Ki |

(1)

2) Szasz inequality



|Kβ |


β⊆N :|β|=l

1

(k−1)
l−1



≥

1

(k−1)
l

|Kβ |

Deﬁnition 3 (Entropic regions): Consider any nonempty ﬁnite ground set N . Deﬁne the following “entropy regions”:

β⊆N :|β|=l+1

(2)

n

Γ∗ = {g ∈ R2 : g is entropic}
n

for any 1 ≤ l < k.

∗
γs,n

As pointed out in [1], [2], many determinant inequalities
(including the above two inequalities) can be proved via an
information-theoretic approach. Despite that many determinant
inequalities can be found in this approach, a complete characterisation of all determinant inequalities is still missing. In
this paper, we aim to understand determinant inequalities by
using the information inequality framework proposed in [3].

= {g ∈ R

2n

: g is s-entropic}.

(3)
(4)

Understanding the above entropic regions is one of the most
fundamental problems in information theory. It is equivalent
to determining the set of all information inequalities [3].
In this paper, we will use the following notation. For any
n
subset S ⊆ R2 , W(S) is deﬁned as the set of all rank
functions g ∗ such that g ∗ = c · g for some c > 0 and g ∈ S.
The closure of W(S) will be denoted by W(S). Finally, the

This work is supported in part by the Australian Research Council
(DP1094571), the University Grants Committee (Project No. AoE/E-02/08) of
the Hong Kong Special Administrative Region, China, and the U.S. National
Science Foundation (Project No. CCF-0644344).

1 All

1

logarithms used in the paper is in the base 2.

smallest closed and convex cone containing S will be denoted
by con(S). Clearly,
S ⊆ W(S) ⊆ W(S) ⊆ con(S).

holds for all positive deﬁnite matrix K if and only if
cα h(Xα ) ≥ 0

(5)

α⊆N

for all scalar jointly Gaussian variables {X1 , . . . , Xn }.

Theorem 1 (Geometric framework [3]): A linear information inequality α⊆N cα H(Xα ) ≥ 0 is valid for all discrete
random variables {X1 , . . . , Xn } if and only if for all g ∈ Γ∗ ,
n
α⊆N cα g(α) ≥ 0.

In fact, the Hadamard inequality and Szasz inequality are
respectively the counterparts of the following basic information inequalities2 [6]

By Theorem 1, characterising the set of all valid information
inequalities is thus equivalent to characterising the set Γ∗ .
n
∗
Similar results can be obtained for the set γs,n . In the
following, we will extend this geometric framework to study
determinant inequalities.

n

h(Xi ) ≥ h(X1 , . . . , Xn )
1
k
l

Deﬁnition 4 (Log-determinant function): A rank function g
over N is called log-determinant if there exists an n × n
positive deﬁnite matrix K such that for all α ⊆ N ,
g(α) = log |Kα |.

(6)

1
k
l+1

β⊆N :|β|=l+1

h(Yβ )
.
l+1

(12)

Corollary 1 (Proving an inequality): Suppose S contains
con(Υs,n ) as a subset. The inequality (7) holds for all positive
deﬁnite matrix K if for all g ∈ S, α⊆N cα g(α) ≥ 0.

Theorem 2: Let {cα , α ⊆ N } be any real numbers. The
determinant inequality

Therefore, any explicit outer bound on con(Υs,n ) can lead
to the discovery of new determinant inequalities. On the other
hand, an inner bound on con(Υs,n ) can be used for disproving
a determinant inequality.

(7)

α⊆N

holds for all positive deﬁnite matrix K if and only if for all
g ∈ con(Ψn )
cα g(α) ≥ 0.

β⊆N :|β|=l

h(Yβ )
≥
l

In the following sections, we will obtain inner and outer
bounds on the set con(Υs,n ). The following corollaries of
Theorem 2 show how these bounds can be used for proving
or disproving a determinant inequality.

Let Ψn be the set of all log-determinant functions over N .
Then, we have the following theorem.

|Kα |cα ≥ 1

(11)

i=1

Corollary 2: Suppose that T ⊆ con(Υs,n ). The determinant inequality (7) does not hold for all positive deﬁnite
matrices if there exists g ∈ T such that α⊆N cα g(α) < 0.

(8)

α⊆N

III. A N INNER BOUND AND AN OUTER BOUND

In other words, the characterisation of the set of all determinant inequalities is equivalent to determining con(Ψn ). In this
paper, we will obtain inner and outer bounds on con(Ψn ).
To achieve our goal, we will take an information theoretic
approach [2]. The idea is very simple: Let {X1 , . . . , Xn }
be a set of scalar jointly Gaussian random variables whose
covariance matrix is equal to (1/2πe)K. Then the differential
entropy of Xα is given by
1
h(Xα ) = log |Kα |.
(9)
2
Deﬁnition 5 (Scalar Jointly Gaussian function): A
n
function g ∈ R2 is called s-Gaussian if there exists
scalar jointly Gaussian variables {X1 , . . . , Xn } where
g(α) = h(Xα ), ∀α ⊆ N .

As log-determinant functions are essentially the same as
s-Gaussian functions, our objective is thus to characterise
con(Υs,n ). Since scalar Gaussian random variables are continuous scalar random variables, the next lemma follows
immediately from the deﬁnition.
∗
Lemma 1 (Outer bound): Υs,n ⊆ γs,n , and consequently,
∗
con(Υs,n ) ⊆ con(γs,n ).

(13)

∗

It is well known that Γn (i.e., the closure of Γ∗ ) is a closed
n
and convex cone [3]. It was established in [7] that
∗

∗
con(γs,n ) = con(Γn , φn , . . . , φn )
1
n

(10)

(14)

where
φn (α) =
i

From (9), a rank function g is log-determinant if and only
if 1 g is s-Gaussian. Let Υs,n be the set of all s-Gaussian
2
functions. Then con(Ψn ) = con(Υs,n ). Consequently, we have
the following theorem.

−1
0

if i ∈ α
otherwise.

2 Han’s inequality was originally proved for discrete random variables.
However, by using the same proving technique, it can also be proved to hold
for all continuous random variables [1]. Alternative, its validity also follows
from [7]: If a balanced information inequality (including Han’s inequality)
holds for all discrete random variables, then its “continuous counterpart” (i.e.,
the inequality by replacing discrete entropies with differential entropies) also
holds for all continuous random variables.

Theorem 3: The determinant inequality
|Kα |cα ≥ 1
α⊆N

2

In the following, we prove an inner bound on con(Υs,n ) by
using representable functions.

g. For any c1 , . . . , cn > 0, deﬁne the set of random variables
{Y1 , . . . , Yn } by Yi = Xi /ci , ∀i ∈ N , and let g ∗ be the
differential entropy function of {Y1 , . . . , Yn }. Then

Deﬁnition 6 (s-representable function): A rank function g
is called s-representable if there exists real-valued vectors (of
the same length) {A1 , . . . , An } such that for all α ⊆ N ,

g ∗ (α) = g(α) +

log ci

(19)

(log ci )φn (α)
i

(20)

i∈α

= g(α) −

g(α) = dim Ai , i ∈ α .

i∈N

Theorem 4 (Inner bound): If g is s-representable, then

for all α ⊆ N . Consequently, if g is s-Gaussian, then so is g ∗ .

g ∈ W(Υs,n ).

Corollary 3:
∗
con(Ωs,n , φn , . . . , φn ) ⊆ con(Υs,n ) ⊆ con(γs,n )
1
n

Proof: Suppose the length of each row vector Ai is k. Let
{W1 , . . . , Wk , V1 , . . . , Vn } be a set of independent standard
Gaussian random variables. Therefore, its covariance matrix
is the (n + k) × (n + k) identity matrix. Let c > 0. For each
i = 1, . . . , n, deﬁne a real-valued continuous random variable
Xi

∗

= con(Γn , φn , . . . , φn ) (21)
1
n
where Ωs,n is the set of all s-representable functions.
Proposition 1 (Tightness): For n ≤ 3,

1
√ Ai [W1 , . . . , Wk ] + Vi .
c

con(Ωs,n , φn , . . . , φn ) = con(Υs,n )
1
n
∗

∗
= con(γs,n ) = con(Γn , φn , . . . , φn ). (22)
1
n

Let X = [X1 , . . . , Xn ] . Then
1
X = √ A[W1 , . . . , Wk ] + V
c

Proof: By Corollary 3, to prove the proposition, it sufﬁces
to prove that for n ≤ 3,

where A is an n × k matrix whose ith row is Ai and V =
[V1 , . . . , Vn ] . Since Xi is zero-mean,
Cov(X) = E[XX ]
1
= E A[W1 , . . . , Wk ] [W1 , . . . , Wk ]A
c
1
= AA + I.
c

∗

con(Γn ) ⊆ con(Ωs,n ).

In [15], the cone
(when n ≤ 3) was explicitly determined
by identifying the set of extreme vectors of the cone. It can
be proved that all the extreme vectors are s-representable3 and
hence is a subset of con(Ωs,n ). Consequently, (23) holds and
the proposition follows.

+I

Proposition 1 does not hold when n ≥ 4. In fact,
con(Ωs,n , φn , . . . , φn ) is in general a proper subset of
n
1
con(Υs,n ) when n ≥ 4. In [11], it was proved that all
s-representable functions satisfy the Ingleton inequalities. It
can also be directly veriﬁed that all the functions φn also
i
satisfy the Ingleton inequalities. Therefore, all the functions
in con(Ωs,n , φn , . . . , φn ) also satisfy the Ingleton inequalities.
n
1
However, in [4], it was proved that there exists g ∈ Υs,n
for n = 4 that violates the the Ingleton inequality. Thus,
con(Ωs,n , φn , . . . , φn ) is indeed a proper subset of con(Υs,n ).
n
1

Consequently, det(Cov(X)) = det 1 D + I where D is the
c
diagonal matrix obtained by using singular-value decomposition (SVD) over AA . Let d1 ≥ d2 ≥ · · · ≥ dn ≥ 0 be the
diagonal entries of D and r be the rank of the matrix AA
(or equivalently, the rank of A). Hence, di > 0 if and only if
i ≤ r. Then
r

det(Cov(X)) =
i=1

di
+1 .
c

(15)

It is easy to see that
h(X1 , . . . , Xn )
log (det(Cov(X)))
= lim
1
c→0
c→0
log 1/c
2 log 1/c
lim

= lim

c→0

r
i=1

log di + 1
c
log 1/c

= r.

(23)

∗
Γn

Remark 1: Theorem 4 is not merely an intermediate result
to proving Proposition 1. It leads to Corollary 3 which gives
a non-trivial inner bound for the set of Gaussian functions.

(16)
(17)

IV. A NOTHER OUTER BOUND

(18)

By deﬁnition, con(Ψn ) (which is the focus of our interest)
is close under addition. However, this is not necessarily true
for Ψn . In fact, W(Ψn ) is not necessarily equal to con(Ψn ). In
the previous section, we showed that the set Ψn is essentially
equivalent to the set of s-Gaussian functions, deﬁned via sets
of scalar jointly Gaussian random variables. It turns out that,

Similarly, for any α ⊆ {1, . . . , n}, we can prove that
h(Xα )
= dim Ai , i ∈ α = g(α).
log 1/c

lim
c→0 1
2

Thus, g ∈ W(Υs,n ) and the theorem is proved.

3 In [15], the extreme vectors are proved to be representable with respect to a
ﬁnite ﬁeld. However, it can be veriﬁed easily that they are also s-representable
with respect to the real ﬁeld R.

Lemma 2: Let {X1 , . . . , Xn } be a set of scalar jointly
continuous random variables with differential entropy function

3

Deﬁnition 9 (m-Quantization): Given m > 0, let the mquantization of any real number x be denoted as:

if we relax the constraint by allowing the jointly Gaussian
random variables to be vectors, instead of scalars, we will
obtain an outer bound for Ψn and also con(Ψn ).

mx
(26)
m
where t denotes the largest integer not exceeding t. Similarly, let the m-quantization of a real vector x = (x1 , . . . , xn )
be the element-wise m-quantization of the vector, denoted by
[x]m , i.e., [x]m = ([x1 ]m , . . . , [xn ]m ).
[x]m =

Deﬁnition 7 (Vector Gaussian function): A function g is
called v-Gaussian if there exists n jointly Gaussian random
vectors {X1 , . . . , Xn } such that g(α) = h(Xα ) for all α ⊆ N .
Lemma 3: con(Υv,n ) = W(Υv,n ).
Theorem 5 (Another outer bound):

Evidently, [x]m can only take values from the set

con(Υs,n ) ⊆ W(Υv,n ).

(24)

0, ±

Proof: Follow from that Υs,n ⊆ Υv,n and Lemma 3.

lim H([X]m ) − n log m = h(X) .

m→∞

∗
con(Υs,n ) ⊆ W(Υv,n ) ⊆ con(γv,n ).

To show that (24) is tighter, we will prove the following result.
∗

∗
∗
Theorem 6: γv,n = γs,n = con(Γn , φn , . . . , φn ).
n
1

Lemma 5: Let {X1 , . . . , Xn } be a set of discrete random
variables such that its entropy function is g. For any positive
numbers c1 , . . . , cn , let g ∗ be deﬁned as g ∗ (α) = g(α) −
∗
i∈α log ci . Then g is s-entropic.

Theorem 6 states that replacing the real-valued random
variables Xi in the vector X by random vectors does not
enlarge the closure of the space of differential entropy vectors.
The discrete counterpart of this result is trivial, because as far
as the probability masses and the entropy are concerned, a
discrete random vector is the same as a scalar discrete random
variable. However, in the continuous domain, it is unclear how
a probability density function on R2 or more generally Rm can
be mapped to a pdf on R without changing the entropies. In
particular, there does not exist a continuous mapping from R2
to R [9]. The proof of Theorem 6 exploits the relationship
between the differential entropy of a continuous vector and
the entropy of a discrete vector obtained through quantisation:
Given the n-tuple Z whose entries are vectors, we “quantise”
Z by a discrete vector and then construct a continuous
vector with n scalar entries whose entropy vector arbitrarily
approximates that of Z. Before we prove the theorem, we need
several intermediate supporting results.

∗
∗
Proof of Theorem 6: Clearly, γs,n ⊆ γv,n . We will now
∗
∗ . Let Z = (Z , . . . , Z ) consist of n
prove that γv,n ⊆ γs,n
1
n
random vectors, where Zi = (Zi,1 , . . . , Zi,ki ). Let us deﬁne
the m-quantization of Zi , denoted as [Zi ]m , be the elementwise m-quantization of Zi , i.e., it consists of [Zi,j ]m for j =
1, . . . , ki . By Proposition 4,

lim

m→∞

Proposition 2: If g ∈

∗

and c > 0, then c · g ∈

H([Zi ]m , i ∈ α) −

ki

log m = h(Zα ). (29)

i∈α
n

Let g ∗ , rm , g m ∈ R2 be such that
g ∗ (α) = h(Zα )

(30)

m

r (α) = H([Zi ]m , i ∈ α)

Lemma 4 (Closeness in addition): If g1 and g2 are ventropic (or entropic) functions over N , then their sum g1 +g2
is also v-entropic (or entropic).
∗
γv,n

(28)

Under the assumption that the pdf of a random variable X
is Riemann-integrable, Proposition 4 is established in [1] by
treating H([X]m ) − n log m as the approximation of the Riemann integration of − fX (x) log fX (x)dx. It is nontrivial to
establish the result in general, where the pdf is not necessarily
Rieman-integrable. An example of such a pdf can be deﬁned
by using the Smith-Volterra-Cantor set. Nonetheless (28) can
be shown to hold using the Lebesgue convergence theorem
along with some truncation arguments [8].

(25)

∗
Clearly, W(Υv,n ) = con(Υv,n ) ⊆ con(γv,n ). Thus,

∗

(27)

Proposition 4 (Renyi [8]): If X is a real-valued random
vector of dimension n with a probability density function, then

Deﬁnition 8: A rank function g is called v-entropic if there
exists a set of random vectors {X1 , . . . , Xn }, not necessarily
of the same length, such that g(α) = h(Xα ). Also, let
n

.

Hence for every real-valued random variable X, [X]m is a
discrete random variable taking value in the set (27).

So far, we have established two outer bounds (13) and (24)
for con(Υs,n ). In the following, we will prove that (24) is in
fact a tighter one.

∗
γv,n (N ) = {g ∈ R2 : g is v-entropic}.

2
1
,± ,...
m m

g m (α) = rm (α) −

ki

(31)
log m.

(32)

i∈α
∗
By (29), limm→∞ g m = g ∗ . Also, since rm ∈ Γ∗ , g m ∈ γs,n
n
∗
by Lemma 5. Consequently, g ∗ ∈ γs,n . We have thus proved
∗
∗
∗
∗
that γv,n ⊆ γs,n and as a result, γv,n = γs,n . Finally, by
∗
Proposition 3, γv,n is a closed and convex cone and is equal

∗
γv,n .

∗
Proposition 3: γv,n is a closed and convex cone.

4

∗

∗
∗
to con(γs,n ). Then by (14), γv,n = con(Γn , φn , . . . , φn ). The
n
1
theorem is proved.

convex, [4] proved that the closure of the set of all normalised
Gaussian entropy functions is indeed closed and convex.

In Theorem 4, we have constructed an inner bound for
con(Υs,n ) by using s-representable functions. The same trick
can also be used for constructing an inner bound for W(Υv,n ).

Proposition 5: Let Υ∗ 4 be the set of all normalised
N,n
Gaussian entropy functions. Then con(Υ∗ ) = con(Υv,n ).
N,n
Remark 3: Our Proposition 1 can also be derived from
[4, Theorem 5], which proved that for any g ∈ Υv,n when
n = 3, there exists a θ∗ > 0 such that for all θ ≥ θ∗ ,
1
θ g is vector Gaussian. However, their proof techniques are
completely different.

Deﬁnition 10: A rank function g over N is called vrepresentable if for i = 1, . . . , n, there exists a set of realvalued vectors (of the same length) {Ai,1 , . . . Ai,ki } such that
for all α ⊆ N , g(α) = dim Ai,j , i ∈ α, j = 1, . . . , ki .
The following theorem is a counterpart of Theorem 4. We
will omit the proof for brevity.

V. C ONCLUSION
In this paper, we took an information theoretic approach to
study determinant inequalities for positive deﬁnite matrices.
We showed that characterising all such inequalities for an
n × n positive deﬁnite matrix is equivalent to characterising
the set of all scalar Gaussian entropy functions for n random
variables. While a complete and explicit characterisation of
the set is still missing, we obtained inner and outer bounds
respectively by means of linearly representable functions and
vector Gaussian entropy functions. It turns out that for n ≤ 3,
the set of all scalar Gaussian entropy functions is the same
as the set of all differential entropy functions. The latter
set is completely characterized by Shannon-type information
inequalities. Consequently, the aforementioned inner and outer
bounds agree with each other. For n ≥ 4, we showed the
contrary, and the problem is seeming very difﬁcult.

Theorem 7 (Inner bound on W(Υv,n )): Suppose that g is
v-representable, then g ∈ W(Υv,n ) .
Characterising the set of v-representable functions has been
a very important (and also extremely difﬁcult) problem in
linear algebra and information theory. For many years, it is
only known that v-representable functions are polymatroidal
and satisﬁes the Ingleton inequalities [10], [11]. The set of
representable functions is only known when n ≤ 4. However,
there were some recent breakthrough in this areas. In [12],
[13], many new subspace rank inequalities which are required
to be satisﬁed by representable functions are discovered.
Via a computer-assisted mechanical approach, the set of all
representable functions when n ≤ 5 has been completely characterised. Properties about the set of v-representable functions
were also obtained [14]. Theorems 4 and 7 thus opens a new
door to exploit results obtained about representable functions
to characterise the set of Gaussian functions.

R EFERENCES
[1] T. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd ed.
Wiley, 2006.
[2] T. Cover, “Determinant inequalities via information theory,” SIAM. J.
Matrix Anal. & Appl., 9(3), pp.384-392, 1988.
[3] R. W. Yeung, “A framework for linear information inequalities,” IEEE
Trans. Inform. Theory, vol. 43, no. 6, pp. 1924–1934, Nov 1997.
[4] B. Hassibi and S. Shadbakht, “The entropy region for three gaussian
random variables,” in Information Theory, 2008. ISIT 2008. IEEE
International Symposium on, july 2008, pp. 2634 –2638.
[5] R. W. Yeung, Information Theory and Network Coding, Springer 2008.
[6] T. S. Han, “Nonnegative entropy measures of multivariate symmetric
correlations,” Inform. Contr., 36: 133-156, 1978.
[7] T. H. Chan, “Balanced information inequalities,” IEEE Trans. Inform.
Theory, vol. 49, pp. 3261 – 3267, 2003.
[8] A. Rényi, Probability Theory. Budapest, Hungary: North Holland –
Academiai Kiado, 1970.
[9] K. Wiboonton, “Bijections from Rn to Rm ,” available online at
https://www.math.lsu.edu/ kwiboo1/talkpaper.pdf, 2010.
[10] L. Guille, T. H. Chan, and A. Grant, “The minimal set of ingleton
inequalities,” IEEE Trans. Inform. Theory, vol. 57, pp. 1849-1864, 2011.
[11] A. W. Ingleton, “Representation of matroids.” London: Academic Press,
1971, pp. 149–167.
[12] R. Dougherty, C. Freiling, and K. Zeger, “Linear rank inequalities on
ﬁve or more variables,” Arxiv preprint cs.IT/0910.0284v3, 2009.
[13] R. Kinser, “New inequalities for subspace arrangements,” J. Combin.
Theory Ser. A, 2010.
[14] T. Chan, A. Grant, and D. Pﬂüger, “Truncation technique for characterising linear polymatroids,” IEEE Trans. Information Theory, pp. 6364–
6378, Oct 2011
[15] Z. Zhang and R. W. Yeung, “On the characterization of entropy function
via information inequalities,” IEEE Trans. Inform. Theory, vol. 44, pp.
pp. 1440–1452, 1998.

Corollary 4 (Inner bound on W(Υv,n )):
con(Ωv,n , φn , . . . , φn ) ⊆ W(Υv,n )
1
n
where Ωv,n is the set of all v-representable functions.
Remark 2: While con(Ωs,n , φn , . . . , φn ) ⊆ con(Υs,n ), it is
n
1
still an open question whether or not
con(Ωv,n , φn , . . . , φn ) ⊆ con(Υs,n ).
1
n
Gaussian rank functions were studied in [4]. However, their
deﬁnitions are slightly different from ours.
Deﬁnition 11 (Normalised joint entropy [4]): Let
{X1 , . . . , Xn } be a set of n jointly distributed vector
valued Gaussian random variables such that each vector Xi
is a vector of length T . Its normalised Gaussian entropy
n
function g is a function in R2 such that g(α) h(Xα )/T .
The only difference between Deﬁnitions 5 and 11 is the
scalar multiplier 1/T . Hence, a normalised Gaussian entropy
function must be contained in the set W(Υv,n ). In one sense,
our proposed deﬁnition is slightly more general as we do not
require all the random vectors Xi to have the same length.
On the other hand, the “normalising factor” 1/T in Deﬁnition
11 can lead to some interesting results. For example, while
we cannot prove that the closure of W(Υs,n ) is closed and

4 The

5

subscript N is a mnemonic for the word “normalised”.

