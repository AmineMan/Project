Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 09:30:36 2012
ModDate:        Tue Jun 19 12:54:44 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      516805 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569552251

The Dispersion of Slepian-Wolf Coding
Vincent Y. F. Tan∗† , Oliver Kosut‡
∗

Institute for Infocomm Research, A*STAR (Email: tanyfv@i2r.a-star.edu.sg)
Department of Electrical and Computer Engineering, National University of Singapore
‡ Stochastic Systems Group (SSG), Massachusetts Institute of Technology, (Email: okosut@mit.edu)
†

rates as a function of (n, ) is. The main tool that we use is a
multidimensional version of the Berry-Ess` en theorem [2].
e

Abstract—We characterize second-order coding rates (or dispersions) for distributed lossless source coding (the SlepianWolf problem). We introduce a fundamental quantity known
as the entropy dispersion matrix, which is analogous to scalar
dispersion quantities. We show that if this matrix is positivedeﬁnite, the optimal rate region under the constraint of a
ﬁxed blocklength and non-zero error probability has a curved
boundary compared to being polyhedral for the Slepian-Wolf
case. In addition, the entropy dispersion matrix governs the rate
of convergence of the non-asymptotic region to the asymptotic
one. As a by-product of our analyses, we develop a general
universal achievability procedure for dispersion analysis of some
other network information theory problems such as the multipleaccess channel. Numerical examples show how the region given by
Gaussian approximations compares to the Slepian-Wolf region.
Index Terms—Slepian-Wolf, Dispersion, Second-order Rates

A. Main Contributions
This paper characterizes the (n, )-optimal rate region for
the SW problem R ∗ (n, ) up to an O( log n ) factor. In the
n
course of doing so, we introduce a fundamental quantity called
the entropy dispersion matrix of pX1 ,X2 and show that if this
matrix is non-singular, the boundary of R ∗ (n, ) is, unlike
that of SW, a smooth curve. We also demonstrate numerically
how our region compares to the SW region and to the problem
of ﬁnite blocklength source coding with side information also
at the encoder. While the SW problem is the focus of this
paper, our achievability technique is general enough to be
applicable to multi-terminal channel coding problems such as
the multiple-access, broadcast and interference channels. The
results for these other problems are not included this paper.
The interested reader may refer to [3] for more details.

I. I NTRODUCTION
Distributed lossless source coding consists in separately
n
n
encoding two (or more) correlated sources (X1 , X2 ) ∼
n
k=1 pX1 ,X2 (x1k , x2k ) into a pair of rate-limited messages
(M1 , M2 ). Subsequently, given these compressed versions of
n
n
the sources, a decoder seeks to reconstruct (X1 , X2 ). One of
the most remarkable results in information theory, proved by
Slepian and Wolf [1], states that the set of achievable rate pairs
(R1 , R2 ) is equal to that when each of the encoders is given
n
knowledge of the other source, i.e., encoder 1 knows X2 and
∗
vice versa. The optimal rate region R is the polyhedron

B. Related Work
The redundancy of SW coding was discussed in [4]–[6].
However, the authors considered a single source X1 to be
compressed and side information X2 available only at the
decoder. Thus, X2 is neither coded nor estimated. They
showed that a scalar dispersion quantity governs the secondorder coding rate. He et al. [5] also analyzed a variable-length
variant of the SW problem and showed that the dispersion
is smaller than in the ﬁxed-length setting. This dispersion
is similar to that for channel coding. Sarvotham et al. [7]
considered the SW problem with two sources to be compressed
but limited their setting to the case the sources are symmetric.
This work generalizes their setting in that we consider all
discrete sources. This paper is a network information theory
analogue of the works on second-order coding rates [8], [9]
and ﬁnite blocklength analysis [10]–[13]. We employ the
information spectrum method [14] in our converse proof. This
was also done in [9].

R1 ≥ H(X1 |X2 )
R2 ≥ H(X2 |X1 )
R1 + R2 ≥ H(X1 , X2 ).

(1)

As with most other statements in information theory, this result
is asymptotic in nature. In this paper, we take a step towards
non-asymptotic results by analyzing the second-order coding
rates of the Slepian-Wolf (SW) problem.
An two-sender SW code is characterized by four parameters; the blocklength n, the rates of the ﬁrst and second sources
(R1 , R2 ) and the probability of error deﬁned as
(n)
n
n
ˆn ˆn
Pe := P((X1 , X2 ) = (X1 , X2 )),

II. P ROBLEM S TATEMENT AND M AIN R ESULTS

(2)

A. Notation

n
ˆn
ˆn
where X1 and X2 are the reconstructed versions of X1 and
(n)
n
X2 respectively. Traditionally, we require Pe → 0 as n →
∞. In this paper, we ﬁx n and require the code to be such
(n)
that Pe ≤ . We then ask what the set of achievable pairs of

Random variables and the values they take on will be
denoted by upper case (e.g., X) and lower case (e.g., x)
respectively. Types (empirical distributions) will be denoted
by upper case (e.g., P ) and distributions by lower case
(e.g., p). For a sequence xn ∈ X n , the type is denoted as
Pxn and conditional types are denoted similarly. The entropy

V. Tan is supported by A*STAR, Singapore. O. Kosut is supported by Shell
Global Solutions International B.V.

1

An illustration is provided in Fig. 1. Henceforth, ∈ (0, 1).

and conditional entropy are denoted as H(X1 ) = H(pX1 )
and H(X2 |X1 ) = H(pX2 |X1 |pX1 ) respectively. For a pair
ˆ
of sequences xn , xn , the notations H(xn ) := H(Pxn ) and
1
2
1
1
ˆ
H(xn |xn ) := H(Pxn |xn |Pxn ) denote, respectively, the empir2 1
1
2
1
ical marginal and conditional entropies. For two vectors u, v ∈
Rd , the notation u ≤ v means ut ≤ vt for all t = 1, . . . , d.
We also use the notation [2nR ] := {1, . . . , 2nR }.

C. Main Result and Interpretation
Theorem 1. The (n, )-optimal rate region R ∗ (n, ) satisﬁes
Rin (n, ) ⊂ R ∗ (n, ) ⊂ Rout (n, ).

for all n sufﬁciently large.
This theorem is proved for V 0 in Section III. Sources for
which V is singular include those which are (i) independent,
i.e., I(X1 ; X2 ) = 0, (ii) either X1 or X2 is uniform over
their alphabets. The authors in [7] dealt with the speciﬁc case
1
where X1 , X2 ∈ F2 , X1 = Bern( 2 ), X2 = X1 ⊕ N with
1
N = Bern(q), q ∈ (0, 2 ), i.e., a discrete symmetric binary
source (DSBS). In Section IV, we comment on how the proof
can be adapted to derive R ∗ (n, ) for a DSBS and all V 0.
The direct part of Theorem 1 is proved using the usual
random binning argument together with a multidimensional
Berry-Ess` en theorem [2]. The converse is proved using an
e
information spectrum technique by Han [14]. Theorem 1
extends to the case where there are more than two senders.
By examining Rin (n, ) and Rout (n, ), it can be seen that
we have characterized the (n, )-rate region up to an O( log n )
n
factor. This residual is a consequence of (i) universal decoding
for the direct part and (ii) approximations resulting from using
the multidimensional Berry-Ess` en theorem [2]. Observe that
e
as n → ∞, the (n, )-rate region approaches the SW region [1]
1
at a rate of O( √n ). This follows from the multidimensional
central limit theorem. However, somewhat unexpectedly, if
V
0, the (n, )-rate region is not-polyhedral [cf. (1)]. Its
boundary is a smooth curve in R2 . This curvature, given
by V, is due to the fact that the three empirical entropies
ˆ n n ˆ n n
ˆ n n
H(X1 |X2 ), H(X2 |X1 ) and H(X1 , X2 ) have to be jointly
smaller than some rate vector. By Taylor’s theorem, we see
that the empirical entropy vector behaves like a multivariate
Gaussian with mean H and covariance V.

B. Deﬁnitions
Let (X1 , X2 , pX1 ,X2 (x1 , x2 )) be a discrete memoryless
n
n
multiple source (DMMS). This means that (X1 , X2 ) ∼
n
pX1 ,X2 (x1k , x2k ). The alphabets X1 , X2 are ﬁnite.
k=1
Deﬁnition 1. An (n, 2nR1 , 2nR2 , )-SW code consists of two
encoders fj,n : Xjn → Mj := [2nRj ], j = 1, 2, and a decoder
n
n
ϕn : M1 ×M2 → X1 ×X2 such that the the error probability
ˆ n , X n ) := ϕn (f1,n (X n ), f2,n (X n )) does not
ˆ
in (2) with (X1 2
1
2
1
exceed . The rates are deﬁned as Rj := n log |Mj |.
Deﬁnition 2. A rate pair (R1 , R2 ) is (n, )-achievable if
there exists an (n, 2nR1 , 2nR2 , )-SW code for the DMMS
pX1 ,X2 (x1 , x2 ). The (n, )-optimal rate region R ∗ (n, ) ⊂ R2
is the set of all (n, )-achievable rate pairs.
For a positive-semideﬁnite symmetric matrix V
random vector Z ∼ N (0, V). Deﬁne the set

0, let the

S (V, ) := {z ∈ R3 : P(Z ≤ z) ≥ 1 − }.

(3)

Note that S (V, ) ⊂ R3 and is analogous to the cumulative
distribution function of a zero-mean Gaussian with covariance
1
matrix V. If ≤ 2 , S (V, ) is a convex, unbounded set in the
positive orthant. The boundary of S (V, ) is a differentiable
manifold if V is positive-deﬁnite (V 0).
Deﬁnition 3. The entropy density vector is deﬁned as


− log pX1 |X2 (X1 |X2 )
h(X1 , X2 ) :=  − log pX2 |X1 (X2 |X1 )  .
− log pX1 ,X2 (X1 , X2 )

(4)

III. P ROOF OF T HEOREM 1
A. Achievability (Inner Bound)
Proof: Let (R1 , R2 ) be a rate pair such that R belongs
to the inner bound Rin (n, ), deﬁned in (5).
Codebook Generation: For j = 1, 2, randomly and independently assign an index f1,n (xn ) ∈ [2nRj ] to each sequence
j
xn ∈ Xjn according to a uniform pmf. The sequences of
j
the same index form a bin, i.e., Bj (mj ) := {xn ∈ Xjn :
j
f1,n (xn ) = mj }. Note that Bj (mj ), mj ∈ [2nRj ] are random
j
sets. The bin assignments are revealed to all parties. In
particular, the decoder knows the bin rates Rj .
Encoding: Given xn ∈ Xjn , encoder j transmits the bin index
j
fj,n (xn ). Hence, for length-n sequence, the rates of m1 and
j
m2 are R1 and R2 respectively.
Decoding: The decoder, upon receipt of the bin indices
(m1 , m2 ) ﬁnds the unique sequence pair (ˆn , xn ) ∈ B1 (m1 )×
x1 ˆ 2
B2 (m2 ) such that the empirical entropy vector
ˆ n n 
H(ˆ1 |ˆ2 )
x x
ˆ x ˆ
ˆ x x
H(ˆn , xn ) :=  H(ˆn |ˆn )  ≤ R − δn 1,
(8)
1
2
2 1
ˆ x ˆ
H(ˆn , xn )

The mean of the entropy density vector is E[h(X1 , X2 )] =
H(pX1 ,X2 ) := [H(X1 |X2 ), H(X2 |X1 ), H(X1 , X2 )]T .
Deﬁnition 4. The entropy dispersion matrix V(pX1 ,X2 ) is the
covariance of the random vector h(X1 , X2 ).
We abbreviate the deterministic quantities H(pX1 ,X2 ) and
V(pX1 ,X2 ) as H and V respectively. Observe that V is an
analogue of the scalar dispersion quantities that have gained
attention in recent years [10]–[13]. We will ﬁnd it convenient
to deﬁne the rate vector R := [R1 , R2 , R1 + R2 ]T ∈ R3 .
Deﬁnition 5. Deﬁne the region Rin (n, ) ⊂ R2 to be the set
of rate pairs (R1 , R2 ) that satisfy
1
ν log n
R ∈ H + √ S (V, ) +
1,
n
n

(5)

where ν := |X1 ||X2 | + 1 and 1 := (1, 1, 1)T . Also let
Rout (n, ) ⊂ R2 be the set of rate pairs (R1 , R2 ) that satisfy
1
log n
1.
R ∈ H + √ S (V, ) −
n
n

(7)

(6)

1

2

2

where δn := (|X1 ||X2 | + 1 ) log(n+1) . Deﬁne the empirical
2
n
entropy typical set T (R, δn ) := {z ∈ R3 : z ≤ R − δn 1}.
ˆ x ˆ
Then, (8) is equivalent to H(ˆn , xn ) ∈ T (R, δn ). If there
1
2
is more than one pair or no such pair in B1 (m1 ) × B2 (m2 ),
declare a decoding error. Note that our decoding scheme is
universal [15], i.e., the decoder does not depend on knowledge
of the true distribution pX1 ,X2 .
Analysis of error probability: Let the sequences sent by the two
n
n
users be (X1 , X2 ) and let their corresponding bin indices be
(M1 , M2 ). We bound the probability of error averaged over the
random code construction. Clearly, the ensemble probability
of error is bounded above by the sum of the probabilities of
the following four events:

where (a) follows from the deﬁnition T (R, δn ) and (b)
follows from the probability relation
P(W + ∆ ≤ R ) ≥ P(W ≤ R − cn 1) − P( ∆
As is shown in [3], P( ∆
With this choice of cn ,

1

1

2

c
P(E1 ) ≥ P

n
n
E4 := {∃ xn ∈ B1 (M1 ) \ {X1 }, xn ∈ B2 (M2 ) \ {X2 } :
˜1
˜2
ˆ x ˜
H(˜n , xn ) ∈ T (R, δn )}
(9)
1
2

(10)

where we made the dependence of the empirical entropy vector
on the type explicit. We now bound the probability in (10). Let
|X ||X |
vec(pX1 ,X2 ) ∈ R+ 1 2 be a vectorized version of the joint
distribution pX1 ,X2 . Consider the Taylor series expansion:

,
pX1 ,X2 (x1 ,x2 )

(12)

(a)

(b)

≥ P H+

1
n
1
n

n
n
n
ˆ x
H(˜n , xn ) ∈ T (R, δn ) (X1 , X2 ) = (xn , xn ), X1 ∈ B1 (1)
1
2
1
2

(13)

(b)

p(xn , xn )
1
2

≤
xn ,xn
1
2
(c)

ˆ x
xn =xn :H(˜n ,xn )∈T (R,δn )
˜1
1
1
2

xn ,xn
1
2
(d)
xn ,xn
1
2
(e)

(Jk − E[Jk ]) ≤ R − (δn + cn )1

Q

(14)

1
ˆ x
xn =xn :H(˜n |xn )≤R1 −δn
˜1
1
1
2

(xn ,xn )∈TQ
1
2

2−nR1
V ∈V (QX2 ):
¯
H(V |Pxn )≤R1 −δn
2

3

2nR1

p(xn , xn )
1
2

≤

k=1

P (˜n ∈ B1 (1))
x1
ˆ x
xn =xn :H(˜n |xn )≤R1 −δn
˜1
1
1
2

p(xn , xn )
1
2

=

k=1
n

P (˜n ∈ B1 (1))
x1

p(xn , xn )
1
2

≤

(Jk − E[Jk ]) + ∆ ≤ R − δn 1

≥ cn ].

(18)

xn ,xn
1
2

k=1

∞

.

n
p(xn , xn )P ∃ xn ∈ B1 (1) \ {X1 } :
˜1
1
2

=

n

− P[ ∆

log n
√
n

n
P(E2 |X1 ∈ B1 (1))

n
n
because the joint type PX1 ,X2 places a probability mass
1/n on each sample (X1k , X2k ). Deﬁne the random vector
Jk := ([J]1,(X1k ,X2k ) , [J]2,(X1k ,X2k ) , [J]3,(X1k ,X2k ) )T . On account of (10), (11) and (13), we have

c
P(E1 ) = P H +

(17)

For the second event, by symmetry and uniformity, P(E2 ) =
n
P(E2 |X1 ∈ B1 (1)). Now consider the chain of inequalities:

x1 ,x2
n

[J]t,(X1k ,X2k )

(16)

k=1

P(E1 ) ≤ − O

n
n
PX1 ,X2 (x1 , x2 )[J]t,(x1 ,x2 )

1
n

z
1
(Jk − E[Jk ]) ≤ √ +ψn 1 − 2
n
n

where (a) follows from Taylor’s approximation theorem. Be1
√n
cause ψn = O( logn ) dominates the O( √n ) term resulting
from the Berry-Ess` en approximation, we conclude that
e

(a)

=

n

400d1/4 ξ
1
√ − 2
3/2 n
n
λmin (V)
(a)
530ξ
1
√ −
≥ 1 − + O (ψn ) −
,
λmin (V)3/2 n n2

where g1 (pX1 ,X2 ) := H(X1 |X2 ), g2 (pX1 ,X2 ) := H(X2 |X1 )
and g3 (pX1 ,X2 ) := H(X1 , X2 ). Because the gt ’s are twice
continuously differentiable, each entry of the second order
correction term ∆ ∈ R3 in (11) is bounded above by
n
n
C vec(PX1 ,X2 − pX1 ,X2 ) 2 for some constant C > 0. Let
[J]t be the t-th row of the matrix J. Now, note that
n
n
[J]t vec(PX1 ,X2 ) =

1
n

c
P(E1 ) ≥ P (Z ≤ z + ψn 1) −

ˆ
n
n
n
n
H(PX1 ,X2 ) = H(pX1 ,X2 ) + J(vec(PX1 ,X2 − pX1 ,X2 )) + ∆.
(11)
where the Jacobian J ∈ R3×(|X1 ||X2 |) is deﬁned entry-wise as
[J]t,(x1 ,x2 )

(15)

where ψn = O( log n ). Now note that the summands above
n
are i.i.d. random vectors. These random vectors have zero
mean, covariance matrix V
0 and ﬁnite third moment
ξ := E h(X1 , X2 ) 3 because X1 , X2 are ﬁnite sets. Since the
2
set integrated over in (16) is convex, by the multidimensional
Berry-Ess` en theorem [2] (dimension d = 3),
e

We bound each of these in turn. Consider

∂gt (pX1 ,X2 )
=
∂pX1 ,X2 (x1 , x2 )

(Jk − E[Jk ]) ≤
k=1

z
because R − H = √n + ν log n 1 for some z such that P(Z ≤
n
z) ≥ 1− for Z ∼ N (0, V) [cf. deﬁnition of S (V, )]. Since
ν > |X1 ||X2 | + 1/2 (the coefﬁcient of δn ), we have

n
ˆ n ˜
E3 := {∃ xn ∈ B2 (M2 ) \ {X2 } : H(X1 , xn ) ∈ T (R, δn )}
˜2
2

ˆ
n
n
P(E1 ) = 1 − P(H(PX1 ,X2 ) ∈ T (R, δn ))

≥ cn ) ≤ 1/n2 if cn = O(1/n).

ν log n
z
1
√ +
1 − (δn + cn )1 − 2
n
n
n

ˆ n n /
E1 := {H(X1 , X2 ) ∈ T (R, δn )}
ˆ x
E2 := {∃ xn ∈ B1 (M1 ) \ {X n } : H(˜n , X n ) ∈ T (R, δn )}
˜
1

≥ cn ).

n

1
n

c
P(E1 ) ≥ P

∞

∞

xn ∈TV
˜1

(xn )
2

(f )

p(xn , xn )
1
2

≤

2

n=200,ε=0.01

nH(V |Pxn ) −nR1
2

2

n=1000,ε=0.01

0.95

(19)

R2

p(xn , xn )(n + 1)|X1 ||X2 | 2n(R1 −δn ) 2−nR1
1
2

≤

0.85

0.8
0.75

xn ,xn
1
2

0.7

0.8

0.9

1

0.6

0.7

R

0.8

0.9

1

R

1

1

Fig. 1. Plots of the SW boundary, the (n, )-SI-ED boundary (sharp corners)
and the (n, )-SW boundary (curved) for = 0.01 neglecting the O( log n )
n
terms in Theorem 1. The legend applies to both plots. Notice that R ∗ (n, )
∗
and RSI−ED (n, ) are quite different near the equal rate and corner points
∗
when n is small. Plots of R ∗ (n, ) and RSI−ED (n, ) as functions of n
∗
along the equal rate and corner point slices of RSI−ED (n, )) are given in
Figs. 2 and 3. These are indicated by the black and the green ×.

with the rate vector. This is likened to maximum-likelihood
decoding. Taylor expansion in (11) would not be required.
Under this decoding strategy, there is symmetry between
the error probabilities in the direct and converse parts. Also
see [14, Lem. 7.2.1-2]. The rate penalty of using a universal
decoder is of the order O( log n ). This is insigniﬁcant compared
n
1
to the dispersion term which is of the order O( √n ).
IV. S INGULAR E NTROPY D ISPERSION M ATRICES
When V is rank-deﬁcient, consider the set S (V, ). Suppose for the moment that rank(V) = 1. This is the case
considered in [7] where the source is a DSBS(q). For such
a DSBS, V = v13×3 for v = Var(− log pX1 |X2 (X1 |X2 )) =
Var(− log pX2 |X1 (X2 |X1 )) = Var(− log pX1 ,X2 (X1 , X2 )).
As such, all the probability mass of the degenerate Gaussian
N (0, V) lies in a subspace of dimension one. Therefore, the
√
set S (V, ) = {z ∈ R3 : z ≥ vQ−1 ( )1} is axis-aligned.
v −1
The quantity
( ) is the rate redundancy [4]–[7] for
nQ
ﬁxed-length SW coding in the ﬁnite blocklength regime for
a DMMS for which rank(V) = 1. In this case, the bounds
in (5) and (6) (up to O( log n ) factors) degenerates to
n

n
n
for all n and for any γ > 0. Recall that h(X1 , X2 ) is the
n
n
entropy density vector in (4) evaluated at (X1 , X2 ). Suppose
that, to the contrary, there exists a rate pair (R1 , R2 ) such
that R ∈ Rout (n, ) but (R1 , R2 ) is (n, )-achievable. Then,
/ √
/
by (6), z := n(R−H+ log n 1) ∈ S (V, ). By the deﬁnition
n
of S (V, ) in (3), z ∈ R3 is such that P(Z ≤ z) < 1 − .
Now consider the probability in (20), denoted as sn :

k=1

0.6
0.6

1
n
n
h(X1 , X2 ) ≤ R + γ1 − 3(2−nγ ), (20)
n

(h(X1k , X2k )−H) ≤ z −

0.7
0.65

0.6

Proof: For the outer bound, [14, Lemma 7.2.2] asserts
(n)
that every (n, 2nR1 , 2nR2 , Pe )-SW code must satisfy

n

0.8

0.7

B. Converse (Outer Bound)

(n)
Pe ≥ 1 − P

SW
(n,ε)−SW
(n,ε)−SI−ED

0.75

0.65

where (a) follows because for xn = xn , the events {xn ∈
˜1
1
1
n
n
B1 (1)}, {˜n ∈ B1 (1)} and {(X1 , X2 ) = (xn , xn )} are
x1
1
2
mutually independent, (b) follows by the union bound and
ˆ x
(c) follows from {˜n : H(˜n , xn ) ∈ T (R, δn )} ⊂ {˜n :
x1
x1
1
2
ˆ xn |xn ) ≤ R1 − δn }. Equality (d) follows from the
H(˜1 2
uniformity in the random binning. In (e), we partitioned the
sum over (xn , xn ) into type classes indexed by Q = QX1 ,X2
¯ ¯
1
2
n
and xn ∈ X1 into sums over stochastic matrices V : X2 → X1
˜1
n
for which the V -shell of a sequence of type QX2 in X2 is not
¯
empty (denoted as V ∈ V (QX2 )). In (f ) we upper bounded
¯
nH(V |Pxn )
2
the cardinality of the V -shell as |TV (xn )| ≤ 2
[15,
2
Lem. 1.2.5]. In (g), we used the Type Counting Lemma [15,
Eq. (2.5.1)]. By using the deﬁnition of δn , (19) gives P(E2 ) ≤
n−1/2 . Similarly P(E3 ) ≤ n−1/2 and P(E4 ) ≤ n−1/2 .
Combining this with (18), the error probability averaged
over the random binning is P(E) ≤ . Hence, there is a
deterministic code whose error probability is no greater than
if the rate pair (R1 , R2 ) belongs to Rin (n, ).

1
(a)
sn = P √
n

0.9

R2

2

(g)

0.95

0.9

V ∈V (QX2 ):
¯
H(V |Pxn )≤R1 −δn

1

0.85

Q (xn ,xn )∈TQ
1
2

1

log n √
√ − nγ 1
n

R≥H+

(v/n) Q−1 ( ) 1,

(22)

where the scalar dispersion v := q(1−q)[log((1−q)/q)]2 . This
reduces to results in previous works [4]–[7]. Our analysis, of
course, applies to all sources. Furthermore, we improve on
the residual term, which is now of the order O( log n ). The
n
case where rank(V) = 2 follows analogously. All the probability mass of N (0, V) is concentrated on a two-dimensional
subspace in R3 and the boundary of the set S (V, ) are not
differentiable. As such only one of the “corners” of S (V, )
will be curved and this will be reﬂected in a result similar
to (22). This argument can be formalized and is done in the
extended version of this work [3].

log n √
530ξ
√
≤ P Z ≤ z − √ − nγ 1 +
n
λmin (V)3/2 n
(c)
log n
1
<1− −O √
+O √
(21)
n
n
(b)

where (a) follows from the deﬁnition of z, (b) follows from the
multidimensional Berry-Ess` en theorem [2] and (c) follows by
e
taking γ := log n and using Taylor’s approximation theorem.
2n
(n)
Uniting (20) and (21) yields Pe > , contradicting the (n, )achievability of (R1 , R2 ) for all n sufﬁciently large.
C. Comments on the proof

V. N UMERICAL E XAMPLES
In this section, we present examples to illustrate R ∗ (n, ).
We neglect the O( log n ) terms throughout; thus we are just
n

Instead of the universal decoder in (8), one could use a
non-universal one by comparing the entropy density vector

4

1.8

−1

1.75

(n,ε)−SW
(n,ε)−SI−ED

1.75

−1.2
(n,ε)−SW
(n,ε)−SI−ED

1.7

−1.4

−2

1.6
1.55

−3

Sum rate

log10(Difference)

Sum rate

1.65

−4

log10(Difference)

1.65

1.7

1.6
1.55
1.5

−1.8
−2
−2.2

1.45

1.5

−1.6

−5

1.4

−2.4

1.4

1.45
500

1000
n

1500

2000

−6

500

1000
n

1500

1.35

2000

2

2.5

3
log (n)

3.5

4

−2.6

2

10

Fig. 2. Comparison between the (n, )-SW equal rate point and the (n, )SI-ED equal rate point and their difference as functions of n. The right plot
shows that the difference decays exponentially.

2.5

3
log (n)

3.5

4

10

Fig. 3. Comparison between the corner rates and their difference. Note that
the x-axis is log10 (n) and the difference decays as Θ(n−1/2 ).

VI. C ONCLUSIONS AND F UTURE W ORK
concerned about Gaussian approximations. The source is taken
to be pX1 ,X2 = [1−3a, a; a, a] where a = 0.1. This source has
a positive-deﬁnite dispersion. In Fig. 1, we plot the boundaries
of the SW region [1] and the boundary of R ∗ (n, ) for =
0.01. We also plot the boundary of the (n, )-region for coding
with side information at encoders and decoder (SI-ED). This
∗
region RSI−ED (n, ) ⊂ R2 is the set of (R1 , R2 ) satisfying
R≥H+

diag(V(pX1 ,X2 )) −1
Q ( ).
n

In this paper, we quantiﬁed the second-order coding rates
of the Slepian-Wolf problem. We showed that these rates are
governed by a so-called entropy dispersion matrix. Admittedly,
our results cannot be described as being ﬁnite blocklength. We
seek to work towards such results in the future and to compare
the accuracy of the Gaussian approximation in Theorem 1 to
upper and lower bounds on the blocklength required to achieve
a target error probability.

(23)

R EFERENCES
[1] D. Slepian and J. K. Wolf, “Noiseless coding of correlated information
sources,” IEEE Trans. on Inf. Th., vol. 19, pp. 471–80, 1973.
[2] V. Bentkus, “On the dependence of the Berry-Esseen bound on dimension,” J. Stat. Planning and Inference, vol. 113, pp. 385 – 402, 2003.
[3] V. Y. F. Tan and O. Kosut, “On the dispersions of three network
information theory problems,” arXiv:1201.3901, Feb 2012, [Online].
[4] D. Baron, M. A. Khojastepour, and R. G. Baraniuk, “Redundancy rates
of Slepian-Wolf coding,” in Allerton Conf., 2004.
[5] D.-K. He, L. A. Lastras-Monta˜ o, E.-H. Yang, A. Jagmohan, and
n
J. Chen, “On the redundancy of Slepian-Wolf coding,” IEEE Trans. on
Inf. Th., vol. 55, no. 12, pp. 5607–27, Dec 2009.
[6] S. Watanabe, R. Matsumoto, and T. Uyematsu, “Strongly secure privacy
ampliﬁcation cannot be obtained by encoder of Slepian-Wolf code,”
IEICE Trans. on Fund. Elec., Comms. and Comp. Sciences, vol. E93.A,
no. 9, pp. 1650–1659, 2010.
[7] S. Sarvotham, D. Baron, and R. G. Baraniuk, “Non-asymptotic performance of symmetric Slepian-Wolf coding,” in Conference on Information Sciences and Systems, 2005.
[8] V. Strassen, “Asymptotische Absch¨ tzungen in Shannons Informationsa
theorie,” in Trans. Third. Prague Conf. Inf. Th., 1962, pp. 689–723.
[9] M. Hayashi, “Second-order asymptotics in ﬁxed-length source coding
and intrinsic randomness,” IEEE Trans. on Inf. Th., vol. 54, pp. 4619–
37, Oct 2008.
[10] Y. Polyanskiy, H. V. Poor, and S. Verd´ , “Channel coding in the ﬁnite
u
blocklength regime,” IEEE Trans. on Inf. Th., vol. 56, pp. 2307 – 59,
May 2010.
[11] V. Kostina and S. Verd´ , “Fixed-length lossy compression in the ﬁnite
u
blocklength regime: Discrete memoryless sources,” in Int. Symp. Inf.
Th., 2011.
[12] A. Ingber and Y. Kochman, “The dispersion of lossy source coding,” in
Data Compression Conference (DCC), 2011.
[13] D. Wang, A. Ingber, and Y. Kochman, “The dispersion of joint sourcechannel coding,” in Allerton Conference, 2011, arXiv:1109.6310.
[14] T. S. Han, Information-Spectrum Methods in Information Theory.
Springer Berlin Heidelberg, Feb 2010.
[15] I. Csisz´ r and J. K¨ rner, Information Theory: Coding Theorems for
a
o
Discrete Memoryless Systems. Akademiai Kiado, 1981.

From Fig. 1, we see that R ∗ (n, ) has a curved boundary,
reﬂecting the correlations among the entropy densities. Also,
it approaches the SW boundary as n grows. The boundaries of
∗
R ∗ (n, ) and RSI−ED (n, ) coincide if R2 meets the condition
in (23) with equality and R1 is large (and vice versa).
There are two interesting “slices” of the plots in Fig. 1.
These are the equal rate slice (along the 45◦ line) and the slice
∗
∗
passing through the origin and a corner point (R1,n , R2,n ) of
∗
RSI−ED (n, ), deﬁned as follows:
∗
∗
R2,n := inf{R2 : (R1 , R2 ) ∈ RSI−ED (n, ) for some R1 }
∗
∗
∗
R1,n := inf{R1 : (R1 , R2,n ) ∈ RSI−ED (n, )}.

(24)

These two slices are indicated by the markers (×, ) in Fig. 1.
The sum rates along both slices are plotted as functions of n in
Figs. 2 and 3 respectively. We observe from Fig. 2 that the two
sum rates on the 45◦ equal rate line approach each other as n
grows. Moreover, empirically we observe (and can prove) that
their difference decays as exp(−Θ(n)), which is subsumed
by the O( log n ) term, i.e., the dispersions are the same. Thus,
n
when n ≥ 103 , there is essentially no loss in performing SW
coding versus cooperative encoding if we wish to optimize
the sum rate. On the other hand, from Fig. 3, we see that the
corresponding difference in corner points decays at a much
slower rate of Θ(n−1/2 ). Thus, the corner rate dispersions
are different and if we wish to operate at this point, SW loses
second-order coding rate relative to the cooperative scenario.
See [3] for further analysis of this point.

5

