Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 18:22:59 2012
ModDate:        Tue Jun 19 12:55:38 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      407740 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566193

Efﬁcient Signature Scheme for Network Coding
Ely Porat∗ and Erez Waisbard∗
∗

Department of Computer Science, Bar-Ilan University, Ramat-Gan, 52900, Israel
Email: {porately,waisbard}@cs.biu.ac.il

creating a new random linear combination of the data that it
already received. Using this method every peer simultaneously
sends and receives partial information about many block, thus
solving the problem of rare blocks.

Abstract—Network coding helps maximize the network
throughput. However, such schemes are also vulnerable to
pollution attacks in which malicious forwarders inject polluted
messages into the system. Traditional cryptographic solution,
such as digital signatures, are not suited for network coding,
in which nodes do not forward the original packets, but rather
linear combinations of the data they received. We describe secure
scheme that uses batch techniques and selective veriﬁcation to
efﬁciently verify the integrity of the received packets. We show
that for real peer-to-peer networks, our scheme is much more
efﬁcient than previously suggested schemes.

A. Securing Network Coding
One can see that network coding improves the network
throughput and works well as long as everyone follows the
protocol. Unfortunately, this is not always the case in real
systems. Real systems need to cope with malicious users who
try to pollute the network. In traditional content distribution
schemes, this problem is easily solved by verifying the integrity of the packets using cryptographic tools such as hash
functions and digital signatures. This is also the case in peerto-peer networks such as BitTorrent where each block of data
has a hash value stored in the .torrent ﬁle. Each packet that is
received by the peer is veriﬁed by computing its hash value
and comparing it to the value that appears in the .torrent ﬁle.
The security of a cryptographic hash function ensures that
it is infeasible to ﬁnd two different blocks that hash to the
same value. The integrity check takes place for each block
separably. This allows early detection of corruption without
having to wait for the entire ﬁle to download and prevents the
further spread of corrupted packets.
Securing network coding schemes is more difﬁcult than
securing traditional peer-to-peer schemes. We ﬁrst note that the
problem of polluting the network is more acute for network
coding schemes as a single corrupted block can corrupt the
entire ﬁle and prevent its reconstruction. Furthermore, such a
pollution quickly propagates throughout the network. Thus, it
is crucial to be able to verify the integrity of incoming packets.
Alas, the effective traditional cryptographic techniques that
secure peer-to-peer networks such as BitTorrent no longer
work. Incoming packets in a network coding scheme are
random linear combinations of the source blocks and can take
many different values. Since one cannot tell in advance which
combination would be used by the forwarding peers, it is
impossible to compute the appropriate hash values in advance.

Index Terms—Network coding; Digital signatures; Homomorphic signatures; Group testing; Peer-to-peer; File sharing

I. I NTRODUCTION
The volume of content that is distributed over the internet
is growing every day. The simple client-server based approach
ﬁnds it difﬁcult to cope with the growing number of clients and
volume of data and there is a growing usage of peer-to-peer
technologies.
A peer-to-peer network uses distributed architecture instead
of relying on a single server. The beneﬁt of this approach is in
the splitting of the workload between many different servers
instead of overloading a single server. BitTorrent [5] is an
example for a popular peer-to-peer network that is used to
distribute large ﬁles over the Internet. It works by splitting
the large ﬁle into smaller blocks and sending these blocks
to peers. Any peer that downloaded a block also becomes a
server to this block and so the number of available sources
increases as more peers download it. However, as efﬁcient as
BitTorrent may be, it may happen that the scheduling scheme
would result in having some blocks available only from a few
peers with slow connection and as a result the completion of
downloading of the entire ﬁle is delayed for many users. Using
network coding as an efﬁcient way to cope with this problem
was suggested in [2], [8].
Network coding [3] was introduced as an efﬁcient alternative to traditional routing that maximizes network throughput
[10], [13], [14]. Network coding improves the spreading of
different blocks in a peer-to-peer network by sending a random
linear combination of the blocks instead of sending a single
block. In such a scheme each peer continues to act as a
server for blocks it has obtained and sends out random linear
combinations of the blocks it has received. After receiving
enough packets a peer would be able to reconstruct the ﬁle
from the system of linear equations. Note that even before
a peer is able to extract the different blocks, it can already
participate in the distributed effort to spread the content by

II. P REVIOUS W ORK
Some attempts have been made to solve this problem. These
attempts largely focus on settings that are different than the
one needed for large distribution ﬁle sharing over the internet.
Trying to solve the problem from an information theoretic
angle, [6], [17] suggested a way to protect the network against
a passive adversary. Clearly this does not provide the required
protection from an adversary in our setting.

1

they publish a commitment to u that allows checking if a
given vector is orthogonal to u without using u directly. We
elaborate on the way it is done when we provide details of our
construction in section III. The number of modular operations
that are required in this scheme is proportional to the size of
the vectors. If one considers a typical ﬁle sharing scenario over
the internet, then a back of the envelope computation indicates
that the veriﬁcation time of an incoming packet is longer than
the communication time to receive it. Our construction aims to
improve the efﬁciency of the veriﬁcation step by signiﬁcantly
reducing the number of modular operations per packet.

Others considered byzantine adversaries [10], [9], [11] and
provide security against a known threshold, but since adversaries can freely join public peer-to-peer network, they can
easily outnumber the byzantine bound.
A different approach is to use homomorphic MACs [1].
Loosely speaking, Homomorphic MACs allows nodes that
do not know the secret key to create a valid tag that can
be authenticated by the end recipient. This allows the end
recipient to discard polluted packets, but does not prevent
their distribution throughout the network. This solution also
assumes the existence of a shared secret between a sender
and a receiver, which is not applicable to many settings. In
particular, most of the popular peer-to-peer networks work in
a mode in which one user shares a copy of his ﬁles with many
users which he does not know in advance.
Homomorphic signatures [18] allow intermediate nodes to
create a signature on a linear combination of the incoming
messages [18] or on any polynomial function of them [4].
Homomorphic signatures start by the source signing the different blocks. Each peer gets a linear combination of the blocks
along with an homomorphic signature of it. The homomorphic
property enables creating a new signature from the previously
received signatures. These signatures are limited to signing
only linear combinations of previously received packets, which
is exactly what we need for network coding. This allows
intermediate nodes to verify the integrity of the messages
they receive and ﬁts well with the adversarial model for peerto-peer ﬁle sharing. However the known schemes are either
inefﬁcient [4] or known to be broken [18].
Another approach by [12] was to use the space orthogonal to
the message space as a mean to verify that incoming messages
are from a valid message space. This method is indeed more
efﬁcient, however, as was noted by [16] the need to securely
distribute the NULL message space before one can efﬁciently
use this scheme puts us in a chicken and egg situation.

III. O UR C ONSTRUCTION
We present a method for boosting the performance of
homomorphic signatures. In this paper we present our construction by applying it to a speciﬁc scheme that was presented
in [19]. Our method signiﬁcantly reduces the number of
modular exponentiations required per packet by introducing
two techniques: Selective veriﬁcation and Batch veriﬁcation.
We start with a large ﬁle F that we wish to share and divide
it into n blocks, denoted v1 , . . . , vn . Each block is viewed as
¯
¯
an element in the m−dimentional vector space Fm , where p
p
and q are large primes such that p is a divisor of q − 1.
Each vector is then assigned with a preﬁx and these augmented vectors v1 , . . . , vn are given by
i

¯
vi = (0, . . . , 1, . . . , 0, vi )
n

m

where the ﬁrst n elements are zero except that the ith one is
1, and vi ∈ Fm is the i-th vector block from the ﬁle.
¯
p
Let us denote the subspace that is spanned by these vectors
as V . Namely:
V = Sp(v1 , . . . , vn )

A. Signing Orthogonal Vector

A. Basic Flow

Zhao, et.al. [19] introduced an interesting scheme in which
a source can ‘sign’ the message in a way that allows intermediate nodes to efﬁciently verify the integrity of the messages
without requiring a prior shared secret. Their scheme works by
splitting the message M into vector blocks (v1 , . . . , vn ) and
looking at the vector space that they span V = Sp(v1 , . . . , vn ).
Then they ﬁnd a random vector u which is orthogonal to the
message space and ‘sign’ it using a standard signature scheme.
The signed vector is sent as part of the public information (the
equivalent of the .torrent ﬁle in the BitTorrent). Using the
linearity property u is guaranteed to be orthogonal to every
linear combination of the message blocks. Upon receiving
an incoming message, the receiving peer veriﬁes that the
incoming message is orthogonal to u as a proof for it being a
valid combination of the original blocks. Note that there are
many vectors that are orthogonal to u, but are not in V . Thus,
it is important to be able to verify that a vector is indeed
orthogonal to u, but without explicitly publishing u. Using
the hardness assumption of the discrete logarithm problem,

The source starts distributing the data by computing a
random linear combinations of the augmented vectors and
sending them to peers. After collecting enough of these vectors
the receiving peer would have n linearly independent blocks
and would be able to reconstruct the original ﬁle. Peers
actively participate in the distribution of the ﬁle (even before
they are able to reconstruct the original ﬁle) by sending new
random linear combinations of the blocks they obtain.
In order to protect against pollution attacks, we use an
orthogonal vector to V as an indication that the incoming
blocks are indeed from V . Noting that the augmented vectors
span a subspace V of Fn+m , there exist many vectors which
p
are orthogonal to V . Let us denote one of these vectors by
u = (u1 , . . . , um+n ). As u ⊥ V , it is also orthogonal to any
linear combination of the augmented vectors. Thus, checking
that an incoming block is orthogonal to u would serve as an
indication that it belongs to V . Naturally, the peers would
need to know u before they can verify the authenticity of the
incoming blocks. The vector u also needs to be authenticated

2

Where each of the Ai s is random2 (m + n) × (1 + )n matrix
over F∗ .
p
2) The Algorithms: We now describe the three key components of our scheme. We start with a description of setting
up the public parameters and in particular the veriﬁcation
vector. We then specify the sending algorithm and veriﬁcation
algorithm. We note that there is no change in the sending
algorithm when compared to [19]. The change is in the
veriﬁcation step.

and this can be done by having u digitally signed by s (using
any standard digital signature scheme).
However, if an attacker knows u then he can easily ﬁnd
v ∈ V for which < u, v >= 0. Thus we hide u by utilizing
/
the hardness assumption of the discrete logarithm problem.
Instead of using u directly, s picks a large prime q, such that
p is a divisor of q − 1 and a generator g of a group of order
p in F∗ . Then s picks a secret set of random elements in
q
F∗ : {αi }i=1,...,m+n and publishes {hi = g αi }i=1,...,m+n after
p
digitally signing it (again, using any standard digital signature
scheme). Then s ﬁnds u1 , . . . , um+n orthogonal to all vectors
u
u
in V and computes a vector x = α1 , . . . , αn+m , which he also
1
n+m
signs.
When a node receives a vector w, it computes

• Computing orthogonal vector: Instead of ﬁnding a vector
u that is orthogonal to every vi in the original ﬁle, we ﬁrst
extend the matrix using error correcting code (ECC).
We look at the resulting matrix E and break it into
blocks of size (1 + )n:

m+n

hxi wi
i

d=

(I|M ) → (B1 |B2 | · · · |B )

i=1

Where each Bi = (I|M )Ai
We then ﬁnd vectors u1 , . . . , u , where uj is orthogonal
to Sp(Bj ). For every vector uj we also pick a secret
j
set of random elements in F∗ : {αi }i=1,...,(1+ )n , compute
p

and veriﬁes that d = 1 and that x is properly signed1 .
B. Selective Veriﬁcation
The ﬁrst idea that we introduce is the selective veriﬁcation.
Loosely speaking, we are going to verify only part of the
wi , but still get a high level of assurance while checking a
signiﬁcantly smaller number of bits. In order to ensure that
one cannot avoid this selective check by corrupting a small
number of bits, we are using error correcting codes that expand
the incoming messages in a way that any number of corrupted
bits is reﬂected in many of the expanded bits.
The use of the error correcting code allows us to break the
resulting expansion into blocks, each of length (1 + )n, and
to do the veriﬁcation in one or more of these blocks. The idea
is that for the right values of , checking over one or more
blocks is much faster than verifying the full length of w. We
use an efﬁcient construction of the code to ensure locality in
the computation. Namely in order to verify a single block, we
do not need to compute the full expansion of all the blocks.
A key observation is that the probability of an attacker to
pass this veriﬁcation using a corrupted vector is small as the
number of possible ”legal” blocks is much smaller than .
We stress that although we use error correcting codes in
the veriﬁcation step, there is no change in the length of the
messages that traverse the network. Thus, with the exception
of the initial signature (that is part of the metadata) the
communication complexity remains the same as in [19].
1) The Error correcting code: We start by deﬁning the error
correcting code that we use. While a known code like the
Reed-Solomon code may be used with a small tweak, we have
chosen to explicitly deﬁne a new code that allows to illustrate
the main ideas of our scheme in the simplest way.
(1+ )n
Our random code is a function h : Fm+n → Fp
,
p
deﬁned as follows:

j

{hj = g αi }i=1,...,(1+
i

)n ,

compute {xj =
1

uj
i
}
αj i=1,...,(1+ )n
i

and publish {xj } and {hj } along with their digital signai
i
tures.
• Sending a message: Sending a message is done as usual
in network coding schemes by picking at random ai ∈ F∗
p
and sending v = ai vi .
i

• Message veriﬁcation: Upon receiving a vector w, the
receiver picks a random j ∈ {1, . . . , } as the index of the
block to verify and applies the ECC locally (i.e. computes
only wAj ).
Let
wAj = (z1 , . . . , z(1+ )n )
It then checks if d =

(1+ )n
i=1

xj z i

hi i

equals to 1.

The speed up we gain by this method is proportional to
the size of the block that we check compared to the size of
the original vector, i.e. roughly (1+ )n . In terms of security,
m+n
it was proven in [19] that under the hardness assumption of
solving the discrete logarithm problem, the scheme is secure
if we were to check all blocks. We are going to check only
portion of the blocks, but we are going to do so after applying
the ECC. Intuitively, the ECC would ensure that even a few
corrupted bits in the input vector would result in plenty of
corrupted blocks. In section IV we show why even under a
worse case scenario we can bound the error probability of our
scheme with δ of our choice.
C. Batch Veriﬁcation
Another performance enhancement comes from performing
batch veriﬁcation. The batch veriﬁcation technique is independent of the selective veriﬁcation technique. It can be used

h(v) = (vA1 | . . . |vA )
1 In practice the signature on x does not need to be veriﬁed each time a
message is received. it sufﬁces to check it once before the ﬁrst messages is
veriﬁed and then only verify that x and the signature remains the same

2 In

3

order to make computations efﬁcient we can pick A to be sparse

combination of the ﬁrst k vectors. After we get an additional
k vectors we would batch all of the 2k vectors using fresh
r1 , . . . , r2k before running selective veriﬁcation on their linear
combination. This way we double check the ﬁrst k vectors
(on different locations), reducing the chance that selective
veriﬁcation fails to spot corrupted vectors.

in conjunction with selective veriﬁcation or independently
(e.g. it can be applied directly to the scheme of [19]). Batch
veriﬁcation works by gathering a few incoming vectors before
performing the veriﬁcation. Only once enough vectors are
gathered, we check all of them in single veriﬁcation operation.
The way we batch the vectors together is done as follows:
Let w1 , . . . , wk be the incoming vectors. We pick random
r1 , . . . , rk and compute w = r1 w1 + . . . + rk wk . Clearly,
if w is a linear combination of vectors that are orthogonal to
u, then w would also be orthogonal to u and it is enough to
verify that w is orthogonal to u. If some of the vectors are
not orthogonal to u, then with high probability w will not be
orthogonal to u.
Since veriﬁcation only occurs once every few packets, the
incoming vectors w1 , . . . , wk should be stored and marked
as unveriﬁed until they pass veriﬁcation. This is important
in cases where veriﬁcation fails and we wish to be able to
identify which of the batched vectors are the corrupted ones
(and perhaps also identify their sender).
As the modular operations are far more expensive than
computing a linear combination of the incoming vectors, if
we batch j vectors, our veriﬁcation time would be j times
faster.
This certainly helps in terms of saving on veriﬁcation
operations, but it also introduces a potential delay. If each node
would need to wait for the arrival of t messages before it can
forward them, then we are introducing a potential bottleneck.
In order to prevent this bottleneck we propose a randomized
forwarding scheme that would forward packets even before
they are checked, but with a lower probability. Before diving
into the details, we use an example to illustrate our point.
Assume that 10% of the incoming packets are polluted and
assume that a forwarding probability of a non veriﬁed packet
is 10%. This means that an unveriﬁed malicious packet would
be forwarded with probability of 1%. The probability further
forwarding corrupted packets quickly drops. In order to make
this enhancement more robust, we suggest not to use a ﬁxed
probability, but rather start with a conservative forwarding
policy that gives little to no chance of forwarding a packet
from an unfamiliar source. Only after a period of time, in
which no corrupted packets received from this source, the
probability of forwarding unveriﬁed packets should increase.

IV. S ECURITY
In this section we prove the security of our construction.
It was proven in [19] that ﬁnding a vector w ∈ V for which
/
m+n xi wi
= 1 is as hard as the discrete log problem. What
hi
i=1
remains to show is that our selective veriﬁcation does not allow
the spreading of polluted ﬁle.
We take a group testing [15] approach to analyzing the
security of our scheme. Using the ECC we deﬁned in III-B
we apply h to all the possible vectors in Sp(V ). Let us denote
the number of vectors in Sp(V ) by k, i.e. k = pn .
Given any input vector y ∈ {x1 , x2 , . . . , xk }, we ask what is
/
the probability that the j−th element in h(y) (denoted h(y)[j])
equals the j−th element in one of the previous xi s. Summing
over all the xi s (using the union bound) we get that
P r[h(y)[j] = h(xi )[j]] =

P r[xi Aj = yAj ]

i

i

Since Aj is random and since xi = y, the event in which
(xi − y)Aj = ¯ occurs with probability
0
1
( )(1+
p

)n

=(

1 1+
1
)
= 1+
n
p
k

Thus
P r[xi Aj = yAj ] = k
i

1
1
=
k 1+
k

The probability that δ elements in h(y) appear in h(xi ) is
1
bounded by δ ( k )δ
By taking the union bound over all possible values of y
(which is bounded by pm+n ) we get that the probability for
1
existence of a ”bad” y is bounded by pm+n δ ( k )δ
In order to give a sense of security for practical parameters,
lets assume that we want to bound3 this probability by 2−40 .
In this case we get that
pm+n

D. Combining Selective and Batch Veriﬁcation
The two techniques introduced above can work in conjunction. It can be readily seen that the overall performance boost is
the product of the boost provided be each individual technique.
We now make a note on how to combine the two techniques
in a way that constantly increases the level of assurance. The
naive way of combining both techniques is to batch k vectors
each time, then perform a selective veriﬁcation on their linear
combination and later to repeat the process with a different
group of incoming vectors. Since selective veriﬁcation may
err, a better approach is to add previously veriﬁed vectors to
the batch. For example, if we batch k vectors at a time, then
initially we would do a selective veriﬁcation on the linear

δ

1
k

δ

< 2−40

i.e.

e δ
< 2−40
δk
Looking at the discrete log of the above equation we get
pm+n

m + n + δ (logp e − logp δ − logp k ) < −40
and since k = (pn ) we get
m + n + δ (logp e − logp δ) − n) < −40
3 We note that the error probability is determined by the ﬁle that we protect
and the ECC that we use and not by the adversary’s strategy.

4

Since logp δ and logp e are constants smaller than 1 (assuming a typical p to be about 21024 ), we get that

In our method, let us take = 1. We only have (1 + )n
modular exponentiations, i.e. 2 · 256 = 512 modular exponentiations, which are at least 512 ∗ 1024 = 524288
modular multiplications. In addition we have our ECC, which
takes (m + n)(1 + )n modular multiplications assuming the
expansion matrix is full. However, if we take a sparse matrix
with only 10 elements in each row5 , then we get that there
are only 10(m + n) = 330240 modular multiplication. So
the total number of modular multiplication in our scheme is
about 854528, which makes our scheme about 40 times faster
than [19] with only using the selective check method. Further
performance gain can be achieved by using batch veriﬁcation.
If we batch in groups of ﬁve, then we would get that our
scheme is about 5 ∗ 40 = 200 faster than [19].

m + n + δ(− n) < −41
Therefore, in order to have an error smaller than δ we would
need to use > 41+m+n .
δ n
We now consider the size of the orthogonal vector that
1
achieves the above security. If we take for example δ = 10
and we wish to use blocks of size 2n then in order to get the
security assurance for = 1 we would require our orthogonal
vector to be of size 21(m + n). If we allow ourselves bigger
blocks of size 3n, then we take = 2 , then we can see that
it is sufﬁcient to stretch our orthogonal vector to the length of
16(m + n).
The different tradeoffs between and the stretch can be
viewed in the graph below.

Fig. 1.

R EFERENCES
[1] S. Agrawal and D. Boneh. Homomorphic MACs: MAC-Based Integrity for Network Coding. In ACNS 2009. pages 292–305, Springer
Verlag.
[2] S. Acedanski, S. Deb, M. Medard, and R. Koetter, How good is
random linear coding based distributed network storage?, in Proc. 1st
Workshop on Network Coding, Theory, and Applications (Netcod05),
Riva delGarda, Italy, April 2005.
[3] R. Ahlswede, N. Cai, S. Li, and R. Yeung. In Network information
ﬂow. IEEE Transactions on Information Theory, 46(4). pages 12041216, 2000.
[4] D. Boneh, D. M Freeman. Homomorphic Signatures for Polynomial
Functions. In EUROCRYPT 2011. pages 149–168
[5] BitTorrent ﬁle sharing protocol. http://www.BitTorrent.com
[6] N. Cai and R. Yeung. Network coding and error correction. In Proc.
2002 IEEE Information Theory Workshop 2002. pages 119-122
[7] R. Gennaro, J. Katz, H. Krawczyk, T. Rabin. Secure Network Coding
over the Integers. In Public Key Cryptography 2010. pages 142–160
[8] C. Gkantsidis and P. Rodriguez, Network coding for large scale
content distribution, in Proc. IEEE INFOCOM05, Miami, FL, March
2005.
[9] T. Ho, B. Leong, R. Koetter, and M. Medard. Byzantine Modiﬁcation
Detection in Multicast Networks using Randomized Network Coding.
In IEEE Proc. ISIT 2004.
[10] T. Ho, M. Medard, M. Effros, and D. Karger, The beneﬁts of coding
over routing in a randomized setting, in Proc. IEEE Symposium on
Information Theory (ISIT03), Kanagawa, Japan, July 2003.
[11] S. Jaggi, M. Langberg, S. Katti, T. Ho, D. Katabi, M. Medard Resilient
network coding in the presence of Byzantine adversaries. In Proc.
INFOCOM, pages 616624, 2007.
[12] E. Kehdi, B. Li, ”Null Keys: Limiting Malicious Attacks Via Null
Space Properties of Network Coding.” in INFOCOM 2009 pages
1224–1232
[13] Z. Li and B. Li, Network coding: the case of multiple unicast sessions,in Proc. 42th Annual Allerton Conference on Communication,
Control, and Computing, September-October, 2004.
[14] D. S. Lun, M. Medard, and R. Koetter, Network coding for efﬁcient
wireless unicast, in Proc. 2006 International Zurich Seminar on
Communications (IZS06), Zurich, Switzerland, February 2006
[15] E. Porat, A. Rothschild, ”Explicit Non-adaptive Combinatorial Group
Testing Schemes”. In ICALP (1) 2008 pages 748–759
[16] Y. Wang, Insecure ”Provable Secure Network Coding”. In IACR
Cryptology ePrint Archive 2009: 504, 2009
[17] R. Yeung and N. Cai. Network Error Correction, In Basic Concepts
and Upper Bounds. Commun. Inf. Syst. 6(1), 2006 pages 19-35
[18] Z. Yu, T. Wei, B. Ramkumar, and Y. Guan. An Efﬁcient Signaturebased Scheme for Securing Network Coding against Pollution
Attacks. In IEEE INFOCOM, 2008.
[19] F. Zhao, T. Kalker, M. Medard, K. Han. Signatures for Content
Distribution with Network Coding. In Proc. 2007 IEEE ISIT

epsilon vs stretch

A. Further security considerations
It was mentioned both in [19] and in [16] that the public
key cannot be used for too many ﬁles as it would allow to
collect enough equations to compute u. Thus a different public
key needs to be associated with every new ﬁle. Since this
information can be part of a .torrent ﬁle the only affect is
that it adds a little overhead to the original data that is being
transferred.
V. P ERFORMANCE
We now analyze the performance gains of our scheme
and compare it to [19]. For this analysis we use common
parameters from the Bittorrent peer-to-peer network.
The ﬁle size that we are going to share is 1GB. We are
assuming a modulus q of 1024 bits4 and we get that the packet
size is m = 4M B (i.e. 4096*8 values from F∗ and there are
p
n = 256 vectors in our matrix).
In the original scheme we had m + n modular exponentiations, which are at least logq ∗ (m + n) = 1024(m + n) =
33816576 modular multiplications.
4 We

5 A good heuristic would be having a few elements in each row. In our
computation we used 10 as a conservative value.

note that p and q are of the same order of magnitude

5

