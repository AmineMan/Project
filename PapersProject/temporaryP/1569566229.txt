Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 11:56:50 2012
ModDate:        Tue Jun 19 12:56:38 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      481911 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566229

Capacity Regions of Partly Asynchronous Multiple
Access Channels
Lóránt Farkas

Tamás Kói

Budapest University of Technology and Economics
Email: lfarkas@math.bme.hu

Budapest University of Technology and Economics
Email: koitomi@math.bme.hu

In words, for the ﬁrst two senders and for the last two senders
the channel is almost a simple additive channel but the output
is completely random if both of the ﬁrst two or both of the
last two senders send 1.
When the synchronized groups of senders are {1, 2} and
{3, 4} then the capacity region of this model is equal to the
capacity region of the synchronous model, given by R1 +R2 ≤
1, R3 + R4 ≤ 1. The strategy to achieve this is simple: one of
the senders of the group {1, 2} and one of those of the group
{3, 4} sends 0 in each time slot. However, if the synchronized
groups are {1, 2}, {3}, {4} then the capacity region becomes
smaller, because senders 3 and 4, if sending with positive rates,
cannot ensure that in each time slot at least one of them sends
0. Note also that in the totally asynchronous case the capacity
region shrinks further.

Abstract—Frame asynchronous discrete memoryless multiple
access channels are analyzed, where some groups of senders are
synchronized but the groups are not synchronized with each
other. A single-letter characterization of the capacity region is
obtained.
Index Terms—capacity region, frame asynchronous channels,
multi-access, partly asynchronous, polymatroid, rate-splitting,
successive decoding

I. I NTRODUCTION
Multiple access channels (MACs) are most frequently studied under the assumption that the senders cannot communicate
with each other but are able to maintain frame synchronism.
An asynchronous MAC (AMAC) arises when this assumption
fails, causing unknown delays between the starting times of
the codewords of the different senders, see [1], [9], [7].
In [4] the authors gave a general formalization of AMACs,
ﬁlled some gaps in the previous literature, and determined
the capacity region for a new (rather artiﬁcial) model, which
was strictly between those of the corresponding synchronous
and totally asynchronous models. The extended version [5]
gave a single-letter characterization of the capacity region
also for a more practical model, with 3 senders, two of them
synchronous and the third one not synchronous with them.
Generalizing the result in [5], here a single letter characterization is given for the capacity region of discrete memoryless
partially asynchronous multiple access channels (PAMACs).
These are AMACs with the senders divided into groups, the
senders belonging to the same group are synchronized but the
groups are not synchronized with each other.

III. M ODEL OF CODING FOR THE PAMAC
This section is a shorter version of Section 2 in [5] with
addition of the formal deﬁnition of PAMAC.
A K-senders asynchronous discrete memoryless multipleaccess channel (K-AMAC) is deﬁned in terms of K ﬁnite
input alphabets Xi , i ∈ {1, 2, . . . , K}, a ﬁnite output alphabet
Y, and a stochastic matrix W : X1 × X2 × · · · × XK → Y
describing the probability distribution of the output given the
inputs.
Deﬁnition 1. A code-book system of block-length n with rate
vector (R1 , R2 , . . . , RK ) for a given K-AMAC W consists
of K code-books C1 , C2 , . . . , CK , where the code-book Cm
of the m-th sender has 2nRm codewords of length n whose
symbols are from Xm .

II. E XAMPLE
The following simple example shows that the capacity region can depend on how the senders cluster into synchronized
groups.
Consider a channel W with 4 senders, with input alphabets
X1 = X2 = X3 = X4 = {0, 1} and output alphabet Y =
{0, 1} × {0, 1}. Let W (y|x1 , x2 , x3 , x4 ) be the following:
 1
 4 if x1 = x2 = 1 or x3 = x4 = 1
1 if
y = (x1 + x2 , x3 + x4 )

0
otherwise.

The system is symbol synchronized but not frame synchronized. The differences between the timescale of the receiver
and the timescales of the senders are represented by a K-tuple
of delays as in Deﬁnition 3.
The senders have two-way inﬁnite sequences of random
messages, and assign codewords to their consecutive messages.
The codewords go through the channel. The sequences of
the senders’ codewords and hence also the output symbol sequence are two-way inﬁnite sequences. Fix the location of the
0-th output symbol. The message of sender m ∈ {1, 2, . . . , K}
whose codeword affects the 0’th output is denoted by Mm,0 .
This restricts the delays to be in the set {0, 1, . . . , n − 1}.
Formally, we use the following deﬁnitions:

The work of Tamás Kói was supported by the project "Talent care and
cultivation in the scientiﬁc workshops of BME" (TÁMOP - 4.2.2.B-10/12010-0009) and by the scientiﬁc program of the "Development of qualityoriented and harmonized R+D+I strategy and functional model at BME"
project (TÁMOP - 4.2.1/B-09/1/KMR-2010-0002).

1

Deﬁnition 5. An informed L-block decoder, L ∈ Z+ ,
is deﬁned as a function which assigns to each
(2Ln + 1)-tuple {yl , l ∈ {−Ln, . . . , 0, . . . , Ln}} of
possible output realizations and each realization of
D(n) = (D1 (n), D2 (n), . . . , DK (n)) a K-tuple of messages
ˆ
{Mm,0 , m ∈ {1, 2, . . . , K}}.
The codebooks and the decoder form an n-length codingdecoding system.
n
Deﬁnition 6. The maximal error Pe is
K

Figure 1: 4-PAMAC with partition {1, 2}, {3, 4}

max
n

dn :Pr{D =dn }>0

Deﬁnition 2. For each integer j ∈ Z and for each m ∈
{1, 2, . . . , K} let Mm,j be a uniformly distributed random
variable taking values in the set {1, 2, . . . , 2nRm }. All these
random variables are independent of each other. The two-way
inﬁnite sequence {Mm,j , j ∈ Z} represents the message ﬂow
sent by the m-th sender. For each integer j ∈ Z and for each
m ∈ {1, 2, . . . , K} let Xm,j denote the codeword assigned to
Mm,j in the code-book of sender m. Let Xm,nj+i be the i-th
symbol of Xm,j where i ∈ {0, 1, . . . , n − 1}.

ˆ
Mm,0 = Mm,0 |Dn = dn

Pr
m=1

.
(1)

Deﬁnition 7. Corresponding to the delay system D, the rate
vector (R1 , R2 , . . . , RK ) is achievable if for every ε > 0, δ >
0 for all N ∈ Z+ there exists a coding system with blocklength
n > N with rates coordinate-wise exceeding (R1 − δ, R2 −
δ, . . . , RK −δ) and with error less than ε. The set of achievable
rate tuples is the capacity region of the K-AMAC.
Remark 1. In the deﬁnition above we used the ’optimistic’
deﬁnition of capacity region, rather than the more usual
’pessimistic one’1 . The reason is that this deﬁnition is better
suited for certain AMAC models (see [4],[5]).

Deﬁnition 3. For each n ∈ Z+ , let
D(n) = (D1 (n), D2 (n), . . . , DK (n))

Note that the above models focus on the decoding of the 0’th
messages of the senders. It is assumed that the same but shifted
decoding procedure occurs at the output points {nk, k ∈ Z}.
After having discussed the K-AMAC in general we give
the deﬁnition of K-PAMAC. This special case is analyzed in
this paper.

be a K-tuple of random variables, not necessarily independent
of each other but independent of all previously deﬁned random
variables, taking values in the set {0, 1, . . . , n − 1}. Dm (n)
will represent the delay of sender m relative to the receiver’s
timescale. The joint distribution of delays is known to the
senders and the receiver. The realizations of the random
variables D1 (n), D2 (n), . . . , DK (n) are not known to the
senders and may be known or unknown to the receiver. The
sequence D = {D(1), D(2), . . . , D(n), . . . } will be called
the delay system.

Deﬁnition 8. The K-PAMAC is deﬁned by the following delay system. Let G1 , G2 , . . . , Gl be a ﬁxed partition of the set {1, 2, . . . , K} of the senders. Let
DG1 (n), DG2 (n), . . . , DGl (n) be independent random variables uniformly distributed on the set {0, 1, . . . , n−1}. For all
m ∈ {1, 2, . . . , K}, deﬁne Dm (n) = DGj (n) where m ∈ Gj .

Deﬁnition 4. Let Ynj+i be the output random variable of
the channel with transition matrix W when the inputs are
X1,nj+i+D1 (n) , X2,nj+i+D2 (n) ,. . . , XK,nj+i+DK (n) where
i ∈ {0, 1, . . . , n − 1}.

Note that the classical synchronous MAC, and the totally
asynchronous MAC are special cases of PAMAC: in the ﬁrst
case all senders belong to the same group, in the latter case
all senders belong to different groups.

It is possible to deﬁne the decoder in several ways depending on whether the receiver is informed or uninformed on
the realization of the delays, and whether the whole inﬁnite
output sequence or only a ﬁnite part may be used to estimate
the messages, see [4],[5]. The error terms can be deﬁned via
averaging or maximizing over delays; averaging over messages
is understood in both cases (for maximum error over messages,
the capacity region is unknown even for synchronous MACs).
Due to space limitation, attention will be restricted to informed
L-block decoders considering maximal error deﬁned below; it
can be shown similarly to [4],[5], that other possibilities lead
to the same capacity region in case of PAMAC. In particular,
additional technical difﬁculties in case of uninformed decoder
can be overcome as in [4], [5].

IV. C ODING THEOREM FOR THE PAMAC
This section relies on the results of Grant, Rimoldi, Urbanke, Whiting in [6] and of Rimoldi in [10]. It is assumed that
the reader is familiar with the concepts of successive decoding
and rate splitting. For a non-empty subset S ⊂ {1, 2, . . . , K}
of the senders, as in [6] write
[i] = {1, 2, . . . , i} , S c = [K] \ S
XS = (Xi )i∈S , R(S) =

Ri ,

(2)
(3)

i∈S
1 In short, in the ’optimistic’ deﬁnition it is enough to show that there is a
"good" coding/decoding system for a sequence of blocklength nk → ∞.

2

and R+ denotes the non-negative real numbers.

this way with appropriate pair (π, t). Note that the vertices
Rπ,t need not be all distinct. See also [8].
The following Lemma is the generalization of Lemma 14.4+
in [2].

Theorem 1. For a K-PAMAC as in Deﬁnition 8, a rate
tuple (R1 , R2 , . . . , RK ) is achievable if and only if there
exist random variables (U1 , U2 , . . . Ul , X1 , X2 , . . . , XK , Y )
with values in U1 × U2 × · · · × Ul × X1 × · · · × XK × Y
where |Ui | ≤ 2K − 1, i = 1, 2, . . . , l, whose joint distribution factorizes as p(u1 , u2 , . . . , ul , x1 , x2 , . . . , xK , y) =
l
i=1 p(ui )
r∈Gi p(xr |ui )W (y|x1 , x2 , . . . , xK ), such that
for all S ⊆ {1, 2, . . . , K}:
0 ≤ R(S) ≤ I(XS ∧ Y |XS c , U1 , U2 , . . . , Ul ).

Lemma 1. For all i ∈ {1, 2, . . . , k} let W i be arbitrary
i
multiple access channels with K senders and let pX1 ×
i
i
pX2 × · · · × pXK be arbitrary product distribution on the
inputs of W i . Then a vector R = (R1 , R2 , . . . , RK ) equals a
convex combination with weight vector α = (α1 , α2 , . . . , αk )
i
i
i
of vectors Ri ∈ R W i ; pX1 × pX2 × · · · × pXK , i ∈
2
{1, . . . , k}, if and only if R is contained in the set R(α) =
R(α1 , α2 , . . . , αk ) deﬁned by the following inequalities:

(4)

Furthermore, if |Gi | = 1 then one can take |Ui | = 1 thus
Ui = constant.

k

Remark 2. Note that Theorem 1 gives back the synchronous
and the totally asynchronous MAC coding theorems.

Remark 3. The proof of Theorem 1 will use the case W 1 =
W 2 = · · · = W k = W , the general case is needed in the
proof of Lemma 3.
Proof of Lemma 1: The set R(α) determined by the
conditions (7) is a polymatroid. By [3] it can be seen that its
k
vertices can be written as vπ,t = i=1 αi Ri;π,t , where Ri;π,t
i
i
i
is the vertex of R W i ; pX1 × pX2 × · · · × pXK described
by (π, t), as in the paragraph after Deﬁnition 9. Hence
the vertices of R(α) are contained in the (convex) set of
convex combinations with weights αi of vectors in the sets
i
i
i
R W i ; pX1 × pX2 × · · · × pXK , therefore so is the whole
R(α). The reverse inclusion is obvious.
The sets R(α) are generalizations of the sets
R [W ; pX1 × pX2 × · · · × pXK ], which correspond to α
degenerate (k = 1). Since R(α) is a polymatroid, it has
a face Dom(R(α)) which dominates3 all points in R(α),
and whose points cannot be dominated by other points
of R(α). This dominant face Dom(R(α)) consists of
the rate tuples (R1 , R2 , . . . , RK ) from R(α) for which
k
i
i
i
i
R1 + R2 + · · · + RK =
i=1 αi I(X1 , X2 , . . . , XK ∧ Y ).
With the notation in the proof of Lemma 1 the vertices of
Dom(R(α)) are vπ,1 , where π runs over the orderings of
[K]. The vertex vπ,1 will be denoted by vπ for the remainder
of this paper.
Proof of Theorem 1, achievability, ﬁrst part: For notational simplicity, the proof will be sketched for the case l = 2,
writing U and V for U1 and U2 .
i,j
i,j
i,j
Let (X1 , X2 , . . . , XK , Y i,j ) be random variables whose
joint distribution is the same as the conditional joint distribution of (X1 , X2 , . . . , XK , Y ) given (U = i, V = j),
i ∈ {1, 2, . . . , |U|}, j ∈ {1, 2, . . . , |V|}. Note that if l ∈
G1 then the distribution of Xli,j doesn’t depend on j, if
l ∈ G2 it doesn’t depend on i. The random variables
i,j
i,j
i,j
X1 , X2 , . . . , XK are independent and Y i,j is connected

I(XS ∧ Y |XS c , U1 , U2 , . . . , Ul ) =
= I(XS , Ui ∧ Y |XS c , U1 , U2 , . . . , Ui−1 , Ui+1 , . . . , Ul )
− I(Ui ∧ Y |XS c , U1 , U2 , . . . , Ui−1 , Ui+1 , . . . , Ul )
(5)

where in the last inequality the Markov relation Ui X[K] Y
is used. Hence in this case the random variable Ui can be
omitted or equivalently it can be assumed that |Ui | = 1. Note
that if {Gi } = {i1 , i2 , . . . , is } with s > 1, then Ui cannot be
omitted because the random variables Xi1 , Xi2 , . . . , Xis are
not necessarily independent.
The achievability proof of Theorem 1 relies on the following
considerations.
Deﬁnition 9. Let R [W ; pX1 × pX2 × · · · × pXK ] be the set
of rate tuples R ∈ (R+ )K such that for all S ⊆ {1, 2, . . . , K}
R(S) ≤ I(XS ∧ Y |XS c ),

(7)

i=1

The converse part of Theorem 1 follows from the general
converse in [5]. The cardinality bounds on the auxiliary random variables follow from the Support Lemma 15.4 of [2]. The
stronger cardinality bounds for auxiliary variables belonging
to one-sender groups are obtained as follows. Let us assume
that the random variables U1 , U2 , . . . Ul , X1 , X2 , . . . , XK , Y
fulﬁll the conditions of Theorem 1, and assume that for some
i ∈ {1, 2, . . . , l} the group Gi consists of one sender, the j’th.
Then the following is true for all S ⊂ [K]:

≤ I(XS ∧ Y |XS c , U1 , U2 , . . . , Ui−1 , Ui+1 , . . . , Ul ),

i
i
αi I(XS ∧ Y i |XS c ), S ⊆ [K] .

0 ≤ R(S) ≤

(6)

where Y and X1 , X2 , . . . , XK are connected by the channel
W , and the joint distribution of X1 , X2 , . . . , XK is the product
distribution pX1 × pX2 × · · · × pXK .
The set R [W ; pX1 × pX2 × · · · × pXK ] is a special convex
polytope: a polymatroid, see [3]. By [3], [6] its vertices are
described as follows. Let π = (π1 , π2 , . . . , πK ) be an ordering
π,t
of [K], and t ∈ [K]. For all i ∈ [K] let Rπi be equal to
I(Xπi ∧ Y |X{π1 ,...,πi−1 } ) if i ≥ t and 0 otherwise. Then the
π,t
π,t
π,t
rate vector Rπ,t = (R1 , R2 , . . . , RK ) is a vertex, and all
vertices of R [W ; pX1 × pX2 × · · · × pXK ] can be written in

2 For

brevity we do not indicate all variables on which this set depends.
˜
˜
is dominated by (R1 , R2 , . . . , RK ) if and only if
˜
Ri ≤ Ri for all i.
3 (R
˜

3

1 , R2 , . . . , RK )

with them through the channel W . Using the deﬁnition of
conditional mutual information we can write:

senders from the group G2 to generate their codewords.
The purpose of this randomization pattern is to make sure
that, irrespective of the delay realization, for all i, j, exactly
the fraction p(U = i)p(V = j) of the output sequence is
i,j
i,j
i,j
created using the input variables (X1 , X2 , . . . , XK ).
Above, the distributions of the independent random symbols
of the codebooks are determined. The number of codewords
in the b th sender’s codebook is the following:

I(XS ∧ Y |XS c , U, V )
|U | |V|
i,j
i,j
p(U = i)p(V = j)I(XS ∧ Y i,j |XS c ).

=

(8)

i=1 j=1

The above equation shows via Lemma 1 that it is
enough to prove that the convex combinations of the sets
R W ; pX i,j × pX i,j × · · · × pX i,j , i ∈ {1, 2, . . . , |U|}, j ∈
1
2
K
{1, 2, . . . , |V|} with weights αij = p(U = i)p(V = j), can
be achieved. The set R(α) in Lemma 1 with α consisting
of these weights will be denoted by R(U, V ). It is enough to
achieve the dominant face of this set (see Deﬁnition 7). Having
this in mind, the proof of Theorem 1 will be continued after
establishing two key lemmas.
In the Appendix of [6] it is shown that the vertex Rπ
of Dom(R [W ; pX1 × pX2 × · · · × pXK ]) can be achieved by
successive decoding with order π in the totally asynchronous
case. The following lemma extends that result. From technical
point of view this is the main result of this paper.

n

2

. . . ,. . . ,. . . ,. . .
. . . ,. . . ,. . . ,. . .

i,j
i,j
p(U =i)p(V =j)I(Xb ∧Y i,j |X{1,...,b−1} )−2δ

. (9)

i,j
W1 (y|x1 ) =

···

=
x2 ∈X2

i,j
pXr (xr )W (y|x1 , x2 , . . . , xK ).

xK ∈XK r∈[K]\{1}

(10)
We have assumed in Deﬁnition 5 that the delay realizations (of senders) are known to the decoder. From the
delay of the ﬁrst sender, the decoder knows that the output
n tuple (Y−D1 (n) , . . . , Yn−1−D1 (n) ) corresponds to the 0 th
message of the ﬁrst sender. For all i ∈ {1, 2, . . . , |U|}, j ∈
{1, 2, . . . , |V|}, from the delays of the other senders, the decoder knows which coordinates of the codeword corresponding
i,j
to the 0 th message went through channel W1 . Denote the
i,j
set of these coordinates by T
⊂ {0, 1, . . . , n − 1}. Note
that |T i,j | is equal to n(p(U = i)p(V = j)) independently
of the delay realizations. Note also that the symbols in
T i,j of the ﬁrst sender’s codewords were generated using
i,j
distribution X1 . The decoder uses joint typicality to decide which is the 0’th message of the ﬁrst sender. We say
that the h’th codeword of the ﬁrst sender is typical in the
window (Y−D1 (n) , Y−D1 (n)+1 . . . , Yn−1−D1 (n) ) if the following is true for all i ∈ {1, 2, . . . , |U|}, j ∈ {1, 2, . . . , |V|}:
the subsequences of the h’th codeword with coordinates
in T i,j and the subsequence of the same coordinates of
(Y−D1 (n) , Y−D1 (n)+1 , . . . , Yn−1−D1 (n) ) are jointly δ typical
i,j
i,j
according to input distribution X1 and the channel W1 . If
h is the only codeword of the ﬁrst sender which is typical,
then the decoder’s estimation is h for the 0’th message. The
decoder decodes not just the 0’th codeword but, in a similar
manner, all codewords which are necessary for the following
successive steps.
The following successive steps work the same way. The
difference is that the already decoded codewords from the
previous steps become part of the output instead of considering
them as noise.
The maximal error over delays of the random code can be
shown to be exponentially small in n as in the Appendix of

Proof: It is enough to prove the statement for the
identity ordering πid = (1, 2, . . . , K). It will be shown
|U |
|V|
(i,j);πid
that vπid =
can
i=1
j=1 p(U = i)p(V = j)R
be achieved with decoding order πid . Using the deﬁnition
π
of R(i,j);πid , the b’th coordinate vb id of vπid is equal to
|U |
|V|
i,j
i,j
i,j
|X{1,...,b−1} ).
i=1
j=1 p(U = i)p(V = j)I(Xb ∧ Y
Without loss of generality assume that p(U = i) and p(V = j)
are rational numbers.
Let the senders choose their codebooks randomly. All
symbols of the codebooks are independent but not of the
same distribution. The key idea is a deterministic randomization pattern which is constructed in the following way. Let
j1 , j2 , . . . , jk be a sequence of elements of V whose empirical
distribution is equal to the distribution of V . Let i1 , i2 , . . . , il
be a sequence of elements of U whose empirical distribution
is equal to the distribution of U . Note that the integers k and l
depend on the distributions of U and V . Using these sequences
let us deﬁne the following randomization pattern of length kl:
i2 ,i2 ,. . . ,i2
j1 ,j2 ,. . . ,jk

|V|
j=1

The senders use the above random codebooks. The decoder
performs successive decoding as in the Appendix of [6] and
as in [5]. First, the decoder decodes the codewords of the ﬁrst
sender considering the inputs of the other senders as noise.
Hence from the receiver’s point of view the codewords of the
ﬁrst sender go through |U||V| different channels according to
the input distributions of the other senders. These channels are
the following (i ∈ {1, 2, . . . , |U|}, j ∈ {1, 2, . . . , |V|}):

Lemma 2. Given a K-PAMAC W : X1 ×X2 ×· · ·×XK → Y
with partition {G1 , G2 } of [K], for each ordering π of [K] the
vertex vπ of Dom(R(U, V )) can be achieved by successive
decoding with order π.

i1 ,i1 ,. . . ,i1
j1 ,j2 ,. . . ,jk

|U |
i=1

il , il ,. . . , il
j1 ,j2 ,. . . ,jk

We will use those n as codeword lengths which are divisible
by the length kl of the pattern. The ﬁrst row shows which
distributions are used by the senders from the group G1 to
generate their codewords in their codebooks. Namely, if sender
b belongs to group G1 , he generates the ﬁrst k symbols
i
i
distributed as Xb1 , the next k symbols as Xb2 , ..., etc.;after the
kl’th symbol, this pattern is repeated periodically. Similarly,
the second row shows which distributions are used by the

4

the convex combination with weight vector α of the sets
i
i
i
i
Dom(R Wτi
|{K+1} ; pX1 × pX2 × · · · × pXKa × pXKb ).
i
i
Here W is obtained from W by splitting the K’th
i
sender, and Wτi
|{K+1} is a channel derived from W
i
whose sender set is τ , the (K + 1)’th input of W is
part of its output, and the remaining inputs of W i are
considered as noise. R [K]\τ is in the dominant face of
the convex combination with weight vector α of the sets
i
i
i
i
i
Dom(R W[K]\τ |{K+1}∪τ ; pX1 × pX2 × · · · × pXKa × pXKb ),
i
where W[K]\τ |{K+1}∪τ is the channel derived from W i with
input set [K] \ τ , when the remaining inputs are available to
the output. Then the induction hypothesis applied for R τ
and for R [K]\τ gives the result. Note that implicitly Lemma
1 is also used.
Proof of Theorem 1, achievability, second part: Lemma
2 shows that each vertex of Dom(R(U, V )) can be achieved
by successive decoding. Lemma 3 applied to W 1 = W 2 =
· · · = W k = W shows that any point on Dom(R(U, V )) can
be represented by a vertex of Dom(R (U, V )) for a larger
PAMAC system with 2K − 1 senders. This larger PAMAC
W is obtained from W by splitting senders; the splitting of
senders in group G1 or G2 results in virtual senders also in
that group. Hence Lemma 3 reduces the proof of Theorem 1
to Lemma 2.

[6]. Note that a genie added model version is necessary for
the exact proof. Hence there exists a sequence of deterministic
coding-decoding systems with exponentially small maximal
error over delays.
Lemma 3 will rely on rate splitting, for deﬁniteness we
use the explicit construction from [6]. A split of sender
i with input distribution pXi on Xi = {0, 1, . . . , Xi − 1}
results in two virtual senders ia, ib with distributions pXia
and pXib , also on Xi , explicitly determined by pXi and a
splitting parameter 0 ≤ ε ≤ 1, such that the splitting function
f (xa , xb ) = max(xa , xb ) maps pXia × pXib into pXi .
Section 2 of [6] shows4 via Rate-Splitting that each R ∈
Dom(R [W ; pX1 × pX2 × · · · × pXK ]) can be represented by
a higher dimensional vertex. This means that there exists an
˜
auxiliary channel WR with 2K −1 virtual senders constructed
by splitting the senders of W , perhaps some of them split
repeatedly and others not at all, and there exist splitting
parameters ε1 , ε2 , . . . , εK−1 and an ordering π on the set
{1, 2, . . . , 2K − 1} with the following property. Note that the
˜
channel WR is uniquely determined by W and the splitting
pattern which describes which inputs are split. The new input
˜
distributions of WR , which are determined by the distributions pX1 , pX2 , . . . , pXK , the splitting pattern and the splitting
parameters, are denoted by pX1 , pX2 , . . . , pX2K−1 . Then the
˜
˜
˜
b’th coordinate of R is the sum of those coordinates of
˜
˜
the vertex Rπ of Dom(R WR ; pX1 × pX2 × · · · × pX2K−1 )
˜
˜
˜
that correspond to the virtual senders into which the b’th
sender has been split, b = 1, 2, . . . , K.

ACKNOWLEDGMENT
The preparation of this article would not have been possible
without the support of Dr. Imre Csiszár. We would like to
thank him for his help and advice within this subject area.

Lemma 3. Let R = (R1 , R2 , . . . , RK ) ∈ Dom(R(α)) (see
Lemma 1). There exists a splitting pattern of K − 1 splits,
splitting parameters ε1 , ε2 , . . . , εK−1 and an ordering π on
k
the set {1, 2, . . . , 2K − 1} such that R = i=1 αi Ri , where
i
i
i
Ri ∈ Dom(R W i ; pX1 × pX2 × · · · × pXK ) is the vector
that is represented by the higher dimensional vertex determined by the given splitting pattern, splitting parameters and
ordering corresponding to channel W i and input distributions
i
i
i
pX1 , pX2 , . . . , pXK .

R EFERENCES
[1] T. M. Cover, R. J. McEliece, E. C. Posner, Asynchronous MultipleAccess Channel Capacity, IEEE Trans. Inform. Theory, vol. IT-27, NO.
4, July 1981.
[2] I. Csiszár, J. Körner, Information theory, Coding theorems for Discrete
Memoryless Systems, 2nd edition, Cambridge University Press, 2011.
[3] J. Edmonds. Submodular functions, matroids, and certain polyhedra.
In Combinatorial optimization - Eureka, you shrink!, pages 11–26.
Springer, 2003.
[4] L. Farkas, T. Kói, Capacity Region of Discrete Asynchronous Multiple
Access Channels, IEEE International Symposium of Information Theory,
Aug. 2011.
[5] L. Farkas, T. Kói, On the Capacity Region of Discrete Asynchronous
Multiple Access Channels, submitted to IEEE Trans. Inform. Theory,
manuscript available on ArXiv.
[6] A. J. Grant, B. Rimoldi, R. L. Urbanke, P. A. Whiting, Rate-Splitting
Multiple Access For Discrete Memoryless Channels, IEEE Trans. Inform. Theory, vol. 47, NO. 3, Mar. 2001.
[7] J. Y. N. Hui, P. A. Humblet, The Capacity Region of the Totally Asynchronous Multiple-Access Channel, IEEE Trans. Information Theory,
vol. IT-31, NO. 2, Mar. 1985.
[8] N. Ninoslav, B. Rimoldi, On the Structure of the Capacity Region of
Asynchronous Memoryless Multiple-Access Channels, submitted to IEEE
Trans. Inform. Theory, manuscript available on ArXiv.
[9] G. Sh. Poltyrev, Coding in an Asynchronous Multiple-Access Channel,
Probl. Peredachi Inf., vol. 19, no. 3, pp. 12-21, 1983.
[10] B. Rimoldi, Generalized Time Sharing: A Low-Complexity CapacityAchieving Multiple-Access Technique, IEEE Trans. Inform. Theory, vol.
47, NO. 6, Sept. 2001.

Remark 4. Note that the subject of the above Lemma is not
the achievability of R, it is about the convex decomposition of
R by vectors which can be represented by identical splitting
pattern, splitting parameters and ordering.
Remark 5. Lemma 3 will be used for the case W 1 = W 2 =
· · · = W k = W , the general formulation is needed for the
inductive proof.
Sketch of proof: The statement is trivial if K = 1.
The general case can be derived following the inductive
proof of [6]. Split the K’th sender of each channel W i ,
i ∈ {1, 2, . . . , k} with the same parameter ε1 , and deﬁne a new
k
i
K + 1 dimensional vector by RK+1 = i=1 αi I(XKa ∧ Y i ),
RK = RK − RK+1 , Ri = Ri if i < K. As in
Section 2 of [6] there exist ε1 and τ ⊂ [K − 1] with
the following properties. R τ is in the dominant face of
4 The

stronger result of Section 3 of [6] is not necessary in this paper.

5

