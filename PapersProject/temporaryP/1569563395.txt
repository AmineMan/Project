Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May  3 00:12:30 2012
ModDate:        Tue Jun 19 12:55:34 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      565204 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569563395

Joint Source-Channel Coding for the
Multiple-Access Relay Channel
Yonathan Murin, Ron Dabora

Deniz G¨ nd¨ z
u u

Department of Electrical and Computer Engineering
Ben-Gurion University, Israel
Email: moriny@bgu.ac.il, ron@ee.bgu.ac.il

Centre Tecnologic de Telecomunicacions
de Catalunya (CTTC), Barcelona, Spain
Email: deniz.gunduz@cttc.es
a simple comparison of the rates of the optimal source code
and the optimal channel code for the respective source and
channel, sufﬁces to conclude whether reliable communication
is feasible. This is called the separation theorem. However,
the optimality of separation does not generalize to multiuser
networks [6], [7], and, in general the source and channel codes
must be jointly designed for every particular combination of
source and channel, for optimal performance.
In this paper we study source-channel coding for the transmission of correlated sources over MARCs. We note that while
the capacity region of the MAC (which is a special case of
the MARC) is known for independent messages, the optimal
joint source-channel code for the case of correlated sources
is not known in general [6]. Single-letter sufﬁcient conditions
for communicating discrete, arbitrarily correlated sources over
a MAC are derived in [6]. These conditions were later shown
by Dueck in [8], to be sufﬁcient but not necessary. This gives
an indication on the complexity of the problem studied in the
present work.
The main technique used in [6] is the correlation preserving
mapping (CPM) in which the channel codewords are correlated
with the source sequences. Since the source sequences are
correlated with each other, CPM leads to correlation between
the channel codewords. The CPM technique of [6] is extended
to source coding with side information for the MAC in [9]
and to broadcast channels with correlated sources in [10].
Transmission of arbitrarily correlated sources over interference
channels (ICs) is studied in [11], in which Liu and Chen apply
the CPM technique to ICs. Lossless transmission over a relay
channel with correlated side information is studied in [12] and
[13]. In [12] a decode-and-forward (DF) based achievability
scheme is proposed and it is shown that separation is optimal
for physically degraded relay channels with degraded side
information, as well as for cooperative relay-broadcast channels with arbitrary side information. Necessary and sufﬁcient
conditions for reliable transmission of a source over a relay
channel, when side information is available at the receiver or
at the relay, are established in [13].
Main Contributions
In this paper we ﬁrst demonstrate the suboptimality of
separate source and channel encoding for the MARC by
considering the transmission of correlated sources over a
discrete memoryless (DM) semi-orthogonal MARC in which
the relay-destination link is orthogonal to the channel from the
sources to the relay and the destination.

Abstract—Reliable transmission of arbitrarily correlated
sources over multiple-access relay channels (MARCs) and
multiple-access broadcast relay channels (MABRCs) is considered. In MARCs, only the destination is interested in a
reconstruction of the sources, while in MABRCs, both the relay
and the destination want to reconstruct the sources. We allow an
arbitrary correlation among the sources at the transmitters, and
let both the relay and the destination have side information that
are correlated with the sources. Two joint source-channel coding
schemes are presented and the corresponding sets of sufﬁcient
conditions for reliable communication are derived. The proposed
schemes use a combination of the correlation preserving mapping
(CPM) technique with Slepian-Wolf (SW) source coding: the ﬁrst
scheme uses CPM for encoding information to the relay and
SW source coding for encoding information to the destination;
while the second scheme uses SW source coding for encoding
information to the relay and CPM for encoding information to
the destination.

I. I NTRODUCTION
The multiple-access relay channel (MARC) models a network in which several users communicate with a single
destination, with the help of a relay [1]. Examples of such a
network include sensor and ad-hoc networks in which an intermediate relay node is introduced to assist the communication
from the source terminals to the destination. The MARC is
a fundamental multi-terminal channel model that generalizes
both the multiple-access channel (MAC) and the relay channel
models, and has received a lot of attention in the recent years.
If the relay terminal is also required to decode the source
messages, the model is called the multiple-access broadcast
relay channel (MABRC).
While in [1],[2] MARCs with independent sources at the
terminals are considered, in [3], [4] we allow arbitrary correlation among the sources to be transmitted to the destination
in a lossless fashion. We also let the relay and the destination
have side information that are correlated with the two sources.
In [3] we address the problem of determining whether a pair
of sources can be losslessly transmitted to the destination with
a given number of channel uses per source sample, using
statistically independent source code and channel code.
In [5] Shannon showed that a source can be reliably
transmitted over a memoryless point-to-point (PtP) channel, if
and only if its entropy is less than the channel capacity. Hence,
This work was partially supported by the European Commission’s Marie
Curie IRG Fellowship PIRG05-GA-2009-246657 under the Seventh Framework Programme. Deniz G¨ nd¨ z is partially supported by the European
u u
Commission’s Marie Curie Fellowship IRG256410, and by the Spanish
Government under project TEC2010-17816 (JUNTOS).

1

Next, we propose two DF-based joint source-channel
achievability schemes for MARCs and MABRCs. Both proposed schemes use a combination of SW source coding and
the CPM technique. While in the ﬁrst scheme CPM is used
for encoding information to the relay and SW source coding is
used for encoding information to the destination; in the second
scheme SW source coding is used for encoding information to
the relay and CPM is used for encoding information to the destination. A comparison of the conditions of the two schemes
reveals a tradeoff: while the relay feasibility conditions of
the former are looser, the destination feasibility conditions of
the latter are looser. These are the ﬁrst joint source-channel
achievability schemes, proposed for a multiuser network with
a relay, which take advantage of the CPM technique.
The rest of this paper is organized as follows: in Section II
we introduce the system model and notations. In Section III
we demonstrate the suboptimality of separate encoding for the
MARC. In Section IV we present two achievability schemes
for DM MARCs and MABRCs with correlated sources and
side information, and derive their corresponding sets of feasibility conditions. We discuss the results in Section V, and
conclude the paper in Section VI.

Fig. 1. Multiple-access broadcast relay channel with correlated side inforn
n
ˆn ˆn
mation. (S1,3 , S2,3 ) are the reconstructions of (S1 , S2 ) at the relay, and
ˆn , S n ) are the reconstructions at the destination.
ˆ
(S1 2

Y, Y3 over ﬁnite output alphabets Y, Y3 , is available. The
MARC is memoryless in the sense
k−1
p(yk , y3,k |y k−1, y3 , xk , xk , xk ) = p(yk , y3,k |x1,k , x2,k , x3,k ).
1
2
3
A source-channel code for MABRCs with correlated side
information consists of two encoding functions at the trans(n)
n
mitters: fi
: Si → Xin , i = 1, 2, a decoding function
n
n
at the destination, g (n) : Y n × W n → S1 × S2 , and a
(n)
n
n
n
n
decoding function at the relay, g3 : Y3 × W3 → S1 × S2 .
Finally, there is a causal encoding function at the relay,
(n) k−1
n
x3,k = f3,k (y3,1 , w3,1 ), 1 ≤ k ≤ n. Note that in the MARC
(n)
ˆn
scenario the decoding function g3 does not exist. Let Si
ˆn denote the reconstruction of S n , i = 1, 2, at the
and Si,3
i
receiver and at the relay respectively. The average probability
of error of a source-channel code for the MABRC is deﬁned as
(n)
n
n
n
n
ˆn ˆ)
ˆn ˆn
Pr (S1 , S2 = (S1 , S2 ) or (S1,3 , S2,3 ) = (S1 , S2 ) .
Pe
For the MARC the deﬁnition is similar except that the decoding error event at the relay is omitted. The sources (S1 , S2 ) can
be reliably transmitted over the MABRC with side information
if there exists a sequence of source-channel codes such that
(n)
Pe → 0 as n → ∞. The same deﬁnition applies to MARCs.
Before presenting the new joint source-channel coding
schemes, we ﬁrst motivate this work by demonstrating the
suboptimality of separate encoding for the MARC.
III. S UBOPTIMALITY OF S EPARATION FOR DM MARC S

II. N OTATIONS AND S YSTEM M ODEL
In the following we denote random variables with upper
case letters, e.g. X, and their realizations with lower case
letters, e.g. x. A discrete random variable X takes values in
a set X . We use pX (x) ≡ p(x) to denote the probability
mass function (p.m.f.) of a discrete RV X on X . We denote
vectors with boldface letters, e.g. x; the i’th element of a
vector x is denoted by xi , and we use xj where i < j to
i
denote (xi , xi+1 , ..., xj−1 , xj ); xj is a short form notation for
∗(n)
(X) to denote the set of -strongly typical
xj . We use A
1
sequences w.r.t. the p.m.f pX (x) on X , as deﬁned in [14,
Ch. 13.6]. When referring to a typical set we may omit the
random variables from the notation, when these variables are
clear from the context. The empty set is denoted by φ.
The MARC consists of two transmitters (sources), a receiver
(destination) and a relay. Transmitter i observes to the source
n
sequence Si , for i = 1, 2. The receiver is interested in the
lossless reconstruction of both source sequences observed by
the two transmitters. The objective of the relay is to help the
n
receiver decode these sequences. Let W3 and W n , denote the
side information at the relay and at the receiver respectively.
The side information sequences are correlated with the source
sequences. For the MABRC both the receiver and the relay
are interested in a lossless reconstruction of both source
sequences. Figure 1 depicts the MABRC with side information
setup.
The sources and the side information sequences,
{S1,k , S2,k , Wk , W3,k }n , are arbitrarily correlated according
k=1
to a joint distribution p(s1 , s2 , w, w3 ) over a ﬁnite alphabet
S1 × S2 × W × W3 , and independent across different sample
indices k. All nodes know this joint distribution.
For transmission, a discrete memoryless MARC with inputs
X1 , X2 , X3 over ﬁnite input alphabets X1 , X2 , X3 , and outputs

Consider the transmission of arbitrarily correlated sources
S1 and S2 over a DM semi-orthogonal MARC (SOMARC),
in which the relay-destination link is orthogonal to the
channel from the sources to the relay and to the destination. The SOMARC is characterized by the joint distribution
p(yR , yS , y3 |x1 , x2 , x3 ) = p(yR |x3 )p(yS , y3 |x1 , x2 ), where
YR and YS are the channel outpus at the destination. The
SOMARC is depicted in Figure 2. In the following we present
a scenario (sources and a channel) in which joint sourcechannel coding strictly outperforms separate source-channel
coding.
We begin with an outer bound on the sum-capacity of the
SOMARC. This is characterized in Proposition 1.

Fig. 2.

2

Semi-orthogonal multiple-access relay channel.

for a joint distribution that factors as
p(s1 , s2 , w3 , w)p(v1 )p(x1 |s1 , v1 )×

Proposition 1. The sum-capacity of the SOMARC, R1 + R2 ,
is upper bounded by
R1 + R2 ≤
max
min I(X1 , X2 ; Y3 , YS ),

p(v2 )p(x2 |s2 , v2 )p(x3 |v1 , v2 )p(y3 , y|x1 , x2 , x3 ).

p(x1 )p(x2 )p(x3 )

Proof: See [4, Subsection VI.D, Appendix C].
n
n
Theorem 2. A source pair (S1 , S2 ) can be reliably transmitted
over a DM MARC with relay and receiver side information
as deﬁned in Section II if,
H(S1 |S2 , W3 ) < I(X1 ; Y3 |S1 , X2 , X3 )
(6a)

I(X3 ; YR ) + I(X1 , X2 ; YS ) . (1)
Proof: Detailed proof is provided in [4, Subsection VI-A].
Next, consider a SOMARC deﬁned by
X1 = X2 = X3 = Y3 = YR = {0, 1}, YS = {0, 1, 2},
YR = X3 , Y3 = X1 ⊕ X2 , YS = X1 + X2 .
(2)
Additionally, consider the sources (S1 , S2 ) ∈ {0, 1}×{0, 1}
with the joint distribution p(s1 , s2 ) = 1 for (s1 , s2 ) ∈
3
{(0, 0), (0, 1), (1, 1)}, and zero otherwise. Then, H(S1 , S2 ) =
log2 3 = 1.58 bits/sample. For the channel deﬁned in
(2) the mutual information expression I(X1 , X2 ; Y3 , YS ) reduces to I(X1 , X2 ; YS ). This is because I(X1 , X2 ; Y3 , YS ) =
I(X1 , X2 ; YS ) + I(X1 , X2 ; Y3 |YS ), and Y3 is a deterministic
function of YS . Therefore,
R1 + R2 ≤ max I(X1 , X2 ; YS )

H(S2 |S1 , W3 ) < I(X2 ; Y3 |S2 , X1 , X3 )

(4c)
(4d)

H(S2 |S1 , W ) < I(X2 , X3 ; Y |S2 , X1 , V1 )

(6f)

p(x3 |s1 , s2 )p(y3 , y|x1 , x2 , x3 ).
(7)
Proof:
1) Codebook construction: For i = 1, 2, assign every si ∈
n
Si to one of 2nRi bins independently according to a uniform
distribution on Ui {1, 2, . . . , 2nRi }. Denote this assignment
by fi , i = 1, 2.
n
For i = 1, 2, for each pair (ui , si ), ui ∈ Ui , si ∈ Si , generate one n-length codeword xi (ui , si ), by choosing the letters
xi,k (ui , si ) independently with distribution pXi |Si (xi,k |si,k )
for all 1 ≤ k ≤ n. Finally, generate one length-n ren
lay codeword x3 (s1 , s2 ) for each pair (s1 , s2 ) ∈ S1 ×
n
S2 by choosing x3,k (s1 , s2 ) independently with distribution
pX3 |S1 ,S2 (x3,k |s1,k , s2,k ) for all 1 ≤ k ≤ n.
2) Encoding: Consider a source sequences of length Bn
Bn
sBn ∈ Si , i = 1, 2. Partition each sequence into B
i
length-n subsequences, si,b , b = 1, . . . , B. Similarly, for
Bn
b = 1, 2, . . . , B, partition the side information sequences w3
Bn
and w into B length-n subsequences w3,b , wb , respectively.
We transmit a total of Bn source samples over B + 1 blocks
of n channel uses each.
At block 1, source terminal i, i = 1, 2, observes si,1 and
ﬁnds its corresponding bin index ui,1 ∈ Ui . It then transmits
n
the channel codeword xi (ui,1 , ai ) where ai ∈ Si is a ﬁxed
sequence. At block b, b = 2, . . . , B, source terminal i, i = 1, 2,
transmits the channel codeword xi (ui,b , si,b−1 ) where ui,b ∈
Ui is the bin index of source vector si,b . At block B + 1,
source terminal i, i = 1, 2, transmits xi (1, si,B ).
At block b = 1, the relay transmits x3 (a1 , a2 ). Assume
that at block b, b = 2, . . . , B, B + 1, the relay obtained the
estimates (˜1,b−1 , ˜2,b−1 ) of (s1,b−1 , s2,b−1 ). It then transmits
s
s
the channel codeword x3 (˜1,b−1 , ˜2,b−1 ).
s
s
3) Decoding: The relay decodes the source sequences
sequentially trying to reconstruct source block si,b , i = 1, 2,
at the end of channel block b as follows: let ˜i,b−1 , i = 1, 2,
s
be the estimate of si,b−1 , at the relay at the end of block
b − 1. Using this information, and its received signal y3,b , the
relay channel decoder at time b decodes (u1,b , u2,b ), i.e., the
bin indices corresponding to si,b , i = 1, 2, by looking for a
unique pair (˜1 , u2 ) such that:
u ˜

(4a)

(4e)

H(S1 , S2 |W ) < I(X1 , X2 , X3 ; Y |S1 , S2 ),

(6e)

H(S1 , S2 |W ) < I(X1 , X2 , X3 ; Y |W ),
for a joint distribution that factors as
p(s1 , s2 , w3 , w)p(x1 |s1 )p(x2 |s2 )×

(4b)

H(S1 |S2 , W ) < I(X1 , X3 ; Y |S1 , X2 , V2 )

(6d)

H(S2 |S1 , W ) < I(X2 , X3 ; Y |S1 , X1 , W )

IV. J OINT S OURCE -C HANNEL C ODING FOR D ISCRETE
M EMORYLESS MARC S AND MABRC S
In this section we present two sets of sufﬁcient conditions for reliable transmission of correlated sources over DM
MARCs and MABRCs with side information. Both achievability schemes use a combination of SW source coding, the
CPM technique, and the DF scheme with successive decoding
at the relay and backward decoding at the destination [1]. We
shall refer to SW source coding as separate source and channel
coding (SSCC). The achievability schemes differ in the way
the source codes are combined. In the ﬁrst scheme (Thm. 1)
SSCC is used for encoding information to the destination
while CPM is used for encoding information to the relay.
In the second scheme (Thm. 2), CPM is used for encoding
information to the destination while SSCC is used for encoding
information to the relay.
n
n
Theorem 1. A source pair (S1 , S2 ) can be reliably transmitted
over a DM MARC with relay and receiver side information
as deﬁned in Section II if,

H(S1 , S2 |W3 ) < I(X1 , X2 ; Y3 |V1 , V2 , X3 , W3 )

(6c)

H(S1 |S2 , W ) < I(X1 , X3 ; Y |S2 , X2 , W )

= 1.5 bits per channel use.
(3)
Hence, we have H(S1 , S2 ) > I(X1 , X2 ; YS ), for any
p(x1 )p(x2 ). We conclude that it is not possible to send the
sources reliably to the destination by using a separation-based
source and channel codes. However, by choosing X1 = S1 and
X2 = S2 a zero error probability is achieved. This example
shows that separate source and channel coding is, in general,
suboptimal for sending arbitrarily correlated sources over
MARCs.

H(S1 |S2 , W3 ) < I(X1 ; Y3 |S2 , X2 , V1 , X3 , W3 )

(6b)

H(S1 , S2 |W3 ) < I(X1 , X2 ; Y3 |S1 , S2 , X3 )

p(x1 )p(x2 )

H(S2 |S1 , W3 ) < I(X2 ; Y3 |S1 , X1 , V2 , X3 , W3 )

(5)

(4f)

3

˜1,b−1 , ˜2,b−1 , x1 (˜1 , ˜1,b−1 ), x2 (˜2 , ˜2,b−1 ),
s
s
u s
u s

In Thm. 2 the LHS of condition (6a) is the entropy
of S1 when (S2 , W3 ) are known. In the RHS of (6a)
the mutual information expression I(X1 ; Y3 |S1 , X2 , X3 ) =
I(X1 ; Y3 |S1 , S2 , X2 , X3 , W3 ) represents the rate for sending
n
the bin index of the source sequence S1 to the relay (see
[4, Subsection VI-E]). This is because S1 , S2 and W3 are
given. The LHS of condition (6d) is the entropy of S1 when
(S2 , W ) are known. In the RHS of condition (6d), as S2 ,
X2 and W are given, the mutual information expression
I(X1 , X3 ; Y |S2 , X2 , W ) represents the available rate that can
n
be used for sending information on the source sequence S1
to the destination.
Remark 3. For an input distribution
p(s1 , s2 , w3 , w, v1 , v2 , x1 , x2 , x3 ) =

∗(n)

x3 (˜1,b−1 , ˜2,b−1 ), y3,b ∈ A
s
s
.
(8)
The decoded bin indices, denoted (˜1,b , u2,b ), are then
u
˜
given to the relay source decoder. Using (˜1,b , u2,b ) and
u
˜
the side information w3,b , the relay source decoder estimates (s1,b , s2,b ) by looking for a unique pair of sequences
(˜1 , ˜2 ) that satisﬁes f1 (˜1 ) = u1,b , f2 (˜2 ) = u2,b and
s s
s
˜
s
˜
∗(n)
(˜1 , ˜2 , w3,b ) ∈ A
s s
(S1 , S2 , W3 ). Let (˜1,b , ˜2,b ) denote the
s s
decoded sequences.
Decoding at the destination is done using backward decoding. The destination node waits until the end of channel
block B + 1. It ﬁrst decodes si,B , i = 1, 2, using the received
signal at channel block B + 1 and its side information wB .
Going backwards from the last channel block to the ﬁrst, we
assume that the destination has estimates (ˆ1,b+1 , ˆ2,b+1 ) of
s
s
(s1,b+1 , s2,b+1 ) and consider decoding of (s1,b , s2,b ). From
(ˆ1,b+1 , ˆ2,b+1 ) the destination ﬁnds the corresponding bin
s
s
indices (ˆ1,b+1 , u2,b+1 ). Using this information, its received
u
ˆ
signal yb+1 and the side information wb , the destination
decodes (s1,b , s2,b ) by looking for a unique pair (ˆ1 , ˆ2 ) such
s s
that:
ˆ1 , ˆ2 , x1 (ˆ1,b+1 , ˆ1 ), x2 (ˆ2,b+1 , ˆ2 ),
s s
u
s
u
s

p(s1 , s2 , w3 , w)p(v1 )p(x1 |v1 )p(v2 )p(x2 |v2 )p(x3 |v1 , v2 ),
the conditions in (4) specialize to [3, Equation (2)], and
the transmission scheme specializes to a separation-based
achievability scheme.
Remark 4. In both Thm. 1 and Thm. 2 the conditions stemming from the CPM technique can be specialized to the MAC
source-channel conditions of [6, Equations (12)]. In Thm. 1
letting V1 = V2 = X3 = W3 = φ, reduces the relay conditions
in (4a)–(4c) to the ones in [6, Equations (12)] with Y3 as
the destination. In Thm. 2 letting X3 = W = φ, reduces the
destination conditions in (4d)–(4f) to the ones in [6, Equations
(12)] with Y as the destination.
Remark 5. Thm. 1 and Thm. 2 establish different achievability conditions. As stated in Section III, SSCC is generally
suboptimal for sending correlated sources over DM MARCs
and MABRCs. In Thm. 1 the CPM technique is used for
sending information to the relay, while in Thm. 2 SSCC is
used for sending information to the relay. This observation
implies that the relay decoding constraints of Thm. 1 are looser
compared to the relay decoding constraints of Thm. 2. Using
similar reasoning we conclude that the destination decoding
constraints of Thm. 2 are looser compared to the destination
decoding constraints of Thm. 1 (as long as coordination is
possible, see Remark 6). Considering the distribution chains
in (5) and (7) we conclude that these two theorems represent
different sets of sufﬁcient conditions, and neither theorem is
a special case of the other.
Remark 6. Figure 3 depicts the cooperative relay broadcast
channel (CRBC) model with correlated relay and destination
side information, which is a special case of the MABRC with
X2 = S2 = φ. For this model the optimal source-channel rate
was obtained in [12, Theorem 3.1]:

x3 (ˆ1 , ˆ2 ), wb , yb+1 ∈ A∗(n) .
s s
(9)
4) Error probability analysis: The error probability analysis is detailed in [4, Appendix D].
V. D ISCUSSION
Remark 1. Constraints (4a)–(4c) in Thm. 1 and (6a)–(6c) in
Thm. 2, are due to decoding at the relay, while constraints
(4d)–(4f) in Thm. 1 and (6d)–(6f) in Thm. 2, are due to
decoding at the destination.
Remark 2. In Thm. 1 V1 and V2 represent the binning
information for S1 and S2 , respectively. Observe that the
left-hand side (LHS) of condition (4a) is the entropy of S1
when (S2 , W3 ) are known. On the right-hand side (RHS) of
(4a), as V1 , S2 , X2 , X3 and W3 are given, the mutual information expression I(X1 ; Y3 |S2 , X2 , V1 , X3 , W3 ) represents
the available rate that can be used for sending to the relay
n
information on the source sequence S1 , in excess of the
bin index represented by V1 . The LHS of condition (4d) is
the entropy of S1 when (S2 , W ) are known. The RHS of
condition (4d) expresses the rate at which binning information
can be transmitted reliably and cooperatively from transmitter
1 and the relay to the destination. This follows as the mutual
information expression on the RHS of (4d) can be written as
I(X1 , X3 ; Y |S1 , X2 , V2 ) = I(X1 , X3 ; Y |S1 , S2 , V2 , X2 , W ),
which, as S1 , S2 and W are given, represents the rate for
n
sending the bin index of source sequence S1 to the destination
(see [4, Subsection VI-D]). This is in contrast to the decoding
constraint at the relay, c.f. (4a). Therefore, each mutual information expression in (4a) and (4d) represents a different type
of information sent by the source: either the source-channel
codeword to the relay in (4a), or bin index to the destination
in (4d). This is because SSCC is used for sending information
to the destination and CPM is used for sending information to
the relay.

Fig. 3. Cooperative relay broadcast channel with correlated side information.

4

n
Proposition ([12, Theorem 3.1]). A source S1 can be reliably
transmitted over a DM CRBC with relay and receiver side
information if
H(S1 |W3 ) < I(X1 ; Y3 |X3 )
(10a)

VI. C ONCLUSIONS
In this paper we considered joint source-channel coding for
DM MARCs and MABRCs. We ﬁrst showed via an explicit
example that joint source-channel coding generally enlarges
the set of possible sources that can be reliably transmitted
compared to separation-based coding. We then derived two
new joint source-channel achievability schemes. Both schemes
use a combination of SSCC and CPM techniques. While in the
ﬁrst scheme CPM is used for encoding information to the relay
and SSCC is used for encoding information to the destination,
in the second scheme SSCC is used for encoding information
to the relay and CPM is used for encoding information to
the destination. The different combinations of binning and
source mapping enable ﬂexibility in the system design by
choosing one of the two schemes according to the quality
of the side information and received signals at the relay and
at the destination. In particular, the ﬁrst scheme has looser
decoding constraints at the relay and is therefore better when
the channels from the sources to the relay are the bottleneck;
while the second scheme has looser decoding constraints at
the destination, and is more suitable for scenarios in which
the channels to the destination are more noisy (at the cost of
more constrained source-relay coordination).
R EFERENCES

H(S1 |W ) < I(X1 , X3 ; Y ),
(10b)
for some input distribution p(s1 , w3 , w)p(x1 , x3 ). Conversely,
n
if a source S1 can be reliably transmitted then the conditions
in (10a) and (10b) are satisﬁed with < replaced by ≤ for some
input distribution p(x1 , x3 ).
The conditions in (10) can also be obtained from Thm. 1 by
letting V1 = X3 , S2 = X2 = V2 = φ, and considering an input
distribution independent of the sources. However, when we
consider Thm. 2 with S2 = X2 = φ, we obtain the following
achievability conditions:
H(S1 |W3 ) < I(X1 ; Y3 |X3 , S1 )
(11a)
H(S1 |W ) < I(X1 , X3 ; Y |W ),
(11b)
for some input distribution p(s1 , w3 , w)p(x1 |s1 )p(x3 |s1 ).
Note that the RHS of the inequalities in (11a) and (11b)
are not greater than the RHS of the inequalities in (10a) and
(10b), respectively. Moreover, not all joint input distributions
p(x1 , x3 ) can be achieved via p(x1 |s1 )p(x3 |s1 ). Hence, the
conditions obtained from Thm. 2 for the CRBC setup are
stricter than those obtained from Thm. 1, illustrating the fact
that the two sets of conditions are not equivalent. We conclude
that the downside of using CPM to the destination as applied
in this work is that it puts constraints on the distribution chain,
thereby constraining the achievable coordination between the
sources and the relay. For this reason, when there is only a
single source, the joint distributions of the source and the relay
(X1 and X3 ) achieved by the scheme of Thm. 2, do not exhaust
the entire space of joint distributions, resulting in generally
stricter source-channel constraints than those obtained from
Thm. 1. However, recall that for SOMARC in Section III the
optimal scheme uses CPM to the destination. Therefore, for
the MARC it is not possible to determine whether either of
the schemes is universally better than the other.

[1] G. Kramer, M. Gastpar, and P. Gupta, “Cooperative strategies and
capacity theorems for relay networks”. IEEE Trans. Inform. Theory,
vol. 51, no. 9, pp. 3037–3063, Sep. 2005.
[2] L. Sankaranarayanan, G. Kramer, and N. B. Mandayam, “Offset
encoding for multiaccess relay channels”. IEEE Trans. Inform. Theory,
vol. 53, no. 10, pp. 3814–3821, Oct. 2007.
[3] Y. Murin, R. Dabora, and D. G¨ nd¨ z, “Source-channel coding for the
u u
multiple-access relay channel”. Proc. Int. Symp. Wireless Commun. Sys.,
Aachen, Germany, Nov. 2011.
[4] Y. Murin, R. Dabora, and D. G¨ nd¨ z,
u u
“Source-channel coding theorems for the multiple-access relay channel”.
Submitted
to the IEEE Trans. Information Theory, May 2011. Available at
http://arxiv.org/abs/1106.3713v2.
[5] C. E. Shannon, “A mathematical theory of communication”. Bell Syst.
Tech. J., vol. 27, pp. 379–423 and pp. 623–656, 1948.
[6] T. M. Cover, A. El Gamal, and M. Salehi, “Multiple access channels
with arbitrarily correlated sources”. IEEE Trans. Inform. Theory, vol.
26, no. 6, pp. 648–657, Nov. 1980.
[7] D. G¨ nd¨ z, E. Erkip, A. Goldsmith, and H. V. Poor, “Source and channel
u u
coding for correlated sources over multiuser channels”. IEEE Trans.
Inform. Theory, vol. 55, no. 9, pp. 3927–3944, Sep. 2009.
[8] G. Dueck, “A note on the multiple access channel with correlated
sources”. IEEE Trans. Inform. Theory, vol. 27, no. 2, pp. 232–235,
Mar. 1981.
[9] R. Ahlswede and T. S. Han, “On source coding with side information via
a multiple access channel and related problems in multiuser information
theory”. IEEE Trans. Inform. Theory, vol. 29, no. 3, pp. 396-412, May
1983.
[10] T. Han and M. H. M. Costa, “Broadcast channels with arbitrarily
correlated sources”. IEEE Trans. Inform. Theory, vol. 33, no. 5, pp.
641–650, Sep. 1987.
[11] W. Liu and B. Chen, “Interference channels with arbitrarily correlated
sources”. IEEE Trans. Inform. Theory, vol. 57, no. 12, pp. 8027–8037,
Dec. 2011.
[12] D. G¨ nd¨ z and E. Erkip, “Reliable cooperative source transmission
u u
with side information”. Proc. IEEE Inform. Theory Workshop, Bergen,
Norway, Jul. 2007, pp. 22–26.
[13] R. Kwak, W. Lee, A. El Gamal, and J. Ciofﬁ, “Relay with side
information”. Proc. IEEE Int. Symp. Inform. Theory, Nice, France, Jun.
2007, pp. 606–610.
[14] T. M. Cover and J. A. Thomas, Elements of Information Theory. John
Wiley and Sons Inc., 1991.

Remark 7. In both Thm. 1 and Thm. 2 we use a combination of SSCC and CPM. Since CPM can generally support
sources with higher entropies, a natural question that arises
is whether it is possible to design a scheme based only on
CPM; namely, encode both cooperation (relay) information
and the new information, using a superposition CPM scheme.
This approach cannot be used directly in the framework of the
current paper. Here, we use joint typicality decoder, which
does not apply to different blocks generated independently
with the same distribution. For example, we cannot test the
joint typicality of sn and s2n , as they belong to different
1,n+1
1,1
time blocks. Using a CPM-only scheme would require such a
test. We conclude that applying the CPM technique for sending
information to both the relay and the destination cannot be
done while using joint typicality decoder as considered in this
paper. It is, of course, possible to construct schemes that use
a different decoder, or apply CPM through intermediate RVs,
which overcome this difﬁculty. Investigation of such coding
schemes is left for future research.

5

