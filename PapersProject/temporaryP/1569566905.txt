Title:          ISIT12_MAC_final.pdf
Author:         ywhuang
Creator:        PScript5.dll Version 5.2.2
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 16:25:44 2012
ModDate:        Tue Jun 19 12:54:41 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      386108 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566905

Finite Blocklength Coding for Multiple Access
Channels
Yen-Wei Huang and Pierre Moulin
Department of Electrical and Computer Engineering
University of Illinois at Urbana-Champaign, Urbana, IL 61801 USA
E-mails: huang37@illinois.edu; moulin@ifp.uiuc.edu

n

Y are ﬁnite and PY|X1 X2 (y|x1 , x2 ) = i=1 W (yi |x1i , x2i ).
Hence, a DM-MAC is determined by the single-lettered conditional distribution W (y|x1 , x2 ). An (n, M1 , M2 ) MAC code
C consists of two codebooks {x1 (m1 )} and {x2 (m2 )} where
mj ∈ {1, . . . , Mj }
Mj for j = 1, 2 and a (possibly randomized) decoder φ(y) = (m1 , m2 ). Code rates are deﬁned
by Rj n−1 log Mj for j = 1, 2, and M M1 M2 .
The error probability for passing a message pair (m1 , m2 )
using MAC code C through channel PY|X1 X2 is denoted by

Abstract—This paper studies the maximum achievable rate
region of multiple access channels (MAC) for a given blocklength
n and a desired error probability . The inner region for
the discrete memoryless MAC is approximated by a single1
lettered expression I − √n Qinv (V, ) where I is associated with
the capacity pentagon bounds by Ahlswede and Liao, V is
the MAC dispersion matrix, and Qinv is the complementary
multivariate Gaussian cumulative distribution region. For outer
regions, we provide general converse bounds for both average
error probability and maximum error probability criteria, and a
single-lettered approximation for the discrete memoryless MAC.

Pe (m1 , m2 , PY|X1 X2 , C)

I. I NTRODUCTION
Shannon’s single-user channel coding problem and other
classical channel problems give us an elegant formulation
of capacities or capacity regions. The theorems state that
the maximum rate of transmission approaches capacity or
capacity region as the blocklength tends to inﬁnity. However
such theorems do not provide convergence rates. This issue
is addressed by asymptotic channel coding analysis conducted
by Strassen [1] and others in the early 1960’s and recently
revisited by Polyanskiy et al. [2]. The past few years have
witnessed several research efforts applying these techniques
to other classical channel coding problems [3], [4].
In this work, we study the ﬁnite blocklength capacity
region of the MAC. The characterization of the average error
capacity region of the discrete memoryless MAC (DM-MAC)
is given independently by Ahlswede [5] and Liao [6]. Unlike
the single-user channel, the maximal error capacity region is
generally smaller the average error capacity region [7] and its
characterization remains an open problem. The strong converse
results in [8], [9] further strengthened the notion of capacity
region in the sense that no rate pairs outside the capacity
region can be -achievable for any < 1. Independently of our
work, Tan and Kosut [10] and MolavianJazi and Laneman [11]
recently studied the ﬁnite blocklength capacity inner region for
DM-MAC. Comparisons with their work are provided in Sec.
IV.
Formally, a two-user MAC1 is characterized by a transition
n
n
distribution PY|X1 X2 where X1 ∈ X1 , X2 ∈ X2 , and
n
Y ∈ Y , and is called discrete memoryless if X1 , X2 , and

y∈Y n

PY|X1 X2 (y|x1 (m1 ), x2 (m2 ))1{φ(y) = (m1 , m2 )}.
The average error probability is deﬁned by
Pe,avg (PY|X1 X2 , C)
=

1
M

M1
m1 =1

M2
m2 =1

Pe (m1 , m2 , PY|X1 X2 , C)

and the maximum error probability is deﬁned by
Pe,max (PY|X1 X2 , C)
= max(m1 ,m2 )∈M1 ×M2 Pe (m1 , m2 , PY|X1 X2 , C).
An (n, M1 , M2 , ) MAC code C under average (resp. maximum) error probability satisﬁes Pe,avg (PY|X1 X2 , C) ≤ (resp.
Pe,max (PY|X1 X2 , C) ≤ ). A rate pair (R1 , R2 ) is (n, )achievable under average (resp. maximum) error probability
if there exists an (n, M1 , M2 , ) MAC code under average
(resp. maximum) error probability.
The capacity region of a DM-MAC W is closely related
to statistics of the information density vector i(U, X1 , X2 , W )
abbreviated by i (should there be no risk of confusion) and
deﬁned by

 log W (Y |X1 ,X2 ) 
i(X1 ; Y |X2 , U )
P (Y |X2 ,U )


i = i(X2 ; Y |X1 , U ) = log W (Y |X1 ,X2 ) 
P (Y |X1 ,U )
|X1 ,X
i(X1 , X2 ; Y |U )
log W (Y(Y |U ) 2 )
P

where U is an auxiliary “time sharing” random variable, X1
and X2 are independent given U , and U ↔ (X1 , X2 ) ↔ Y
forms a Markov chain, i.e.,

This work was supported by DARPA under the ITMANET program and
presented in part at the ITMANET meeting in Stanford, CA, Jan. 2011.
1 Generalizations to k-user MAC are straightforward and thus omitted.

PU X1 X2 Y = PU PX1 |U PX2 |U W.

1

The marginal distributions P (Y |X1 , U ), P (Y |X2 , U ) and
P (Y |U ) are obtained by marginalizing out X2 , X1 ,
and (X1 , X2 ), respectively. The mutual information vector
I(PU X1 X2 , W ) or I is deﬁned by the expectation of the
information density vector under PU X1 X2 W :


I(X1 ; Y |X2 , U )
(1)
I(PU X1 X2 , W ) E [i] = I(X2 ; Y |X1 , U ) .
I(X1 , X2 ; Y |U )

1.5

1

0.5 σ Q−1(ε)

The (n, )-capacity region Rn, is deﬁned by the set of
all (n, )-achievable rate pairs. For the DM-MAC, as
0
and n → ∞, this capacity region (average error probability)
coincides with the MAC capacity region by Ahlswede [5] and
Liao [6]:
C(W ) =

PU PX1 |U PX2 |U

R(PU PX1 |U PX2 |U )

−1

σ Q (ε)
0

Fig. 1.

The region Qinv (Σ, ) for Σ =

ˆ
PU − PU

1.5

1
.3

.3
and
1

= .3.

L

= O(1/n)

(6)

{(m1 , m2 ) ∈ M1 × M2 : Zn (m1 , m2 ) ≥ τ n }

where
n

i(ui , X1i (m1 ), X2i (m2 ), W ).

(7)

Theorem 1: For any ﬁxed distribution PU X1 X2
PU PX1 |U PX2 |U let

=

Zn (m1 , m2 ) =
i=1

II. I NNER R EGION
In this section, we characterize an asymptotic inner bound
for the (n, )-capacity region of DM-MAC. The characterization utilizes the mutual information vector of (1), the secondorder dispersion matrix of the information density vector, and
the complementary multivariate Gaussian cumulative distribution region (deﬁned below).
Under PU X1 X2 W , we deﬁne the dispersion matrix
V(PU X1 X2 , W ) or V by the conditional covariance matrix
EPU CovPX1 X2 |U W [i|U ] .

1

where · 1 denotes the L1 norm. For each message pair
(m1 , m2 ) ∈ M1 × M2 , draw Xji independently from
PXj |U (·|ui ) for 1 ≤ i ≤ n, j = 1, 2.
Decoder: Deﬁne the (variable-size) list L of all messages
(m1 , m2 ) whose score exceeds a prescribed threshold vector
τ n = [τ1,n , τ2,n , τ3,n ] :

Given a sequence of vectors an , a sequence of regions
Fn , and a sequence of functions gn , the asymptotic notation
an ∈ Fn + O(gn ) means there exists a constant c such that
an ∈ Fn + cgn 1 for sufﬁciently large n. Here 1 denotes the
vector comprised of all ones, and ‘+’ denotes Minkowski set
addition.

Rin (PU X1 X2 , W )
n,

I(PU X1 X2 , W ) −

1
√ Qinv (V(PU X X , W ),
1 2
n

)

(8)

and
Rin (W )
n,

∪PU X1 X2 Rin (PU X1 X2 , W ).
n,

(9)

Then for sufﬁciently large n any rate vector R ∈ Rin (W ) −
n,
O

log n
n

is (n, )-achievable (average error probability).
Sketch of the proof: Analysis of the threshold decoding
scheme is based on a generalized multidimensional BerryEsseen theorem,2 which approximates the distribution of the
sum of independent random vectors to a multivariate Gaussian
distribution (details to be provided in the full version), and
large deviations analysis.
Without loss of generality, we assume m1 = m2 = 1. The
goal is to ﬁnd the threshold vector τ n such that

(5)

Let Z be a d-dimensional Gaussian vector with mean 0 and
covariance matrix Σ. For any ∈ (0, 1) let
z ∈ Rd : Pr(Z ≤ z) ≥ 1 −

1

Encoder: Let {ui }n , n ≥ 1 be deterministic sequences such
i=1
ˆ
that for large enough n the empirical distribution PU (u)
n
1
i=1 1{ui =u} approaches PU (u) in O(1/n), i.e.,
n

The rate vector is deﬁned by R = [R1 , R2 , R1 + R2 ] and
the inequality denotes element-wise inequality. The strong
converse by Dueck [8], later sharpened by Ahlswede [9], stated
that every sequence of (n, M1 , M2 , ) MAC codes (average
error probability) satisﬁes
√
(4)
[log M1 , log M2 ] ∈ nC(W ) + O n log n .

Qinv (Σ, )

0.5

(2)

where R(PU PX1 |U PX2 |U ) is the set of all rate pairs [R1 , R2 ]
satisfying
(3)
R ≤ I(PU X1 X2 , W ).

V(PU X1 X2 , W )

0

.

2

Note that for d = 1, we have Qinv (σ , ) = {z ≥ σQ−1 ( )}
where Q−1 is the complementary Gaussian cumulative distribution function. An example of a two-dimensional Qinv region
is shown in Fig. 1.
For any ﬁxed distribution PU X1 X2 = PU PX1 |U PX2 |U , we
consider the following random codes:

Pr(L = ∅) ≤ − αn

Pr(∃ [(m1 , m2 ) = (1, 1)] ∈ L) ≤ αn
2 For

2

the i.i.d. case, see [12].

(10)
(11)

for some αn = O(n−1/2 ). Now Zn of (7) is a sum of
independent (but not identically distributed) random vectors.
If we let
Cn

1
1
E[Zn ] =
n
n

A. General Converse Bound: Average Error Probability
Fix distributions P and {Qj }k (k = 3 in Theorem 2
j=1
below) and consider the composite hypothesis problem:

n

ˆ
PU (u)E[i(u, X1 , X2 , W )]

A randomized hypothesis test is denoted by δ(y) =
Pr(Say H0 |Y = y), y ∈ Y.
Deﬁnition 1: The set of achievable error exponent vectors
is deﬁned by

u

ˆ
= I(PU PX1 |U PX2 |U , W )
and
Σn

n

1
1
Cov(Zn ) = Cov
n
n
=

1
n

E

i(ui , X1i , X2i , W )

k

[E1 , . . . , Ek ] : ∃test δ s.t.

P, {Qj }j=1

EP [δ(Y )] ≥ 1 − and − log EQj [δ(Y )] ≥ Ej , 1 ≤ j ≤ k .

i=1

n

This is a generalization of the binary hypothesis testing
performance function βα of [2, (100)], which can also be
written as:

Cov (i(ui , X1i , X2i , W ))
i=1

ˆ
PU (u)Cov (i(u, X1 , X2 , W ))

=

Y ∼ Qj for some 1 ≤ j ≤ k.

H1 :

i=1

=

Y ∼P

H0 :

E[i(ui , X1i , X2i , W )]

βα (P, Q) =

u

{ : ∃test δ s.t.
EP [δ(Y )] ≥ α and EQ [δ(Y )] ≤ } .

min

ˆ
= V(PU PX1 |U PX2 |U , W )
ˆ
(both linear functions of PU ), and we pick τ n arbitrarily from
√
nCn − nQinv (Σn , − αn − γn ),

Indeed for k = 1, we have
E (P, Q) = {E : E ≥ − log β1− (P, Q)}.
Theorem 2: For

then by the generalized multidimensional Berry-Esseen theorem, there exists γn = O(n−1/2 ) such that

Qj 1 X2 Y
X

j

four

channels

PY|X1 X2

and

and a MAC code C (possibly randomized

= avg. error prob. with Qj 1 X2 , j = 1, 2, 3
Y|X

PX1 X2 = PX1 PX2 = Qj 1 X2 = encoder output
X

Pr(Zn ≥ τ n ) ≥ 1 − + αn

distribution with indep. equiprobable codewords.

and therefore (10) is satisﬁed. On the other hand, using large
deviations analysis we obtain

 Pr(∃((m1 = 1), 1) ∈ L) ≤ M1 exp {−τ1,n + O (1)}


Pr(∃(1, (m2 = 1)) ∈ L) ≤ M2 exp {−τ2,n + O (1)}
 Pr(∃((m1 = 1), (m2 = 2)) ∈ L)


≤ M1 M2 exp {−τ3,n + O (1)} .

Then we have

− log(1 −
− log(1 −
− log(1 −

1)
2)
3)



∈E

PX 1 X 2 Y , Q j 1 X 2 Y
X

3

.

j=1

Proof: This is a generalization of [2, Theorem 26].
Consider the following test δ for deciding between P and
Qj : denote the observed pair by (x1 , x2 , y); y is fed into
the decoder and the test declares P if the message is decoded
correctly. The probability that the test is correct if P is the
actual distribution is EP [δ(X1 , X2 , Y)] = 1 − , and the probability that the test is incorrect if Qj is the actual distribution
is EQj [δ(X1 , X2 , Y)] = 1− j , j = 1, 2, 3. Therefore the error
exponent vector [− log(1 − 1 ), − log(1 − 2 ), − log(1 − 3 )]
is achievable.
Theorem 3: Every (n, M1 , M2 , ) MAC code (average error
probability) satisﬁes

Therefore (11) is satisﬁed if

log n
n

any

= avg. error prob. with PY|X1 X2

where Z ∼ N (0, Σn ). Also by the choice of τ n we have

τn
−O
n

j=1

decoder), let

1
Pr(Zn ≥ τ n ) − Pr(Z ≥ √ (τ n − nCn ) ≤ γn
n

[R1 , R2 , R1 + R2 ] ∈

3

.

Finally, by combining Cn − I(PU X1 X2 , W ) 1 = O(1/n)
and Σn −V(PU X1 X2 , W ) 1 = O(1/n) (which can be shown
using (6)) and the fact that PU X1 X2 and τ n are arbitrary, the
theorem is proved.
III. O UTER R EGION
We provide general outer regions for discrete multiple
access channels (not necessarily memoryless), for both average
and maximum error probability criteria. The results can be
regarded as generalizations of the general converses for singleuser channels in [2]. For DM-MAC, a single-lettered converse
bound is also provided for the maximum error probability
criterion.

[log M1 , log M2 , log M ] ∈

PX 1 X 2 Q 1
,Q2
,Q3
Y
Y|X
Y|X
2

E

PX1 X2 × Q2 1 , PX1 X2 × Q3
Y
Y|X

3

1

PX1 X2 × W n , PX1 X2 × Q1 2 ,
Y|X
(12)

where Q1 2 , Q2 1 , and Q3 range over all distributions on
Y
Y|X
Y|X
Y n.
Sketch of the proof: By particularizing Q1 1 X2 in
Y|X
Theorem 2 to be independent of X1 , the error probability 1
is at least 1 − 1/M1 . Similarly we have 2 ≥ 1 − 1/M2 and
3 ≥ 1 − 1/M1 M2 . Hence the theorem.

where V11 and V22 are the respective ﬁrst and second entries
on the diagonal of the dispersion matrix V(PU X1 X2 , W ) of
(5).
Sketch of the proof: We prove (14) and (15) follows
in a similar manner. Let m2 = 1 and divide the codeword
x2 (1) into |X2 | subsets where within each subset x2 (m2 )
has the same symbol. Now within each subset, there are
at most (n + 1)|X1 |−1 types for x1 , so there exists Ω ⊆
{(m1 , 1) : 1 ≤ m1 ≤ M1 } such that each codeword pair
(x1 (m1 ), x2 (1)) where (m1 , 1) ∈ Ω has the same joint type
within each subset. If we let U = x2 (1), then each codeword
pair (x1 , x2 ) associated with message pair in Ω has the same
conditional joint type Tx1 x2 |U . Now we particularize QY in
Theorem 5 to be PY|U , then β1− (W n (·|x1 x2 ), PY|U ) is
independent of x1 and x2 . On the other hand, Ω has size
at least M1 (n + 1)−|X2 |(|X1 |−1) . Therefore (14) follows from
Theorem 5 and [2, Lemma 58].
Note that Theorem 6 provides bounds only on M1 and
M2 . Although a similar bound on M = M1 M2 is strongly
desirable, we have not yet been able to generalize the result
at this time. Since messages m1 and m2 are independent, it
is difﬁcult to ﬁnd a large set Ω where each codeword pair has
the same joint type. Also the bounds on M1 and M2 do not
match the inner region in Theorem 1.

B. General Converse Bound: Maximum Error Probability
Theorem 4: For any two channels PY|X1 X2 and QY|X1 X2 ,
a MAC code C (possibly randomized decoder), and a subset
Ω of the set of message pairs M1 × M2 , let
Ω

=

Ω

=

max

Pe (m1 , m2 , PY|X1 X2 )

max

Pe (m1 , m2 , QY|X1 X2 ).

(m1 ,m2 )∈Ω
(m1 ,m2 )∈Ω

Then
min

(x1 ,x2 )∈C(Ω)

β1− Ω (PY|X1 =x1 ,X2 =x2 ,

QY|X1 =x1 ,X2 =x2 ) ≤ 1 −

Ω.

Proof: Let (m∗ , m∗ ) ∈ Ω be the message pair
1
2
with Pe (m∗ , m∗ , QY|X1 X2 ) =
1
2
Ω . Consider the test
δ for deciding between PY|X1 =x1 (m∗ ),X2 =x2 (m∗ ) and
1
2
QY|X1 =x1 (m∗ ),X2 =x2 (m∗ ) that declares P if the message is
1
2
decoded correctly. The probability that the test is correct if P is
the actual distribution is EP [δ(x1 (m∗ ), x2 (m∗ ), Y)] ≥ 1− Ω ,
1
2
and the probability that the test is incorrect if Q is the actual
distribution is EQ [δ(x1 (m∗ ), x2 (m∗ ), Y)] = 1− Ω . Therefore
1
2
β1− Ω (PY|X1 =x1 (m∗ ),X2 =x2 (m∗ ) ,
1
2
QY|X1 =x1 (m∗ ),X2 =x2 (m∗ ) ) ≤ 1 −
2
1

IV. N UMERICAL E XAMPLES AND C OMPARISONS
In this section we compare our inner and outer regions
in Sec. II and III with two concurrent inner region results
obtained by Tan and Kosut [10] and by MolavianJazi and
Laneman [11]. Our inner region is the largest among the
three, and numerical results are provided for two DM-MAC
examples.
Our inner region (denoted by Rin,H-M in the sequel) is given
n,
by

Ω.

Theorem 5: Let C be an (n, M1 , M2 , ) MAC code (maximum error probability) and let Ω be a subset of the set of
message pairs M1 × M2 . Then
|Ω| ≤ inf

max

QY (x1 ,x2 )∈C(Ω)

β1− (PY|X1 X2 , QY )

−1

Rin,H-M (W ) = ∪PU X1 X2 Rin,H-M (PU X1 X2 , W )
n,
n,

(13)

where

where QY ranges over all distributions on Y n .
Proof: By particularizing QY|X1 X2 in Theorem 4 independent of X1 and X2 , the maximum error probability Ω is
at least 1 − 1/|Ω|. Also Ω is trivially upper-bounded by .
The claim follows.

1
Rin,H-M (PU X1 X2 , W ) = I − √ Qinv (V, ).
n,
n

√

˜
V(PU X1 X2 , W )

sup [nI(X1 ; Y |X2 , U )−
(14)

nV22 Q−1 ( ) + |X1 ||X2 | log n + O(1)

(15)

√

Cov [i(U, X1 , X2 , W )] .

(18)

Tan and Kosut’s inner region has a form similar to (16)
and (17) while MolavianJazi and Laneman’s is a union of
pentagons. When no time sharing is necessary, our inner
region coincides with Tan and Kosut’s, and MolavianJazi and
Laneman’s is slightly smaller. When time sharing is necessary,
it can be shown (omitted here) by the law of total covariance
that our inner region is strictly bigger than the other two.
The three inner regions coincide only under some degenerate
cases.3

PU X 1 X 2

nV11 Q−1 ( ) + |X1 ||X2 | log n + O(1)
log M2 ≤ sup [nI(X2 ; Y |X1 , U )−

(17)

In [10, Theorem 2] and [11, Theorem 1], the inner regions are
characterized in terms of the unconditional covariance matrix
of the information density vector

C. Single-Lettered Converse Bound: Maximum Error Probability
The formulas of (12) and (13) are hard to evaluate in
general. Here we provide a single-lettered converse bounds for
DM-MAC W under the maximum error probability criterion.
Theorem 6: Every sequence of (n, M1 , M2 , ) MAC codes
(maximum error probability) satisﬁes
log M1 ≤

(16)

PU X 1 X 2

3 One can show that the necessary condition for the three regions to coincide
˜
is V = V has rank at most one.

4

0.6

2

2

0.6
0.4

0.4
0.2

0.2
0
0

0.2

0.4

0.6

0.8

0
0

1

Rate R , bits/ch. use

with probability 1 − δ
with probability δ

δ(1 − δ)
δ(1 − δ)
3
2 δ(1 − δ)

1

The dispersion matrix V is trivially 0, but for any p1 we have


p1 p2 −p1 p2 0
˜
V = −p1 p2 p1 p2 0 .
0
0
0

The capacity region C(W ), inner and outer regions are shown
in Fig. 3. Our inner region is much larger than the other two.
R EFERENCES
[1] V. Strassen, “Asymptotische absch¨ tzungen in Shannons informationsa
theorie,” Trans. Third. Prague Conf. Inf. Th., pp. 689–723, 1962.
[2] Y. Polyanskiy, H. V. Poor, and S. Verdu, “Channel coding rate in the
ﬁnite blocklength regime,” IEEE Trans. Inf. Theory, vol. 56, no. 5, pp.
2307–2359, 2010.
[3] Y. Polyanskiy, H. Poor, and S. Verdu and, “Dispersion of the GilbertElliott channel,” Information Theory, IEEE Transactions on, vol. 57,
no. 4, pp. 1829 –1848, April 2011.
[4] A. Ingber and M. Feder, “Finite blocklength coding for channels with
side information at the receiver,” in Convention of Electrical and
Electronics Engineers in Israel (IEEEI), 2010.
[5] R. Ahlswede, “Multi-way communication channels,” in Proceedings of
2nd International Symposium on Information Theory, 1971, pp. 23–52.
[6] H. H. J. Liao, “Multiple access channels,” Ph.D. dissertation, University
of Hawaii, Honolulu, 1972.
[7] G. Dueck, “Maximal error capacity regions are smaller than average
error capacity regions for multi-user channels,” Problems of Control
and Information Theory, vol. 7, no. 1, pp. 11–19, 1978.
[8] ——, “The strong converse to the coding theorem for the multiple-access
channel,” J. Comb. Inform. Syst. Sci., vol. 6, pp. 187–196, 1981.
[9] R. Ahlswede, “An elementary proof of the strong converse theorem for
the multiple-access channel,” J. Comb. Inform. Syst. Sci, vol. 7, no. 3,
pp. 216–230, 1982.
[10] V. Y. F. Tan and O. Kosut. (2012, Feb.) On the dispersions
of three network information theory problems. [Online]. Available:
http://arxiv.org/abs/1201.3901v2
[11] E. MolavianJazi and J. N. Laneman, “Multiaccess communication in
the ﬁnite blocklength regime,” in Information Theory and Applications
Workshop (ITA), 2012, Feb. 2012.
[12] V. Bentkus, “On the dependence of the Berry-Esseen bound on dimension,” J. Stat. Planning and Inference, vol. 113, pp. 385–402, 2003.
[13] T. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd ed.
Hoboken, NJ: John Wiley & Sons, 2006.

3
I = [1 − δ, 1 − δ, (1 − δ)]
2
δ(1 − δ)
˜
V = V =  δ(1 − δ)
3
2 δ(1 − δ)

0.8

and by ranging p1 from 0 to 1 we obtain the capacity region
as
C(W ) = {R1 + R2 ≤ 1}.

where δ is the erasure error probability. The channel is a
noisy version of the binary erasure MAC discussed in [13,
Example 15.3.3]. Its capacity region is uniquely achieved by
input distributions PX1 = PX2 = Bernoulli(1/2) (no time
sharing necessary). The mutual information vector and the
dispersion matrix are given respectively by



0.6

Fig. 3. Noiseless XOR MAC: The Ahslwede-Liao capacity region C(W ), the
inner regions Rin (W ) (three constructions), and the outer region (combining
n,
Theorem 6 and (4)). Parameters: Blocklength n = 200 and error probability
−3 . In this example, our inner region is much larger than that of Tan
= 10
and Kosut and that of MolavianJazi and Laneman.

Example 1 (Noisy Binary Erasure MAC): Let X1 = X2 =
{0, 1} and a quaternary output Y = {0, 1, 2, e}. The channel
W is deﬁned by
X1 + X2 ,
e,

0.4
1

Fig. 2. Noisy binary erasure MAC: The Ahslwede-Liao capacity region
C(W ), the inner regions Rin (W ) (three constructions), and the outer region
n,
(combining Theorem 6 and (4)). Parameters: Erasure probability δ = 0.1,
blocklength n = 50, and error probability = 10−3 . In this example, our
inner region coincides with that of Tan and Kosut while MolavianJazi and
Laneman’s is slightly smaller.

Y =

0.2

Rate R , bits/ch. use

1

and

Outer bound [H−M]
Inner bound [H−M]
Inner bound [T−K]
Inner bound [M−L]
Capacity region [A−L]

0.8
Rate R , bits/ch. use

0.8
Rate R , bits/ch. use

1

Outer bound [H−M]
Inner bound [H−M]
Inner bound [T−K]
Inner bound [M−L]
Capacity region [A−L]

1


− δ)
.
− δ)
1
4 (1 − δ)(1 + 9δ)
3
2 δ(1
3
2 δ(1

The capacity region C(W ) and the inner and outer regions
are shown in Fig. 2.4 The three inner regions are quite close
˜
because V = V.
Example 2 (Noiseless XOR MAC): Let X1 = X2 = Y =
{0, 1}. The channel W is deterministic and is deﬁned by
Y = X1 ⊕ X2 .
Let U = 1 with probability p1 and U = 2 with probability
p2 = 1 − p1 . Let Xj ≡ 0 if U = j and PXj = Bernoulli(1/2)
if U = j for j = 1, 2. Now the mutual information vector
equals
I = [p1 , p2 , 1]
4 The capacity region and the inner regions are under the average error
probability criterion, while the outer region the maximum error probability
criterion.

5

