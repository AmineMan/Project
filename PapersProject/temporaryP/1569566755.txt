Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 18:43:07 2012
ModDate:        Tue Jun 19 12:55:48 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      598070 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566755

The Sensitivity of Compressed Sensing Performance
to Relaxation of Sparsity
David Donoho and Galen Reeves
Department of Statistics
Stanford University

the ease of certain calculations in the strict sparsity case,
along with the ease of explaining the strict sparsity model,
are dominant considerations. Dramatically less is known about
the behavior of procedures in the case where only approximate
sparsity holds; for references, see below.
In this paper we consider a formal model of approximate
sparsity which, roughly speaking, assumes that x0 is close
in p-th power mean to a k-sparse vector – the case p = 2
concerns mean-squared distance from strict sparsity. We are
interested in the sensitivity of performance to the sparsity
assumption, namely, in how rapidly the performance degrades
as we go away from the strict sparsity model.
We study a very popular approach in the compressed sensing
literature, which estimates x0 via the solution x1,λ of the
following convex optimization problem

Abstract—Many papers studying compressed sensing consider
the noisy underdetermined system of linear equations: y = Ax0 +
z, with n × N measurement matrix A, n < N , and Gaussian
white noise z ∼ N(0, σ 2 I). Both y and A are known, both x0
and z are unknown, and we seek an approximation to x0 ; we
let δ = n/N ∈ (0, 1) denote the undersampling fraction. In the
popular strict sparsity model of compressed sensing, such papers
further assume that x0 has at most a speciﬁed fraction ε of
nonzeros.
In this paper, we relax the assumption of strict sparsity by
assuming the vector x0 is close in mean p-th power to a sparse
signal. We study how this relaxation affects the performance of
ˆ1,λ
1 -penalized 2 minimization, in which the reconstruction x
solves min y − Ax 2 /2 + λ x 1 .
2
We study asymptotic mean-squared error (AMSE), the largesystem limit of the MSE of x1,λ . Using recently developed tools
ˆ
based on Approximate Message Passing (AMP), we develop
∗
expressions for minimax AMSE Mε,p (δ, ξ, σ) – max over all
approximately sparse signals, min over penalizations λ, where
ξ measures the deviation from strict sparsity. There is of course
a phase transition curve δ ∗ = δ ∗ (ε); only above this curve,
δ > δ ∗ (ε), can we have exact recovery even in the noiselessdata strict-sparsity setting. It turns out that the minimax AMSE
can be characterized succinctly by a coefﬁcient sens∗ (ε, δ) which
p
we refer to as the sparsity-relaxation sensitivity. We give explicit
∗
expressions for sensp (ε, δ), compute them, and interpret them.
Our approach yields precise formulas in place of loose order
bounds based on restricted isometry property and instance
optimality results. Our formulas reveal that sensitivity is ﬁnite
everywhere exact recovery is possible under strict sparsity, and
that sensitivity to added random noise in the measurements y is
smaller than the sensitivity to adding a comparable amount of
noise to the estimand x0 . Our methods can also treat the mean
q-th power loss. The methods themselves are based on minimax
decision theory and seem of independent interest.

(P2,λ,1 )

2
2

+ λ x 1.

(2)

This approach has been called LASSO, Basis Pursuit, or more
prosaically, 1 -penalized least-squares [1]–[3].
We obtain information about the behavior of x1,λ by largeˆ
system analysis; let n, N tend to inﬁnity so that1 n ∼ δN
and correspondingly let the number of nonzeros k ∼ ε · N ;
thus we have a phase space 0 ≤ δ, ε ≤ 1, expressing different
combinations of undersampling δ and sparsity ε.
Our model deﬁnes approximate sparsity by saying that the
k-term approximation to x0 has an error whose p-th power
mean is at most ξ p . Equivalently, for the empirical distribution
ν of entries in x0 , ν = (1 − ε)ν0 + εν1 , where ν0 has p-th
moment bounded by ξ p and ν1 is unconstrained. We let Fε,p,ξ
denote the class of such distributions.
Adopting this viewpoint, we use previous work [4]–[6]
which developed expressions for the so-called asymptotic
mean-squared error (AMSE). We evaluate the minimax AMSE
over all approximately sparse sequences:

I. I NTRODUCTION
The following strict sparsity model is heavily studied in
the developing literature of compressed sensing. Consider the
noisy underdetermined system of linear equations:
y = Ax0 + z,

1
y − Ax
2

minimize

(1)

∗
Mε,p (δ, ξ, σ) = max min AMSE(ˆ1,λ , ν, σ 2 );
x
ν

where the matrix A is n × N , n < N , the N -vector x0 is ksparse (i.e. it has at most k non-zero entries), and z ∈ Rn is a
Gaussian white noise z ∼ N(0, σ 2 I). Both y and A are known,
both x0 and z are unknown, and we seek an approximation
to x0 . The term strict sparsity refers to the fact that in this
model all but k of the entries in x0 are vanishing.
It is frequently remarked in this literature that strict sparsity
is too strong a condition for realistic applications; however,

λ

(3)

here ν is chosen from Fε,p,ξ , the data have noise level σ 2 ,
and λ denotes the tuning parameter of the 1 -penalized 2
∗
minimization. The case Mε,p (δ, 0, 1) concerns the minimax
MSE at noise level σ = 1 in the strict sparsity model [4].
1 Here and below we write a ∼ b if a/b → 1 as both quantities tend to
inﬁnity.

1

Contours of Sens (!,")
2

It turns out that the minimax MSE degrades gracefully as
ξ increases from 0 to bigger values. We may write:

50

1

0.9

where r = min(p, 2). We call the coefﬁcient sens∗ (ε, δ), the
p
sparsity-relaxation sensitivity. A formula for the sensitivity
will be given in terms of a corresponding much more elementary and concrete notion: the sensitivity of the minimax
MSE of soft thresholding to relaxation of sparsity. Let Mε,p (ξ)
denote the minimax MSE of scalar soft thresholding over class
Fε,p,ξ deﬁned in Section II below. This, it turns out, has the
following behavior

0.7

50

10

5
1.5

2

5

3

0.2

2
10

0.1

5 3
2

0.1

1.5
1.25

1.5
1.25
1.1

.5
11.25
1.1

0.2

0.3

1.1

0.4

0.5
!

0.6

0.7

0.8

0.9

1

Fig. 1. Contour lines of the sparsity-relaxation sensitivity sens∗ ( , δ)
2
as a function of ε/δ and δ. The dotted black curve graphs the phase
boundary: above this curve, the minimax MSE is unbounded.

ε → 0.

The most precise information about compressed sensing
algorithms has been developed not in applied mathematics,
but instead in information theory. Examples include [10]–[12],
much of which applies the replica method. The closest to the
present work is the paper of Xu and Hassibi [13], which in
essence precisely determines C1,1 , which may be called the
case q = 1, p = 1 of our general setting; our paper focuses on
inequalities of the q = 2, p type.
We note that relaxations with respect to perturbations of the
matrix have also been considered previously [14]–[16].

There is by now a massive amount of literature discussing
aspects of the compressed sensing problem. Much of this work
is seemingly very different from the work described here;
however, there are strong links which may not at ﬁrst be
visible. For example, some applied mathematics have worked
on the problem of instance optimality [7], seeking to provide
inequalities of the form
≤ Cq,p x0 − xk

10

3

A. Related Work

q

50

0.3

Now Mε ∼ 2 log(1/ε) as ε → 0, while sensp (ε) → 1 as
ε → 0.
In this paper we derive precise formulas for the basic objects
just introduced, isolate some important structural features and
numerical properties, and discuss implications. The ability
to do this provides precise quantitative information about a
number of issues in compressed sensing that were previously
not possible to study in any detail. For a detailed statement of
the main results, see section III.

x∗ − x0

10

0.5

0.4

where r = min(p, 2) and Mε and sensp (ε) are derived in
Section II below. In particular, we have Mε → 0 as ε → 0
and, for p ≥ 2,
sensp (ε) → 1,

50

2

"/!

0.6

Mε,p (ξ) = Mε + sensp (ε) · ξ r + o(ξ r ),

0 ≤ sensp (ε) ≤ 1,

5

0.8

3

∗
∗
Mε,p (δ, ξ, 1) = Mε,p (δ, 0, 1) + sens∗ (ε, δ) · ξ r + o(ξ r ),
p

II. M INIMAX MSE OF S CALAR S OFT T HRESHOLDING
Following the work in [5], it is now well understood that
the MSE of LASSO in the compressed sensing problem can
be characterized by the MSE of soft thresholding in a related
scalar problem. In this section, we provide a new analysis of
the scalar problem that allows us to characterize the sensitivity
to relaxed sparsity.
To begin, let Fε,ξ,p denote the set of probability measures
ν on the real line expressible as mixtures of two components:
ﬁrst, a component ν0 with p’th moment bounded by ξ and large
mixing parameter (1 − ε); second, an unrestricted component
ν1 , with small mixing parameter ε:

p

∗

where x is the reconstruction and xk denotes the best k-term
approximation. Since this has to be true for all x0 , evaluating
Cq,p is effectively the same thing as asking for the minimax qmean error subject to x0 having a given p-mean discrepancy
from strict k-sparsity. In other words, this paper determines
C2,p 2 .
At least initially, much of the work of the type comparable
to this paper came from applied mathematicians, who aimed
largely to give qualitative estimates or loose quantitative
estimates; much of the work on restricted isometry property
estimates is of this character [8]; while this literature derives
the strongest implications available from the RIP condition,
those implications are seemingly not close to describing the
actual best bounds, i.e. precisely determining Cq,p [9].

Fε,ξ,p = ν : ν = (1−ε)ν0 + εν1 ; Eν0 |X|p ≤ ξ p .

(4)

This family of distributions is naturally associated with signals
that are approximately sparse. For example, the empirical
distribution νN of an N -vector x is an element of Fk/n,ξ,p
if and only if there exists a k-sparse approximation x such
ˆ
that x − x lies in an p ball of radius (N − k)1/p ξ, or in other
ˆ
words
νN ∈ Fk/n,ξ,p

2 Modulo some ﬁne points about the asymptotic model, which are more
carefully discussed in [6].

2

⇔

x−x
ˆ

inf
xk : x
ˆ

0 =k

p
p

≤ (N − k) ξ p .

Minimax MSE Mp(ε,ξ), various p, ε = 0.01
1

The mixing parameter ε corresponds to the fraction of nonzeros in the strict sparsity setting.
Given an observation y and threshold τ the soft thresholding
function is given by

y − τ, if y ≥ τ

η(y; τ ) = 0,
(5)
if −τ < y < τ .


y + τ, if y ≤ −τ

0.9

0.8

0.7

Mp(ε,ξ)

0.6

0.5

0.4

For a given probability distribution ν and threshold τ , the soft
thresholding MSE is deﬁned by
mse(ν, τ ) = E |η(X + Z; τ ) − X|2

0.3

0.25
0.50
0.75
1
1.25
1.50
1.75
2

0.2

(6)

0.1

where the expectation is with respect to independent random
variables X ∼ ν and Z ∼ N (0, 1).
The fundamental concept needed for our results is the
minimax MSE of soft thresholding over the class Fε,ξ,p , which
is given by
Mε,p (ξ) = inf

sup

τ ≥0 ν∈F (ε,ξ,p)

mse(ν, τ ).

0

0

0.1

0.2

0.3

0.4

0.5
ξp

0.6

0.7

0.8

0.9

1

Fig. 2. Minimax MSE of scalar soft thresholding Mε,p (ξ) as a function of
ξ p for ε = 0.01 and various p.

(7)
A. The Case 0 < p < 2

The minimax MSE can be viewed as the value of a game
against nature where the statistician chooses the threshold τ ,
nature chooses the distribution ν, and the statistician pays
nature an amount equal to MSE. The special case of exact
sparsity, Mε,p (0), was studied extensively in [4], [17]. The
special case of p balls, M0,p (ξ), with 0 < p < 1 was studied
in [17].
It may seem difﬁcult to numerically solve the problem of
maximizing over ν ∈ Fε,ξ,p because, after all, this is an
inﬁnite-dimensional problem. The following results simplify
things considerably.

Our ﬁrst result gives a precise expression for the worst case
MSE corresponding to a given threshold τ , which is given by
Mε,p (ξ; τ ) =

sup

mse(ν, τ ).

(8)

ν∈F (ε,ξ,p)

The proof follows from Lemmas 1 and 2 and the fact that
mse(δµ , τ ) is an increasing function of µ.
Lemma 3. For 0 < p < 2, the worst case MSE obeys:
Mε,p (ξ; τ ) = Mε (τ ) + sε,ξ,p (τ ) · ξ p

(9)

where

Lemma 1. The minimax problem in Eq. (7) has a saddle point,
i.e. a pair (νε,ξ,p , τε,ξ,p ) such that

sε,ξ,p (τ ) = (1 − ε) sup
µ≥ξ

mse(ν, τε,ξ,p ) ≤ mse(νε,ξ,p , τε,ξ,p ) ≤ mse(νε,ξ,p , τ ).

mse(δµ , τ ) − mse(δ0 , τ )
.
µp

(10)

Using Lemma 3, the minimax MSE Mε,p (ξ) can be computed numerically by maximizing over τ on the right-hand
side of (9). Figure 2 displays its behavior as a function of ξ
for various values of p.
To characterize the sensitivity to small perturbations away
from sparse signals, we deﬁne sensp (ε) to be the leading
coefﬁcient in the power expansion of Mε,p (ξ) at the point
ξ = 0.

Moreover, for any threshold τ , the least-favorable distribution
νε,ξ,p (τ ) that achieves supν∈Fε,ξ,p mse(ν, τ ) is supported on
at most 3 points.
Lemma 2. For 0 < p < 2, there exits up > 0 such that the
mapping u → mse(δu1/p , τ ) is convex over the interval [0, up ],
and concave over the interval [up , ∞) for some up > 0. For
p ≥ 2, the mapping u → mse(δu1/p , τ ) is concave.

Lemma 4. For 0 < p < 2, the minimax MSE obeys

The proof of Lemma 1 is closely related to results in [6] and
is omitted. The proof of Lemma 2 follows straightforwardly
from computing the ﬁrst and second derivatives of the function
mse(δµ , τ ) with respect to µ.
It turns out that the cases 0 < p < 2 and p ≥ 2 have
dramatically different behavior. We consider them separately
in the following sections. For notational convenience, we use
the shorthand Mε and τε to denote the minimax MSE and
threshold, respectively, for the class Fε of all distributions ν
obeying ν({0}) ≥ (1 − ε).

Mε,p (ξ) = Mε + sensp (ε) · ξ p + o(ξ p ),

ξ → 0,

(11)

where the sensitivity is given by
mse(δµ , τε ) − mse(δ0 , τε )
.
µp
µ≥0

sensp (ε) = (1 − ε) · sup

(12)

Furthermore, the minimax MSE is bounded by
Mε ≤ Mε,p (ξ) = Mε + sensp (ε) · ξ p .

3

(13)

The sensitivity sensp (ε) can be evaluated numerically using
the fact that the supremum on the right-hand side of (12) is
attained at the solution µ to the ﬁxed point equation
2 2
µ Pr[|µ + Z| ≤ τε ] = mse(δµ , τε ) − mse(δ0 , τε )
p

III. M AIN R ESULTS
The papers [4]–[6] developed a formalism for evaluating
operating characteristics of LASSO; for example [4] did so in
the case of Fε . We now give the comparable development for
case Fε,ξ,p , without pausing to repeat the concepts of [4].
The fundamental object of interest is the minimax LASSO
AMSE which is given by:

(14)

where Z ∼ N (0, 1). With a bit of work, it can shown that
sens(ε, p) ∼

2 log(ε−1 ) − Φ−1 ( p )
2

2−p

ε → 0, (15)

,

where Φ−1 (p) is the functional inverse of Φ(x) =

x
−∞

∗
Mε,p (δ, ξ, σ) =

φ(t)dt.

inf AMSE(λ; ν; σ).

(25)

By a simple rescaling of the problem we obtain

B. The Case p ≥ 2

∗
∗
Mε,p (δ, ξ, σ) = Mε,p (δ, ξ/σ, 1) · σ 2 .

We now perform a similar analysis for the case p > 2.
Lemma 5. For p > 2, the worst case MSE obeys:
Mε,p (ξ; τ ) = (1 − ε) mse(δξ , τ ) + ε (1 + τ 2 ).

(16)

lim

N →∞

ξ→0

(17)

∗
Mε,p (δ, 0, σ) =

where the sensitivity is given by
sensp (ε) = (1 − ε)[Φ(τε ) − Φ(−τε )].

Mε ≤ Mε,p (ξ) = Mε + sensp (ε) · ξ p .

(27)

Mε
· σ2
1 − Mε /δ

(28)

(19)

A. Relaxation of Sparsity: The Noiseless Case
Our ﬁrst main result is an analogue of the noise sensitivity
for the relaxation of sparsity in the noiseless setting.
Theorem 1. For any pair (δ, ε) with δ > Mε the minimax
LASSO AMSE is given by

balls

The characterization of the minimax MSE given in the
previous sections is sharp for small ξ. This section provides a
complementary analysis for the case of small ε.
Since mse(δµ , τ ) → (1 + τ 2 ) as µ → ∞, we obtain
2

Mε,p (ξ; τ ) = (1 − ε)M0,p (ξ; τ ) + ε (1 + τ ).

∗
Mε,p (δ, ξ, 0) =

Mε,p (ξ) ≥ (1 − ε)M0,p (ξ) + ε

(20)

Mε,p (ξ) ≤ (1 − ε)M0,p (ξ) + ε(1 +

(22)

Since the least-favorable distribution νε,ξ,p converges weakly
to ν0,ξ,p as ξ becomes small, we have
2
Mε,p (ξ) = (1 − ε)M (0, ξ, p) + ε(1 + τξ,p ) + o(ε).

(23)

In [17], it is shown that, for 0 < p < 2:
ξ → 0.

· ξ2,

(29)

It is interesting to note that the sparsity sensitivity in (29)
is ﬁnite everywhere that exact recovery is possible under
strict recovery (i.e. everywhere that δ > Mε ). Furthermore,
in conjunction with (28), our analysis allows us to compare
the effects of noise added directly to the signal, prior to
multiplication by a measurement matrix A, with the effects of
noise added to the measurements3 . Near the critical boundary
δ ≈ Mε , the sparsity sensitivity is strictly greater then the
noise sensitivity, thus demonstrating that noise added directly
to the signal is more detrimental. Conversely, for relatively
large values of δ, the opposite behavior occurs.

(21)
2
τξ,p ).

δ
2
−1
Mε,p (δ)

−1
for all ξ > 0 where Mε,p (δ) denotes the functional inverse of
the minimax MSE of scalar soft thresholding deﬁned in Eq. (7).

If we let τξ,p denote the minimax threshold for the class F0,ξ,p ,
we then obtain the bounds:

2 log(ξ −p ),

= AMSE(λ; ν; σ).

for all σ > 0. Here, the ﬁrst term on the right-hand side is
referred to as the noise sensitivity.

The sensitivity is upper bounded by one for all p and ε and
scales as sensp (ε) → 1 as ε → 0.

2
τξ,p ∼

2

(18)

Furthermore, the minimax MSE is bounded by

p

1 1,λ
x − x0
N

In this sense, the minimax LASSO AMSE in (25) provides
an upper bound on the MSE of the optimally tuned LASSO
estimate.
The special case of ξ = 0 corresponds to the classical notion
of strict sparsity. In [4] it is shown that, for any pair (δ, ε) with
δ > Mε , the minimax LASSO AMSE is given by

Lemma 6. For p ≥ 2, the minimax MSE obeys
Mε,p (ξ) = Mε + sensp (ε) · ξ 2 + o(ξ 2 ),

(26)

One implication of the large-system limit is that if the
entries of x0 are i.i.d. ν, then the following limit holds almost
surely and in expectation:

Since the right-hand side of (16) does not depend on p, the
minimax MSE Mε,p (ξ) is independent of p ≥ 2. Its behavior
is displayed in Figure 2.
As before, we deﬁne sensp (ε) to be the scaling coefﬁcient
in the power expansion of Mε,p (ξ). This time, however, the
power of the leading term does not depend on p.

C. Minimaxity over

sup

ν∈Fε,ξ,p λ∈R+

(24)

3 This

4

comparison was studied previously in [18].

Contours of M* (!,"), $ = 0.05, # = 0.3
#,$,2

0.9

Mε
1−Mε /δ

σ2 +

noise sensitivity

0.9
1.
1.
2

0.7

0.3

0.4

1

0.1

0.2

0.

0.4

1.

9

6

0.

0.

sens2 (ε)
1−Mε /δ

ξ 2 + o(ξ 2 )

sens∗ ( ,δ)
2

as ξ → 0. The sparsity-relaxation sensitivity sens∗ ( , δ) can
2
be evaluated numerically using the results given in Section II.
Its contours are displayed in Figure 1 as a function of δ and
ε/δ. Finally, for 0 < p < 2 we have, as ξ → 0,

3

1.1

2

1.4

1

∗
Mε,p (δ, ξ, σ) =

0.8

0 05

0.5

0.6

0.8
"

1.5

1.2

0.7

0.4

0.3

1

0.6

1.3

0.2
0.1

0.8

1.1

0.6

0.5

0 05
1.2

Combining Theorem 3 with (26) gives the scaling behavior
for general σ > 0. For p ≥ 2, we obtain the simple expression:

1

0.

∗
∗
Mε,p (δ, ξ, σ) = Mε,p (δ, 0, σ) + sens∗ ( , δ) ξ p σ 2−p + o(ξ p ).
p

8

5

0.2

0.

0.

0.05

2

0.
0

0.

3

1

0

1

7

0.2

ACKNOWLEDGMENT

0.

4

0.4

0.6

0.8

1

1.2

This work was supported by grants from NSF-DMS and
an NSF VIGRE Postdoctoral Fellowship. Thanks to Andrea
Montanari and Iain Johnstone for helpful discussions.

1.4

2

!

∗
Contour lines of the minimax LASSO AMSE Mε,2 (δ, ξ, σ)
as a function of ξ 2 and σ 2 with ε = 0.05 and δ = 0.3.

Fig. 3.

R EFERENCES
[1] R. Tibshirani, “Regression shrinkage and selection with the lasso,” J.
Royal. Statist. Soc B, vol. 58, pp. 267–288, 1996.
[2] S. Chen and D. Donoho, “Examples of basis pursuit,” in Proceedings
of Wavelet Applications in Signal and Image Processing III, San Diego,
CA, 1995.
[3] S. S. Chen, D. L. Donoho, and M. A. Saunders, “Atomic decomposition
by basis pursuit,” SIAM Journal on Scientiﬁc Computing, vol. 20, pp.
33–61, 1998.
[4] D. Donoho, A. Maleki, and A. Montanari, “The Noise Sensitivity Phase
Transition in Compressed Sensing,” IEEE Trans. Info. Theory, vol. 57,
pp. 6920–6942, 2011.
[5] M. Bayati and A. Montanari, “The LASSO risk for gaussian matrices,”
2010, arXiv:1008.2581.
[6] D. Donoho, I. Johnstone, and A. Montanari, “Compressed Sensing over
p Balls: Minimax Mean Squared Error,” 2011, arXiv:1103.1943v2.
[7] R. DeVore, G. Petrovaa, and P. Wojtaszczyk, “Instance-optimality in
probability with an 1 -minimization decoder,” vol. 27, pp. 275–288,
2009, http://dx.doi.org/10.1016/j.acha.2009.05.001.
[8] E. J. Candes and T. Tao, “Decoding by linear programming,” IEEE
Trans. on Inform. Theory, vol. 51, pp. 4203–4215, 2005.
[9] J. D. Blanchard, C. Cartis, and J. Tanner, “Compressed sensing: How
sharp is the restricted isometry property?” CoRR, vol. abs/1004.5026,
2010.
[10] Y. Kabashima, T. Wadayama, and T. Tanaka, “A typical reconstruction
limit for compressed sensing based on lp-norm minimization,” J.Stat.
Mech., p. L09003, 2009.
[11] D. Guo, D. Baron, and S. Shamai, “A single-letter characterization of
optimal noisy compressed sensing,” in 47th Annual Allerton Conference,
Monticello, IL, Sept. 2009.
[12] S. Rangan, A. K. Fletcher, and V. K. Goyal, “Asymptotic analysis of
map estimation via the replica method and applications to compressed
sensing,” 2009, pUT NIPS REF.
[13] W. Xu and B. Hassibi, “Precise stability phase transitions for 1
minimization: A uniﬁed geometric framework,” IEEE Transactions on
Information Theory, vol. 57, no. 10, pp. 6894–6919, 2011.
[14] M. Herman and T. Strohmer, “General deviants: An analysis of perturbations in compressed sensing,” IEEE J. Selected Topics Sig. Proc.,
vol. 4, no. 2, 2010.
[15] M. Herman and D. Needell, “Mixed operators in compressed sensing,”
in 44th Annual Conf. on Info. Sciences and Systems (CISS), Princeton,
NJ, Mar. 2010.
[16] Y. G. J. Ding, L. Chen, “Performance analysis of orthogonal matching
pursuit under general perturbations,” arXiv:1106.3373v2.
[17] D. L. Donoho and I. M. Johnstone, “Minimax risk over lp balls,” Prob.
Th. and Rel. Fields, vol. 99, pp. 277–303, 1994.
[18] G. Reeves and M. Gastpar, “Differences between observation and
sampling error in sparse signal reconstruction,” in SSP, Madison, WI,
Aug. 2007.

B. Relaxation of Sparsity: The Noisy Case
Our next result gives a precise characterization of the
minimax LASSO AMSE in the noisy setting.
Theorem 2. For any pair (δ, ε) with δ > Mε , the minimax
LASSO AMSE is given by
∗
Mε,p (δ, ξ, σ) = δ · npi(ξ/σ) − 1 · σ 2

(30)

where the function npi(γ) is given by the unique positive
solution to4
1
npi(γ) =
.
(31)
1 − Mε,p (γ/ npi(γ))/δ
∗
The function Mε,p (δ, ξ, σ) can be evaluated numerically
using the results given in Section II. Its contours are displayed
in Figure 3 as a function of ξ 2 and σ 2 for δ = 0.3, ε = 0.05,
and p = 2.
To characterize the sensitivity to small perturbations away
from sparse signals, we deﬁne sens∗ (ε, δ) to be the leading
p
∗
coefﬁcient in the power expansion of Mε,p (δ, ξ, σ) at the point
ξ = 0. We refer to this coefﬁcient as the sparsity-relaxation
sensitivity.

Theorem 3. Fix any pair (δ, ε) with δ > Mε . As ξ → 0, the
minimax LASSO AMSE obeys:
∗
∗
Mε,p (δ, ξ, 1) = Mε,p (δ, 0, 1) + sens∗ (ε, δ) ξ r + o(ξ r ), (32)
p

where r = min(p, 2) and
sens∗ (ε, δ) =
p

sensp (ε)
1 − Mε /δ

min(1,p/2)

(33)

and sensp (ε) is the minimax MSE of scalar soft thresholding
given in Section II.
4 The function npi(γ) corresponds to an effective noise-plus-interference
term; its signiﬁcance is discussed at length in [4].

5

