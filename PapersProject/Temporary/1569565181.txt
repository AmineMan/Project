Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 16:23:15 2012
ModDate:        Tue Jun 19 12:56:11 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      521125 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565181

Recognition Capacity Versus Search Speed in Noisy
Databases
Ertem Tuncel
Department of Electrical Engineering
University of California, Riverside, CA
Email: ertem.tuncel@ucr.edu

Abstract—The tradeoff between the number of distinguishable
objects and search speed in a data management system is investigated in an information-theoretic framework. In the discussed
scenario, incoming high-dimensional (and noisy) data vectors
are enrolled in (possibly multiple) clusters to be accessed later.
Upon receiving a random query, which is the noisy version of
an enrolled vector, the search engine retrieves only a subset of
the clusters to compare against the query. This creates tension
between the search speed (determined by the expected number
of retrieved entries) and recognition capacity (maximum possible
number of entries that can be reliably recognized). A singleletter achievable rate region is characterized and it is shown
with examples that search can be performed much faster in
the discussed scenario than in non-clustered linear scan without
compromising maximum recognition capacity.

In this paper, we take a different approach and propose
a system in which we allow more than one cluster to be
retrieved, and characterize an achievable tradeoff of only the
capacity versus the search speed. Although we also allow
an entry to be enrolled in multiple clusters, this assignment
is probabilistic, and the resultant tradeoff enrolls each entry
in only one cluster on average. Somewhat surprisingly, we
observe from the resultant tradeoff that up to a certain point,
the reduction in search speed does not come with any loss
in the capacity of the database. This is in contrast with the
capacity-storage tradeoff where any reduction from the lossless
compression rate comes at the expense of reduced recognition
capacity [4], [6].

I. I NTRODUCTION

II. P RELIMINARIES AND N OTATION

With the proliferation of high-dimensional data used for
identiﬁcation purposes, such as ﬁngerprints, iris scans, or other
biometric or behavioral patterns, comes the need for huge
disk storage space, and more importantly, the need for data
management systems (DMSs) with fast search algorithms for
reliable identiﬁcation of the entries. In any such system, there
is obvious tension between (i) the number of entries that can be
reliably identiﬁed, (ii) the speed of search, and (iii) the amount
of storage in the disk. For example, more data entries imply
slower search and larger disk space. Similarly, if the storage
space is reduced, it will result in less number of entries that
can be reliably identiﬁed due to the inherent information loss.
The information-theoretic analysis of the performance of
DMSs was initiated by [3], [7], where the maximum growth
rate of number of entries that can be reliably identiﬁed (i.e., the
capacity) was characterized. Later, in [4], [6], the analysis was
generalized to the tradeoff between capacity and storage space.
Other extensions include analysis of the tradeoff between
capacity, storage, and reconstruction distortion [5], and the
capacity-storage tradeoff over multiple databases where the
query is a function of one entry from each database [2].
As for search speed, Willems [8] introduced a clusteringbased DMS, whereby entries are clustered (in an overlapping
fashion) using a quantizer, and upon receiving a query, only
one cluster is retrieved. The tradeoff between the capacity,
the number of clusters, and the number of entries in each
cluster was derived. It turns out that in this scheme, the
optimal tradeoff comes at the expense of a signiﬁcant amount
of redundancy in storage.

We assume that M data vectors X n (m) ∈ X n for m =
1, 2, . . . , M are generated independently and according to
n
n

n

Pr[X (m) = x ] =

PX (xt ) .
t=1

In the enrollment phase, noisy versions of X n (m), denoted
by Y n (m), are observed and enrolled in one or more of the
K clusters. We model Y n (m) as the output of a discrete
memoryless channel (DMC) governed by PY |X with the
output alphabet Y. The enrollment is performed using the
enrollment function
fn : Y n → 2K − ∅
where K = {1, 2, . . . , K} and 2K is the set of all subsets of
K. Now let
Mk = {m : k ∈ fn (Y n (m))}
and
Ck = {Y n (m) : m ∈ Mk )} .
That is, Ck is the collection of Y n (m) which are enrolled in
cluster k.
Let W be independent from {X n (m), Y n (m)}M , unim=1
formly distributed in M = {1, 2, . . . , M }, and unknown
to the user of the data management system. In the identiﬁcation phase, the user observes Z n ∈ Z n , which is the
noisy version of X n (W ) corrupted by the DMC PZ|X with
Z n − X n (W ) − Y n (W ) forming a Markov chain, and desires

1

Fig. 1. The illustration of the recognition system. For example, as shown in the ﬁgure, the enrollment function chooses clusters 1 and K for Y n (m), and
upon receiving the query Z n , the noisy version of Y n (W ), the selector function chooses clusters 1 and 2. Therefore, D(Z n ) = C1 ∪ C2 , and the cost of
retrieval is α(|C1 | + |C2 |) + βK.

to identify W . For that purpose, only a subset of the clusters
indicated by the selector function

real cost of retrieving data from disk during a sequential scan
of the selected clusters.
Deﬁnition 1: An (M, K, L, n, ) data management system
consists of an enrollment function fn , using which M data
entries of dimension n are enrolled into K ≤ M clusters, a
selector function gn , and a recognition function hn such that



gn : Z n → 2K
is retrieved and compared against Z n . For any z n ∈ Z n , we
denote by D(z n ) the union of the data in the selected clusters,
i.e.,
D(z n ) =
Ck .
k∈gn

E

(z n )

k∈gn (Z n )

Identiﬁcation of the unknown W is performed by the recognition function

and
ˆ
Pr[W = W ] ≤ .

∗

hn : Z n × (Y n ) → M

Remark 1: The data management systems that were fully
treated in [3] and [7] correspond to the special case of
(M, 1, M, n, ) systems here.
Our goal in this paper is to characterize the asymptotic
behavior of data management systems in terms of rates of
exponential growth of the database size M and of the average
time spent for answering the query. For the time spent, we
propose to use a cost model which takes into account not only
the total number of retrieved entries, but also the number of
clusters. It is logical to assume that the higher the number of
clusters, the more time it takes to compute the selector function
gn . For example, it is easy to conceive a model where gn
operates by comparing Z n with cluster representatives (such
as mean, median, etc.) In this model, cluster representatives,
since exponentially many, would probably be stored in the
disk also, and therefore will have comparable cost to retrieve.
Thus, we propose a cost model given by

where for any set A, A∗ denotes the set of arbitrarily many
∞
Cartesian products of A, i.e., A∗ = i=0 Ai . More speciﬁcally, for a random query Z n , hn computes
ˆ
W = hn (Z n , D(Z n ))
as the estimate of W . The quality of this estimate is measured
by
ˆ
Pe = Pr[W = W ] .
See Figure 1 for an illustration of the system.
For ﬁnite n, the goal of the designer of a system should
be to minimize the expected number of data vectors retrieved
from disk for each query, which is given by


E

|Ck |

|Ck | = L

(1)

k∈gn (Z n )

Retrieval cost = αL + βK

subject to a given number of entries M , while ensuring Pe ≤
for some small . Note that (1) is in general larger than
E (|D(Z n )|) because of potentially multiple countings of the
same Y n (m) in (1). We will use (1) because it reﬂects the

which imposes the exponential rate of growth
1
log [Retrieval cost] ≈ max
n

2

1
1
log L, log K
n
n

.

Deﬁnition 2: A rate pair (Ri , Rs ) is achievable if for every
> 0, there exists large enough n such that there is an
(M, K, L, n, ) system satisfying
Ri −

≤

Rs +

≥

1
log M
n
1
1
log L, log K
max
n
n

Cluster selection: Similar to the enrollment phase, for given
z n ∈ Z n , the cluster selection function gn checks whether
n
(z n , T n (k)) ∈ S[ZT ]

for k = 1, 2, . . . , K − 1, and includes in the output set gn (z n )
all k for which the pair is typical. If there are no k for which
the pair is typical, gn (z n ) = ∅.
Identiﬁcation: The identiﬁcation function hn goes through
all y n in its argument set, and checks whether any of them
satisﬁes
n
(z n , y n ) ∈ S[ZY ] .

.

We close this section by introducing our typicality notation.
We use the strong typicality deﬁnitions of [1]. That is, a
sequence xn is δ-typical with respect to PX if
|π(x|xn ) − PX (x)| ≤ δPX (x)

If there is none, or more than one y n that is jointly typical
with z n , an identiﬁcation error is declared (i.e., hn outputs 0).
If there is a unique y n that satisﬁes typicality, hn outputs the
index of that y n in the database1 .
Expected number of retrieved vectors: For large enough
n,



for all x, where π(x|xn ) is the frequency of occurrences of
x in xn . The set of all typical sequences will be denoted as
n
S[X] . Dependence of this set on δ will be suppressed with the
understanding that it will be synchronized with other small
entities such as .
III. M AIN R ESULT

Ri

≤ I(Y ; Z)

Rs

≥ I(Y ; T )





= E

|Ck | W = 1

(a)

k∈gn (Z n )




(b)

≤

n
|Ck | W = 1, Z n ∈ S[Z] 

KM + E 
k∈gn

(3)

≤ I(Z; T ) .

k∈gn (Z n )

(2)
(4)

Ri − Rs

|Ck |

L = E

We are ready to present our main result which is a singleletter achievable rate region in the (Ri , Rs )-plane. Although
this result is not accompanied with a converse, and hence is
not complete, it will be crucial in understanding the advantage
of structured data management.
Theorem 1: A pair (Ri , Rs ) is achievable if there exists an
auxiliary random variable T such that T − Y − X − Z and

(c)

=

(Z n )

K−1 M

Pr Y n (m) ∈ Ck ,

KM +
k=1 m=1

n
k ∈ gn (Z n ) W = 1, Z n ∈ S[Z]

Proof: Fix > 0. For a random variable T that satisﬁes
(2)-(4) for given (Ri , Rs ), randomly generate vectors T n (k)
for k = 1, 2, . . . , K − 1 in an i.i.d. fashion according to PT ,
where K (to be chosen later) satisﬁes
1
log K ≤ Rs + .
(5)
n
These vectors will serve as the cluster representatives as
discussed in the previous section. Then enroll M noisy data
entries Y n (m) using the enrollment function fn as described
below, where
1
(6)
Ri − ≤ log M ≤ Ri − .
n
2
Enrollment: For any given y n ∈ Y n , the enrollment
function fn checks for all k whether y n and T n (k) are jointly
typical, i.e., whether

(7)

where (a) follows from symmetry in W , and (b) follows
because even when Z n is not typical, which happens with
probability less than any > 0 for large enough n, the number
of retrieved data vectors cannot be larger than KM . Also, in
(c), k = K is excluded from the summation because CK is
never retrieved. Now, the sum in (7) can be split into two
parts: m = 1 and m = 1. For m = 1, since Z n , Y n (m), and
T n (k) are independent,
n
Pr[Y n (m) ∈ Ck , k ∈ gn (Z n )|W = 1, Z n ∈ S[Z] ]
n
n
= qn Pr[(Y n (m), T n (k)) ∈ S[Y T ] ] Pr[(Z n , T n (k)) ∈ S[ZT ] ]

≤ qn 2−n[I(Y ;T )− ] 2−n[I(Z;T )−

]

.

As for m = 1, because of the assumption W = 1, (Y n (1), Z n )
is generated i.i.d.∼ PY Z . Therefore, due to the Markov lemma,
if (Y n (1), T n (k)) is jointly typical, so is (Z n , T n (k)) with
high probability. Hence, we use an upper bound which cannot
be improved exponentially:

n
(y n , T n (k)) ∈ S[Y T ] .

For each k where the pair is typical, a random decision is
made: k is included in the output set fn (y n ) with probability qn , where qn is to be determined later. If every
k = 1, 2, . . . , K − 1 ends up being excluded from the output
set, then f n (y n ) is set to {K} where the sole purpose of this
last cluster K is to collect the outlier data for which the system
concedes an identiﬁcation error from the beginning.

n
Pr[Y n (1) ∈ Ck , k ∈ gn (Z n )|W = 1, Z n ∈ S[Z] ]
n
≤ qn Pr[(Y n (1), T n (k)) ∈ S[Y T ] ]

≤ qn 2−n[I(Y ;T )−
1 It

]

.

is assumed that together with data vectors, clusters store the corresponding data IDs.

3

Bringing everything together, and further upper bounding
K − 1 and M − 1 by K and M , respectively,
L ≤

Finally,
M

E4 (m)

Pr

KM
+Kqn 2−n[I(Y ;T )−

M 2−n[I(Z;T )−

]

]

m=2
M

+1
≤

and therefore,

Pr[E4 (m)]
m=2

1
1
1
log L ≤ 3 + log K + log qn − I(Y ; T )
n
n
n
1
+ log M − I(Z; T )
n
for large enough n. Now impose on K the condition

≤ M Pr[E4 (2)]
n
≤ M Kqn Pr (Y n (2), T n (1)) ∈ S[Y T ] ,
n
n
(Z n , T n (1)) ∈ S[ZT ] , (Z n , Y n (2)) ∈ S[ZY ]

where Y n (2), Z n , and T n (1) are independently generated. It
is then easy to see

1
log K ≥ I(Y ; T ) + ,
n
which can be done whenever ≤ due to (3) and (5), and
set
2n[I(Y ;T )+ ]
qn =
K
to further obtain
1
1
log L ≤ 4 + log M − I(Z; T )
n
n
(a)
1
≤ 4 + log M + Rs − Ri
n

M

E4 (m)

Pr

≤

m=2
n
(Y n (2), T n (1)) ∈ S[Y T ]

=

≤

Rs +

=
<

where (a) follows from (4), and (b) follows from (6) whenever
is chosen to satisfy ≤ 3 .
8
Probability of identiﬁcation error: Due to symmetry, we
can assume W = 1 again. The possible events that are sources
of error are
E1

which vanishes if

E3

n

n

= {∀k < K, Y (1) ∈ Ck or (Z , T (k)) ∈

n
S[ZT ] }

n
Y n (m) ∈ Ck and (Z n , T n (k)) ∈ S[ZT ] } .

Now, Pr[E1 ] → 0 because (Z n , Y n (1)) is generated i.i.d.∼
PZY . Deﬁning
n
pn = Pr[(Y n (1), T n (k)) ∈ S[Y T ] ] ,

the second type of error event satisﬁes

exp(−Kpn qn )

=

n[I(Y ;T )+ ] −n[I(Y ;T )+

exp(−2

n[ −

exp(−2

2

]

< 6.

]

≥ I(Y ; U )

Ri

(1 − pn qn )

≤

2

Rc

K−1

≤

]

which is the same recognition capacity and retrieval cost
as in unstructured (i.e., non-clustered and linear scan) data
management systems analyzed in [3], [7].
Secondly, the resemblance of the region of Theorem 1 to [4,
Theorem 1] can be exploited in the analysis of the achievability
region. More speciﬁcally, in [4, Theorem 1], it was shown that
a compression/identiﬁcation rate pair (Rc , Ri ) is achievable if
and only if

n
= {(Z n , Y n (m)) ∈ S[ZY ] and ∃k < K with

=

−2

−n[ 2 −3 ]

Rs = Ri = I(Y ; Z)

n

Pr[E2 ]

2−n[ 2 −

]

IV. A NALYSIS

= {Y n (1) stored in CK }

E4 (m)

2n[I(Y ;Z)− 2 ] · 2n[I(Y ;T )+ ]
·2−n[I(Y ;Z)− ] · 2−n[I(Y ;T )−

The ﬁrst thing to notice about Theorem 1 is that the choice
T =constant achieves the point

n
= {(Z n , Y n (1)) ∈ S[ZY ] }

E2

n
M Kqn Pr (Z n , Y n (2)) ∈ S[ZY ]
n
· Pr T n (1) ∈ S[T |Y ] (Y n (2))

(b)

≤

n
M Kqn Pr (Z n , Y n (2)) ∈ S[ZY ] ,

≤ I(Z; U )

for some U − Y − X − Z. Deﬁning the recognition capacity
as
C(Rc ) =
max
I(Z; U ) ,

)

)

Z−X−Y −U
I(Y ; U ) ≤ Rc

which can be brought to 0 if
< . The third error event E3
c
conditioned on E2 can be understood as “for all clusters k in
which Y n (1) is stored, T n (k) is not jointly typical with Z n .”
The probability of this also vanishes as n → ∞ because of
the fact that for all such clusters Y n (1) is jointly typical with
T n (k) and using the Markov lemma.

we can restate (2)-(4) as
Ri ≤ I(Y ; Z)
Ri − Rs ≤ C(Rs )
This conversion is illustrated in Figure 2.

4

(8)
.

(9)

Example 1: PX (x) = 1 for x ∈ X = {0, 1}, and
2
PY |X and PZ|X are binary symmetric channels (BSC) with
crossover probabilities p1 and p2 , respectively. This implies
that PY (y) = 1 for y ∈ {0, 1} and PZ|Y is a also a BSC
2
with crossover probability γ = p1 p2 , where a b =
a(1 − b) + b(1 − a). Therefore,
I(Y ; Z) = 1 − H(γ)

(12)

where H(p) is the binary entropy function. It was shown in [4]
that
C(Rs ) = 1 − H(γ H−1 (1 − Rs ))
(13)

Achievable

for all 0 ≤ Rs ≤ 1, where H−1 returns values between 0
and 1 . Bringing (11), (12), and (13) together, Rs,min is the
2
solution to
Rs,min + H(γ) = H(γ

H−1 (1 − Rs,min )) .

Of course, this is not easy to solve in general, but it is not
hard to see that at the extreme of γ → 0 (no noise during
enrollment or identiﬁcation), Rs,min → 0.5 and I(Y ; Z) → 1.
1
Example 2: PX (x) = 2 for x ∈ X = {0, 1}, Y = X, and
PZ|X is the erasure channel with erasure probability . It is
clear that
I(Y ; Z) = 1 −
(14)

Achievable

and it was shown in [4] that
C(Rs ) = (1 − )Rs .

(15)

It then follows from (11), (14), and (15) that
Rs,min + (1 − )Rs,min = 1 −

Fig. 2. The connection between the achievable (Ri , Rs ) and the capacity
C(Rc ) of [4] where Rs plays the role of Rc and Ri − Rs plays that of
Ri . On the top, the achievability region is shown directly in the analogous
(Rs , Ri −Rs )-plane, and on the bottom, it is translated further into the desired
(Ri , Rs )-plane.

yielding
Rs,min =

1−
2−

which is again signiﬁcantly smaller than I(Y ; Z) = 1 − .
R EFERENCES

Of course, a direct application of [4, Lemma 1] to the
current problem yields that the achievability region in (2)-(4)
is convex, and can be characterized completely by focusing
on alphabets T of size not exceeding |Y| + 1.
Another immediate conclusion is that even when Ri =
I(Y ; Z), i.e., we do not want to compromise from maximum
recognition capacity, the search rate Rs can be signiﬁcantly
small compared to I(Y ; Z), which was the minimum search
rate in all the uncompressed and unstructured data storage
schemes before [7], [6], [4]. To achieve this maximum Ri , it
follows from (8) and (9) that Rs must satisfy
Rs + C(Rs ) ≥ I(Y ; Z) .

[1] A. E. Gamal and Y.-H. Kim. Network Information Theory, Cambridge,
2012.
[2] D. Gunduz, E. Tuncel, A. Goldsmith, and H. V. Poor, “Identiﬁcation
over multiple databases,” Proc. IEEE Int’l Symp. Inform. Theory, Seoul,
S. Korea, July 2009.
[3] J. A. O’Sullivan and N. A. Schmid, “Large deviations performance
analysis for biometrics recognition,” Proc. of Allerton Conf. on Comm.,
Control, and Computing, Oct. 2002, Monticello, IL.
[4] E. Tuncel, “Capacity/storage tradeoff in high-dimensional identiﬁcation
systems,” IEEE Trans. Inform. Theory, vol. 55, no. 5, pp. 2097-2106,
May 2009.
[5] E. Tuncel and D. Gunduz, “Identiﬁcation and lossy reconstruction in noisy
databases,” Proc. IEEE Int’l Symp. Inform. Theory, Austin, Texas, June
2010.
[6] M. B. Westover and J. A. O’Sullivan, “Achievable rates for pattern
recognition,” IEEE Trans. Inform. Theory, vol. 54, no. 1, pp. 299-320,
Jan. 2008.
[7] F. Willems, T. Kalker, J. Goseling and J.-P. Linnartz, “On the capacity
of a biometrical identiﬁcation system,” Proc. IEEE Int’l Symp. Inform.
Theory, Yokohama, Japan, July 2003.
[8] F. Willems, “Search methods for biometric identiﬁcation systems: Fundamental limits,” Proc. IEEE Int’l Symp. Inform. Theory, Seoul, South
Korea, July 2009.

(10)

Denote by Rs,min the minimum Rs that satisﬁes (10). Since
the left-hand side of (10) is increasing in Rs , we need to solve
Rs,min + C(Rs,min ) = I(Y ; Z)

(11)

to ﬁnd Rs,min .
We next go over the two examples in [4] and characterize
Rs,min .

5

