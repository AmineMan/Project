Title:          IRC_GDoF.dvi
Creator:        dvips(k) 5.991 Copyright 2011 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 09:22:06 2012
ModDate:        Tue Jun 19 12:55:13 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      359856 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566129

Lattice Coding and the Generalized Degrees of
Freedom of the Interference Channel with Relay
Anas Chaaban and Aydin Sezgin
Chair of Communication Systems
RUB, 44780 Bochum, Germany
Email: anas.chaaban@rub.de, aydin.sezgin@rub.de

consequence of this result, the DoF of the IRC is the same as
that of the IC, i.e., DoF=1. One question which immediately
arises with this result is whether this is also true in terms of
GDoF.
In this paper, we investigate the GDoF for the symmetric Gaussian IRC. Shortly, our contribution includes deriving new upper bounds on the sum-capacity, providing new
achievable sum-rates by proposing a “functional decode-andforward” [12] (FDF) scheme. The distinct feature of the
achievable strategy is that the overall message is split in three
parts, namely a private, a common and a cooperative public
part. While the former two are in use already in the basic IC,
the latter one, encoded using nested lattices, is of particular
value to overcome the multiple-access-bottleneck at the relay.
We characterize the GDoF of the IRC for all cases in which
the interference link is stronger than the source-relay link.
This characterized regime covers half the space of all possible
channel parameters for the IRC, and is especially interesting
for the IRC with weak interference. It turns out that while a
relay does not increase the DoF of the IC, it does increase its
GDoF. In the next section, we formally deﬁne the IRC and
the notation used in the paper.

Abstract—The generalized degrees of freedom (GDoF) of the
symmetric two-user Gaussian interference relay channel (IRC)
is studied. While it is known that the relay does not increase the
DoF of the IC, this is not known for the more general GDoF.
For the characterization of the GDoF, new sum-capacity upper
bounds and lower bounds are derived. The lower bounds are
obtained by a new scheme, which is based on functional decodeand-forward (FDF). The GDoF is characterized for the regime in
which the source-relay link is weaker than the interference link,
which constitutes half the overall space of channel parameters.
It is shown that the relay can indeed increase the GDoF of the
IRC and that it is achieved by FDF.

I. I NTRODUCTION
The exact characterization of the capacity of interference
networks is an open problem for several decades now. Given
the difﬁculty of the problem, there is shift of paradigm
to provide an approximate characterization of the capacity,
referred to as the degrees of freedom (DoF), which gets asymptotically tight for high signal-to-noise power ratios (SNR).
While the DoF provides interesting insights into the behaviour
of the system, the so-called generalized degrees of freedom,
or GDoF [1], is a much more powerful metric, as it allows
different signal strengths and thus captures a large variety of
scenarios.
The setups gets even more interesting for cases, in which
some of the nodes are dedicated relays. It is known that
relaying in wireless networks can play a vital role in improving
its performance in terms of coverage and achievable rates. As
such, a relay can help the network by establishing cooperation
between the nodes in the network. Interestingly enough, the capacity of even the basic point-to-point (P2P) relay channel [2]
(without interference) is an open problem, although there exist
good approximations of the capacity of the Gaussian P2P relay
channel within one bit [3].
The improvements obtained depend heavily on the capability of and restrictions at the relay, such as cognition and
causality. For example, for a network with two transmitters,
two receivers, and a relay referred to as the interference relay
channel (IRC), several capacity bounds have been derived with
a causal or with a cognitive relay [4]–[10]. As for the relay
and the interference channel (IC) individually, the capacity of
the IRC remains an open problem.
Surprisingly, in characterizing the gains in terms of DoF by
deploying a relay in a wireless interference network, it was
shown in [11] that relaying does not increase the DoF. As a

II. N ETWORK M ODEL AND N OTATION
In the symmetric Gaussian IRC (Fig. 1), transmitter i, i ∈
{1, 2}, has a message mi uniformly distributed over the set
Mi
{1, . . . , 2nRi }, to be sent to receiver i. The message
n
mi is encoded into an n-symbol codeword Xi , where Xik is
a real valued random variable, and transmits this codeword.
At time instant k, the input-output equations of this setup are
given by
yik = hd xik + hc xjk + hr xrk + zik ,
yrk = hsr x1k + hsr x2k + zrk .
for i = j, i, j ∈ {1, 2}. The coefﬁcients hd , hc , hr , hsr ≥ 0
are real valued channel gains, and xrk is the transmit signal at
the relay at time instant k. The relay is causal, which means
that xrk is a function of the previous observations at the relay,
k−1
i.e., xrk = fr (yr ). The source and relay signals must satisfy
2
a power constraint E[Xi ] ≤ P , i ∈ {1, 2, r}. The receivers’
additive noise z1 , z2 , and zr is Gaussian with zero mean and
unit variance.

1

2
2

1+β−α
2 + 2γ − 2α

α

d

2α

1+β−α
β

10
Fig. 1.

γ

1 β

The 2-user interference relay channel (IRC).

2

α

Fig. 2. β = 1.1 and γ = 0.2. A case where the GDoF of the IRC is larger
than that of the IC in both the weak and the strong interference regimes.
n
After receiving yi , receiver i uses a decoder to detect mi by
n
processing yi . The messages set, encoders, and decoders deﬁne a code denoted (2nR1 , 2nR2 , n), with an error probability
Pe deﬁned by Pe = P (m1 = m1 or m2 = m2 ). A rate pair
ˆ
ˆ
(R1 , R2 ) is said to be achievable if there exists a sequence
of (2nR1 , 2nR2 , n) codes such that Pe → 0 as n → ∞. The
capacity region C of the IRC is deﬁned as the closure of the
set of these achievable rate pairs, and the sum-capacity CΣ
is the maximum achievable sum-rate RΣ = R1 + R2 , i.e.,
CΣ = max(R1 ,R2 )∈C RΣ . The GDoF of the IRC is deﬁned as
follows.

2

log(h2 P )
c
,
log(h2 P )
d

β=

log(h2 P )
r
,
log(h2 P )
d

γ=

log(h2 P )
sr
.
log(h2 P )
d

d
10

CΣ (α, β, γ)
.
P →∞ 1 log(h2 P )
d
2

1

γ

α

β

2

Fig. 3. β = 1.4 and γ = 1.2. A case where the GDoF of the IRC is larger
than that of the IC in both the strong and the very strong interference regimes.

(1)
that is used to achieve it, and in Section VI we sketch the
proof of Theorem 1.

We deﬁne the GDoF d(α, β, γ) or simply d as
d = lim

α
β

Deﬁnition 1. Let the following variables represent the channel
strength (as in [1])
α=

2γ

2γ

IV. U PPER B OUNDS

(2)

We start by providing the following bounds that can be
obtained from the cut-set bounds [13] applied to the IRC. The
cut-set bounds for the IRC are expressed in [4], [14].

Throughout the paper, we use xn to denote the length-n
sequence (x1 , . . . , xn ), and we use C(x) = (1/2) log(1 + x),
C + (x) = max {0, C(x)}.

Theorem 2. CΣ is upper bounded by
CΣ ≤ 2C((|hd | + |hr |)2 P ) and CΣ ≤ 2C(h2 P + h2 P ).
sr
d

III. M AIN R ESULT
The main statement of the paper is characterizing the GDoF
of the IRC for all cases where h2 ≤ h2 . The GDoF in this
sr
c
case is given in the following theorem.

Using the deﬁnition of β in (1) and the ﬁrst bound in
Theorem 2, we can write CΣ ≤ 2C((|hd | + |hr |)2 P ) ≤
2 max{C(h2 P ), C((h2 P )β )+2, which, by using (2) translates
d
d
to the ﬁrst argument in the min in (3). The second argument
in (3) can be obtained similarly from the second bound
in Theorem 2. Using a similar method, the third and ﬁfth
arguments in (3) can be obtained from the bounds in [14,
Theorems 1 and 2]. The remaining expressions in (3) are
obtained from the following theorem.

Theorem 1. The GDoF of the IRC with γ ≤ α is given by


2 max{1, β}








2 max{1, γ}






max{1, α, β} + max{1, α} − α
d = min
. (3)
2 max{1, α} + γ − α








2 max{α, β, 1 − α}






2 max{α, 1 + γ − α}

Theorem 3. CΣ is upper bounded by

Figures 2 and 3 show the GDoF for two examples of the
IRC. The GDoF of the IC is also shown (dash-dotted) for
comparison. The characterization of the GDoF in the shaded
area, where α < γ, is not considered in this paper. The proof
of this theorem is provided in the next section. Namely, in
Section IV we provide the sum-capacity upper bounds that
translate to this GDoF, in Section V we describe the scheme

CΣ ≤ C(2h2 P ) + C(h2 P + h2 P ) + C + h2 /h2 − 1
sr
d
c
d
c
CΣ ≤

2C(h2 /h2
c
sr

2

+ (1 − hd /hc ) ) +

2C(2h2 P ).
sr

(4)
(5)

Due to space limitations, we only provide a sketch of the
proof of (4) in Appendix A. In the next section, we provide a
GDoF achieving scheme for the IRC.

2

Using nested-lattice coding and lattice alignment, we establish a cooperation strategy between the relay and the users.
This scheme is denoted “Functional Decode-and-Forward”
(FDF) using the terminology of [12]. We use three kinds of
messages in FDF, private (P), common (C), and cooperative
public (CP) messages. The private and the common messages
are the same as those used by Etkin et al. in the IC [1].
The CP message itself is also split into K sub-messages. The
superposition of the CP messages is decoded by the relay, and
forwarded to the destinations. Using backward decoding, the
sum-rates given in the following theorems are achievable.

xn (b) = xn (b) + xn (b) +
j
j,p
j,c

Rc ≤ C
2Rc ≤ C
K

h2 Pp
d
,
1 + h2 Pp
c
min{h2 , h2 }Pc
c
d
1 + (h2 + h2 )Pp
c
d
(h2 + h2 )Pc
c
d
1 + (h2 + h2 )Pp
c
d

,

In this scheme, the relay only decodes the CP messages.
More precisely, the relay decodes the superposition of CP
messages as follows. The relay starts decoding at the end of
(k)
(k)
block b = 1 where the sum u(k) (1) = (λ1,cp (1) + λ2,cp (1))
(k)
mod Λc is decoded, starting with k = 1 and ending with
k = K (see successive compute-and-forward [19]). Decoding
this superposition of codewords is possible as long as the rate
constraint (9) is satisﬁed.
Notice that the set of all possible values of u(k) (1) ∈
(k)
U (k) has size U (k) = 2nRcp . The relay combines all
u(k) (1), k = 1, . . . , K, into one message mr ∈ Mr .
Then the message set Mr has a size which is equal to
the size of the Cartesian product of all U (k) , i.e., |Mr | =
(k)
K
U (1) × U (2) × · · · × U (K) = 2n k=1 Rcp . The relay then
maps the message tuple (u(1) (1), . . . , u(K) (1)) to a message
mr (2) ∈ Mr to be sent in block b = 2. This message
(1)
(2)
(1)
(2)
is split into mr (2) and mr (2) with rates Rr and Rr ,
(1),n
respectively. The relay messages are then encoded to xr (2)
(2),n
(1)
and xr (2), two Gaussian codewords with powers Pr and
(2)
(1)
(2)
Pr , respectively, such that Pr + Pr ≤ P . The sum of
these codewords is sent in block 2. This process is repeated
for every block b = 1, . . . , B − 1. The relay sends in blocks
b = 2, . . . , B and does not send any signal in block 1.

(8)

(k)

(2)

(1)

(6)

where Pp + Pc + k=1 Pcp = P , Pr + Pr ≤ P , Rcp =
(k)
K
k=1 Rcp , K ∈ N, and where the constraints (9)-(11) on the
next page are satisﬁed.
Theorem 5. The
where Rc ≤ min
and Rcp satisﬁes
(k)
K
k=1 Pcp = P ,
N.

j ∈ {1, 2},

C. Relay Processing

(7)

,

(k),n

xj,cp (b)

at each block b ∈ {1, . . . , B − 1}. No messages are sent in
block B. This incurs a rate loss which, however, becomes
negligible for large B.

Theorem 4. The sum-rate RΣ = 2(Rp + Rc + Rcp ) is
achievable where
Rp ≤ C

(k)

K

the power constraint, we set Pp + Pc + k=1 Pcp = P .
Same is done at transmitter 2, using the same nested-lattices.
This enables the relay to decode the sum [17] u(k) (b) =
(k)
(k)
(k)
λ1,cp (b) + λ2,cp (b) modulo Λc . The transmitters then send
the superposition of their codes as

V. ACHIEVABILITY: F UNCTIONAL
D ECODE - AND -F ORWARD

sum-rate RΣ = 2(Rc + Rcp ) is achievable
C min{h2 , h2 }Pc , 1 C h2 Pc + h2 Pc ,
c
c
d
d
2
(12)-(14) on the next page, such that Pc +
(1)
(2)
(k)
K
Pr + Pr ≤ P , Rcp = k=1 Rcp , K ∈

Proof: Due to the lack of space, we refer the reader to
[15]–[17] for more details about nested-lattice coding. In this
work, we need nested-lattice codes with a ﬁne lattice Λf and
a coarse lattice Λc ⊆ Λf denoted (Λf , Λc ). The nested-lattice
codewords are constructed as xn = (λ − d) mod Λc where
λ ∈ Λf ∩ V(Λc ) (V(.) for fundamental Voronoi region) and d
is a random dither.

D. Decoding
The receivers wait until the end of block B where decoding
starts. Let us focus on receiver 1. At the end of block B
n
where only the relay is active, receiver 1 has y1 (B) =
(1),n
(2),n
n
hr (xr (B) + xr (B)) + z1 since the transmitters do not
(1)
(2)
send in this block. Then, mr (B − 1) and mr (B − 1) are
decoded successively in this order, which is reliable if

A. Message splitting
For a transmission block b, user 1 splits its message m1 (b)
into three parts, a private (P), a common (C), and a cooperative
public (CP) [18] part denoted m1,p (b), m1,c (b), and m1,cp (b),
respectively. Moreover, the CP message is divided into K
(k)
CP sub-messages m1,cp (b), k = 1, . . . , K. The rates of these
(1)
(2)
(K)
messages are denoted Rp , Rc , Rcp , Rcp , . . . , Rcp .

(1)

(1)
Rr ≤ C

h2 Pr
r
1+

(2)
h2 Pr
r

(2)
(2)
and Rr ≤ C(h2 Pr ).
r

(15)

Now, the receiver knows (u(1) (B − 1), . . . , u(K) (B − 1)).
Decoding proceeds backwards to block B − 1 where

B. Encoding

n
y1 (B − 1)

Brieﬂy, m1,p (b) and m1,c (b) are encoded into xn (b)
1,p
and xn (b), respectively, where X1,p ∼ N (0, Pp ) and
1,c
(k)
X1,c ∼ N (0, Pc ). Each CP message m1,cp (b) is encoded
(k),n
(k)
(k)
(k)
into x1,cp (b) = (λ1,cp (b) − d1,cp ) mod Λc using a nested(k)
(k)
(k)
lattice code (Λf , Λc ) with power Pcp . In order to satisfy

= hd xn (B − 1) + hd xn (B − 1) + hd
1,p
1,c
+ hc xn (B − 1) + hc xn (B − 1) + hc
2,p
2,c

(k),n

x1,cp (B − 1)

n
+ hr x(1),n (B − 1) + hr x(2),n (B − 1) + z1 .
r
r

3

(k),n

x2,cp (B − 1)

(k)

(k)
Rcp ≤ C +

h2 Pcp
sr
K
i=k+1

1 + 2h2
sr

(i)
Pcp

+ 2h2 Pc + 2h2 Pp
sr
sr

−

1
2

∀k ∈ {1, . . . , K}

(9)

∀k ∈ {1, . . . , K}

(10)

(k)

(k)
Rcp ≤ C

Rcp ≤ C

h2 Pcp
d
(i)
K
2
i=k+1 Pcp + hc
(1)
h 2 Pr
r
(2)
h2 Pr + h2 P + h2 P
r
c
d

K
i=k

1 + h2
d
1+

(i)

(2)

Pcp + h2 Pc + h2 Pc + h2 Pp + h2 Pp + h2 Pr
c
c
r
d
d
(2)

+C

h2 Pr
r
1 + h 2 Pc + h 2 Pc + h 2 Pp + h 2 Pp
c
c
d
d

(k)

(k)
Rcp ≤ C +

(k)
Rcp ≤ C

Rcp ≤ C

h2 Pcp
sr
1+

2h2
sr

−

(i)
Pcp

+ 2h2 Pc
sr
(k)
h2 Pcp
c
(i)
K
h2 i=k Pcp
c

(i)
K
i=k+1 Pcp +
(1)
h2 Pr
r
(2)
h2 Pr + h2 P + h2 P
r
c
d

1 + h2
d
1+

K
i=k+1

.

(11)

1
2

∀k ∈ {1, . . . , K}
∀k ∈ {1, . . . , K}

(2)

+ h2 Pc + h2 Pc + h2 Pr
c
r
d

(12)
(13)

(2)

+C

h2 Pr
r
1 + h 2 Pc + h 2 Pc
c
d

The receiver decodes the messages successively in this order:
(1)
(1)
(2)
(K)
(2)
mr → m1,cp → m1,cp → · · · → m1,cp → mr →
(1)
(m1,c , m2,c ) → m1,p . The message mr (B − 1) is ﬁrst
decoded while treating the other signals as noise, leading to the
ﬁrst term in the rate constraint (11). Next, the receiver decodes
(1)
m1,cp (B − 1) while treating the other signals as noise. Thus,
we have the rate constraint in (10) with k = 1.
Recall that u(1) (B − 1) is known at the receiver from the
decoding process in block B. Now interference cancellation is
(1)
performed. Since the receiver now knows both m1,cp (B − 1)
(1)
and u(1) (B −1), then, it can extract m2,cp (B −1) (see [17]). It
(1),n
n
thus removes its contribution, hc x2,cp (B−1), from y1 (B−1).
(k)
Therefore, after decoding each m1,cp (B−1), interference from
(k)
m2,cp (B−1) is cancelled. This continues until all CP messages
are decoded, leading to the rate constraint (10). At this stage,
the receiver can calculate

.

(14)

similarly, except that the interfering CP messages are decoded
ﬁrst at each receiver instead of the desired CP messages.
At this point, it is worth to remark that a lattice strategy for
the IRC was also proposed in [8]. The ﬁrst difference between
our scheme and the one in [8] is that we use P, C, and a set of
CP messages, while in [8] each user sends only a CP message.
The relay processing of the CP messages is the similar in both
cases. The fundamental difference however is the decoding at
the destination. We use interference cancellation described in
Section V-D which is not used in [8].
To examine the performance of the FDF scheme, one has to
carefully choose K (the inﬂuence of which is explained in the
next section) and the power allocations, plug in the FDF rate
constraints, and compare to the upper bounds. In this way, it
is possible to prove that the GDoF in Theorem 1 is achievable.
Due to space constraints, we use an example to illustrate the
proof.

hd xn (B − 1) + hd xn (B − 1) + hc xn (B − 1)
1,p
1,c
2,p
n
+ hc xn (B − 1) + hr x(2),n (B − 1) + z1 .
2,c
r

VI. GD O F: A N E XAMPLE

(1)
(k)
by subtracting the contribution of mr (B − 1), m1,cp (B − 1),
(k)
n
and m2,cp (B − 1), for k = 1, . . . , K, from y1 (B − 1). The
(2)
receiver then decodes mr (B−1), (m1,c (B−1), m2,c (B−1))

Consider an IRC with β − 1 < γ ≤ α ≤ 1 ≤
and 2α > 1 + γ. In this case, from (3) we obtain d
min{2α, 1 + β − α}. Let us set the FDF parameters
(1)
(2)
Pp = 1/h2 , Pc = h2 P/h2 − Pp , Pr = P , Pr = 0,K
c
r
d

(jointly), and m1,p (B − 1) successively in this order, each
time treating the remaining signals as noise. This leads to the
second term in the rate constraint (11), and the constraints (6)(8). Notice that the ﬁrst and second terms in (11) are more
binding than (15), thus the latter are ignored. Additionally,
(1)
(2)
K
since we have Rr + Rr = Rr =
k=1 Rcp (k) = Rcp ,
then, we can write the bound (11).
Decoding then proceeds backwards till block 1 is reached
and the same is done at the second receiver, which proves
the achievability of Theorem 4. Theorem 5 can be proved

(K)

log h2 /h2 / log h2 /h2 ,Pcp
r
c
d
d
(k)

h2
c
h2
d

k−1

h2
c
h2
d

=P
k

h2
c
h2
d

K−1

β,
=
to
=

− Pc − Pp ,

and Pcp = P
−P
, for k = 1, . . . , K − 1.
Evaluating the expressions stated in Theorem 4, gives the
achievable private GDoF dp = 1 − α. For the common
messages we get dc = min {2α − β, (1 + α − β)/2} where
we used γ > β − 1 and 2α > 1 + γ. For the cooperative
public messages, by plugging the chosen parameters in (10)

4

n
n
n
˜
˜
we can write Z1 /hc = Z n + Z2 /hd , where Z n and Z2 are
independent. Then we can write

we get
d(k) ≤ 1 − α,
cp
(K)
dcp

∀k = 1, . . . , K − 1,

≤ (K − 1)(α − 1) − 1 + β.

(16)

h

(17)
(k)

K−1

P = h2 h2 /h2
c
d
d

K

−h

n
Sc n
|S
hc sr

n

n
˜ S
˜
= −I Z n ; d + Z n |Ssr
hd

,

which is negative. As a result, by letting n → ∞, and using the
Gaussian distribution for X1 and X2 to maximize the upper
bound, we obtain (4). If h2 > h2 , then the bound (4) can be
c
d
obtained by enhancing receiver 2 by replacing the noise Z2
by hd Z2 /hc , and proceeding as above.

Here comes the importance of the choice of Pcp and K. The
(k+1)
choice of the powers of the CP signals leads to h2 Pcp
=
d
(k)
2
hc Pcp , i.e., while decoding the k-th CP message, the interference power from the (k +1)-th desired CP message is equal
to that of the kth interfering CP message. Thus, the (k + 1)(k)
th desired CP message does not affect dcp . This allows the
achievability of 1 − α. Now notice that without CP message
splitting, that is all we could achieve. By splitting the CP
messages, after decoding the k-th desired CP message, we can
cancel the interference of the k-th interfering CP message, and
then proceed to decode the (k + 1)-th desired CP message
(k+1)
(k+2)
where we have h2 Pcp
= h2 Pcp , achieving another
c
d
1 − α. By an appropriate choice of K, the ﬁrst K − 1 CP
messages have 1 − α GDoF, leading to (16). While decoding
the K-th desired CP message, the strongest interferer is the
desired C message since
h2 h2 /h2
c
d
c

n
Sd n
|S
hd sr

R EFERENCES
[1] R. H. Etkin, D. N. C. Tse, and H. Wang, “Gaussian interference channel
capacity to within one bit,” IEEE Trans. on Info. Theory, vol. 54, no. 12,
pp. 5534–5562, Dec. 2008.
[2] T. M. Cover and A. El-Gamal, “Capacity theorems for the relay
channel,” IEEE Trans. on Info. Theory, vol. IT-25, no. 5, pp. 572–584,
Sep. 1979.
[3] A. S. Avestimehr, S. Diggavi, and D. Tse, “A deterministic approach to
wireless relay networks,” in Proc. of Allerton Conference, 2007.
[4] I. Maric, R. Dabora, and A. Goldsmith, “An outer bound for the
Gaussian interference channel with a relay,” in IEEE Info. Theory
Workshop (ITW), Taormina, Italy, Oct. 2009.
[5] O. Sahin and E. Erkip, “Achievable rates for the Gaussian interference
relay channel,” in Proc. of 2007 GLOBECOM Communication Theory
Symposium, Washington D.C., Nov. 2007.
[6] ——, “On achievable rates for interference relay channel with interference cancellation,” in Proc. of 41st Annual Asilomar Conference on
Signals, Systems and Computers, Paciﬁc Grove, California, USA, Nov.
2007.
[7] S. Sridharan, S. Vishwanath, S. A. Jafar, and S. Shamai, “On the capacity
of cognitive relay assisted Gaussian interference channel,” in Proc. of
IEEE ISIT, Toronto, Ontario, Canada, Jul. 2008.
[8] Y. Tian and A. Yener, “The Gaussian interference relay channel:
improved achievable rates and sum rate upper bounds using a potent
relay,” IEEE Trans. on Info. Theory, vol. 57, no. 5, p. 2865, May 2011.
[9] S. Rini, D. Tuninetti, and N. Devroye, “Capacity to within 3 bits for a
class of gaussian interference channels with a cognitive relay,” in IEEE
International Symposium on Info. Theory (ISIT), St. Petersburg, July
31-Aug. 5 2011.
[10] ——, “Outer bounds for the interference channel with a cognitive relay,”
in Proc. of ITW, Dublin, Sep. 2010.
[11] V. R. Cadambe and S. A. Jafar, “Degrees of freedom of wireless
networks with relays, feedback, cooperation and full duplex operation,”
IEEE Trans. on Info. Theory, vol. 55, no. 5, pp. 2334–2344, May 2009.
[12] L. Ong, C. Kellett, and S. Johnson, “Capacity theorems for the AWGN
multi-way relay channel,” in Proc. of IEEE ISIT, 2010.
[13] T. Cover and J. Thomas, Elements of information theory. John Wiley
and Sons, Inc., 1991.
[14] A. Chaaban and A. Sezgin, “Achievable rates and upper bounds for
the interference relay channel,” in 44th Annual Asilomar Conference on
Signals, Systems, and Computers, Paciﬁc Grove, California, November
7-10 2010.
[15] B. Nazer and M. Gastpar, “Compute-and-Forward: Harnessing interference through structured codes,” IEEE Trans. on Info. Theory, vol. 57,
no. 10, pp. 6463 – 6486, Oct. 2011.
[16] U. Erez and R. Zamir, “Achieving 1/2 log(1 + SNR) on the AWGN
channel with lattice encoding and decoding,” IEEE Trans. on Info.
Theory, vol. 50, no. 10, pp. 2293–2314, Oct. 2004.
[17] K. Narayanan, M. P. Wilson, and A. Sprintson, “Joint physical layer
coding and network coding for bi-directional relaying,” in Proc. of the
Forty-Fifth Allerton Conference, Illinois, USA, Sep. 2007.
[18] V. M. Prabhakaran and P. Viswanath, “Interference channels with source
cooperation,” IEEE Trans. on Info. Theory, vol. 57, no. 1, pp. 156 – 186,
Jan. 2011.
[19] B. Nazer, “Successive compute-and-forward,” in Proc. of the 22nd
International Zurich Seminar on Communication (IZS 2012), Zurich,
Switzerland, March 2012.

P ≤ h4 P/h2 ,
d
r

which follows from the choice of K. In fact, K is chosen as
the largest number such that K(1 − α) ≥ β − 1 leading to
the total CP GDoF dcp = β − 1. Interestingly, this is as if
there were no CP interference at all, where β − 1 would be
achievable by decoding the CP messages while treating only
the C and the P messages as noise. CP message splitting and
interference cancellation therefore provide dcp = β −1 instead
of dcp = 1 − α. Similar CP GDoF expressions are obtained
by evaluating the bounds (9) and (11). Consequently, (9) and
(11) do not decrease the achievable CP GDoF, which is still
β −1. By adding dp , dc , and dcp , we get the overall achievable
GDoF of d ≤ min{2α, 1 + β − α}.
A PPENDIX A
P ROOF OF (4) IN T HEOREM 3
The ﬁrst bound in Theorem 3, i.e., (4) is obtained by giving
Yrn and (Yrn , m1 ) as side information to receivers 1 and 2,
respectively. Using classical information theoretic procedures,
it is possible to write
n
n
n
n(RΣ − ǫn ) ≤ I(m1 , m2 ; Yrn ) + h(hd X1 + hc X2 + Z1 |Yrn )
n n
n n
n
− h(Sc |Ssr ) + h(Sd |Ssr ) − h(Z2 )

with ǫn → 0 as n → ∞, Ssr = hsr X2 +Zr , Sc = hc X2 +Z1 ,
and Sd = hd X2 + Z2 . We proceed by writing
n
n
n
n(RΣ − ǫn ) ≤ I(m1 , m2 ; Yrn ) + h(hd X1 + hc X2 + Z1 )
n
n
n
n
− h (Sc /hc |Ssr ) + h (Sd /hd |Ssr )
n
+ (n/2) log h2 /h2 − h(Z2 ),
d
c

which follows since conditioning does not increase entropy,
and since h(aX) = h(X) + 1 log(a2 ). Now if h2 ≤ h2 , then
c
d
2

5

