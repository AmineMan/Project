Title:          ./ISIT2012-May18.dvi
Creator:        dvips(k) 5.991 Copyright 2011 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 22:42:52 2012
ModDate:        Tue Jun 19 12:55:39 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      453982 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569566831

Alternating Markov Chains for Distribution
Estimation in the Presence of Errors
Farzad Farnoud (Hassanzadeh)

Narayana P. Santhanam

Olgica Milenkovic

Department of Electrical and
Computer Engineering
UIUC
Urbana, IL 61801
Contact: www.ifp.uiuc.edu/~hassanz1

Department of
Electrical Engineering
University of Hawaii at Manoa
Honolulu, HI 96822
Email: nsanthan@hawaii.edu

Department of Electrical and
Computer Engineering
UIUC
Urbana, IL 61801
Email: milenkov@uiuc.edu

It is important to observe that although alternating sequences are generated by a Markov chain, their redundancy
cannot be accurately estimated using the methods developed
in [6]. This is due to the fact that the bound of [6] are
too general to give useful redundancy characterizations for
special classes of Markov chains. Surprisingly, the special
class of alternating Markov sequences has properties that
may be analyzed using tools developed for i.i.d. sequences,
with some appropriate modiﬁcations. Our analysis reveals that
alternating Markov chains have sub-linear pattern redundancy
√
in the sequence length n, scaling as c n + log(n), for some
constant c. This is a counterpart to the examples described
in [6], where Markov chains of redundancy of order n log n
were constructed using permutation patterns.
The paper is structured as follows. In Section II, we
introduce the problem of distribution estimation over noisy
channels and the notion of alternating sequences. In Section
III, we review the basic ideas and terminology behind the
proposed estimation method, including the notions of patterns
and proﬁles. Sections IV-A and IV-B are devoted to deriving
upper and lower bounds on the redundancy of block estimators
for alternating sequences, respectively. A sequential estimator
for alternating sequences is presented in Section V. Section VI
describes how to determine the source distribution, observed
through a noisy channel, based on the estimated probabilities
of alternating sequences.
Due to space limitations, for a number of results the proofs
are either omitted or outlined only. The complete proofs may
be found in the archive version of the manuscript.

Abstract—We consider a class of small-sample distribution
estimators over noisy channels. Our estimators are designed for
repetition channels, and rely on properties of the runs of the
observed sequences. These runs are modeled via special types of
Markov chains, termed “alternating Markov chains”. We show
that alternating chains have redundancy that scales sub-linearly
with the lengths of the sequences, and describe how to use a
distribution estimator for alternating chains for the purpose of
distribution estimation over repetition channels.

I. I NTRODUCTION
Estimating the distribution of a source with a large alphabet
based on a small number of observations is a problem of signiﬁcant interest in molecular biology, neuroscience, physics,
statistics, and learning theory. In order to address this problem,
throughout the years a number of sophisticated estimators were
developed, including those by Good and Turing [1], [2] and
Orlitsky et al. [3], [4], to cite a few. The idea behind these
estimators is to use frequencies of symbol frequencies, rather
than simple frequency counts standardly used for Maximum
Likelihood (ML) estimation.
An additional problem in this estimation setting arises when
some of the observations are inaccurate. Since most known
distribution estimators are based on frequency counts, errors
that change these counts may have a signiﬁcant bearing on
the accuracy of the method. A particularly interesting case
is when the counts are changed by consecutive repetitions
of some symbols. In [5], we described a collection of distribution estimators, based on Expectation Maximization (EM),
both for channels with known and channels with unknown
repetition parameters. The focal point of the study was a
class of sequences, termed alternating sequences, generated
by a Markov chain of special topology. The goal of this work
is twofold. The ﬁrst goal is to establish a rigorous analytical framework for evaluating the redundancy of alternating
Markov chains. The second goal is to describe how to use
alternating sequence distribution estimators for distribution
estimation over repetition channels. In particular, we exhibit
block and sequential estimators for alternating sequences that
have vanishing redundancy and provide good quality estimates
in the presence of repetition errors.

II. S MALL -S AMPLE D ISTRIBUTION E STIMATION
Consider a sample sequence x generated by an i.i.d. source
S deﬁned over a large-cardinality alphabet A. Suppose that
the source S has distribution pS . The estimator observes an
erroneous version of x, denoted by y. The errors are modeled
as arising from a channel C with input x and output y. What are
the ultimate performance limits for estimating the distribution
of the source, given that the length of x is small compared to
|A| or comparable to |A|?
Estimating the distribution of a source based on a noise-free
short sequence x is a challenging task. ML estimators perform

1

n

poorly in this setting as they typically overestimate probabilities of seen symbols, while they underestimate probabilities of
unseen symbols. More appropriate solutions for this scenario
are due to Good and Turing [1], [2] and Orlitsky et al. [3],
[4].
There are two approaches one can follow to address an even
more difﬁcult family of problems, namely that of small-sample
distribution estimation in the presence of errors.
In one scenario, one may try to ﬁrst denoise the output
ˆ
sequence y, so as to obtain a reduced-noise estimate x of
ˆ
x, and then apply a small-sample distribution estimator to x.
ˆ depends on both y and pS , and thus one needs
Note that x
to estimate pS in order to estimate x and vice versa. This
“estimation loop” may be resolved via the use of iterative
methods that alternate in improving estimates for x and pS
(see our companion paper [5]). In another scenario, one may
try to ﬁrst estimate the distribution of y and then reconstruct
the distribution of x by “inversion” of the noisy channel. We
henceforth pursue the second line of reasoning.
The focal point of our inversion study is a special class
of Markov sequences that arise in the study of distribution
estimation over repetition channels. A repetition channel is a
channel which outputs several copies of each input symbol.
The number of copies is a random variable with a predetermined distribution of known or unknown parameters. One
important property of repetition channels is that they maintain
the identity and order of symbols in the sequence, and only
alter the symbols’ runlengths. As an example, the sequence
x =‘committee’ passed through a repetition channel may be
observed as y =‘ccommmiitttee’. The alternating sequence
of a sequence x, V (x), is a sequence obtained from x by
replacing each run of x by one single symbol. We refer to
V (x) = V (y) as the alternating sequence of x and denote it
by v. Note that v is a Markov sequence and its corresponding
Markov chain is referred to as an alternating Markov chain.
Throughout the rest of the paper, we reserve the symbol
N to denote the length of the source output x. We also use
m to denote the cardinality of the alphabet A, which may be
inﬁnite, and n to denote the length of the alternating sequence
v.

parts of size i is ϕi and i iϕi = n. In the example above,
ϕ = (2, 1, 1, 0, 0, 0, 0) corresponds to an (unordered) partition
of 7 into two parts of size 1, one part of size 2, and one part
of size 3, i.e., 7=1+1+2+3. In the correspondence between
partitions and proﬁles, the number of parts of size µ equals
the number of symbols that appear µ times. The number of
partitions of n is denoted by p (n).
It is clear that ψ is the pattern of an alternating sequence v
of length n if and only if ψi = ψi+1 , for 1 ≤ i ≤ n − 1.
Let I n be the collection of i.i.d. distributions over length n
sequences. Consider a probability distribution p over A that
assigns probability 0 < ps ≤ 1 to s ∈ A. Then, the distribution
induced by p over An is denoted by pn and for all x ∈ An ,
it equals
n

pn (x) :=

px j .
j=1

Note that pn ∈ I n .
n
Every distribution p over A also induces a distribution pIΨ ,
n
pIΨ ψ := pn

x : Ψ (x) = ψ

,

over patterns of length n. The set of all such induced distrin
butions is denoted by IΨ . We simply write p (x) and p ψ if
dropping the subscripts and superscripts causes no confusion.
Furthermore, p induces a probability distribution over alternating sequences of length n, which for v = v1 · · · vn takes
the form
n
pvj
pV n (v) := pv1
.
(1)
1 − pvj−1
j=2
The set of all such induced distributions is denoted by V n . The
n
induced distribution pVΨ over alternating patterns of length n
n
and the set VΨ are deﬁned similarly to their unconstrained
sequence counterparts. Note that the family pV n (v) is Markovian.
A. Properties of Alternating Sequences
The ﬁrst issue we address is the relationship between the
length of the source sequence and the length of the corresponding alternating sequence. The following lemma will be
useful four our subsequent derivations.

III. PATTERNS , P ROFILES , AND T ECHNICAL
P RELIMINARIES

Lemma 1. Assuming all symbol probabilities are smaller than
1/2, we have
√
1
P [n ≤ N/2 − 8N ln N ] < ,
(2)
N
√
so that with high probability, n ≥ N/2 − 8N ln N .

The pattern ψ = Ψ (x) of a sequence x is obtained by
replacing each symbol by its order of appearance in x. The
proﬁle of ψ of a pattern is a vector Φ(ψ) = (ϕ1 , · · · , ϕn ),
where ϕi is the number of symbols that appear i times in
ψ. We use the shorthand notation Φ(x) to denote Φ(Ψ(x)).
Notational confusion can be avoided by noting whether the
argument of Φ(·) is a sequence or a pattern.
For example, the proﬁle of the pattern ψ = 1232421 is
ϕ = (2, 1, 1, 0, 0, 0, 0), since 3 and 4 appear once, 1 appears
twice, and 2 appears three times. Note that many patterns may
have the same proﬁle.
Remark: Observe that every proﬁle of patterns of length
n corresponds to a partition of the integer n. The number of

Sketch of proof: By deﬁning the exposure martingale
of the number of runs of x adapted to the ﬁltration obtained
by revealing elements of x one by one, and using Azuma’s
inequality, one can show that
√
2
P [n ≤ E[n] − 2λ N ] < e−λ /2 .
(3)
Furthermore, assuming all symbol probabilities are smaller
than 1/2, it can be shown that E[n] ≥ N/2. Then, the claimed
√
result follows from (3), with λ = 2 ln N .

2

The second property, stated in the following lemma, concerns patterns with same probability. The lemma that follows
provides the means for analyzing alternating sequences using
the techniques developed for i.i.d sequences, which allows
for signiﬁcant simpliﬁcations when compared to the general
Markov process analysis.

Markov processes. Prior results by Dhulipala and Orlitsky [6]
showed that in general, the per-symbol pattern redundancy
of a ﬁrst-order Markov process may be unbounded. For the
particular case of alternating sequences, however, we show
that the per-symbol pattern redundancy tends to zero.
Suppose that P is a collection of distributions over patterns
of length n and let Ψ(P) denote the set of patterns with
positive probability with respect to some distribution in P.
Denote the set of proﬁles of patterns in Ψ(P) by Φ (P). Also,
for a pattern ψ n of length n, let the set of all alternating
patterns with the same proﬁle as the proﬁle of ψ n be denoted
˜
by Ψ−1 (Φ (ψ n )).
If Ψ(P) is partitioned into M classes such that any distribution in P assigns the same probability to all patterns in the
same class, then the worst case redundancy is bounded by [7]

Lemma 2. Let ψ = ψ1 ψ2 · · · ψn and ψ = ψ1 ψ2 · · · ψn be two
alternating patterns with proﬁle ϕ such that the multiplicity of
ψn is equal to the multiplicity of ψn . For any i.i.d. distribution
p, we have p(ψ) = p(ψ ).
Proof: Let the alphabet be A = {a1 , a2 , · · · } and let the
probability of ai be pi . Assume ψn = a and ψn = b, with
a, b ∈ [k], where k is the number of elements appearing in
ψ. Let the multiplicity of i ∈ [k] in ψ be denoted by µi and
the multiplicity of i in ψ be denoted by µi . Note that, by
assumption, µa = µb .
Furthermore, let f be a bijection between the set
{1, 2, · · · , k} and a subset A ⊆ A of size k. This bijection
determines which symbol of the alphabet goes to which
symbol in the pattern. Then the probability assigned to pattern
ψ is

ˆ
R P ≤ log M.

n
n
˜
˜
Let Ψn := Ψ(VΨ ) and Φn := Φ(VΨ ). It is easy to see
that every proﬁle of an alternating sequence corresponds to a
˜
unique partition and hence Φn ≤ p (n).
n
Theorem 3. The worst case redundancy of VΨ grows at most
√
linearly with n. More precisely,

n

p ψ =

pf (ψ1 )
f

pf (ψj )
1 − pf (ψj−1 )
j=2

pf (a)

=

1 − pf (a)

f

µa

µa −1
i∈[k]\{a}

ˆ n
R(VΨ ) ≤

µi

pf (i)
1 − pf (i)

µa
µa −1

1 − pf (a)
=

pf

i∈[k]\{a}
(b)

1 − pf

(b)

pf (i)
1 − pf (i)

µb
µb −1

j∈[k]\{b}

FOR

√
2
log e
n + log n
3

µi

pf (j)
1 − pf (j)

where L(ψ) denote the number of patterns with the same
proﬁle and the same multiplicity of the last element as ψ.
Consider an estimator q that assigns probability

µj

·

˜
ψ ∈ Ψn

(7)

1/L(ψ )

˜
to ψ ∈ Ψn . We have that
sup sup

A LTERNATING S EQUENCES

p

A. Upper Bound on Redundancy of Alternating Patterns

˜
ψ∈Ψn

p ψ
q ψ

≤
≤

We start by deriving an upper bound on the worst case
redundancy of alternating sequences, deﬁned, with respect to
a collection of distributions P, as
p(u)
ˆ
,
R(P) = inf sup sup log
q p∈P u∈U (p)
q(u)

1/L(ψ)

q ψ =

By summing both sides of the equality over all bijections
between the set {1, 2, · · · , k} and a subset A ⊆ A of size
k, it follows that p ψ = p ψ .
IV. B LOCK E STIMATORS

π

Proof: From Lemma 2, all patterns with the same proﬁle
and the same multiplicity of the last element, have the same
probability. Hence, for any distribution p and any pattern ψ ∈
˜
Ψn ,
1
(6)
p(ψ) ≤
L(ψ)

where the summation is over all bijections between the set
{1, 2, · · · , k} and a subset A ⊆ A of size k. There exists a
permutation g over [k] with g(b) = a and, g(j) = i for all j ∈
[k]\{b}, such that µi = µj . Then, by letting f (·) = f (g(·)),
we have
pf (a)

(5)

(8)

1/L(ψ )
˜
ψ ∈ Ψn
1/L(ψ ).
˜
ϕ∈Φn m∈[n]

(9)

ψ :Φ(ψ )=ϕ,

µψ (ψn )=m

In the triple summation above, the index m corresponds to
the multiplicity of the last element ψn of ψ . By deﬁnition of
L(ψ ), we have

(4)

where U(p) is the support set of the distribution p, q(u) is
the probability assigned to u by the estimator q, and p(u)
is the probability of u with respect to the distribution p.
As already pointed out, alternating sequences are ﬁrst-order

1/L(ψ )
ψ :Φ(ψ )=ϕ,

µψ (ψn )=m

3

≤1

(10)

Next suppose n is odd. Then,

and thus
p ψ

sup sup
p

˜
ψ∈Ψn

q ψ

≤

˜
ϕ∈Φn m∈[n]

n/2

˜
1 ≤ n Φn

(11)

j=1

which implies that
≥

1

The theorem then follows from p (n) ≤ e
8–102].

1

π( 2 ) 2 n 2
3

[8, pp.

In subsection IV-A, we saw that the redundancy of patterns
√
of alternating sequences is O ( n). Here, we show that it is
bounded from below by a constant multiple of n1/3 .

n
p∈VΨ

n/2

n/2

µ=1

n/2

µ(i,v)
nrn ,
1
1 − 2rn ,

≥

pa
1 − pa

n/2

j=2

ˆ n
R (VΨ ) ≥ log 


i = a,
i = a,


≥ log 

pvj =
j=1

2rn − 1
2rn

n/2 n/2
µ=1

µ
n/2

= log 

1
2

2rn − 1
2rn

n/2 n/2

ϕµ !
µ=1

µ
n/2

µ
n/2

µ=1

µϕµ

n
ϕ∈Φ(VΨ ) ψ∈Ψϕ


sup p ψ  .

n
p∈VΨ

ϕ∈Φn/2

ψ∈Ψϕ

sup p ψ 

n
p∈VΨ

ψ∈Ψϕ ,µ(1,ψ )=n/2

ψ∈Ψϕ




sup p ψ 

n
p∈VΨ



sup p ΨA ψ  ,

n
p∈VΨ

is
the
alternating
pattern
where
ΨA ψ
1, ψ1 + 1, 1, ψ2 + 1, · · · , 1, ψn/2 + 1 induced by ψ.
Using (12), it can be shown that

µϕµ

.

e23/12
√
2π
The proof for odd n is similar.
ˆ n
R (VΨ ) ≥

The position of a is ﬁxed in v, but the symbols {s1 , · · · , sm }
that appear the same number of times can be swapped without changing the pattern Ψ (v) of v. This can be done in
n/2−1
n/2−1
ϕµ ! ways. Hence, there are µ=1 ϕµ ! sequences
µ=1
with pattern ψ and probability p (v). Since ϕn/2 ≤ 2, we
n/2
n/2−1
have µ=1 ϕµ ! ≥ 1 µ=1 ϕµ !. Thus,
2


n/2
1
sup p ψ ≥ 
ϕµ ! p (v)
n
2 µ=1
p∈VΨ
=

n/2

log

ϕ∈Φn
n/2

ϕ∈Φn
n/2



pvj
pa
1 − pvj−1 1 − pa

n/2 n/2

n/2

where Ψϕ is the set of all patterns with proﬁle ϕ.
Suppose ﬁrst that n is even. Let Φn be the set of proﬁles
k
n
whose largest parts are of size k. Since Φn ⊆ Φ (VΨ ),
n/2



.

where µ (i, v) is the number of occurrences of i in v. First,
suppose that n is even. We then have
pa pv1
p (v) =
1 − pa

1

21/3


ˆ n
R (VΨ ) = log 

,

Proof: Note that since ψ is an alternating pattern, we
have ψj = 1 for 1 ≤ j ≤ n/2 . Consider the alphabet A =
{a, s1 , · · · , sm } , where m is the largest number appearing in
ψ minus one. Let v be a sequence with pattern ψ starting with
symbol a. Let p be a distribution deﬁned as
pi =

pvj
j=1

e23/12
√
n1/3 (1 + o (1)) .
2π
Sketch of proof: From Starkov’s sum [7] we have that



(12)

where ϕ is the proﬁle of the pattern ψ1 ψ2 · · · ψ

2rn − 1
2rn

ˆ n
R (VΨ ) ≥

µϕµ

µ
n/2

ϕµ !

1
2

n/2

n/2

n
Theorem 5. For the collection of distributions VΨ ,

Lemma 4. Let ψ = 1ψ1 1ψ2 · · · 1ψn/2 , for even n, and ψ =
1ψ1 1ψ2 · · · 1ψ n/2 1, for odd n, be an alternating pattern. For
a function rn ≥ 1 of n, we have
2rn − 1
2rn

pa
1 − pa

where the inequality follows since pa ≥ 1/2. The number of
n/2
sequences with pattern ψ and starting with a is µ=1 ϕµ !.

B. Lower Bound on the Redundancy of Alternating Patterns

1
2

1
2

=

ˆ n
˜
R (VΨ ) ≤ log Φn + log n ≤ log p (n) + log n.

sup p ψ ≥

pvj
pa
1 − pa 1 − pvj−1

p (v) = pa

1

21/3

log

n1/3 (1 + o (1)) .

V. S EQUENTIAL E STIMATORS FOR A LTERNATING
S EQUENCES
In the previous section we studied the problem of assigning
probabilities to patterns of a certain length without prior information. In this section, we address a more practical problem.
Namely, given a pattern ψ n−1 of length n−1, what is our best
estimate q ψn |ψ n−1 of the probability of ψn being the next
observed symbol, for ψn ∈ {1, 2, · · · , 1 + maxi≤n−1 ψi }.
This sequential estimator assigns probabilities to patterns of
length n in a natural way, q (ψ n ) = n q ψi |ψ i−1 , where
i=1
ψ 0 is the empty string.

µϕµ

·

4

VI. E STIMATING

We present a sequential estimator q1/2 for patterns of
alternating sequences which is based on a sequential estimator
for patterns of i.i.d. sequences presented in [7] by Orlitsky et
al. Let
pψn (ψ n ) : = sup p (ψ n )
ˆ
be the largest probability assigned to ψ n by any distribution
n
in VΨ and let q be as deﬁned in (7), i.e.,
1/L(ψ n )

˜
ψ∈Ψn

,

(13)

2√
n .
3

(14)

1/L(ψ)

for which we have
pψn (ψ n )
ˆ
≤ n exp π
q (ψ n )

˜n

i

p a2

1 2

p aj

Proof: By deﬁnition,

pψ1 (ψ1 )
ˆ n n
n ·
q1/2 (ψ1 )

R EFERENCES

n
pψ1 (ψ1 )
ˆ n n
pψ1 (ψ1 ) q hn (ψ1 )
ˆ n n
= hn n ·
n
n ·
q1/2 (ψ1 )
q (ψ1 ) q1/2 (ψ1 )

[1] I. J. Good, “The population frequencies of species and the estimation of
population parameters,” Biometrika, vol. 40, no. 3-4, pp. 237–264, 1953.
[2] W. A. Gale and G. Sampson, “Good-Turing smoothing without tears,”
Journal of Quantitative Linguistics, vol. 2, 1995.
[3] A. Orlitsky, N. Santhanam, and J. Zhang, “Always good turing: asymptotically optimal probability estimation,” in Foundations of Computer
Science, 2003. Proceedings. 44th Annual IEEE Symposium on, 2003, pp.
179–188.
[4] A. Orlitsky, N. Santhanam, K. Viswanathan, and J. Zhang, “Convergence
of proﬁle based estimators,” in Proc. IEEE Int. Symp. Information Theory,
Adelaide, SA, Sep. 2005.
[5] F. Farnoud, O. Milenkovic, and N. Santhanam, “Small-sample distribution
estimation over sticky channels,” in IEEE Int. Symp. Information Theory,
Jun./Jul. 2009, pp. 1125 –1129.
[6] A. Dhulipala and A. Orlitsky, “Universal compression of markov and
related sources over arbitrary alphabets,” Information Theory, IEEE
Transactions on, vol. 52, no. 9, pp. 4182 –4190, Sep. 2006.
[7] A. Orlitsky, N. Santhanam, and J. Zhang, “Universal compression of
memoryless sources over unknown alphabets,” Information Theory, IEEE
Transactions on, vol. 50, no. 7, pp. 1469–1481, 2004.
[8] G. E. Andrews, The theory of partitions. Cambridge University Press,
1998.

From Lemmas 7 and 8, the proofs of which are omitted, we
obtain
√
pψ1 (ψ1 )
ˆ n n
2 2 hn
1+(log hn −1)/2
√
·
exp π
n ≤ hn
q1/2 (ψ1 )
32− 2
The theorem follows after some simple algebra and by noting
that hn < 2n.
n
Lemma 7. For an alternating pattern ψ1 , we have

2
3

2

pa a
= p a2 1 j
p a1 a2

gives the probabilities paj for j ≥ 3.
Although the presented estimators for alternating sequences
do not ﬁnd probabilities with zero error, we are justiﬁed in
assuming that the estimates obtained from these estimators
are “close” to the correct values since their redundancy is
vanishing. Hence, the estimates of the probabilities pai aj of
the alternating sequence may be used to obtain estimates for
probabilities pi of the source sequence as explained in the
previous sections.
Acknowledgment: The work was supported by the NSF
STC-CSoI 2011 and NSF CCF 0809895, and AFRLDL-EBS
AFOSR Complex Networks Grant.

q hi ψ i
q1/2 (1) = 1,
q1/2 ψ |ψi−1 = hi i−1 ·
q (ψ )
Theorem 6. The worst case redundancy of the sequential
estimator q1/2 is bounded by
√
4π
n
ˆ VΨ , q1/2 ≤ 2 + 5 log n + 1 log2 n + √ log e √n .
R
2
2
3 2− 2
i

pψ1 (ψ1 )
ˆ n n
n ≤ hn exp π
q hn (ψ1 )

1 − p a1 a2
,
1 − p a1 a2 p a2 a1
1 − p a2 a1
= p a1 a2
·
1 − p a1 a2 p a2 a1

Given pa1 aj for j ≥ 2, the remaining probabilities may be
pa a
pa
obtained by noting that pa1 aj = paj and thus

˜
over Ψi for i ≤ n.
Finally, deﬁne the estimator q1/2 , as

Write

S OURCE

p a1 = p a2 a1

˜
z∈Ψn (ψ i )

n ˜
ψ 1 ∈ Ψn

OF A

which implies that pa1 and pa2 can be found as

i

For an alternating pattern ψ , let Ψ ψ
:=
˜ n : z1 z2 · · · zi = ψ i
z∈Ψ
be the set of alternating
patterns of length n ≥ i whose ﬁrst i elements are the
˜
same as ψ i . Accordingly, from q (z) , z ∈ Ψn , we deﬁne the
distribution
q (z)
q n ψ i :=

ˆ n
R VΨ , q1/2 ≤ max log

D ISTRIBUTION

In this section, we explain how to reconstruct the noiseless
source probabilities from estimates of probabilities provided
by alternating sequences. First, recall from Lemma 1, that with
high probability, n is of the same order as N and thus, with
high probability, the length n of the alternating sequence is
large if the length of the source sequence is large.
Assume that the source has alphabet A = {a1 , a2 , · · · } with
probability paj for element aj . Suppose pai aj is the probability
of observing aj after ai in the alternating sequence and assume
that the correct values of pa1 a2 and pa2 a1 are given. We have
p a2
p a1
p a1 a2 =
,
p a2 a1 =
1 − p a1
1 − p a2

n
p∈VΨ

q (ψ n ) =

THE

hn ·

n
Lemma 8. For n ≥ 2 and an alternating pattern ψ1 , we have
√
n
2
hn
q hn (ψ1 )
√
.
≤ h(log hn −1)/2 exp π
n
n
q1/2 (ψ1 )
3 2−1

5

