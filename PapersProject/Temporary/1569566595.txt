Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 16:20:20 2012
ModDate:        Tue Jun 19 12:56:21 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      513767 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566595

Learning Markov Graphs Up To Edit Distance
Abhik Kumar Das, Praneeth Netrapalli, Sujay Sanghavi and Sriram Vishwanath
Department of ECE, The University of Texas at Austin, USA
e-mail:{akdas,praneethn}@utexas.edu, sanghavi@mail.utexas.edu, sriram@ece.utexas.edu

the number of available samples exceeds some threshold, the
probability of error in learning the structure of a Markov
network goes to one as the problem size increases. Such
information-theoretic limits are important as they provide an
understanding of the settings where recovery is impossible,
regardless of the algorithm or cleverness of its design.
In this paper, we focus on reconstruction of the graph structure of a Markov network within a pre-speciﬁed distortion,
rather than exact reconstruction. As an overarching goal, we
are interested in characterizing the rate-distortion limits of the
problem of graphical model selection. We restrict ourselves to
Markov networks whose underlying graphical structures have
bounded degree. We derive results for two well-known families
of Markov networks – Ising models and Gaussian Markov
networks. The distortion metric we choose is edit distance
between graphs, that we deﬁne in the next section.

Abstract—This paper presents a rate distortion approach to
Markov graph learning. It provides lower bounds on the number
of samples required for any algorithm to learn the Markov graph
structure of a probability distribution, up to edit distance. We
ﬁrst prove a general result for any probability distribution, and
then specialize it for Ising and Gaussian models. In particular,
for both Ising and Gaussian models on p variables with degree
s
at most d, we show that at least Ω((d − p ) log p) samples are
required for any algorithm to learn the graph structure up to
edit distance s. Our bounds represent a strong converse; i.e.,
we show that for a lower number of samples, the probability of
error goes to 1 as the problem size increases. These results show
that substantial gains in sample complexity may not be possible
without paying a signiﬁcant price in edit distance error.
Index Terms—Markov networks, graphical models, strong
converse, Ising model, Gaussian Markov model

I. I NTRODUCTION
Markov networks, also known as (undirected) graphical
models, describe the interdependencies (or the lack thereof)
among a collection of random variables using an undirected
graph. As such, they have been used for modeling and designing applications in a multitude of settings, for example, social
network modeling [1], [2], image processing/computer vision
[3], [4] and computational biology [5], [6]. The problem of
learning the graph structure of a Markov network from samples
generated by the underlying probability distribution is a wellstudied one and is referred to as the problem of graphical
model selection or Markov graph learning. There is diverse
literature on various aspects of learning Markov graphs, ranging from statistical physics to computational learning theory. It
is only relatively recently that the information theoretic limits
for this learning problem are being better understood.
An understanding of the information theoretic limits of
high-dimensional learning problems in general, and the problem of learning Markov graphs in particular, provides us with
sample complexity bounds corresponding to lower bounds for
learning. A useful tool used for obtaining these bounds is
Fano’s inequality and its generalizations [7]. However, Fano’s
inequality results in weak converse bounds – the typical result
obtained for the problem of learning Markov graphs is that
if the number of observed samples available to a learning
algorithm falls below a certain threshold, the probability of
error in learning the structure of a Markov network is bounded
away from zero. Therefore, alternate information-theoretic
tools are required for stronger bounds on sample complexity.
This motivates the formulation of strong converse type results,
in the same spirit as in the case of noisy channel coding
[8]. In other words, we desire results that say that unless

A. Related Work
There is a signiﬁcant body of literature in the context
of deriving the information-theoretic limits on the sample
complexity for exact learning of Markov networks, especially
the specialized cases of Ising models [9], [10], and Gaussian
Markov networks [11], [12]. The graph ensembles that have
been considered include degree-bounded graphs [9]–[12],[13],
graphs with limited edges [9] and random graphs [10], [12].
A common theme in deriving these theoretical bounds is to
treat the graphical model selection problem as a noisy channel
coding problem and apply Fano’s inequality to characterize
the limits. The graphical model selection problem in presence
of distortion has been examined in [12] for the ensemble
of Erd¨ s-R´ nyi graphs. The only known strong converse
o e
results have been derived in [10] and [14], for the cases of
exact reconstruction of Erd¨ s-R´ nyi graph based Ising models
o e
and degree bounded Gaussian Markov networks respectively.
The performance of graphical model learning algorithms, that
output a list of graph structures instead of a single one, is
examined in [15] for Gaussian and Ising models.
B. Summary of Results
We provide a comparison of our results against the existing
ones in literature in Table I, for ensemble of Markov networks
based on graphs with p nodes and degree bounded by d.
Our results are highlighted in bold face. Note that s denotes
the maximum allowed edit distance between the original and
recovered graphs. All existing results in literature are for the
case of exact graph structure recovery i.e., s = 0.

1

TABLE I
C OMPARISON WITH EXISTING RESULTS
1
d

Edge weight = Θ

Model

Ω(d2 log p) [9]

Edge weight = Θ

computer vision [19], game theory [20] and many other topics.
In this paper, we restrict ourselves to a special case of Ising
model – the zero ﬁeld binary Ising model. Here, the alphabet
is chosen as A = {−1, 1}. Given an undirected graph G on
p nodes and weight θij ∈ R assigned to edge (i, j) ∈ E, the
probability of vector x = (x1 , x2 , . . . , xp ) ∈ Ap is given by

1
√
d

√ √
Ω( de d log p) [9]

Ising
d−

Ω

8s
p

log p

Ω

Ω

√

d d−

4s
p

log p

8s
p

log p

exp

(i,j)∈E

p (x) =

Ω d2 log p [11]
Gaussian

d−

Ω (d log p) [11]
Ω

d−

4s
p

z∈Ap

exp

θij xi xj

(i,j)∈E

,
θij zi zj

where vector z = (z1 , z2 , . . . , zp ) ranges over Ap .
Gaussian Markov Model: This is one of the most well
known families of continuous Markov networks. Here, vector
X possesses a multivariate Gaussian distribution over reals.
Without loss of generality, it can be assumed that X has the
zero vector as its mean. Given an undirected graph G on p
nodes and a p × p positive deﬁnite matrix Θ ∈ Rp×p such that
Θ(i, j) = 0 iff (i, j) ∈ E, the p.d.f. of vector X is given by

log p

It is known that edge weights play an important role in
determining the complexity of learning graphical models [16].
However this dependency is complex in general. For instance,
very low edge weights or very large edge weights tend to
increase the number of samples needed to learn the graphical
model. The presence of low edge weights causes difﬁculty
in determining the existence of edges, while the presence
of high edge weights results in long range correlations in
graphical models. Existing results show signiﬁcant difference
1
in lower bounds for edge weight scaling as Θ( d ) as opposed
1
to Θ( √d ), see Table I. However, in this paper, we are able to
show different lower bounds for the Gaussian case but not for
the Ising model. A detailed comparison and discussion of our
results with the existing ones is done in Section VI.
The rest of this paper is organized as follows. We discuss
some preliminaries and introduce the system model in Section
II. We consider the problem of learning graph structure up to
a pre-speciﬁed distortion value (edit distance) and give strong
limits on the sample complexity for arbitrary ensembles, Ising
models and Gaussian Markov networks in Sections III, IV and
V respectively. We ﬁnally conclude the paper with Section VI.
Due to lack of space, complete proofs of all the results are not
presented in this paper. They can be found in [17].

p(x) =

1
(2π)p |Θ−1 |

1
exp − xT Θx ,
2

where x = (x1 , . . . , xp ) ∈ Rp and Θ is the inverse covariance
matrix. Θ is also called the potential matrix, since Θ(i, j)
can be interpreted as the potential of edge (i, j) ∈ E. The
following quantity, called the minimum magnitude of partial
correlation coefﬁcient, plays an important role in determining
the sample complexity of the Markov graph learning problem:
λ∗ (Θ) := min
(i,j)∈E

|Θ(i, j)|
Θ(i, i)Θ(j, j)

.

This quantity is invariant to rescaling of the random variables
and can be thought of as the minimum magnitude of a nonzero entry after normalizing the diagonal terms of Θ.
In this paper we restrict our attention to the ensemble of
degree bounded graphs in light of the fact that extensive
work on learning Markov networks focus on these graphs.
We denote the set of graphs on p nodes and maximum degree
d by Gp,d . We also denote the set of all graphs on p nodes
by Up . For any two graphs G and H on the same set of
nodes, we deﬁne the edit distance, ∆(G, H), between them
as the minimum number of edge deletions/insertions required
to convert G to H. Thus, ∆(G, H) is the cardinality of the
symmetric difference between edge sets of G and H.

II. P RELIMINARIES
We consider an undirected graph G = (V, E), where V =
{1, . . . , p} is the set of nodes and E ⊆ V × V is the set of
edges. A Markov network is obtained by associating a random
variable Xi to node i, that takes values from an alphabet set A,
and specifying a joint probability distribution p(·) over vector
X = (X1 , X2 , . . . , Xp ) that satisﬁes the following property:

A. Learning Algorithm and Error Criterion
We consider an ensemble of undirected graphs on a common
set of p nodes, G = {G1 , . . . , GM }, and an ensemble of
Markov networks K = {K1 , . . . , KM }, such that Ki has
Gi as its underlying graph and the random variables in
X = (X1 , . . . , Xp ) draw values from alphabet A. We choose
a Markov network K ∈ K uniformly at random and obtain
n i.i.d. vector samples X n = (X (1) , . . . , X (n) ) from the
distribution speciﬁed by K. The problem we consider is to
reconstruct the graph G associated with K given the samples
X n . This is referred to as the problem of graphical model
selection or Markov graph learning. A learning algorithm is

p(xA , xB |xC ) = p(xA |xC )p(xB |xC ),
where A, B and C are any disjoint subsets of V such that every
path in G from A to B passes through C, and xA , xB , xC
denote the restrictions of (x1 , . . . , xp ) ∈ Ap to indices in
A, B, C respectively. Here, p(·) denotes p.m.f. for the discrete
case (A is a ﬁnite set) and p.d.f. for the continuous case
(A = R and continuous distribution). Next, we present
examples of discrete and continuous Markov networks.
Ising Model: This family of discrete probability distributions is widely studied and used in statistical physics [18],

2

any function φ : Anp → Up that maps the observed samples to
ˆ
an estimated graph G = φ(X n ) ∈ Up . Given a pre-speciﬁed
s, we deﬁne the error event for the learning algorithm as
{∆(G, φ(X n )) ≥ s}, i.e., error is declared if the edit distance
between the actual graph and the reconstructed version is
greater than or equal to s. Then the probability of error of
the learning algorithm is given by the following expression:

Here, error refers to the event that the original graph and the
recovered graph are at an edit distance of more than s, and
A (K) = max var log
1≤i≤M

1
M

,

where p(·) stands for p.m.f. in the discrete case and p.d.f. in
the continuous case, and dependent on ensemble K.

(n)
Pe (φ) = P (∆(G, φ(X n )) ≥ s)

=

p X (1) K = Ki
K = Ki
p(X (1) )

M

If we can ﬁnd a good upper bound for A (K) for given
(n)
ensemble K, then we can use Theorem 1 to show that Pe →
R
1 in the high dimensional setting as p → ∞ and n < C . We
pursue this approach in the next two sections to prove results
for ensembles of Ising and Gaussian graphical models.

P (∆(Gi , φ(X n )) ≥ s|K = Ki ) .
i=1

In this paper, we derive lower bounds on sample size n, in
terms of the ensemble parameters, for any learning algorithm
to reliably recover the underlying graph of a Markov network
(n)
up to an edit distance of s. We do this by bounding Pe from
below in terms of n and the ensemble parameters.

IV. L OWER B OUNDS FOR I SING M ODELS
Our main result in this section is the following theorem that
characterizes a lower bound on the number of samples required
for consistent recovery of the Markov graph structure for an
ensemble of Ising models whose construction is described
below. For each graph in Gp,d , we consider the corresponding
zero ﬁeld binary Ising model with all edge weights equal to
1
I
θ, where θ ∈ (0, √d ). We refer to this ensemble as Kp,d . Note
I
that there is a bijective mapping between Gp,d and Kp,d .

III. L OWER B OUNDS FOR A RBITRARY E NSEMBLES
In this section, we state our result for lower bounds on
the sample complexity for arbitrary ensembles of graphical
models. We consider the same setup as described in Section
II-A. We have an ensemble of Markov networks K and the
corresponding ensemble of undirected graphs G on p nodes.
We choose K ∈ K uniformly at random and obtain n i.i.d.
sample vectors from its joint distribution. Our aim is to analyze
the performance of an arbitrary learning algorithm φ : Anp →
Up . For this, we deﬁne the following quantities for G ∈ G:

Theorem 2. Suppose K is chosen uniformly at random from
I
Kp,d . If for some α < 1, d = o(pα ), s < (1 − α) pd and the
16
number of samples, obtained from distribution of K, satisﬁes

B(s, G) := {H : ∆(G, H) < s, H ∈ Up } ,
B(s, G) := max |B(s, G)| .
G∈G

n<

B(s, G) denotes the maximum number of graphs that are at an
edit distance of at most s from any graph in G. We also deﬁne
another quantity, similar in structure to mutual information:

1
2

=Ω

d 2s
−
4
p

log p −

(1 − α)d −

8s
p

s
2s log s
d
log 8d + log
−
4
p
e
p

log p ,

H(X (1) ) − H(X (1) |K = Ki ), |A| < ∞,
h(X (1) ) − h(X (1) |K = Ki ),
A = R.

then for any arbitrary graphical model learning algorithm, its
(n)
probability of error satisﬁes Pe → 1 as p → ∞.

H(·) and h(·) represent the entropy and differential entropy
functions respectively. For given K, G, we deﬁne a lower
bound R and an upper bound C on the following quantities:

Proof strategy for Theorem 2: The proof of Theorem 2
follows from establishing the bounds R and C in (1) and (2)
and then using Theorem 1. Lemmas 1 and 2 establish such
bounds R and C respectively. A complete proof of Theorem
2 can be found in the appendix and [17]. We list the graphs
in Gp,d as Gp,d = {G1 , . . . , GM } and the corresponding Ising
I
models in Kp,d as {K1 , . . . , KM }. Now we state the following
two lemmas bounding R and C for these ensembles.

I(Ki ; X (1) ) :=

R ≤ log M − log B(s, G),
C ≥ max I(Ki ; X
1≤i≤M

(1)

).

(1)
(2)

Then the following theorem establishes a necessary condition
on the number of samples n for consistent recovery of the
structure of Markov networks using any learning algorithm.

Lemma 1. For Gp,d with d ≤

Theorem 1. Consider an ensemble of Markov networks K =
{K1 , . . . , KM } and the corresponding ensemble of undirected
graphs on p nodes, G = {G1 , . . . , GM }. Suppose the random
variables take values from alphabet A. If the number of
R
samples satisﬁes n < C , then for any learning algorithm, we
have the following lower bound on the probability of error:
(n)
Pe ≥ 1 −

pd
p
log M ≥
log ,
4
8d

p−1
2 ,

s≤

p(p−1)
,
4

B(s, Gp,d ) < s

we have
p2
2

s

.

Lemma 2. Suppose K is chosen uniformly at random from
I
Kp,d = {K1 , . . . , KM }. Then we have the following bound:

(R−nC)
4nA (K)
− 2− 2 .
2
(R − nC)

max I(Ki ; X (1) ) ≤ p.

1≤i≤M

3

number of samples, obtained from distribution of K, satisﬁes

V. L OWER B OUNDS FOR G AUSSIAN M ARKOV M ODELS

d−

Our main result in this section is a lower bound on the
number of samples required for consistent recovery of the
Markov graph structure for an ensemble of Gaussian Markov
G
networks Kp,d , whose construction is described below.
Without any loss of generality, we assume that p is even. We
choose d perfect matchings on p nodes, each perfect matching
chosen uniformly at random, and form a multigraph resulting
from the union of the edges in the matchings. We refer to
the set of all such multigraphs on p nodes, constructed in this
fashion, as H. The uniform distribution over the choices of
perfect matchings also deﬁnes a probability distribution over
H. We have the following lemma for this distribution [21]:

n<

= Ω

d−1+1
2

(1 − 2α)d −
log 1 +

4s
p

√
4 d√

λ−1 −4

−

2 log s
p

−

2
p


log p ,

d

then for any arbitrary graphical model learning algorithm, its
(n)
probability of error satisﬁes Pe → 1 as p → ∞.
Proof strategy for Theorem 3: Analogous to the proof of
Theorem 2, the proof of Theorem 3 follows from establishing
the bounds R and C in (1) and (2) and then using Theorem 1.
Lemmas 5 and 6 establish such bounds R and C respectively.
The complete proof of Theorem 3 can be found in the appendix
and [17]. We list the graphs in Gp,d as Gp,d = {G1 , . . . , GM }
G
and the corresponding Gaussian Markov networks in Kp,d as
{K1 , . . . , KM }. Then the following two lemmas hold.
Lemma 5. For Gp,d with large enough p, s ≤

√
c
P (ρ(A) < 3 d) ≥ 1 − τ ,
p
√

2s
2s
p log e
√
4 d√
λ−1 −4 d

log p − 2d log 2d +
2 log 1 +



Lemma 3. Consider a multigraph H on p nodes, formed from
the union of d random perfect matchings on the nodes that
are chosen according to a uniform distribution. Suppose the
eigenvalues of the (weighted) adjacency matrix of H, denoted
by A, are d = λ1 (A) ≥ λ2 (A) ≥ · · · ≥ λp (A). Deﬁne
ρ(A) := max2≤i≤p |λi (A)|. Then the following result holds:

where c is a positive constant and τ =

4s
p

log M ≥

p
pd
log 2 − 1,
2
4d

p(p−1)
,
4

B(s, Gp,d ) < s

p2
2

s

we have
.

Lemma 6. Suppose K is chosen uniformly at random from
G
Kp,d = {K1 , . . . , KM }. Then we have the following bound:
√
p
4 d
(1)
√
max I(Ki ; X ) ≤ log 1 +
.
1≤i≤M
2
λ−1 − 4 d

− 1.

Next, we eliminate those multigraphs from H√whose
(weighted) adjacency matrices A satisfy ρ(A) ≥ 3 d and
get a reduced subset H . By Lemma 3, H\H forms a small
1
fraction of H. We ﬁx constants λ ∈ (0, 4√d ), δ > 0 and
δ
deﬁne µ := λ−1 −4√d . Then for every multigraph H ∈ H , we
√
generate a p × p matrix Θ = (4 dµ + δ)Ip + µA, where Ip
is the p × p identity matrix and A is the (weighted) adjacency
matrix of multigraph H. We refer to the resulting set of these
matrices as T . Then the following property holds for this set:

VI. C ONCLUSION & D ISCUSSION
Remarks about Theorems 2 & 3: Specializing our result
to the case of exact recovery, i.e., s = 0 yields weaker lower
bounds than the existing results. However, for the case of
1
Gaussian Markov models [11] with edge weights Θ( √d ), our
result matches the existing result and for both the cases of
1
Ising [9] and Gaussian [11] models with edge weights Θ( d ),
√
our result is only a factor of d and d away respectively
from existing results. This gap is either due to a limitation
of our proof technique or due to the difference in the kind
of guarantee. Speciﬁcally, the lower bound results in [9] and
[11] use Fano’s inequality to obtain a weak converse i.e., if the
number of samples n scales below a certain threshold then the
probability of error is lower-bounded by a constant as p → ∞.
On the other hand our result establishes a strong converse i.e.,
if the number of samples n scales below a certain threshold
then the probability of error goes to 1 as p → ∞.
In this paper, we develop a rate-distortion framework for
the problem of learning Markov graphs, where we characterize
lower bounds on sample complexity within a given distortion
criterion. We use a strong converse framework to derive
these bounds, indicating that it is near-impossible to learn the
graphical model with fewer samples. Our results show that, for
both Ising and Gaussian models on p variables with maximum
s
degree d, at least Ω((d − p ) log p) samples are required for
any algorithm to recover the graph within edit distance s. As

Lemma 4. Every Θ ∈ T is symmetric and positive deﬁnite.
Proof: By construction, given any Θ ∈ T , we have
√
Θ = (4 dµ + δ)Ip + µA, where A is the (weighted)
adjacency matrix of some multigraph H ∈ H , which makes it
symmetric. Also, the construction of H ensures that ρ(A) <
√
3√d. Therefore, the minimum eigenvalue of Θ is at least
√
dµ + δ > 0. This along with
4 dµ + δ − ρ(A)µ >
the symmetry of Θ ensure that all the eigenvalues of Θ are
positive. Hence, Θ is also a positive deﬁnite matrix.
Note that the choice of µ ensures λ∗ (Θ) = λ for every
Θ ∈ T . Lemma 4 suggests that the matrices of T can
potentially be the inverse covariance matrices of Gaussian
Markov networks. By construction, the underlying graph of
each of these Markov networks comes from Gp,d . We denote
G
this ensemble of Gaussian Markov networks by Kp,d and the
corresponding graphical ensemble by Gp,d ⊆ Gp,d .
Theorem 3. Suppose K is chosen uniformly at random from
G
Kp,d . If for some α < 1 , d = o(pα ), s < (1 − 2α) pd and the
2
8

4

future work, we hope to derive stronger bounds on probability
of error in learning, and derive results for Markov networks
based on other graph ensembles, like random graphs.

Using

R
≥
C

R EFERENCES

a·e b
,
b

d 2s
−
4
p

we obtain the following lower bound:
log p −

d
s
2s log s
log 8d + log
−
.
4
p
e
p

I
8A(Kp,d )
R
− 2− 4 .
(5)
RC
We need to show that the last two terms of the RHS of (5) go
to 0 as p → ∞. Since d = o(pα ) and s < (1−α)pd , we have
16
(n)
Pe ≥ 1 −

R = Θ(pd log p).

2

I
A(Kp,d ) ≤ 4p2 θ2 d2 (log e) = O(p2 d).

(7)

1
Here, we use the fact that θ = O( √d ). Using (4), (6) and (7)
we see that the second term of the RHS of (5) goes to 0 as
p → ∞ and hence probability of error goes to 1.

B. Proof of Theorem 3
Following the approach of proof of Theorem 2, we deﬁne
lower bound R and upper bound C, given by (1) and (2), as:
p2 /2
p
pd
log 2 − log s
s
2
4d
√
p
4 d
√
C := log 1 +
.
2
λ−1 − 4 d
R :=

− 1,

(8)
(9)

Lemmas 5 and 6 show that R and C satisfy (1) and (2). After
some simpliﬁcations, we obtain the following lower bound:
d−
R
≥
C

4s
p

2s
2s
p log e
√
4 d√
λ−1 −4 d

log p − 2d log 2d +
log 1 +

−

2 log s
p

−

2
p

.

R
Under the hypothesis of the theorem, we have n < 2C . Using
(n)
Theorem 1, we see that the probability of error Pe satisﬁes
G
8A(Kp,d )
R
− 2− 4 .
(10)
RC
We need to show that the last two terms of the RHS of (10)
go to 0 as p → ∞. To constrain the second term we use the
following upper bound whose proof can be found in [17]:
(n)
Pe ≥ 1 −

G
A(Kp,d ) ≤

A. Proof of Theorem 2

3p2
2

1+

5d
√
−1 − 4 d
λ

2

.

(11)

1
G
For λ = O( √d ), we obtain A(Kp,d ) = O(p2 d). Also note that

We make use of Lemmas 1 and 2 to construct lower bound
R and upper bound C, as deﬁned in (1) and (2), as follows:

since d = o(pα ) for some α <

1
2

and s <

R = Θ(pd log p).
,

(6)

This shows that the last term of the RHS of (5) goes to 0 as
p → ∞. To show the same for the second term of the RHS of
(5), we use the following bound whose proof is given in [17]:

A PPENDIX

p
p2 /2
pd
log
− log s
4
8d
s
C := p.

≤

R
Under the hypothesis of the theorem, we have n < 2C . Using
(n)
Theorem 1, we see that the probability of error Pe satisﬁes

[1] A. Grabowski and R. Kosinski, “Ising-based model of opinion formation in a complex network of interpersonal interactions,” Physica A:
Statistical Mechanics and its Applications, vol. 361, pp. 651–664, 2006.
[2] F. Vega-Redondo, Complex social networks. Cambridge Press, 2007.
[3] J. Besag, “On the statistical analysis of dirty pictures,” Journal of the
Royal Statistical Society Series B, vol. 48, pp. 259–279, 1986.
[4] M. Choi, J. J. Lim, A. Torralba, and A. S. Willsky, “Exploiting
hierarchial context on a large database of object categories,” in IEEE
CVPR, 2010.
[5] N. Friedman, “Inferring cellular networks using probabilistic graphical
models,” Science, Feb 2004.
[6] A. Ahmedy, L. Song, and E. P. Xing, “Time-varying networks: Recovering temporally rewiring genetic networks during the life cycle of
drosophila melanogaster,” tech. rep., 2008. arXiv.
[7] T. Cover and J. Thomas, Elements of Info. Theory. Wiley Interscience,
2006.
[8] R. Gallager, Info. Theory and Reliable Communication. Wiley, 1968.
[9] N. Santhanam and M. J. Wainwright, “Information-theoretic limits of
selecting binary graphical models in high dimensions,” arXiv, 2009.
[10] A. Anandkumar, V. Y. F. Tan, and A. Willsky, “High-dimensional
structure learning of Ising models: Tractable graph families,” arXiv
Preprint, 2011.
[11] W. Wang, M. J. Wainwright, and K. Ramchandran, “Informationtheoretic bounds on model selection for Gaussian Markov random
ﬁelds,” in IEEE ISIT, 2010.
[12] A. Anandkumar, V. Y. F. Tan, and A. Willsky, “High-dimensional
Gaussian graphical model selection: Tractable graph families,” arXiv
Preprint, 2011.
[13] G. Bresler, E. Mossel, and A. Sly, “Reconstruction of Markov random
ﬁelds from samples: Some observations and algorithms,” in APPROX,
pp. 343–356, 2008.
[14] I. Mitliagkas and S. Vishwanath, “Strong information-theoretic limits for
source/model recovery,” in Proc. of Allerton Conf. on Communication,
Control and Computing, Monticello, USA, 2010.
[15] D. Vats and J. Moura, “Necessary conditions for consistent set-based
graphical model selection,” in IEEE ISIT, 2011.
[16] A. Montanari and J. A. Pereira, “Which graphical models are difﬁcult
to learn?,” in Advances in Neural Information Processing Systems
22 (Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and
A. Culotta, eds.), pp. 1303–1311, 2009.
[17] A. Das, P. Netrapalli, S. Sanghavi, and S. Vishwanath, “Learning Markov
graphs up to edit distance,” http://uts.cc.utexas.edu/∼akdas/mglearning
techreport12.pdf, 2012.
[18] L. Reichl and J. Luscombe, “A modern course in statistical physics,”
American Journal of Physics, vol. 67, p. 1285, 1999.
[19] S. Geman and C. Grafﬁgne, “Markov random ﬁeld image models and
their applications to computer vision,” in Proceedings of the International Congress of Mathematicians, vol. 1, p. 2, AMS, Providence, RI,
1986.
[20] Y. Zhang, “Modeling market mechanism with evolutionary games,”
Arxiv preprint cond-mat/9803308, 1998.
[21] J. Friedman, “A proof of Alon’s second eigenvalue conjecture and related
problems,” arXiv, 2004.

R :=

a
b

(3)

(1−2α)pd
,
8

we have
(12)

Using (9), (11) and (12), we see that the last two terms of the
(n)
RHS of (10) goes to 0 as p → ∞, hence Pe → 1.

(4)

5

