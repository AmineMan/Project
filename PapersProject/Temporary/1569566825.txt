Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 20:16:43 2012
ModDate:        Tue Jun 19 12:55:30 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      613630 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566825

Adaptive sensing using deterministic
partial Hadamard matrices
S. Haghighatshoar, E. Abbe and E. Telatar
EPFL, Switzerland
Email: {saeid.haghighatshoar, emmanuel.abbe, emre.telatar} @epﬂ.ch
Recently, [1] introduced a more reasonable framework to
study analog compression from an information-theoretic perspective. By requiring the encoder to be linear and the decoder
to be Lipschitz continuous, the fundamental compression limit
is shown to be the R´ nyi information dimension. The setting
e
of [1] also raises a new interesting problem: in the same way
that coding theory aims at approaching the Shannon limit with
low-complexity schemes, it is a challenging problem to devise
efﬁcient schemes to reach the R´ nyi dimension. For example,
e
in compressed sensing, with O(k log(n/k)) instead of O(k)
measurements, k-sparse signals can be reconstructed using l1
minimization, which is a convex optimization problem, rather
than l0 minimization, which is intractable [6], [7]. Hence, in
general, complexity requirements may raise the measurement
rate.
The scope of this paper is precisely to investigate what
measurement rates can be achieved by taking into account the
complexity of the sensing matrix, which in turn, inﬂuences
the complexity of the reconstruction algorithm. Our goal is
to consider signals that are memoryless and drawn from a
probability distribution on R, which may be purely atomic,
purely continuous or mixed. It is legitimate to attempt reaching
this goal by borrowing tools from coding theory, in particular
from codes achieving least compression rates in the discrete
setting. Our approach is based on using Hadamard matrices for
encoding (taking measurements) and developing a counter-part
of the polar technique [2], [3] with arithmetic over R (or Z for
atomic distributions) rather than F2 or Fq . The proof technique
uses a martingale argument as in polar codes and a new form
of entropy power inequality for discrete distributions. Rigorous
results are obtained and the sensing matrix construction is
deterministic. A nested property is also investigated which
allows one to adapt the measurement rate to the sparsity level
of the signal.
Recently, spatially-coupled LDPC codes have allowed to
achieve rigorous results in coding theory. This approach has
also been exploited by [4], [5], which proposes the use of
spatially coupled matrices for sensing. In [5], the mixture case
is covered and further analysis on the reconstruction algorithm
is provided. However, the sensing matrix is still random. It
is known that Hadamard matrices truncated randomly afford
desirable properties for compressed sensing. We extend this
work and show that by knowing signal distribution, Hadamard
matrices can be truncated deterministically to achieve a minimal measurement rate.

Abstract—This paper investigates the construction of deterministic measurement matrices preserving the entropy of a random
vector with a given probability distribution. In particular, it is
shown that for a random vector with i.i.d. discrete components,
this is achieved by selecting a subset of rows of a Hadamard
matrix such that (i) the selection is deterministic (ii) the fraction
of selected rows is vanishing. In contrast, it is shown that for
a random vector with i.i.d. continuous components, no entropy
preserving measurement matrix allows dimensionality reduction.
These results are in agreement with the results of Wu-Verdu on
almost lossless analog compression and provide a low-complexity
measurement matrix. The proof technique is based on a polar
code martingale argument and on a new entropy power inequality
for integer-valued random variables.
Index Terms—Entropy-preserving matrices, Analog compression, Compressed sensing, Entropy power inequality.

I. I NTRODUCTION
Information theory has extensively studied the lossless
and lossy compression of discrete time signals into digital sequences. These problems are motivated by the model
of Shannon, where an analog signal is ﬁrst acquired, by
sampling it at a high enough rate to preserve all of its
information (Nyquist-Shannon sampling theorem), and then
compressed. More recently, it was realized that proceeding
to “joint sensing-compression” schemes can be beneﬁcial. In
particular, compressed sensing introduces the perspective that
sparse signals can be compressively sensed to decrease measurement rate. As for joint source-channel coding schemes,
one may wonder why this would be useful? Eventually, the
signal is represented with the same amount of bits, so why
would it be preferable to proceed jointly or separately? In a
nutshell, if measurements are expensive (such as for example
in certain bio-medical applications), then compressed sensing
is beneﬁcial.
From an information-theoretic perspective, compressed
sensing can be viewed as a form of analog to analog compression, namely, transforming a higher dimensional discrete
time signal into a lower-dimensional one over the reals,
without “losing information”. The key point is that, since
measurements are analog, one may as well pack as much
information in each measurement (whereas in the compression
of discrete signals, a measurement on a larger alphabet is more
expensive than a measurement in bits). However, compressing
a vector in Rn into a vector in Rm , m < n, without regularity
constraints is not an interesting problem, since Rn and Rm
have the same cardinality.

1

1) Given a probability distribution pX over a ﬁnite set, and
> 0, is there a family of measurement matrices that
is -REP and has measurement rate ρ? What is the set
of all possible ( , ρ) pairs? Is it possible to construct a
family of truncated Hadamard matrices with a minimal
measurement rate? How is the truncation adapted to the
distribution pX ?
2) Is it possible to obtain an asymptotic measurement rate
below 1 for continuous distributions?

II. R ELATED W ORK
Let X1 , X2 , . . . , XN be i.i.d. Bernoulli(p) random variables, where N = 2n for some n ∈ Z+ . We use the notation
aj for the column vector (ai , ai+1 , . . . , aj )t and set aj to null
i
i
if j < i. We also deﬁne [r] = {i ∈ Z : 1 ≤ i ≤ r}. Let
⊗n
1 1
GN =
, where ⊗ denotes the Kronecker product,
0 1
N
N
and let Y1 = GN X1 , with arithmetic over F2 . Deﬁne
i−1
Hi = H(Yi |Y1 ), i ∈ [N ], to be the conditional entropy
of Yi given Y1i−1 . In [3], Arikan shows that for any δ > 0
and for large N , the values Hi , i ∈ [N ], polarize to 0 or
1. This provides a compression scheme achieving the least
compression rate, since for any δ ∈ (0, 1)

Remark 1. The RIP notion, introduced in [6], [7], is useful
in compressed sensing, since it guarantees l2 -stability of the
recovery algorithm. We consider truncated Hadamard matrices
satisfying -REP condition and since they have a Kronecker
structure, we obtain a low-complexity reconstruction algorithm
using the maximum likelihood decoding. However, this part
is not emphasized in this paper, and we mainly focus on
the construction of the truncated Hadamard matrices. Section
VI provides numerical simulations of a divide and conquer
ML decoding algorithm and illustrates the robustness of the
recovery to noise.

#{i ∈ [N ] : Hi ∈ (1 − δ, 1]}
→ H(X).
(1)
N
From another point of view, every Yi is associated with
a speciﬁc row of the matrix GN and (1) indicates that
the “measurement” rate required to extract the informative
components is close to the entropy of the source H(X) for
large N .
In signal acquisition, measurements are analog. Hence, one
N
could consider Y1N = GN X1 with arithmetic over the real
ﬁeld and investigate if any “polarization phenomenon” occurs.
The difference is that, in this case, the measurement alphabet
is unbounded. In particular, the Hi values are not bounded
from above.

IV. M AIN R ESULTS
The main results of this paper are summarized here.
⊗n

1 1
, N = 2n , n ∈ Z+ } be
−1 1
N
the family of Hadamard matrices. Suppose X1 are discrete
i.i.d. random variables with distribution pX over a ﬁnite subset
N
of Z. Let Y1N = JN X1 and deﬁne Hi = H(Yi |Y1i−1 )
and mN = #{i ∈ [N ] : Hi > }. The ( , pX )- truncated
¯
Hadamard family {JN }, is the set of matrices of dimension
mN × N obtained by selecting those rows of JN with Hi > .
Deﬁnition 3. Let {JN =

III. P ROBLEM S TATEMENT
N
Deﬁnition 1 (Restricted iso-entropy property). Let X1 be
discrete i.i.d. random variables with a probability distribution
pX supported on a ﬁnite set. The family {ΦN } of measurement
matrices, where ΦN has dimension mN × N , is -REP(pX )
with measurement rate ρ if

and lim supN →∞

Theorem 1 (Absorption phenomenon). Let X be a discrete
random variable with a probability distribution pX supported
on a ﬁnite subset of Z. For a ﬁxed > 0, the family of
¯
( , pX )-truncated Hadamard matrices {JN , N = 2n , n ∈ Z+ }
(deﬁned above) are -REP(pX ) with measurement rate 0. In
other words,
mN
= 0.
lim sup
N →∞ N

N
N
H(X1 |ΦN X1 )
≤ ,
N
mN
N = ρ.

In general, the labeling N can be any subsequence of Z+ .
We will consider N = 2n , n ∈ Z+ .

¯
Remark 2. Although all of the measurement matrices JN
are constructed by truncating the matrices JN , the order and
¯
number of the selected rows, mN , to construct JN depends
on the distribution pX .

N
Deﬁnition 2. Let X1 be continuous (or mixture) i.i.d. random
variables with probability distribution pX . The family of
measurement matrices {ΦN } of dimension mN × N is ( , γ)REP(pX ) with measurement rate ρ if
1) there exists a single letter quantizer Q : R → Z such
that M.M.S.E. of X given Q(X) is less that γ,
2) for any N ,

The following result is used to prove Theorem 1.
Theorem 2 (An EPI over Z). For any probability distribution
p over Z,

N
N
H(Q(X1 )|ΦN X1 )
< ,
N

H(p p) − H(p) ≥ g(H(p)),

where g : R+ → R+ is strictly increasing, limx→∞ g(x) =
1
8 log(2) and g(x) = 0 if and only if x = 0.

N
where Q(X1 ) = (Q(X1 ), Q(X2 ), . . . , Q(XN ))t ,

3)
lim sup
N →∞

(2)

mN
= ρ.
N

Figure 1 shows the EPI gap for discrete random variables.
Remark 3. This theorem complements the work in [8] to obtain an entropy power inequality for discrete random variables.

We address the following questions in this paper:

2

EPI gap for discrete random variables
0.18

Lemma 4. Assuming the hypotheses of Lemma 3,

0.16
0.14

H(p p) − c ≥

g(H(p))

0.12
0.1

α2
p p1 − p p2 2 .
1
2 log(2)

Lemma 5. Assuming the hypotheses of Lemma 3,

0.08
0.06

H(p p) − c ≥

0.04

2α4
.
log(2)

0.02
0

0

5

10

Proof of Theorem 2: Suppose that p is a distribution over
Z with H(p) = c. Set y = p ∞ . There is an α ≥ 1−y and an
2
integer n such that α ≤ p((−∞, n]) ≤ 1 − α. Using Lemma
2 and Lemma 5, it results that H(p p) − c ≥ g(c) where

15

H(p)

Fig. 1: EPI gap for discrete random variables

For continuous distributions, and for any ﬁxed distortion γ,
the measurement rate approaches 1 as tends to 0. This result
has been shown in [1] in a more general context. We recover
this result in our setting for the case of a uniform distribution
over [−1, 1].

g(c) = min max(
y∈[0,1]

It is easy to check that g(c) is a continuous function of c. The
monotonicity of g follows from the fact that cy − (1 + y)h2 (y)
is an increasing function of c for every y ∈ [0, 1]. For strict
positivity, note that (1 − y)4 is strictly positive for y ∈ [0, 1)
and it is 0 when y = 1, but limy→1 cy − (1 + y)h2 (y) = c.
Hence for c > 0, g(c) > 0. If c = 0 then

Lemma 1. Let pU be the uniform distribution over [−1, 1] and
let Q : [−1, 1] → {0, 1, . . . , q − 1} be a uniform quantizer for
X with M.M.S.E. less than γ. Assume that {ΦN } is a family
of full rank measurement matrices of dimension mN × N . If
{ΦN } is ( , γ)-REP(pU ), then the measurement rate, ρ, goes
to 1 as tends to 0.

max(

A. An EPI for Discrete Random Variables
The entropy power inequality for continuous and independent random variables X and Y is
(3)

where h denotes the differential entropy. If X and Y have the
same density p, then (3) becomes

1
(1 − )4
≥ lim sup g(c) ≥ lim inf g(c) ≥
,
c→∞
8 log(2)
8 log(2)
c→∞

1
h(p p) ≥ h(p) + ,
2
which implies a guaranteed increase of the differential entropy.
For this reason, we call (2) an EPI for discrete random
variables.

and limc→∞ g(c) =

N
Assume that X1 , N = 2n , n ∈ Z+ , is a set of i.i.d.
random variables with probability distribution pX over a ﬁnite
⊗n
1 1
N
subset of Z. Let Y1N = JN X1 , where JN =
−1 1
is the Hadamard matrix of dimension N and let Hi =
H(Yi |Y1i−1 ), i ∈ [N ], be the conditional entropy values.

H(p p) − c ≥ cpi − (1 + pi )h2 (pi ),
where h2 (x) = −x log2 (x) − (1 − x) log2 (1 − x) is the binary
entropy function and pi denotes the probability of i.

N
N
Lemma 6. Let Z1 = GN X1 where GN is the same as
i−1
˜
before. Assume that Hi = H(Zi |Z1 ), i ∈ [N ], then Hi =
˜
Hi , i ∈ [N ].

Lemma 3. Let c > 0, 0 < α ≤ 1 and n ∈ Z. Assume that p is
2
a probability measure on Z such that α ≤ p((−∞, n]) ≤ 1−α
and H(p) = c, then
1

1
8 log(2) .

B. Conditional Entropy Martingale

Lemma 2. Let c > 0 and suppose p is a probability measure
over Z such that H(p) = c. Then, for any i ∈ Z,

p p1 − p p2

(1 − y)4
(1 − y)4
, cy − (1 + y)h2 (y)) =
,
8 log(2)
8 log(2)

and its minimum over [0, 1] is 0. For asymptotic behavior, note
(1−y)4
1
that at y = 0, cy − (1 + y)h2 (y) = 0 and 8 log(2) = 8 log(2) .
1
Hence, from continuity, it results that g(c) ≤ 8 log(2) for any
c ≥ 0. Also for any > 0 there exists a c0 such that for
1
any < y ≤ 1, cy − (1 + y)h2 (y) ≥ 8 log(2) . Thus for any
> 0 there is a c0 such that for c > c0 , the outer minimum
over y in g(c) is achieved on [0, ]. Hence, for any c > c0 ,
(1− )4
g(c) ≥ 8 log(2) . This implies that for every > 0,

V. P ROOF OVERVIEW

22h(X+Y ) ≥ 22h(X) + 22h(Y ) ,

(1 − y)4
, cy − (1 + y)h2 (y)).
8 log(2)

N
˜
Lemma 7. Let Z1 and Hi , i ∈ [N ], be as in Lemma 6.
N
N
Suppose W1 = BN Z1 , where BN is the bit shufﬂing matrix
i−1
˜
introduced in [3] and deﬁne Hi = H(Wi |W1 ), then Hi =
Hi , i ∈ [N ].

≥ 2α,

1
1
where p1 = p((−∞,n]) p|(−∞,n] and p2 = p([n+1,∞)) p|[n+1,∞)
are scaled restrictions of p to (−∞, n] and [n + 1, ∞)
respectively.

Remark 4. Lemma 6 demonstrates the equivalence of J and G
to compute the conditional entropies. However, in application,

3

it is preferred to use J because its rows are orthogonal. Lemma
N
7 shows that shufﬂing Z1 by BN does not change the entropy
values. For simplicity of the proof, we use the matrix BN GN
and relate to the polar code notations in [2], [3].
Notice that we can represent BN GN in a recursive way. Let
us deﬁne two binary operation ⊕ and as follows

C. Main Theorem
N
Proof of Theorem 1: Assume that Y1N = JN X1 , for
i−1
n
N = 2 , n ∈ Z+ , and Hi = H(Yi |Y1 ), i ∈ [N ]. Also ﬁx
> 0. Let us deﬁne
Kn = {i : i ∈ [N ], Hi > },

Y[Kn ] = {Yj : j ∈ Kn }.

(a, b) = a + b

¯
Hence, by Deﬁnition 3, |Kn | = mN and JN is obtained from
JN by selecting the rows with index in Kn . We have

⊕(a, b) = b,
where + is the usual integer addition. It is easy to see that
we can do the multiplication by BN GN in a recursive way.
Figure 2 shows a simple case for B4 G4 . The − or + sign
on an arrow shows that the result for that arrow is obtained
by applying a or ⊕ operation to two input operands. If we
X1

+

+

X2

X3
X4

−

−−

+

−

≤

N ¯
N
(N − mN )
H(X1 |JN X1 )
≤
≤ .
N
N
¯
This shows that the family {JN } is -REP. Now we show that
the measurement rate of this family is 0. To prove this, we use
Lemma 6 and Lemma 7 and construct the martingale In by (4).
In is a positive martingale and converges to a random variable
I∞ almost surely. Our aim is to show that for any two positive
numbers a and b where a < b, µ(I∞ ∈ (a, b)) = 0, which
implies that µ(I∞ ∈ {0, ∞}) = 1. Since In is a martingale,
E{In } = E{I0 } = H(X) < ∞. Using Fatou’s lemma, we
obtain

Y2
Y4

Fig. 2: Recursive structure for multiplication by B4 G4
consider a special output Ym , there is a sequence of ⊕ and
operations on the input random variables which results in Ym .
An easy way to ﬁnd this sequence of operations is to write
the binary expansion of m − 1. Then, each 0 in this expansion
corresponds to a
operation and each 1 corresponds to a
⊕ operation. Using this binary labeling, we deﬁne a binary
stochastic process. Assume that Ω = {0, 1}Z+ , and F is the
σ-algebra generated by the cylindrical sets
1

Saeid Haghighatshoar

Wednesday, January 11, 2012

E{I∞ } ≤ lim inf E{In } = H(X1 ) < ∞,
which implies that µ(I∞ = ∞) = 0. Hence, In converges
almost surely to 0 and it also converges to 0 in probability. In
other words, given > 0,

S(i1 ,i2 ,··· ,is ) = {ω ∈ Ω such that ωi1 = 1, . . . , ωis = 1},

|Kn |
mN
= lim sup
= 0.
n
2
n→∞
n→∞
N →∞ N
This implies that for any > 0 the measurement rate ρ is
0. Hence, it is sufﬁcient to prove that for any two positive
numbers a and b, where a < b, µ(I∞ ∈ (a, b)) = 0. Fix a
δ > 0 then for every ω in the convergence set there is an n0
such that for n > n0 , |In+1 (ω) − In (ω)| < δ. This implies
that for n > n0

for every integer s and i1 , i2 , · · · , is . We also construct the
ﬁltration {Fn , n ≥ 0}, where Fn ⊂ F is the σ-algebra
generated by the ﬁrst n coordinates of ω and F0 = {∅, Ω}
is the trivial σ-algebra. We deﬁne the uniform probability
measure µ over the cylindrical sets by µ(S(i1 ,i2 ,··· ,in ) ) = 21 .
n
This measure can be uniquely extended to F.
Let [ω]n = ω1 ω2 . . . ωn denote the ﬁrst n coordinates of
ω = ω1 ω2 . . . , and let Y[ω]n denote the random variable Yi ,
where the binary expansion of i − 1 is [ω]n . Let us deﬁne
Y [ω]n = {Y[η]n : [η]n < [ω]n }. We also deﬁne the random
variable In by
In (ω) = H(Y[ω]n |Y

H(Yi |Y1i−1 )

which implies that

++

[ω]n

c
i∈Kn

c
≤ |Kn | = (N − mN ) ,

Y3

−+

+

+

c
= H(Y[Kn ] |Y[Kn ] )

Y1

+−

+

N ¯
N
H(X1 |JN X1 ) = H(Y1N ) − H(Y[Kn ] )

),

lim sup µ(In > ) = lim sup

|In+1 (ω) − In (ω)| = |In+1 ([ω]n , 0) − In ([ω]n )| < δ.
Using (5) and the entropy power inequality (2), it results that
0 ≤ In (ω) < ρ(δ) where ρ(δ) can be obtained from the EPI
curve in Figure 1. This implies that In must converge to 0 and
this completes the proof.

(4)

where it is seen that In (ω) depends on the ﬁrst n coordinates
of ω. It is also important to note that if ωn+1 = 0
In+1 (ω) = In+1 ([ω]n , 0)
˜
˜
= H(Y[ω]n + Y[ω]n |Y [ω]n , Y [ω]n )

VI. N UMERICAL S IMULATIONS
For simulation, we use a binary random variable, where
pX (0) = 1 − p for some 0 < p ≤ 1 .
2

(5)

where ˜ denotes an independent copy of the corresponding
random element.

A. Absorption Phenomenon
Figure 3 shows the absorption phenomenon for p = 0.05
and N = 256, 512.

Theorem 3. (In , Fn ) is a martingale.

4

Absorption Scheme for N=256, p=0.05

Absorption Scheme for N=512, p=0.05

3
3
2.5

2

Conditional Entropy

Conditional Entropy

2.5

1.5

1

2

1.5

1

0.5

0.5

0

50

100

150
Output Number

200

0

250

50

100

(a) N = 256

150

200

250
300
Output Number

350

400

450

500

(b) N = 512

Fig. 3: Absorption trace for p = 0.05
Absorption Scheme for N=512

Absorption Scheme for N=512
p= 0.04
p= 0.02
p= 0.01

3

p= 0.04
p= 0.02
p= 0.01

2

2.5
Conditional Entropy

Conditional Entropy

1.5
2

1.5

1

1
0.5
0.5

0

50

100

150

200

250
300
Output Number

350

400

450

0

500

130

135

140
145
Output Number

150

Fig. 4: Nested property for N = 512 and different p
Stability analysis for ML decoder

B. Nested Property

20
18

As illustrated in Figure 4, for N = 512 and different values
of p, the conditional entropies at a ﬁxed component are ordered
when p varies. We call this the “nested” property. This allows
to construct robust decoders when the value of p is not exactly
known by using the ‘worst-case’ value of p.

16

SNRout[dB]

14
12
10
8
6
4

C. Robustness to Measurement Noise

2
0

Figure 5 shows the stability analysis of the ML reconstruction algorithm to i.i.d. N (0, σ 2 ) measurement noise. For
simulation, we used N = 512, p = 0.05 and a 0.01-REP
measurement matrix by keeping all of the rows of the matrix
JN with indices in the set K. Note that the ML decoder
exploits the recursive structure of the Hadamard matrix to
perform computations more efﬁciently. We deﬁne the signal
to noise ratio at the input and output of the decoder as:
SNRin =
SNRout =

10
SNRin[dB]

15

20

Fig. 5: Stability analysis

R EFERENCES
[1] Y. Wu and S. Verd´ ,“R´ nyi information dimension: fundamental limits of
u e
almost lossless analog compression”, IEEE Transaction on Information
Theory, vol. 56, no. 8, pp. 3721–3747, August 2010.
[2] E. Arikan, “Channel polarization: A method for constructing capacityachieving codes for symmetric binary-input memoryless channels”, IEEE
Transaction on Information Theory, vol. 55, p. 3051, July 2009.
[3] E. Arikan, “Source polarization”, http://arxiv.org/abs/1001.3087, 2010.
[4] F. Krzakala, M. M´ zard, F. Sausset, Y. Sun, and L. Zdeborova, “Statistical
e
physics-based reconstruction in compressed sensing”, arXiv:1109.4424,
2011.
[5] D. L. Donoho, A. Javanmard, A. Montanari, “Information-Theoretically
optimal compressed sensing via spatial coupling and approximate message passing”, http://arxiv.org/abs/1112.0708v1, 2011.
[6] E. J. Cand` s, T. Tao, “Decoding by linear programming”, IEEE Transace
tion on Information Theory, vol.51, pp. 4203–4215, December 2005.
[7] E. J. Cand` s, T. Tao, “Near-optimal signal recovery from random projece
tions: universal encoding strategies”, IEEE Transaction on Information
Theory, vol. 52, pp. 5406–5425, December 2006.
[8] O. Johnson, Y. Yu, “Monotonicity, thinning, and discrete versions of the
entropy power inequality”, IEEE Transaction on Information Theory, vol.
56, pp. 5387– 5395, November 2010.

E(Yi2 )
,
|K| σ 2

i∈K

N
i=1

N
i=1

5

2
E(Xi )
,
ˆ
E(|Xi − Xi |2 )

ˆ
where Xi is the output of the ML decoder. The result shows
approximately 4 dB loss in SNR for high SNR regime.
ACKNOWLEDGEMENT
E.A. would like to thank M. Madiman and A. Barron for
stimulating discussions on this problem at Yale University, and
P. Vandergheynst for helpful comments.

5

