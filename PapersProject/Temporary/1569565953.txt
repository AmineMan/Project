Title:          paper_ISIT.dvi
Creator:        dvips(k) 5.99 Copyright 2010 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Sat May 19 02:14:16 2012
ModDate:        Tue Jun 19 12:55:20 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      313331 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565953

Short Message Noisy Network Coding
for Multiple Sources
Jie Hou and Gerhard Kramer
Institute for Communications Engineering
Technische Universit¨ t M¨ nchen, 80290 Munich, Germany
a
u
Email: {jie.hou, gerhard.kramer}@tum.de

W2 → (X2 , Y2 )

Abstract—Short message noisy network coding (SNNC) transmits independent short messages in blocks rather than using
long message repetitive encoding. SNNC is shown to achieve
the same rates as noisy network coding (NNC) for discrete
memoryless networks where each node transmits a multicast
message. One advantage of SNNC is that backward decoding may
be used which simpliﬁes the analysis and understanding of the
achievability proof. The analysis reveals that each decoder may
ignore certain other nodes rather than including their message
in the decoding procedure. Additionally, SNNC enables early
decoding at nodes if the channel quality happens to be good.

W1 → (X1 , Y1 )

WK → (XK , YK )

W3 → (X3 , Y3 )

.
.
.

P (y1 , . . . , yK |x1 , . . . , xK )

.
.
.

···

I. I NTRODUCTION
Recently, Lim et al. [1] proposed Noisy Network Coding
(NNC) for communications over noisy networks and developed a lower bound on the capacity. Their result recovers
previous results in [2]–[5] as special cases.
The paper [1] uses three techniques from [5], namely
• Relays quantize without hashing (or binning), which simpliﬁes the relay operation and is referred to as quantizeforward (QF).
• Destinations decode messages and quantization bits
jointly rather than sequentially.
• Repetitive encoding with long messages at the sources.
One drawback of long messages is that they inhibit decodeforward (DF) at the relays even if the channel conditions are
good [6]. For example, if one relay is close to the source
and has a strong source-relay link, the natural operation is DF
which removes the noise at the relay. But this is generally not
possible with a long message because of its high rate.
Fortunately, a short message variant called SNNC gives the
same rate as NNC for one unicast session [7] (see also [8]).
SNNC has:
1) Sources transmit independent short messages in every
block (see [9, Appendix D]).
2) Relays perform QF.
3) Destinations decode with backward decoding or joint
decoding or any other type of decoding.
Note that the name SNNC does not depend on what kind
of decoder is used but is a generic name for the encoding
procedure. Indeed, we will see that the choice of decoder is
important for certain problems.
The main contribution of this work is to show that SNNC
achieves the same rates as NNC for general discrete memory-

Wk → (Xk , Yk )

Fig. 1.

A K-node discrete memoryless network.

less networks (DMNs) with multiple multicast sessions. However, we will see that SNNC seems to have extra constraints
that do not appear for NNC. We show that these constraints
are redundant by using the same approach as in [8].
This paper is organized as follows. In Section II, we state
the problem and our main result. In Section III, we present
SNNC for DMNs with multicast-sessions and show that SNNC
achieves the same rates as NNC.
II. P RELIMINARIES

AND

M AIN R ESULT

Random variables are written with upper case letters and
their realizations with the corresponding lower case letters.
Bold upper and lower case letters refer to random vectors
with speciﬁed dimensions and their respective realizations.
Subscripts on a symbol denote the symbol’s source and the position of the symbol in a sequence. For instance, Xki denotes
the i-th output of the k-th encoder. Superscripts denote ﬁnitelength sequences of symbols, e.g., xn = {xk1 , . . . , xkn }. Set
k
subscripts denote vectors of letters, e.g., XS = [Xk , k ∈ S].
We use Tǫn (PX ) to denote the set of letter-typical sequences
with respect to PX .
Consider the K-Node discrete memoryless network (DMN)
depicted in Fig. 1, where each node has one message only.
Node k ∈ K, K = {1, . . . , K} has message Wk destined for
nodes in the set Dk , Dk ⊆ K, while acting as a relay for
messages of the other nodes. The messages are statistically

1

for all subsets S ⊂ K s.t. S ∩ Dk = ∅ and k ∈ S c for any
joint distribution that factors as

independent and each Wk , k ∈ K, is uniformly distributed
over the set {1, . . . , 2nRk }, where nRk is assumed to be an
integer. We write R(S) = k∈S Rk and D(S) = ∪k∈S Dk .
The channel is described by the conditional probabilities
K

K

K

P (y |x ) = P (y1 , . . . , yK |x1 , . . . , xK )

k=1

III. P ROOF

where Xk and Yk , k ∈ K, are the corresponding input and
output alphabets, i.e., (y1 , . . . , yK ) ∈ Y1 × · · · × YK and
(x1 , . . . , xK ) ∈ X1 × · · · × XK . All alphabets are ﬁnite sets
for now, we extend the results to Gaussian channels later.
Node k transmits xkt ∈ Xk at time t and receives ykt ∈ Yk .
The channel is memoryless in the sense that
(1)

for all t.
We could also represent the DMN as a directed graph G =
{K, E}, where E ⊆ K × K is the set of edges. Edges (links)
are denoted as (i, j) ∈ E, i, j ∈ K, i = j. Suppose that
every edge (i, j) is associated with a non-negative real number
Cij = max I(Xi ; Yj |XK\i ) called the capacity of the link.
p(xK )

Let Path(i,j) be a path that starts from node i and ends at
node j. Let Γ(i,j) to be the set of all Path(i,j) . We assume
|Γ(i,j) | ≥ 1 and write (k, ℓ) ∈ Path(i,j) if (k, ℓ) lies on the
path Path(i,j) . We may communicate reliably between nodes i
Ckl is positive. We
and j if R(i,j) =
max
min

ˆ

wkj = 1, . . . , 2nRk , lk(j−1) = 1, . . . , 2nRk , k ∈ K}
for j = 1, . . . , B.
There is a minor modiﬁcation of the codebooks used in
block j = B + 1, . . . , B + K · (K − 1). In block j =
B+(k−1)·(K −1)+1, . . . , B+k·(K −1), k = 1, . . . , K, gen′ ˆ
erate for all nodes 2n Rk independently identically distributed
′ ˆ
(i.i.d.) codewords xkj (1, lkj ), lkj = 1, . . . , 2n Rk , according
n′
to i=1 PXk (x(kj)i ). We choose

assume that every node k ∈ K can communicate to any other
node in the network with a positive rate in such a multihop
fashion.
We deﬁne two types of functions for each node k:
n
• n encoding functions fk = (fk1 , . . . , fkn ) that generate
channel inputs based on the message and past channel
outputs

n′ = max

Xkt = fkt (Wk , Ykt−1 ), t = {1, . . . , n}.

k

ˆ (k)
One decoding function gk (Ykn , Wk ) = [Wi , i ∈ Dk ]
that puts out estimates of the messages, where Dk =
{i ∈ K, k ∈ Di }, that is the set of nodes whose signals
node k is interested in decoding correctly.
The average error probability for the DMN is deﬁned as
= Pr[∪k∈D ∪i∈Dk

Rk =

ˆ
ˆ
− I(YS ; YS |XK YS c Yk T )

nBRk
nB + [K · (K − 1) · n′ ]

(4)

which approaches Rk as B → ∞.
Encoding: Each node k upon receiving ykj at the end of
the block j, tries to ﬁnd an index lkj s.t. the following event
occurs

where D = ∪K Dk . A rate tuple (R1 , . . . , RK ) is achievable
k=1
for the DMN if for any ǫ > 0, there exist some integer n and
(n)
n
some functions {fk }K and {gk }K such that (s.t.) Pe <
k=1
k=1
ǫ. The capacity region is the closure of the set of achievable
rate tuples.
The following theorem is the main result of this paper.
Theorem 1: For a K-node DMN with multiple multicast
K
sessions, SNNC achieves rate tuples RSNNC = (R1 , . . . , RK )
satisfying
k∈D

(3)

that is independent of B. The overall rate of user k is

= Wi }]

ˆ
R(S) < min I(XS ; YS c Yk |XS c T )

ˆ
nRk
min R(k,i)
i∈K

•

ˆ (k)
{Wi

T HEOREM 1

Cj ={xkj (wkj , lk(j−1) ), ykj (lkj |wkj , lk(j−1) ),
ˆ

Path(i,j) ∈Γ(i,j) (k,l)∈Path(i,j)

(n)
Pe

OF

For clarity, we set the time-sharing random variable T to be
a constant below. The encoding process of SNNC is depicted
in table I.
The message wk , k ∈ K, of 2nBRk bits is split into B
equally sized blocks, wk1 , . . . , wkB , each of nRk bits and is
transmitted over B + K · (K − 1) blocks.
Random Code:
K
Fix a distribution k=1 P (xk )P (ˆk |yk xk ). For each block
y
ˆ
j = 1, . . . , B and k ∈ K, generate 2n(Rk +Rk ) codewords
ˆ
xkj (wkj , lk(j−1) ), wkj = 1, . . . , 2nRk , lk(j−1) = 1, . . . , 2nRk ,
n
according to i=1 PXk (x(kj)i ). For each wkj and lk(j−1) ,
ˆ
ˆ
generate 2nRk ykj (lkj |wkj , lk(j−1) ), lkj = 1, . . . , 2nRk , acˆ
n
cording to i=1 PYk |Xk (ˆ(kj)i |x(kj)i (wkj , lk(j−1) )). This dey
ˆ
ﬁnes the codebook

t−1
t−1
P (y1t , . . . , yKt |w1 , . . . , wK , xt , . . . , xt , y1 , . . . , yK )
1
K

= P (y1t , . . . , yKt |x1t , . . . , xKt )

P (xk |t)P (ˆk |yk xk t) P (y K |xK ).
y

P (t)

E0(kj) : ykj (lkj |wkj , lk(j−1) ), xkj (wkj , lk(j−1) ), ykj
ˆ
∈ Tǫn PYk Xk Yk
ˆ

(5)

where lk0 = 1 by convention. If there is no such index, set
lkj = 1. If there is more than one, choose one. Each node k
transmits xkj (wkj , lk(j−1) ) in block j = 1, . . . , B.
In block j = B + (k − 1)(K − 1) + 1, . . . , B + k · (K − 1),
k = 1, . . . , K, node k tries to convey lkB reliably to all other
ˆ
nodes by sending xkj (1, lkB ) with multihop at rate Rk through
the network.

(2)

2

Block
X1
ˆ
Y1
.
.
.
XK
ˆ
YK

1
x11 (w11 , 1)
y11 (l11 |w11 , 1)
ˆ
.
.
.
xK1 (wK1 , 1)
yK1 (lK1 |wK1 , 1)
ˆ

···
···
···
.
.
.
···

B
x1B (w1B , l1(B−1) )
y1B (l1B |w1B , l1(B−1) )
ˆ
.
.
.
xKB (wKB , lK(B−1) )
yKB (lKB |wKB , lK(B−1) )
ˆ

B + 1 · · · B + K · (K − 1)

Multihop K messages to K − 1 nodes
in K · (K − 1) · n′ channel uses

TABLE I
S HORT MESSAGE NOISY NETWORK CODING FOR A MULTI - SOURCE MULTICAST DMN.

are jointly typical is

A. Backward Decoding
At the end of block B + K · (K − 1), every node k ∈ D has
reliably recovered lB = (l1B , . . . , lKB ), via the multihopping
of the last K(K − 1) blocks.
For block j = B, . . . , 1, node k tries to ﬁnd tuples
(k)
(k)
(k)
(k)
(k)
(k)
l
l
l
ˆ
ˆ
wj = (w1j , . . . , wKj ) and ˆj−1 = (ˆ1(j−1) , . . . , ˆK(j−1) )
ˆ
s.t. the following event occurs

Pr E1(kj) (wj , lj−1 , lj )
≤2

−n(

PXK YK Yk
ˆ

(6)

j

j

j



ˆ c
ˆ
ˆ
H(Yi |Xi ) − H(YSj |XK YSj Yk )

+

i∈Sj

where lj = (l1j , . . . , lKj ) has already been reliably recovered
from the previous block j + 1.
Error Probability: Assume without loss of generality that
wj = (w1j , w2j , . . . , wKj ) = (1, 1, . . . , 1) and lj−1 =
(l1(j−1) , l2(j−1) , . . . , lK(j−1) ) = (1, 1, . . . , 1). In each block
j, the error events at node k ∈ D are as follows:

Hence, Pr E(kj)2 can be upper bounded as follows
Pr E(kj)2 = ∪(wj ,lj−1 ): Pr E1(kj) (wj , lj−1 , 1)
wj =1 or lj−1 =1
(a)

=

c
E(kj)0 : ∪k∈K ∪lkj E0(kj)

E(kj)1 :

j

ˆ c
c
I(Sj ) = I(XSj ; YSj Yk |XSj )


(k) (k)
(k) (k)
ˆ
ˆ
l
. . . , xKj (wKj , ˆK(j−1) ), yKj (lKj |wKj , ˆK(j−1) ), ykj )
ˆ
l

∈

ˆ
ˆ
ˆ
ˆ
H(Xi Yi )+H(XS c YS c Yk )−H(XSj YSj XS c YS c Yk )−3ǫ)

= 2−n(I(Sj )−3ǫ)

where
(k) (k)
(k) (k)
(k) (k)
l
ˆ
ˆ
l
ˆ
l
E1(kj) (wj , ˆj−1 , lj ) : (x1j (w1j , ˆ1(j−1) ), y1j (l1j |w1j , ˆ1(j−1) ),
ˆ
Tǫn

i∈Sj

Pr E1(kj) (wj , lj−1 , 1)
(wj ,lj−1 ):
wj =1 or lj−1 =1

c
E1(kj) (1, 1, 1)

E(kj)2 :∪(wj ,lj−1 ):wj =1 or lj−1 =1 E1(kj) (wj , lj−1 , 1)

(wj ,lj−1 ):
wj =1 or lj−1 =1

The error probability Pr Ek at node k is bounded as
B

2

Pr Ek = Pr ∪B ∪2 E(kj)i ] ≤
j=1 i=0

2−n(I(Sj )−3ǫ)

≤

2−n(I(S)−3ǫ)

=
Pr E(kj)i ]

S:k∈S c
S=∅

j=1 i=0

(wj ,lj−1 ):wj =1 or lj−1 =1
Sj (wj ,lj−1 )=S

2n

<

Pr E(kj)0 can be made small with large n, as long as [10]

i∈M

Ri n

2

i∈Q

ˆ
Ri −n(I(S)−3ǫ)

2

S:k∈S c M⊆S,Q⊆S
S=∅ M∪Q=S

ˆ
ˆ
Rk > I(Yk ; Yk |Xk ) + 3ǫ.

(b)

3|S| 2n(

≤

Similarly, Pr E(kj)1 can be made small with large n.
To bound Pr E(kj)2 , for each wj and lj−1 deﬁne
Sj (wj , lj−1 ) ⊂ K where Sj (wj , lj−1 ) = {i ∈ K : wij =
1 or li(j−1) = 1}. Further, we deﬁne M(wj ) = {i ∈ K :
wij = 1} and Q(lj−1 ) = {i ∈ K : li(j−1) = 1}. By deﬁnition,
we have M(wj ) ⊆ Sj (wj , lj−1 ), Q(lj−1 ) ⊆ Sj (wj , lj−1 ),
c
Sj (wj , lj−1 ) = M(wj ) ∪ Q(lj−1 ) and k ∈ Sj (wj , lj−1 ).
Deﬁne XSj to be the set of Xij (wij , li(j−1) ), i ∈
Sj (wj , lj−1 ), where wij and li(j−1) are the corresponding
elements in wj and lj−1 .
ˆ
ˆ c
ˆ
c
Similarly deﬁne XSj , YSj and YSj . Then, (XSj , YSj ) is
ˆ c
c
independent of (XSj , YSj , Ykj ) and the probability that they

ˆ

i∈S (Ri +Ri )−(I(S)−3ǫ)

)

S:k∈S c
S=∅

where (a) is because all error events are disjoint and (b) is
because for every node i ∈ S, we must have one of the
following three cases occur:
1) i ∈ M and i ∈ Q
/
2) i ∈ M and i ∈ Q
3) i ∈ M and i ∈ Q
/
Deﬁning |S| to be the size of S , there are 3|S| different ways
choosing of M and Q s.t. M ∪ Q = S.

3

c
Suppose that for one subset B ⊆ Dk , the constraint (10) is
violated, i.e., R(B) ≥ RK (B). Using the same argument as in
[8], we have for any B ⊂ S ⊂ K s.t. S ∩ Dk = ∅ and k ∈ S c

Therefore, Pr[E(kj)2 tends to zero as n → ∞ if
R(S) < I(S) −
i∈S

log2 3|S|
ˆ
− 3ǫ
Ri −
n

R(S \ B) < RK (S) − R(B)

ˆ
Ri − ǫ′

= I(S) −

(7)

≤ RK (S) − RK (B)

i∈S

= RK\B (S \ B)

for all subsets S ⊂ K s.t. S = ∅ and k ∈ S c .
ˆ
ˆ
Since we require Rk ≥ I(Yk ; Yk |Xk ), ∀k ∈ K, we have

where
RK\B (S \ B)
ˆ
ˆ
ˆ
= I(XS\B ; YS c Yk |XS c ) − I(YS\B ; YS\B |XK\B YS c Yk )

ˆ
Ri

I(S) −
i∈S

ˆ
I(Yi ; Yi |Xi )

≤I(S) −

(12)

The bounds in (12) are the NNC rate bounds describing the
rates for the nodes in K \ B while treating the signals from
the nodes in B as noise. This suggests that even for NNC, if
any of the constraints is violated, the destination node should
treat the signals from the corresponding nodes as noise rather
than decoding them. In this way, we get better rates.
Now repeat the above argument for the nodes in K \B, until
we ﬁnally reach a set Kk ⊆ K, for which we have no more
violation of constraints, which means

i∈S

ˆ
ˆ
ˆ
=I(XS ; YS c Yk |XS c ) − I(YS ; YS |XK YS c Yk )
Therefore, (7) can be written as
R(S) <RK (S)

(8)

for all subsets S ⊂ K s.t. S = ∅ and k ∈ S c , where
ˆ
ˆ
ˆ
RK (S) = I(XS ; YS c Yk |XS c ) − I(YS ; YS |XK YS c Yk ) − ǫ′

R(B) < RKk (B)
ˆ
ˆ
ˆ
RKk (B) = I(XB ; YBc Yk |XBc ) − I(YB ; YB |XKk YBc Yk )

We can split the bounds in (8) for two different kinds of
subsets S ⊂ K and B ⊂ K:
1) S : S ∩ Dk = ∅
c
2) B : B ∩ Dk = ∅ or equivalently B : B ⊆ Dk
Hence, we can rewrite the bounds in (8) as follows:

for all subsets B ⊆ (Kk \ Dk ) ⊂ Kk s.t. k ∈ B c , where B c is
now the complement set of B in Kk .
The error probability tends to zero as n → ∞ if

R(S) < RK (S)

(9)

R(S) < RKk (S)

subject to R(B) < RK (B)

(10)

for all subsets S ⊂ Kk s.t. S ∩ Dk = ∅ and k ∈ S c , where S c
is the complement set of S in Kk .
We remark that there is a subtle addition to [8] and difference to [11], namely that each k ∈ D may have a different
set Kk of nodes whose messages and quantization indices it
includes in its typicality test. But we can achieve the same rates
as in (13) at node k with SNNC using backward decoding by
treating the signals from the nodes in K \ Kk as noise. Hence
we may ignore the constraints in (10) associated with SNNC
at node k.
By the union bound, the error probability for all destinaK
tions tends to zero as n → ∞ if the rate tuple RSNNC =
(R1 , . . . , RK ) satisﬁes

for all subsets S ⊂ K s.t. S ∩ Dk = ∅ and k ∈ S c and all
c
subsets B ⊆ Dk ⊂ K s.t. k ∈ B c .
With (10), it is guaranteed that the quantization indices lj−1
for the next (backward) decoding step are reliably decoded.
Continuing this way, node k ∈ D successively decodes all
messages wj and quantization indices lj−1 , j = 1, . . . , B.
Note that for any S ⊂ K and k ∈ D s.t. k ∈ S c , we have
k ∈ S c ∩ D(S) if and only if S ∩ Dk = ∅:
1) If S ∩ Dk = ∅, there exists at least one i ∈ S s.t.
k ∈ Di ⊆ D(S) which implies that k ∈ S c ∩ D(S).
2) If k ∈ S c ∩ D(S), there exists at least one i ∈ S s.t.
k ∈ Di which implies that i ∈ Dk . We hence have
S ∩ Dk = ∅.
Hence, in [1, Theorem 2], with long-message NNC the error
probability at node k ∈ D tends to zero as n → ∞ if
R(S) < RK (S)

(13)

ˆ
ˆ
ˆ
R(S) <min I(XS ; YS c Yk |XS c ) − I(YS ; YS |XK YS c Yk )
k∈D

for all subsets S ⊂ K s.t. S ∩ Dk = ∅ and k ∈ S c for any
joint distribution that factors as

(11)

K

P (xk |t)P (ˆk |yk xk t) P (y K |xK ).
y

P (t)

c

for all subsets S ⊂ K s.t. S ∩ Dk = ∅ and k ∈ S .
The difference between the SNNC and NNC bounds are
the constraints (10). If all constraints in (10) are satisﬁed, it is
obvious that SNNC achieves the same rates as NNC. In this
case the decoder should decode the signals from the nodes in
c
Dk , since they can be decoded with the signals from the nodes
in Dk and thereby help remove interference.

k=1

If D = D1 = · · · = DK , then SNNC achieves the same
rates as in [1, Theorem 1] (see also [11, Chapter 19, Theorem
5])
ˆ
ˆ
ˆ
R(S) < min I(XS ; YS c Yk |XS c ) − I(YS ; YS |XK YS c Yk )
k∈D

4

1

1

B. Joint decoding

d

2

2

(a) Both nodes 1 and 2
are sources and relays

It turns out that SNNC with joint decoding also achieves
the same rates as in (2). Recently, the authors of [12] claimed
that SNNC with joint decoding fails to achieve the NNC rates
for multi-source networks because long message repetition
encoding is needed to do so. This is not true. With a minor
modiﬁcation of the decoding procedure, namely, ﬁrst decoding
the last quantization indices and then performing joint decoding with the messages and remaining quantization bits, SNNC
with joint decoding performs as well as SNNC with backward
decoding and NNC. This makes sense, since intuitively one
would expect joint decoding to perform at least as well as
backward decoding. Due to the similarity to the proof with
backward decoding, details are omitted.

d

(b) Node 2 acts as a
relay for node 1

Fig. 2.

Example of a three-node channel.

R2

Point 2

ACKNOWLEDGMENT

J. Hou and G. Kramer were supported by an Alexander
von Humboldt Professorship endowed by the German Federal
Ministry of Education and Research. G. Kramer was also
supported by NSF Grant CCF-09-05235.

Point 1

R1
Fig. 3.

Qualitative illustration of the achievable rates

R EFERENCES
for all subsets S ⊂ K s.t. S ∩ D = ∅ and k ∈ S c .
Remark 1: Intuitively, for some channels it may not be
K
K
so surprising that RSNNC and RNNC are equivalent. Consider
the following network in Fig. 2. If both nodes 1 and 2 act
as sources as well as relays for each other in transmitting
information to the destination node d (Fig. 2(a)), referring to
Theorem 1 and [1, Theorem 2 ], the SNNC and NNC bounds
are the same (see Fig. 3).

[1] S.H. Lim, Y.-H. Kim, A. El Gamal, and S.-Y. Chung. Noisy network
coding. IEEE Trans. inf. Theory, 57(5):3132 –3152, May 2011.
[2] R. Ahlswede, Ning Cai, S.-Y.R. Li, and R.W. Yeung. Network information ﬂow. IEEE Trans. inf. Theory, 46(4):1204 –1216, July 2000.
[3] A.F. Dana, R. Gowaikar, R. Palanki, B. Hassibi, and M. Effros. Capacity
of wireless erasure networks. IEEE Trans. inf. Theory, 52(3):789 –804,
March 2006.
[4] N. Ratnakar and G. Kramer. The multicast capacity of deterministic relay
networks with no interference. IEEE Trans. inf. Theory, 52(6):2425 –
2432, june 2006.
[5] A.S. Avestimehr, S.N. Diggavi, and D.N.C. Tse. Wireless network
information ﬂow: A deterministic approach. IEEE Trans. inf. Theory,
57(4):1872 –1905, April 2011.
[6] G. Kramer and J. Hou. Short-message quantize-forward network coding.
In 2011 8th International Workshop on Multi-Carrier Systems Solutions
(MC-SS), pages 1 –3, May 2011.
[7] X. Wu and L.-L. Xie. On the optimal compressions in the compressand-forward relay schemes. submitted to IEEE Trans. inf. Theory, Feb.
2011.
[8] G. Kramer and J. Hou. On message lengths for noisy network coding. In
IEEE Information Theory Workshop (ITW), pages 430 –431, Oct. 2011.
[9] G. Kramer, M. Gastpar, and P. Gupta. Cooperative strategies and capacity theorems for relay networks. IEEE Trans. inf. Theory, 51(9):3037–
3063, Sept. 2005.
[10] T. Cover and J. Thomas. Elements of Information Theory. WileyInterscience, New York, NY, USA, 2006.
[11] A. El Gamal and Y.-H. Kim. Network Information Theory. Cambridge
University Press, 2011.
[12] P. Zhong, A.A.A. Haija, and M. Vu. On compress-forward without
wyner-ziv binning for relay networks. arXiv:1111.2837v1, 2011.

ˆ
ˆ
ˆ
R1 < I(X1 ; Y2 Yd |X2 ) − I(Y1 ; Y1 |X1 X2 Y2 Yd )
ˆ
ˆ
ˆ
R2 < I(X2 ; Y1 Yd |X1 ) − I(Y2 ; Y2 |X1 X2 Y1 Yd )
ˆ ˆ
R1 + R2 < I(X1 X2 ; Yd ) − I(Y1 Y2 ; Y1 Y2 |X1 X2 Yd )

(14)

However, interesting cases arise when one of the two nodes
acts only as a relay node and the constraints kick in. Suppose
now node 2 acts as a relay node only (Fig. 2(b)), we have
ˆ
R1(SNNC) <min I(X1 ; Y2 Yd |X2 ),
ˆ
I(X1 X2 ; Yd ) − I(Y2 ; Y2 |X1 X2 Yd )
ˆ
subject to I(X2 ; Yd |X1 ) − I(Y2 ; Y2 |X1 X2 Yd ) > 0

(15)
(16)

ˆ
R1(NNC) <min I(X1 ; Y2 Yd |X2 ),
ˆ
I(X1 X2 ; Yd ) − I(Y2 ; Y2 |X1 X2 Yd )

(17)

Now we ask whether R1(SNNC) in (15) with the constraint (16)
is equivalent to R1(NNC) in (17). This is equivalent to asking
K
K
whether RSNNC = RNNC in point 1 and point 2 in Fig. 3. To
K
K
see this, choose R2 = ǫ, ǫ > 0, we have RSNNC = RNNC .
K
K
Let ǫ → 0, we must have RSNNC → RNNC in point 1. We
K
K
also have RSNNC → RNNC in point 2 with the same argument.
Therefore, it suggests that we might ignore the quantization
constraint in (16).

5

