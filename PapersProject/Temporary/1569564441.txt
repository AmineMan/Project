Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 12:40:06 2012
ModDate:        Tue Jun 19 12:56:17 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      458706 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569564441

Tractable Bayesian Social Learning on Trees
Yashodhan Kanoria

Omer Tamuz

Microsoft Research New England and
Department of Electrical Engineering
Stanford University
Email: ykanoria@stanford.edu

Microsoft Research New England
and
Weizmann Institute
Email: omer.tamuz@weizmann.ac.il

and picking an action that is myopically optimal, given
their information. Gale and Kariv [5] showed that, in this
model, agents converge to the same action under some
conditions. Related work [6] sheds more light on the
phenomenon of agreement on actions and the conditions
in which it arises.
We are interested in the following questions in the
context of this model, which have not been previously
addressed:
(I) What action do the agents converge to, e.g., what
is the distribution of this consensus action?
(II) What are the dynamics of such interactions, e.g.,
what is the rate of agreement/convergence?
(III) Are the computations required of Bayesian agents
feasible?
Even in the simple case of two states of the world,
binary private signals and two possible actions, the
required calculations appear to be very complicated.
A na¨ve dynamic programming algorithm (cf. Proposiı
tion III.1) is exponential in the number of individuals.
Since at iteration t one may consider only agents at
distance t, then in graphs of maximum degree d (on
which we focus) the number of individuals to consider
is O(min(n, dt )), and the computational effort required
of each individual to compute their action at time t
t
is t2O(min(n,d )) . This grows very rapidly, restricting
previous simulations to networks of three nodes [5]
We describe a novel algorithm for the agents’ calculation in our model, when the social network graph is a
tree. This algorithm has running time that is exponentially smaller than the na¨ve dynamic program, reducing
ı
the computational effort to 2O(min(n,td)) .
Using our algorithm we are able to run numerical
simulations of the social learning process. This extends
the work of Gale and Kariv [5], who simulated the
process for three agents, to much larger networks1 . We
use our algorithm to investigate questions (I) and (II):
We numerically evaluate the probability that the agents
learn the optimal action, and its progress with time. We
observe rapid learning of the optimal action in certain

Abstract—We study a model of Bayesian agents in social
networks who learn from the actions of their neighbors.
Agents attempt to iteratively estimate an unknown ‘state
of the world’ s from initial private signals, and the past
actions of their neighbors in the network. We investigate
the computational problem the agents face in implementing
the (myopic) Bayesian decision rule. When private signals
are independent conditioned on s, and when the social
network graph is a tree, we provide a new ‘dynamic cavity
algorithm’ for the agents’ calculations, with computational
effort that is exponentially lower than a naive dynamic
program.
We use this algorithm to perform the ﬁrst numerical
simulations of Bayesian agents on networks with hundreds
of nodes, and observe rapid learning of s in some settings.

I. I NTRODUCTION
The importance of social learning in networks has
been demonstrated in a wide variety of settings, such
as the adoption of agricultural technology in Ghana [1],
and choice of contraceptives by European women [2].
Accordingly, developing and understanding models of
social learning has been a goal of theoretical economics
for the past few decades (cf., [3], [4] and references
therein). Typical models in this context assume a pure
information externality; agent payoffs depend only on
the action they choose and an underlying ‘state of
the world’, and not on the actions of others. Agents
observe the actions of their ‘neighbors’, but payoffs are
not observed (or observed with noise) ex interim. The
premise in such models is that “actions speak louder
than words” – agents learn by observing each others’
actions, and not by communicating directly.
We consider a model that features repeated bidirectional interaction between fully Bayesian agents connected by a social network. Our model is a specialization
of the model of Gale and Kariv [5]. We consider a
group of Bayesian agents, each with a private signal that
carries information on an unknown state of the world
s. The individuals form a social network, so that each
observes the actions of some subset of others, whom we
call her neighbors. The agents must repeatedly choose
between a set of possible actions, the relative merit of
which depends on the state of the world s. The agents
iteratively learn by observing their neighbors’ actions,

1 In our numerical analyses, agents receive information (directly or
indirectly) from hundreds of distinct nodes.

1

previously unexplored settings.
We conjecture that on regular trees, the error probability under Bayesian updates is no larger than the error
probability under a different ‘majority’ update rule, in
which agents adopt the opinion of the majority of their
neighbors in the previous round. Our numerical results
support this conjecture. We prove that for the majority
update rule, the number of iterations needed to estimate
s correctly with probability 1 − is O(log log(1/ )),
for regular trees of degree at least ﬁve. Assuming the
conjecture, the computational effort required of Bayesian
agents drops from quasi-polynomial in 1/ (using the
na¨ve dynamic program) to polynomial in log(1/ ) (i.e.,
ı
polylogarithmic in 1/ ), making Bayesian learning computationally tractable. Thus, our results shed new light
on question (III), suggesting a positive answer in the case
of trees.
The restriction of the discussion to tree or tree-like
social networks certainly excludes many natural settings that tend to exhibit highly clustered social graphs.
However, in some cases artiﬁcially constructed networks
have no or few loops by design; these include highly
hierarchical organizations, as well as some physical
communication networks. Furthermore, the fact that this
non-trivial class of networks does not present a major
computational hurdle for fully Bayesian calculations is
in itself somewhat surprising.
See the full version of the paper [7] for a more detailed
discussion, literature survey and proof details.
Technical contributions. A key technique used in
this paper is the dynamic cavity method, introduced by
Kanoria and Montanari [8] in their study of ‘majority
updates’ on trees. This technique is a dynamical version
of the cavity method of statistical physics. Our algorithmic and analytical approach leveraging the dynamic
cavity method should be applicable to a range of models
involving iterative updates on trees.
Our second main technical contribution is our proof,
using a dynamic cavity type approach, of doubly exponentially fast convergence of majority dynamics on regular trees. This result should be of independent interest.

to them are their private signals xi , where xi ∈ X
and X is ﬁnite. We assume a general distribution of
(s, x1 , . . . , xn ), under the condition that private signals
are independent conditioned on s, i.e. P [s, x1 , . . . , xn ] =
P [s] i∈V P [xi |s] .
In each discrete time period (or round) t = 0, 1, . . . ,
agent i chooses action σi (t) ∈ S, which we call a
‘vote’. Agents observe the votes cast by their neighbors
in G. Thus, at the time of voting in round t ≥ 1,
the information available to an agent consists of the
private signal she received initially, along with the votes
cast by her neighbors in rounds up to t − 1. In each
round, each agent votes for the state of the world that
she currently believes is most likely, given the Bayesian
posterior distribution she computes.
t
Formally, let σi = (σi (0), σi (1), . . . , σi (t)) denote all
of agent i’s votes, up to and including time t. Let ∂i
denote neighbors of agent i, not including i, i.e., ∂i =
t
t
t
{j : (i, j) ∈ E}. We write σ∂i = {σj }j∈∂i , i.e., σ∂i are
the votes of i’s neighbors up to and including time t.
Then the agents’ votes σi (t) are given by

II. M ODEL

A fairly straightforward dynamic programming algorithm can be used to compute the actions chosen by
agents in our model. The proposition below states the
computational complexity of this algorithm.

t−1
σi (t) = arg max P s xi , σ∂i ,
s∈S

where, if the maximum is attained by more than one
value, some deterministic tie breaking rule is used. We
denote σi = (σi (0), σi (1), . . .).
Note that σi (t) is a deterministic function of xi and
t−1
σ∂i . We denote this function gi (t) : X × |S|t|∂i| → S:
t−1
σi (t) = gi,t (xi , σ∂i )
t
For convenience, we also deﬁne the vector function gi
that returns the entire history of i’s votes up to time t,
t
gi = (gi,0 , gi,1 , . . . , gi,t ), so that
t−1
t
t
σi = gi (xi , σ∂i ) .

The full version [7] motivates our model in the context
of rational agents, and also presents a detailed comparison with the model of Gale and Kariv [5].
III. M AIN RESULTS
A. Efﬁcient computation

The model we consider is a simpliﬁed version of
the model of social learning introduced by Gale and
Kariv [5].
Consider a directed graph G = (V, E), representing a
network of n = |V | agents, with V being the set of
agents and E being the social ties between them. A
directed edge (i, j) indicates that agent i observes agent
j. In most of this paper, we study the special case of
undirected graphs, where relationships between agents
are bidirectional.
Agents attempt to learn the true state of the world
s ∈ S, where S is ﬁnite. The information available

Proposition III.1. On any graph G, there is a dynamic
programming (DP) based algorithm that allows agents
to compute their actions up to time t with computational
t
effort t2O(min(n,(d−1) )) , where d is the maximum degree
of the graph.
The algorithm and proof is included in the full version
[7] of this paper. This proposition provides the benchmark that we compare our other algorithmic results to.

2

where b > 1 is some constant.
The following is an immediate corollary of Theorem
III.2.

In particular, we do not consider this algorithm a major
contribution of this work.
A key advantage of the DP algorithm is that it works
for any graph G. The disadvantage is that the computational effort required grows doubly exponentially in the
number of iterations t.
Our main result concerns the computational effort
needed when the graph G is a tree (i.e., a graph with no
loops). We show that computational effort exponentially
lower than that of the naive DP sufﬁces in this case.

Corollary III.3. Consider iterative Bayesian learning
on a tree of with maximum degree d. If we have doubly
exponential convergence to s, then computational effort
that is polynomial in log(1/ ) (i.e., polylogarithmic in
1/ ) sufﬁces to achieve error probability P [σi (t) = s] ≤
for all i in V .
Note that if weaken our assumption to doubly exponential convergence in only a subset Vc ⊆ V of
nodes, i.e., maxi∈Vc P [σi (t) = s] = exp − Ω(bt ) , we
still obtain a similar result with nodes in Vc efﬁciently
learning s.
We state below, and provide numerical evidence for, a
conjecture that implies doubly exponential convergence
of iterative Bayesian learning.
1) Bayesian vs. ‘majority’ updates: We conjecture
that on regular trees, iterative Bayesian learning leads
to lower error probabilities (in the weak sense) than
a very simple alternative update rule we call ‘majority
dynamics’[8]. Under this rule, the agents adopt the action
taken by the majority of their neighbors in the previous
iteration. Our conjecture seems natural since the iterative
Bayesian update rule chooses the vote in each round
that (myopically) minimizes the error probability. We use
σi (t) to denote votes under the majority dynamics.

Theorem III.2. In a tree graph G with maximum degree
d, each agent can calculate her actions up to time t with
computational effort t2O(min(n,td)) .
The algorithm we use employs a technique called
the dynamic cavity method [8], previously used only in
analytical contexts. Section IV contains a description of
the algorithm and analysis leading to Theorem III.2.
We would like to thank our anonymous referee for
pointing out that it may also be possible to prove
Theorem III.2 using Bayesian Networks (BN). The proof
would involve constructing the appropriate BN and
showing that its tree-width is min(n, td).
An apparent issue is that the computational effort
required is exponential in t; typically, exponentially
growing effort is considered as large. However, in this
case, we expect the number of iterations t to be typically
quite small, for two reasons: (1) In many settings, agents
appear to converge to the ‘right’ answer in a very small
number of iterations [5]. If is the desired probability
of error, then assuming a reasonable conjecture (Conjecture III.4), we show that computational effort only
polylog(1/ ) is required on trees. Having obtained an
approximately correct estimate, the agents would have
little incentive to continue updating their beliefs. (2)
In many situations we would like to model, we might
expect only a small number (e.g., single digit) number
of iterative updates to occur, irrespective of network
size etc. For instance, voters may discuss an upcoming
election with each other over a short period of time,
ending on the election day when ballots are cast.

Conjecture III.4. Consider binary s ∼ Bernoulli(1/2),
and binary private signals that are independent identically distributed given s, with P [xi = s] = 1−δ for some
δ ∈ (0, 1/2). Let the majority dynamics be initialized
with the private signals, i.e., σi (0) = xi for all i ∈ V .
Then on any inﬁnite regular tree, for all t ≥ 0, we have
P [σi (t) = s] ≤ P [σi (t) = s] .

Using a dynamic cavity approach, we show doubly exponential convergence for majority dynamics on regular
trees (the full version [7] contains a proof):
Theorem III.5. Consider binary s ∼ Bernoulli(1/2),
and binary initial votes σi (0) that are independent identically distributed given s, with P [σi (0) = s] = 1−δ for
some δ ∈ (0, 1/2). Let i be any node in an (undirected)
d regular tree for d ≥ 5. Then, under the majority
dynamics,

B. Convergence
Since an agent gains information at each round, and
since she is Bayesian, then the probability that she votes
correctly is non-decreasing in t, the number of rounds.
We say that the agent converges if this probability
converges to one, or equivalently if the probability that
the agent votes incorrectly converges to zero.
We say that there is doubly exponential convergence
to the state of the world s if the maximum single
node error probability maxi∈V P [σi (t) = s] decays with
round number t as
max P [σi (t) = s] = exp − Ω(bt ) ,
i∈V

(2)

P [σi (t) = s] = exp − Ω

1
2 (d

− 2)

t

.

d−2

when δ < (2e(d − 1)/(d − 2))− d−4 .
Thus, if Conjecture III.4 holds:
• We have doubly exponential convergence for iterative Bayesian learning on regular trees with d ≥ 5,
implying that for any > 0, an error probability

(1)

3

•

can be achieved in O(log log(1/ )) iterations with
iterative Bayesian learning.
Combining with Corollary III.3), we see that the
computational effort that is polylogarithmic in (1/ )
sufﬁces to achieve error probability 1/ .

A. The Dynamic Cavity Method
We consider a modiﬁed process where agent i is
replaced by a zombie agent who takes a ﬁxed sequence
of actions τi = (τi (0), τi (1), . . .), and the true state of
the world is assumed to be some ﬁxed s. Furthermore,
this ‘ﬁxing’ goes unnoticed by the agents (except i,
who is a zombie anyway) who perform their calculations
assuming that i is her regular Bayesian self. Formally:

This compares favorably with the quasi-poly(1/ )
(i.e., exp polylog(1/ ) ) upper bound on computational effort that we can derive by combining Conjecture
III.4 and the na¨ve dynamic program described. Indeed,
ı
based on recent results on subexponential decay of error
probability with the number of private signals being
aggregated [9], it would be natural to conjecture that
the number of iterations T needed to obtain an error
probability of obeys (d − 1)T ≥ C log(1/ ) for any
C < ∞, for small enough. This would then imply
that the required computational effort using the na¨ve
ı
DP on a regular tree of degree d grows faster than any
polynomial in 1/ .
Since we are unable to prove our conjecture, we
instead provide numerical evidence for it (see the full
version of the paper), which is consistent with our
conjecture over different values of d and P [xi = s].
The full version also discusses difﬁculties in proving the
conjecture.
We would like to emphasize that several of the error
probability values could be feasibly computed only because of our new efﬁcient approach to computing the
decision functions employed by the nodes. Our numerical results indicate very rapid decay of error probability
on regular trees (cf. questions (I) and (II) in Section I).
Figure 1 plots decay of error probabilities in regular
trees for iterative Bayesian learning with P [xi = s] =
0.3 Each of the curves (for different values of d) in
the plot of log(− log P [σi (t) = s]) vs. t appear to be
bounded below by straight lines with positive slope,
suggesting doubly exponential decay of error probabilities with t. The empirical rapidity of convergence,
particularly for d = 5, 7, is noteworthy. See the full
version [7] for more numerical results.

σj (t) =

τi (t)
for j = i ,
t−1
gj,t (xj , σ∂j ) for j = i .

We denote by Q [A||τi , s] the probability of event A
in this modiﬁed process.
This modiﬁed process is easier to analyze, as the
processes on each of the subtrees Vj→i for j ∈ ∂i are
independent: Recall that private signals are independent
conditioned on s, and the zombie agent ensures that
the subtrees stay independent of each other. This is
formalized in the following claim, which is immediate
to see:
Claim IV.1. For any i ∈ V , s ∈ S and any trajectory
τi , we have
t
Q σ∂i τi , s =

t
Q σj τit , s .

(3)

j∈∂i
t
(Since σj is unaffected by τi (t ) for all t > t, we
only need to specify τit , and not the entire τi .)
Now, it might so happen that for some number of
steps the ‘zombie’ agent behaves exactly as may be
t−1
expected of a rational player. More precisely, given σ∂i ,
t−1
t
it may be the case that τit = gi xi , σ∂i for some xi .
This event provides the connection between the modiﬁed
process and the original process, and is the inspiration
for the following theorem.

Theorem IV.2. Consider any i ∈ V , s ∈ S, t ∈ N,
t−1
trajectory τi and σ∂i . For any xi such that P [xi |s] > 0,
we have
t−1
t−1
t
P σ∂i s, xi 1 τit = gi xi , σ∂i

Q

IV. T HE DYNAMIC C AVITY A LGORITHM ON T REES

t−1
σ∂i

τi , s 1

τit

=

t
gi

=
t−1
xi , σ∂i

.

(4)

Using Eqs. (3) and (4), we can write the posterior
on s computed by node i at time t, in terms of the
probabilities Q [·||·]:

In this section we develop the dynamic cavity algorithm leading to Theorem III.2. We present the core
construction and key technical lemmas in Section IV-A.
In Section IV-B, we show how this leads to an efﬁcient
algorithm for the Bayesian computations on tree graphs,
and prove Theorem III.2.
Assume in this section that the graph G is a tree
with ﬁnite degree nodes. For j ∈ ∂i, let Gj→i =
(Vj→i , Ej→i ) denote the connected component containing node j in the graph G with the edge (i, j) removed.
That is, Gj→i is j’s subtree when G is rooted at i.

t−1
t−1
P s|xi , σ∂i ∝ P [s] P xi , σ∂i |s

t−1
= P [s] P [xi |s] P σ∂i |s, xi

= P [s] P [xi |s]

t−1
t−1
Q σj
σi , s

(5)

j∈∂i

Recall that
t−1
t−1
σi (t) = gi,t (xi , σ∂i ) = arg max P s|xi , σ∂i . (6)
s∈S

4

3
d=3
d=5
d=7

0.2

log(− log(P[σi (t) = s])

P[σi (t) = s]

0.3

0.1

0

0

1

2

3

4

5

6

1

0

7

t

Fig. 1.

2

0

1

2

3

4

5

6

7

t

Error probability decay on regular trees for iterative Bayesian learning, with P [xi = s] = 0.3 .

We have therefore reduced the problem of calculating
σi (t) to calculating Q [·||·]. The following theorem is
the heart of the dynamic cavity method and allows us to
perform this calculation:

the (myopic) Bayes optimal action in rounds up to
t + 1, based on her neighbors’ past actions. A simple
calculation yields the following lemma.
Lemma IV.4. In a tree graph G with maximum degree
d, the agents can calculate their actions up to time t
with computational effort n2O(td) .

Theorem IV.3. For any i ∈ V , j ∈ ∂i, s ∈ S, t ∈ N,
t
τit and σj , we have
t
Q σj τit , s =

In fact, each agent does not need to perform calculations for the entire graph. It sufﬁces for node i to
P [xj |s] · 1
=
·
calculate quantities up to time t for nodes at distance
t−1 xj
t−1
σ1 ... σd−1
t − t from node i (there are at most (d − 1)t−t such
d−1
nodes). A short calculation yields an improved bound
t−1
t−1
σj , s ,
(7) on computational effort, stated in Theorem III.2. The
·
Q σl
l=1
proof of Theorem III.2 is relatively straightforward and
where neighbors of node j are ∂j = {i, 1, 2, . . . , d − 1}. is provided in the full version [7] of this paper.
The proof of this theorem is similar to the proof of Acknowledgments. We would like to thank A. MontaLemma 2.1 in [8], where the dynamic cavity method is nari, E. Mossel and A. Sly for valuable discussions, and
the anonymous referees for their valuable comments.
introduced and applied to a different process.
Y. Kanoria was supported by a 3Com Corporation
B. The Agents’ Calculations
Stanford Graduate Fellowship. Omer Tamuz is supported
We now describe how to perform the agents’ cal- by a Google Europe Fellowship in Social Computing.
culations. At time t = 0 these calculations are trivial.
R EFERENCES
Assume then that up to time t each agent has calculated
[1] T. Conley and C. Udry, “Social learning through networks: The
the following quantities:
adoption of new agricultural technologies in ghana,” American
t−1
t−1
1) Q σj
τi , s , for all s ∈ S, for all i, j ∈ V
Journal of Agricultural Economics, vol. 83, no. 3, 2001.
t−1
[2] H. Kohler, “Learning in social networks and contraceptive choice,”
such that j ∈ ∂i, and for all τit−1 and σj .
Demography, vol. 34, no. 3, pp. 369–383, 1997.
t−1
t−1
t
2) gi (xi , σ∂i ) for all i, xi and σ∂i .
[3] S. Goyal, Connections: An Introduction to the Economics of
Networks. Princeton University Press, 2007.
Note that these can be calculated without making any
Lobel, and
observations – only knowledge of the graph G, P [s] and [4] D. Acemoglu, M. A. Dahleh, I. The ReviewA. Ozdaglar, “Bayesian
learning in social networks,”
of Economic Studies,
P [x|s] is needed.
vol. 78, no. 4, 2011.
At time t + 1 each agent makes the following calcu- [5] D. Gale and S. Kariv, “Bayesian learning in social networks,”
Games and Economic Behavior, vol. 45, no. 2, pp. 329–346, 2003.
lations:
[6] D. Rosenberg, E. Solan, and N. Vieille, “Informational externalit
t
ties and emergence of consensus,” Games and Economic Behavior,
1) Q σj τit , s for all s, i, j, σj , τit . These can be
vol. 66, no. 2, pp. 979 – 994, 2009.
calculated using Eq. (7), given the quantities from
[7] Y. Kanoria and O. Tamuz, “Tractable bayesian social learning,”
the previous iteration.
2011, preprint at http://arxiv.org/abs/1102.1398.
t+1
t
t
2) gi (xi , σ∂i ) for all i, xi and σ∂i . These can be [8] Y. Kanoria and A. Montanari, “Majority dynamics on trees and the
dynamic cavity method,” Annals of Applied Probability., vol. 21,
calculated using Eqs. (5) and (6) and the the newly
no. 5, 2011.
t
t
calculated Q σj τi , s .
[9] ——, “Subexponential convergence for information aggregation
t+1
on regular trees,” in CDC-ECC, 2011.
Since agent j calculates gi
for all i, then she, in
t+1
particular, calculates gj . This allows her to choose
t
σj

t
gj

t−1
xj , (τit−1 , σ∂j\i )

5

