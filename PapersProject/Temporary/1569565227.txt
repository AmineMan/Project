Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May  9 08:47:10 2012
ModDate:        Tue Jun 19 12:55:49 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      934142 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569565227

Combinatorial Selection and Least Absolute
Shrinkage via the C LASH Algorithm
Anastasios Kyrillidis and Volkan Cevher
´
Laboratory for Information and Inference Systems, Ecole Polytechnique F´ d´ rale de Lausanne
e e
the majority of the existing approaches that enforce such
constraints in selection are inherently continuous. For instance,
a prevalent approach is to tailor a sparsity inducing norm to
the constraints on the support set (c.f., [6]). That is, we create
a structured convex norm by mixing basic norms with weights
over pre-deﬁned groups or using the Lov´ sz extension of nona
decreasing submodular set functions of the support. As many
basic norms have well-understood behavior in sparse selection,
reverse engineering such norms is quite intuitive.
While such structure inducing, convex norm-based approaches on the LASSO are impressive, our contention in
this paper is that, in order to truly make an impact in
structured sparsity problems, we must fully leverage explicitly
combinatorial approaches to guide LASSO’s subset selection
process. To achieve this, we show how Euclidean projections
with structured sparsity constraints correspond to an integer
linear program (ILP), which can be exactly or approximately
solved subject to matroid (via the greedy algorithm), and
certain linear inequality constraints (via convex relaxation
or multi-knapsack solvers). A key actor in this process is
a polynomial-time combinatorial algorithm that goes beyond
simple selection heuristics towards provable solution quality
as well as runtime/space bounds.
Furthermore, we introduce our combinatorial selection and
least absolute shrinkage (C LASH) operator and theoretically
characterize its estimation guarantees. C LASH enhances the
model-based compressive sensing (model-CS) framework [7]
by combining `1 -norm and combinatorial constraints on the
regression vector. Therefore, C LASH uses a combination of
shrinkage and hard thresholding operations to signiﬁcantly
outperform the model-CS approach, LASSO, or continuous
structured sparsity approaches in learning performance of
sparse linear models. Furthermore, C LASH establishes a regression framework where the underlying tractability of approximation in combinatorial selection is directly reﬂected in
the algorithm’s estimation and convergence guarantees.
The organization of the paper is as follows. In Sections II
and III, we set up the notation and the exact projections with
structured sparsity constraints. We develop C LASH in Section
IV and highlight the key components of its convergence proof
in Section V. We present numerical results in Section VI. We
provide our conclusions in Section VII.

Abstract—The least absolute shrinkage and selection operator
(LASSO) for linear regression exploits the geometric interplay
of the `2 -data error objective and the `1 -norm constraint to
arbitrarily select sparse models. Guiding this uninformed selection process with sparsity models has been precisely the center
of attention over the last decade in order to improve learning
performance. To this end, we alter the selection process of LASSO
to explicitly leverage combinatorial sparsity models (CSMs) via
the combinatorial selection and least absolute shrinkage (C LASH)
operator. We provide concrete guidelines how to leverage combinatorial constraints within C LASH, and characterize C LASH’s
guarantees as a function of the set restricted isometry constants
of the sensing matrix. Finally, our experimental results show
that C LASH can outperform both LASSO and model-based
compressive sensing in sparse estimation.

I. I NTRODUCTION
The least absolute shrinkage and selection operator
(LASSO) is the de facto standard algorithm for regression
[1]. LASSO estimates sparse linear models by minimizing the
empirical data error via:
n
o
2
xLASSO = arg min ky
b
xk2 : kxk1 
,
(1)
where k · kr is the `r -norm. In (1), 2 Rm⇥n is the sensing
matrix, y 2 Rm are the responses (or observations), x 2 Rn
is the loading vector and 2 R++ governs the sparsity of the
solution. Along with many efﬁcient algorithms for its solution,
the LASSO formulation is now backed with a rather mature
theory for the generalization of its solutions as well as its
variable selection consistency [2]–[5].
While the long name attributed to (1) is apropos,1 it does
not capture the LASSO’s arbitrariness in subset selection
via shrinkage to best explain the responses. In fact, this
uninformed selection process not only prevents interpretability
of results in many problems, but also fails to exploit key
prior information that could radically improve learning performance. Based on this premise, approaches to guide the
selection process of the LASSO are now aplenty.
Surprisingly, while the prior information in many regression
problems generate fundamentally discrete constraints (e.g., on
the sparsity patterns or the support of the LASSO solution),
This work was supported in part by the European Commission under
Grant MIRG-268398, ERC Future Proof, and DARPA KeCoM program #11DARPA-1055. VC also would like to acknowledge Rice University for his
Faculty Fellowship.
1 Many of the optimization solutions to LASSO leverage shrinkage operations (e.g., as projections onto the `1 -ball) for sparse model selections.
However, the geometric interplay of the `2 -data error objective and the `1 norm constraint inherently promotes sparsity, independent of the algorithm.

II. P RELIMINARIES
Notation: We use [x]j to denote the j-th element of x, and
let xi represent the i-th iterate of C LASH. The index set of n

1

dimensions is denoted as N = {1, 2, . . . , n}. Given S ✓ N ,
we deﬁne the complement set S c = N \ S. Moreover, given
a set S ✓ N and a vector x 2 Rn , (x)S 2 Rn denotes
a vector with the following properties: [(x)S ]S = [x]S and
[(x)S ]S c = 0. The support set of x is deﬁned as supp(x) =
{i : [x]i 6= 0}. We use |S| to denote the cardinality of the set
S. The empirical data error is denoted as f (x) , ky
xk2 ,
2
T
with gradient deﬁned as rf (x) , 2 (y
x), where T
is the transpose operation. The notation rS f (x) is shorthand
for (rf (x))S . I represents the identity matrix.
Combinatorial notions of sparsity: We provide some
deﬁnitions on combinatorial sparse models, and elaborate on
a subset of interesting models with algorithmic implications.

their sparse recovery algorithms inherit strong approximation
guarantees for that CSM. To better identify the CSMs that live
within the model-CS assumptions, we ﬁrst state the following
key observation—the proof can be found in [12].
Lemma 1 (Euclidean projections onto CSMs). The support of
the Euclidean projection onto Ck in (3) can be obtained as a
solution to the following discrete optimization problem:
supp (PCk (x)) = arg max F (S; x),

P
2
where F (S; x) = kxk2 k(x)S xk2 = i2S |[x]i | is the
2
2
b
modular, variance reduction set function. Moreover, let S 2 Ck
be the minimizer of the discrete problem. Then, it holds that
PCk (x) = (x)S , which corresponds to hard thresholding.
b

Deﬁnition 1 (Combinatorial sparsity models (CSMs)). We
deﬁne a combinatorial sparsity model Ck = {Sm : 8m, Sm ✓
N , |Sm |  k} with the sparsity parameter k as a collection
of distinct index subsets Sm .

The following proposition reﬁnes this observation to further
accentuate the algorithmic implications for CSMs:
Proposition 1 (CSM projections via ILP’s). The problem (4)
is equivalent to the following integer linear program (ILP):

Throughout the paper, we assume that any CSM Ck is
downward compatible, i.e., removing any subset of indices
of any given element in Ck , it is still in Ck .
Properties of the regression matrix: Deriving approximation guarantees for C LASH behooves us to assume the
restricted isometry property (RIP) (deﬁned below) on the
regression matrix
[8]. While the RIP and other similar
conditions for deriving consistency properties of LASSO and
its variants, such as the unique/exact representation property or
the irrepresentable condition [5], [6], [9]–[11], are unveriﬁable
a priori without exhaustive search, many random matrices
satisfy them with high probability.

supp arg min

2
k )kxk2

 k xk2  (1 +
2

2
k )kxk2 ,

T
8supp(x) 2 Ck , where k = maxS2Ck
I
S S
as column-indexed by S.
S is a submatrix of

Deﬁnition 3 (PMAP✏ [12]). A CSM has the PMAP✏ with
constant ✏, if the modular subset selection problem (4) or the
ILP (5) admit an ✏-approximation scheme with polynomial
or pseudo-polynomial time complexity as a function of n,
8x 2 Rn . Denoting the ✏-approximate solution of (4) or (5)
b
b
as S✏ , this means F (S✏ ; x) (1 ✏) maxS2Ck F (S; x).

, and

In this paper, we focus and elaborate on CSMs with PMAP0 .

A. Example CSMs with PMAP0
Matroids: By matroid, we mean that Ck = (N , I) is a ﬁnite
collection of subsets of N that satisﬁes three conditions: (i)
{;} 2 I, (ii) if S is in I, then any subset of S is also in I,
and (iii) for S1 , S2 2 I and |S1 | > |S2 |, there is an element
s 2 S1 \ S2 such that S2 [ {s} is in I. As a simple example,
the unstructured sparsity model (i.e., x is k-sparse) forms a
uniform matroid as it is deﬁned as the union of all subsets of
N with cardinality k or less. When Ck forms a matroid, the
greedy basis algorithm can efﬁciently compute (3) by solving
(4) [13] where sorting and selecting the k largest elements in
absolute value is suffcient to obtain the exact projection.
Moreover, it turns out that this particular perspective provides a principled and tractable approach to encode an interesting class of matroid-structured sparsity models. The recipe
is quite simple: we seek the intersection of a structure provider

III. E XACT AND APPROXIMATE PROJECTIONS ONTO CSM S
The workhorse of the model-CS approach is the following
non-convex projection problem onto CSMs, as deﬁned by Ck ,
which is a basic subset selection problem:
w2Rn

xk2 : supp(w) 2 Ck ,
2

(5)

The proof of Proposition 1 is straightforward and is omitted.
Regardless of whether we use a dynamic program, a greedy
combinatorial algorithm, or an ILP solver, the formulations (4)
or (5) make the underlying tractability of the combinatorial
selection explicit. We highlight this notion via the polynomialtime modular ✏-approximation property (PMAP✏ ):

Here, we also comment on the scaling of (k, m, n) for
the desired level of isometry. When the entries of
can
be modeled as independent and identically distributed (iid)
with respect to a sub-Gaussian distribution, we can show that
m = O k 2 (log(2M ) + k log(12 k 1 )) with overwhelming
probability [7]. Here, M is the minimum number of subspaces
covering Ck . While m explicitly depends on n, for certain
restricted CSMs, such as the rooted connected tree of [7], this
dependence can be quite weak, e.g., m = O(k).

PCk (x) = arg min kw

|[x]i |2 ,

where [z]i , (i = 1, . . . , n), are support indicator variables.

(2)
2!2

wT z : [w]i =

z:[z]i 2{0,1},
supp(z)2Ck

Deﬁnition 2 (RIP [7], [8]). The regression matrix has the kRIP with an isometry constant k when
(1

(4)

S:S2Ck

(3)

where PCk (x) is the projection operator. [7] shows that as long
as PCk (·) is exactly computed in polynomial time for a CSM,

2

When available, using the CSM constraint Ck in addition to
the `1 -norm constraint enhances learning in two important
ways. First, the combinatorial constraints restricts the LASSO
solution to exhibit true model-based supports, increasing the
interpretability of the solution without relaxing Ck into a
convex norm. Second, it empirically requires much fewer
number of samples to obtain the true solution than both the
LASSO and the model-CS approaches.
We provide a pseudo-code of an example implementation
of C LASH in Algorithm 1. One can think of alternative ways
of implementing C LASH, such as single gradient updates in
Step 2, or removing Step 4 altogether. While such changes
may lead to different—possibly better—approximation guarantees for the solution of (6), we observe degradation in the
empirical performance of the algorithm as compared to this
implementation, whose guarantees are as follows:

Algorithm 1: C LASH Algorithm
Input: y, , , PCk , Tolerance ⌘, MaxIterations
Initialize: x0
0, X0
{;}, i
0
repeat
1:
Si
supp(PCk (rXic f (xi ))) [ Xi
2:
vi
arg minv:kvk1  , supp(v)2Si ky
vk2
2
3:
PCk (vi ) with i
supp( i )
i
4:
xi+1
arg minx:kxk1  , supp(x)2 i ky
xk2
2
5:
Xi+1
supp(xi+1 )
i
i + 1.
until kxi xi 1 k2  ⌘kxi k2 or MaxIterations.
matroid (e.g., partition, cographic/graphic, disjoint path, or
matching matroid) with the sparsity provider uniform matroid.
While the intersection of two matroids is not a matroid in
general, we can prove that the intersection of the uniform
matroid with any other matroid satisﬁes the conditions above.
Linear support constraints: Many interesting CSMs Ck
can be encoded using linear support constraints of the form:
[
Ck =
supp (z) , Z := {[z]i 2 {0, 1} : Az  b} ,

Theorem 1 (Iteration invariant). Let x⇤ 2 Rn be the true
vector that satisﬁes the constraints of (6) and let y = x⇤ + "
be the set of observations with additive error " 2 Rm . Then,
the i-th iterate xi of C LASH satisﬁes the following recursion:
kxi+1

8z2Z

3k
where ⇢ , p +

where [A, b] is an integral matrix, and the ﬁrst row of A is all
1’s and [b]1 = k. As a basic example, the neuronal spike model
of [14] is based on linear support constraints where each spike
respects a minimum refractory distance to each other.
A key observation is that if each of the nonempty faces of Z
contains an integral point (i.e., forming an integral polyhedra),
then convex relaxation methods can exactly obtain the correct
integer solutions in polynomial time. In general, checking the
integrality of Z is NP-Hard. However, if Z is integral and
non-empty for all integral b, then a necessary condition is
that A be a totally unimodular (TU) matrix [13]. A matrix is
totally unimodular if the determinant of each square submatrix
is equal to 0,1, or -1. Example TU matrices include interval,
perfect, and network matrices [13]. As expected, the constraint
matrix A of [14] is TU. Moreover, it is easy to verify that
the sparse disjoint group model of [15] also deﬁnes a TU
constraint, where groups have individual sparsity budgets.

1

2k
2
2k

2
1+3 3k
2
1 3k

x⇤ k2 + c1 (
and c1 (

2k , 3k )k"k2

2k , 3k )

deﬁned in [16]. The iterations contract when

is a constant
3k

< 0.3658.

Theorem 1 shows that the isometry requirements of C LASH
are competitive with the mainstream hard thresholding methods, such as CoSaMP [17] and Subspace Pursuit [18], even
though it incorporates the `1 -norm constraints, which, as
Section VI illustrates, improves learning performance.
Remark 1. [Model mismatch and selection] Let us assume
a generative model y =
+ ". Let x⇤ be the best ap˜
proximation of in Ck within `1 -ball of radius . Then, we
can show that the iteration invariant of Theorem 1 still holds
⇤
with SNR = kx k2 , where k"k2  k˜k2 + k (
"
x⇤ )k2 ,
k"k2
where the latter quantity (the impact of mismatch) can be
analyzed using the restricted ampliﬁcation property of [7].
For instance, when Ck is the uniform sparsity model, then
⇣
⌘
⇤
p
k (
x⇤ )k2  1 + k k
x⇤ k2 + k px k1 , which
k
should presumably be small if the model is selected correctly.

B. How about PMAP✏ ?

In the absence of prior information, we automate the parameter selection by using the Donoho-Tanner phase transition
[19] to choose the maximum k allowed for a given (m, n)-pair,
and then by using cross validation to pick [20].

For completeness and due to lack of space, we only
mention PMAP✏ , which extends the breath of the model-CS
approach. For a detailed treatment of PMAP✏ and C LASH,
c.f. [12], which describes multi-knapsack CSMs as a concrete
example. Moreover, for many of the PMAP0 examples above,
we can employ ✏-approximate—randomized—techniques to
reduce computational cost.

V. P ROOF OF T HEOREM 1
We sketch the proof of Theorem 1 a l´ [17] and [21]
a
assuming the general case of PMAP✏ . The details of the proof
can be found in an extended version of the paper [16].

IV. T HE C LASH ALGORITHM

Lemma 2 (Active set expansion - Step 1). The support set
Si , where |Si |  2k, identiﬁes a subspace in C2k such that:
p
c
k(xi x⇤ )Si k2  ( 3k + 2k + ✏(1 + 2k ))kxi x⇤ k2
p
p
+
2(1 + 3k ) + ✏(1 + 2k ) k"k2 (7)

The new C LASH algorithm obtains approximate solutions
to the LASSO problem in (1) with the added twist that the
solution must live within the CSM, as deﬁned by Ck :
xC LASH = arg min f (x) : kxk1  , supp(x) 2 Ck .
b

x⇤ k2  ⇢kxi
r

(6)

3

Lemma 2 states that, at each iteration, Step 1 of C LASH
identiﬁes a 2k support set such that the unrecovered energy
of x⇤ is bounded. For ✏ = 0, C LASH exactly identiﬁes the
support where the projected gradient onto Ck can make most
impact on the loading vector in the support complement of its
current solution, which are subsequently merged together.

(v) Subspace Pursuit (SP) algorithm [18], as integrated with
the model-CS approach. We emphasize here that when ! 1
in (6), C LASH must converge to the model-based SP solution.
The LASSO algorithm ﬁnds a solution to the problem
deﬁned in (1), where we use a Nesterov accelerated projected
gradient algorithm. The BPDN algorithm in turn solves the
following optimization problem:
xBPDN = arg min {kxk1 : k x yk2  } ,
b
(9)

Lemma 3 (Greedy descent with least absolute shrinkage - Step
2). Let Si be a 2k-sparse support set. Then, the least squares
solution vi in step 2 of Algorithm 1 satisﬁes
p
1
1 + 2k
c
kvi x⇤ k2  p
k(xi x⇤ )Si k2 +
k"k2 .
2
1
1
3k
3k

where
represents prior knowledge on the energy of the
additive noise term. To solve (9), we use the spectral projected
gradient method SPGL1 algorithm [23].
In the experiments below, the nonzero coefﬁcients of x⇤ are
generated iid according to the standard normal distribution
with kx⇤ k2 = 1. The BPDN algorithm is given the true
values. While C LASH is given the true value of k for
the experiments below, additional experiments (not shown)
shows that our phase transition heuristics is quite good and
the mismatch is graceful as indicated in Remark 1. All the
algorithms use a high precision stopping tolerance ⌘ = 10 5 .
Experiment 1: Improving simple sparse recovery. In this
experiment, we generate random realizations of the model y =
x⇤ +" for n = 800. Here, is a dense random matrix whose
entries are iid Gaussian with zero mean and variance 1/m.
We consider two distinct generative model settings: (i) with
additive Gaussian white noise with k"k2 = 0.05, m = 240 and
k = 89, and (ii) the noiseless model (k"k2 = 0), m = 250 and
sparsity parameter k = 93. For this experiment, we perform
500 Monte Carlo model realizations.
We sweep
and illustrate the recovery performance of
C LASH (6). Figures 1(a)-(b) illustrate that the combination of
hard thresholding with norm constraints can improve the signal recovery performance signiﬁcantly over convex-only and
hard thresholding-only methods—both in noisy and noiseless
problem settings. For k"k = 0, C LASH perfectly recovers the
signal when is close to the true value. When ⌧ kx⇤ k1 ,
the performance degrades due to the large norm mismatch.
Experiment 2: Improving structured sparse recovery We
consider two signal CSMs: in the ﬁrst model, we assume ksparse signals that admit clustered sparsity with coefﬁcients
in C-contiguous blocks on an undirected, acyclic chain graph
[24]. Without loss of generality, we use C = 5 (Figure 1(c)).
The second model corresponds to a TU system where we
partition the k-sparse signals into uniform blocks and force
sparsity constraints on individual blocks; in this case, we solve
the set optimization problem optimally via linear programming
relaxation (Figure 1(d)). Here, the noise energy level satisﬁes
k"k2 = 0.05, and n = 500, m = 125, and k = 50. In both
cases, we conduct 100 Monte Carlo iterations and perform
sparse estimation for a range of values.
In Figure 1(c), we observe that clustered sparsity structure
provides a distinct advantage in reconstruction compared to
LASSO formulation and the sparse-C LASH algorithm. Furthermore, note that when is large, norm constraints have no effect
and the model-C LASH provides essentially the same results as
the model-CS approach [7]. On the other hand, the sparse-

We borrow the proof of Lemma 3 from [21]. This step
improves the objective function f (x) as much as possible on
the active set in order to arbitrate the active set. The solution
simultaneously satisﬁes the `1 -norm constraint.
Step 3 projects the solution onto Ck , whose action is
characterized by the following lemma. Here, we show the ✏approximate projection explicitly:
Lemma 4 (Combinatorial selection - Step 3). Let vi be a 2ksparse proxy vector with indices in support set Si , Ck be a
CSM and i the projection of vi under Ck . Then:
k

i

vi k2  (1
2

✏)k(vi

x⇤ )Si k2 + ✏kvi k2 .
2
2

Step 4 requires the following Corollary to Lemma 3:
Corollary 1 (De-bias - Step 4). Let i be the support set of
a proxy vector i where | i |  k. Then, the least squares
solution xi+1 in Step 4 satisﬁes
p
1
1+ k
⇤
⇤
kxi+1 x k2  p
k i x k2 +
k"k2 .
2
1
1
2k
2k
Step 4 de-biases the current result on the putative solution
support. Its characterization connects Lemmas 3 and 4:

Lemma 5. Let vi be the least squares solution of the greedy
descent step (step 5) and i be a proxy vector to vi after
applying Combinatorial selection step. Then, k i x⇤ k2 can
be expressed in terms of the distance from vi to x⇤ as follows:
k

i

q

x ⇤ k2

p
✏) + 2 1

p
2
✏ 3k + 2 3k ✏ + ✏ · kvi
p
+ D1 k"k2 + D2 kx⇤ k2 + D3 kx⇤ k2 k"k2 ,


1 + (1

where D1 , D2 , D3 are constants depending on ✏,

x⇤ k2

(8)

2k , 3k .

Finally, the proof of Theorem 1 follows by concatenating
Corollary 1 with Lemmas 2, 3, and 5, and setting ✏ = 0.
VI. E XPERIMENTS
In the following experiments, we compare algorithms from
the following list: (i) the LASSO algorithm [1], (ii) the
Basis Pursuit DeNoising (BPDN) [22], (iii) the sparse-C LASH
algorithm, where Ck is the index set of k-sparse signals, (iv)
the model-C LASH algorithm2 , which explicitly carries Ck , and
2 C LASH

codes are available for MATLAB at http://lions.epﬂ.ch/CLASH.

4

1

1

Lasso
sparse-Clash
2

0.6

x - x∗
ˆ

x - x∗
ˆ

Lasso
sparse-Clash

0.8

2

0.8
SP

0.4
BPDN

SP

0.4
BPDN

0.2

0.2
ε

0
0.6

0.6

2

= 0.05

0.8
(a)

1
λ

1.2

0
0.6

1.4

0.8
(b)

(c)

1
λ

1.2

1.4

(d)

x⇤ k2 . Top row: simple sparsity model under noisy k"k2 = 0.05 (left column) and noiseless k"k2 = 0
(right column) settings. Bottom row: the (k, C)-clustered sparsity model (left column) and the TU model (right column).
Fig. 1. Median values of signal error kˆ
x

C LASH improves signiﬁcantly beyond the LASSO solution
thanks to the `1 -norm constraint.
In Figure 1(d) however, the situation is radically changed:
while the TU constraints enhance the reconstruction of modelCS approach over simple sparse recovery, the improvement
becomes quite large as the `1 -norm constraint kicks in. We
also observe the improvement in sparse-C LASH but it is not
as accentuated as the model-C LASH.

[6] R. Jenatton, J.-Y. Audibert, and F. Bach. Structured variable selection
with sparsity-inducing norms. The Journal of Machine Learning
Research, 2777–2824, 2011.
[7] R.G. Baraniuk, V. Cevher, M.F. Duarte, and C. Hegde. Model-based
compressive sensing. IEEE Trans. on Info. Theory, 2010.
[8] E. J. Cand` s and T. Tao. Near optimal signal recovery from random
e
projections: Universal encoding strategies? IEEE Trans. on Info. Theory,
2006.
[9] J.A. Tropp. Algorithms for simultaneous sparse approximation. part ii:
Convex relaxation. Signal Processing, 86(3):589–602, 2006.
[10] L. Jacob, G. Obozinski, and J.P. Vert. Group lasso with overlap and
graph lasso. In ICML, 2009.
[11] F. Bach. Structured sparsity-inducing norms through submodular functions. In Advances in Neural Information Processing Systems, 2010.
[12] A. Kyrillidis and V. Cevher. Sublinear time, approximate model-based
sparse recovery for all. EPFL Technical report, 2011.
[13] G.L. Nemhauser and L.A. Wolsey. Integer and combinatorial optimization, volume 18. Wiley New York, 1988.
[14] C. Hegde, M.F. Duarte, and V. Cevher. Compressive sensing recovery
of spike trains using a structured sparsity model. SPARS, 2009.
[15] J. Friedman, T. Hastie, and R. Tibshirani. A note on the group lasso
and a sparse group lasso. Arxiv preprint arXiv:1001.0736, 2010.
[16] A. Kyrillidis and V. Cevher. Combinatorial selection and least absolute
shrinkage via the C LASH algorithm. EPFL Technical Report, 2011.
[17] D. Needell and J. Tropp. CoSaMP: Iterative signal recovery from incomplete and inaccurate samples. Applied and Computational Harmonic
Analysis, 26(3):301–312, 2008.
[18] W. Dai and O. Milenkovic. Subspace pursuit for compressive sensing
signal reconstruction. IEEE Trans. on Inf. Theory, 2009.
[19] D. L. Donoho and J. Tanner. Neighborliness of randomly projected
simplices in high dimensions. Proceedings of the National Academy of
Sciences, 102, 2005.
[20] R. Ward. Compressed sensing with cross validation. IEEE Trans. on
Info. Theory, 2009.
[21] S. Foucart. Sparse recovery algorithms: sufﬁcient conditions in terms of
restricted isometry constants. In Proceedings of the 13th International
Conference on Approximation Theory, 2010.
[22] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition
by basis pursuit. SIAM Journal on Scientiﬁc Computing, 20:33–61, 1998.
[23] E. van den Berg and M. P. Friedlander. Probing the pareto frontier for
basis pursuit solutions. SIAM Journal on Scientiﬁc Computing, 2008.
[24] V. Cevher, P. Indyk, C. Hegde, and R.G. Baraniuk. Recovery of clustered
sparse signals from compressive measurements. SAMPTA, 2009.

VII. C ONCLUSIONS
C LASH establishes a regression framework where efﬁcient
algorithms from combinatorial and convex optimization can
interface for interpretable sparse solutions. Our experiments
demonstrate that, while the model-based selection can greatly
improve sparse recovery over the approaches based on uniform
sparsity alone, the `1 -norm shrinkage operations have an
undeniable, positive impact on the learning performance. Understanding the tradeoffs between the complexity of approximation and the recovery guarantees of C LASH is a promising
theoretical and practical direction. Finally, joint combinatorial
and norm constrained scenarios appear in many problems of
interest such as portfolio optimization.
R EFERENCES
[1] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal.
Statist. Soc B, 58(1):267–288, 1996.
[2] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efﬁcient
projections onto the `1 -ball for learning in high dimensions. In ICML,
2008.
[3] P.J. Bickel, Y. Ritov, and A.B. Tsybakov. Simultaneous analysis of lasso
and dantzig selector. The Annals of Statistics, 37(4):1705–1732, 2009.
[4] M.J. Wainwright. Sharp thresholds for high-dimensional and noisy
sparsity recovery using `1 -constrained quadratic programming (Lasso).
IEEE Trans. on Info. Theory, 2009.
[5] P. Zhao and B. Yu. On model selection consistency of lasso. The Journal
of Machine Learning Research, 7:2541–2563, 2006.

5

