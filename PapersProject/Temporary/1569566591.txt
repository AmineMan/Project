Title:          ISIT 518.dvi
Creator:        dvips(k) 5.90a Copyright 2002 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 16:06:20 2012
ModDate:        Tue Jun 19 12:54:20 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      356244 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566591

The Sufﬁciency Principle for Decentralized Data Reduction
Ge Xu and Biao Chen
Department of EECS
Syracuse University, NY, USA
gexu{bichen}@syr.edu

Abstract— This paper develops the sufﬁciency principle suitable for data reduction in decentralized inference systems. Both
parallel and tandem networks are studied and we focus on the
cases where observations at decentralized nodes are conditionally
dependent. For a parallel network, through the introduction of a
hidden variable that induces conditional independence among the
observations, the locally sufﬁcient statistics, deﬁned with respect
to the hidden variable, are shown to be globally sufﬁcient for
the parameter of inference interest. For a tandem network, the
notion of conditional sufﬁciency is introduced and the related
theories and tools are developed. Finally, connections between the
sufﬁciency principle and some distributed source coding problems
are explored.

X
θ

p(x, y|θ)
T (Y)

Y

Fig. 1.

X
θ

Parallel network.

X

T (X)

p(x, y|θ)

γ(·)
Y

The sufﬁciency principle has played a prominent role in
designing data processing methods for statistical inference. A
sufﬁcient statistic is a function of the data that contains all the
information in the data about the parameter of interest. The
primary goal of sufﬁciency-based data reduction is dimensionality reduction to facilitate subsequent inferences based on the
reduced data [1]–[3].
Suppose θ is the parameter of inference interest and X
{X1 , · · · , Xn } is a vector of random variables, whose distribution is given by p(x|θ)1 . If T (X) is a sufﬁcient statistic
for θ, then any inference about θ should depend on X only
through T (X) [2]. A useful tool to identify sufﬁcient statistics
is the Neyman-Fisher factorization theorem [2, Theorem 6.2.6]
which states that a statistic T (X) is sufﬁcient for θ if and only
if there exist functions g(t|θ) and h(x) such that

ˆ
θ

γ(·)
Y

I. I NTRODUCTION

T (X)

X

Fig. 2.

ˆ
θ

Y
Tandem network.

sufﬁcient statistic [5]. As such, whether a statistic at a local
node is globally sufﬁcient is not determined solely by the
statistical characterization of local data but also depends on
the joint distribution of the whole data and how data/statistics
are passed along within the network.
For conditionally independent observations (e.g., X and Y
are independent given θ in Figs. 1 and 2), local sufﬁciency
implies global sufﬁciency. This result was established in [5]–
[7] for parallel networks (Fig. 1) and it is straightforward
to show that the same result holds for tandem networks
(Fig. 2). An interesting manifestation of the above result is
in decentralized detection. It is well known that for a binary
hypothesis testing problem, the likelihood ratio (LR) is a
sufﬁcient statistic for the underlying hypothesis. Therefore, it
is not surprising that likelihood ratio quantizers are globally
optimal for decentralized detection with conditionally independent observations [8], even with non-ideal, possibly coupling
channels between the sensors and the fusion center [9], [10].
Without the conditional independence assumption, decentralized inference becomes considerably more complex. For
the decentralized detection, the optimal solution becomes NP
complete when the observations are conditionally dependent
[11]. The primary focus of this paper is to develop theories
and tools for decentralized data deduction with conditionally
dependent observations for both parallel and tandem networks.
For parallel networks, we investigate the sufﬁciency principle under a hierarchical conditional independence (HCI)
model, which is a new framework recently proposed to deal

p(x|θ) = g(T (x)|θ)h(x).
If the parameter θ is itself random, the sufﬁciency principle
can also be reframed using the data processing inequality [4,
Section 2.9]. That is, a function T (X) is a sufﬁcient statistic
if and only if the following Markov chain holds:
θ − T (X) − X.
For decentralized inference, data reduction is done locally
without access to the global data. Therefore, the contrasting
notions of local sufﬁciency and global sufﬁciency [5] need
to be treated with care. A sufﬁcient statistic that is deﬁned
with respect to local data is referred to as locally sufﬁcient
statistic while a sufﬁcient statistic deﬁned with respect to
the global data in the network is referred to as a globally
1 We do not distinguish between probability density and probability mass
function. Its meaning will become clear in the context of speciﬁc problems.

1

Therefore, T (X, Y) is sufﬁcient for θ.
Lemma 1 is not useful in itself as T (X, Y) is a function of
the global data which is not available in either of the nodes.
Its use is main for establishing the following result.
Theorem 1: Let X, Y ∼ p(x, y|θ) and suppose there exists
a random variable W such that θ − W − (X, Y). Let T (W)
be a sufﬁcient statistic for θ, i.e., θ − T (W) − W.
1) If a pair of statistics (Tx (X), Ty (Y)) are globally sufﬁcient for T (W), they are globally sufﬁcient for θ.
2) If T (W) induces conditional independence between X
and Y, and (Tx (X), Ty (Y)) are locally sufﬁcient for
T (W), then (Tx (X), Ty (Y)) are globally sufﬁcient for
θ.
Proof: To prove 1), from Lemma 1, we only need to show
that the Markov chain θ−T (W)−(X, Y) holds. However, the
Markov chain T (W)−(θ, W)−(X, Y) together with θ−W−
(X, Y) results in the Markov chain (θ, T (W))−W−(X, Y).
Combined with the Markov chain θ − T (W) − W, we get
θ − T (W) − W − (X, Y) which implies θ − T (W) − (X, Y).
For the second one, since conditional independence ensures that locally sufﬁcient statistics are globally sufﬁcient,
(Tx (X), Ty (Y)) are thus sufﬁcient for T (W). The ﬁrst result
then establishes that they are also sufﬁcient for θ.
Remark 1: It is given in [12] that any general distributed
inference model can be represented as a HCI model and vice
versa, where the HCI model is constructed by introducing a
hidden variable W such that the following Markov chains
hold: θ − W − (X, Y) and X − W − Y. Therefore, Theorem
1 indicates that under the HCI model, local sufﬁciency with
respect to the hidden variable implies global sufﬁciency.
From the above result, it is clear that whether Tx (X) is
globally sufﬁcient depends also on Ty (Y) and vice versa. This
coupling effect makes it rather difﬁcult in studying the global
sufﬁciency property. In the following, we consider a somewhat
simpliﬁed situation where one is interested in data reduction
at one node provided that a locally sufﬁcient statistic from the
other node is available at the fusion center. That is, if Ty (Y)
is known to be a locally sufﬁcient statistic, what should node
X transmit such that Tx (X) may form a globally sufﬁcient
statistic together with Ty (Y).
Theorem 2: Let X, Y be distributed according to p(x, y|θ).
Assume Ty (Y) is a locally sufﬁcient statistic for θ, then
(Tx (X), Ty (Y)) are globally sufﬁcient for θ if and only if
there exist functions g(t1 |t2 , θ) and h(x, y) such that, for
all sample points (x, y) and all parameter values θ, the
conditional probability p(x|y, θ) satisﬁes

with distributed detection with conditionally dependent observations [12]. The main idea is to inject a hidden variable W
such that the sensor observations are conditionally independent
with respect to this new variable regardless of the dependence
structure of the original model. Suitable conditions are identiﬁed under this HCI model such that local sufﬁciency implies
global sufﬁciency.
For tandem networks such as that described in Fig. 2, Y is
fully available at the decision node. As such, the novel notion
of conditional sufﬁciency is deﬁned to capture the difference in
network structure with that of the parallel network. A new set
of theories and tools corresponding to conditional sufﬁciency
are then developed.
Finally, the developed notion of sufﬁciency is applied to
some classical distributed source coding problems. There,
sufﬁciency-based data reduction prior to a source encoder is
shown to incur no penalty on the corresponding rate region or
the rate distortion function.
The rest of the paper is organized as follows. Section II
develops the sufﬁciency principle in parallel networks with
conditionally dependent observations. Section III deals with
tandem networks where the notion of conditional sufﬁciency
is introduced and associated theories are developed. In section
IV, the connection between the developed sufﬁciency principle and two distributed source coding problems is explored.
Section V concludes the paper.
II. S UFFICIENCY FOR PARALLEL N ETWORK
This section considers only a parallel network of two
sensors as illustrated in Fig. 1. The result extends naturally to
the case with arbitrary numbers of sensors. Let data available
at node X be X while data available at node Y be Y.
Assume the parameter θ is random. (Tx (X), Ty (Y))
are globally sufﬁcient for θ if the Markov chain θ −
(Tx (X), Ty (Y)) − (X, Y) holds.
Identifying local statistics that are globally sufﬁcient can
be accomplished in theory via the factorization theorem.
The process of using the factorization theorem may become
cumbersome in a decentralized system or not applicable when
the precise joint distribution of the data in the network is not
available at local nodes. The following theorem provides certain relation between local sufﬁciency and global sufﬁciency
for a class of distributed inference problem.
Lemma 1: Let X, Y ∼ p(x, y|θ) and suppose there exists
a random variable W such that
θ − W − (X, Y).

(1)

A statistic T (X, Y) that is sufﬁcient for W is also sufﬁcient
for θ.
Proof: The Markov chain (1) implies that θ − W −
(X, Y, T (X, Y)) forms a Markov chain for any statistics
T (X, Y). That T (X, Y) is sufﬁcient for W implies the
Markov chain W−T (X, Y)−(X, Y). It is straightforward to
show that these two Markov chains give rise to a long Markov
chain

p(x|y, θ) = g(Tx (x)|Ty (y), θ)h(x, y).

(2)

Proof: Directly from the factorization theorem for
(X, Y) and by rewriting p(x, y|θ) = p(y|θ)p(x|y, θ).
Remark 2: Given a locally sufﬁcient statistic Ty (Y), it is
possible that there does not exist a Tx (X) forming a globally
sufﬁcient statistic together with Ty (Y).
Remark 3: The above result is shown under the assumption
that θ is a random variable, similar result can be obtained for

θ − W − T (X, Y) − (X, Y).

2

θ is not random by resorting to factorization theorem instead
of data processing inequality.

sufﬁciency principle to this network is as follows: the inference
performance should remain the same whether the inference
is based on (X, Y) or (T (X), Y). From the data processing
inequality, the sufﬁciency of T (X) can thus be characterized
using the Markov chain θ − (T (X), Y) − (X, Y). Given that
T (X) is a function X, it is straightforward to show that that
the Markov chain θ − (T (X), Y) − (X, Y) is equivalent to
θ − (T (X), Y) − X. This motivates the following deﬁnition
of conditional sufﬁciency.
Deﬁnition 1: A statistic T (X) is a conditional sufﬁcient
statistic for θ, conditioned on Y, if the conditional distribution
of the sample X given the value of T (X) and Y does not
depend on θ.
The deﬁnition allows us to generalize a number of classical
results related to sufﬁcient statistics.
Theorem 3: Let X, Y be distributed according to p(x, y|θ).
Let q(T (x), y|θ) be the joint distribution of T (X) and Y, then
T (X) is a conditional sufﬁcient statistic for θ, conditioned on
p(x,y|θ)
Y, if for every (x, y) pair, the ratio q(T (x),y|θ) is constant as
a function of θ.
Similarly, the Neyman-Fisher factorization theorem can also
be generalized to the conditional case.
Theorem 4: Let X, Y be distributed according to p(x, y|θ).
A statistic T (X) is conditionally sufﬁcient for θ, conditioned
on Y, if and only if there exist functions g(t, y|θ) and h(x, y)
such that,

Example 1: For i = 1, · · · , n, let
Xi

=

Z + Ui

Yi

=

Z + Vi ,

where Z, U1 , · · · , Un , V1 , · · · Vn are mutually independent
Gaussian random variables such that Z ∼ N (θ, ρ), Ui ∼
N (0, 1 − ρ), Vi ∼ N (0, 1 − ρ). Thus, Xi , Yi ∼ N (θ, θ, 1, 1, ρ).
The parameter of inference interest is θ. X and Y are not
conditionally independent given θ.
Let T (W ) = W = Z. Thus, Z depends on θ through its
mean value. Clearly, Z satisﬁes the Markov chains θ − Z −
(X, Y) and X − Z − Y as required by the HCI model. Thus,
from Theorem 1, the locally sufﬁcient statistic pair for Z,
( i Xi , i Yi ), is globally sufﬁcient for θ.
Example 2: Consider the hypotheses test where the observations Xi , i = 1, · · · , k, satisfy the following model
H0 :

Xi = Ni ,

H1 :

X i = hi S + N i ,

where hi ’s are complex Gaussian and independent of each
other and of other variables, S is a QAM signal taking values
in the set sm = rm ejθm with probability πm where θm = m 2π
M
for m = 1, · · · , M , and Ni is the independent observation
noise at the ith sensor with Ni ∼ N (0, σ 2 ). The above model
describes the problem of detecting the presence of a QAM
signal in independent Rayleigh fading using k sensors, e.g.,
as in cooperative spectrum sensing. Each sensor makes a local
decision Ui = γ(Xi ) and sends it to a fusion center which
makes a ﬁnal decision regarding the hypothesis under test.
The observations are not conditionally independent given
H1 . Let W = S which induces conditional independence
among observations under both hypotheses. It is easy to see
that T (W ) = |S| is sufﬁcient for H given S. Thus, the Markov
chain H − |S| − S − (X1 , · · · , Xk ) holds.
On the other hand, given |S|, the observations are conditionally independent of each other under the QAM and
Rayleigh fading assumptions. For any i, |Xi | is a minimal
sufﬁcient statistics for |S|. This can be easily veriﬁed by the
ratio p(xi ||s|) for two sample points xi and xi . Therefore, by
p(xi ||s|)
Theorem 1, {|Xi |} is globally sufﬁcient for H.
The above observation can be used to establish that the
optimal detector at each local sensor is an energy detector for
the model described in Example 2 [13].

p(x, y|θ) = g(T (x), y|θ)h(x, y),
for all sample points (x, y) and all parameter values θ.
The proof can be constructed similarly to that of the
factorization theorem in [2, Theorem 6.2.6]. In fact, this result
can be viewed as a special case of Theorem 2 using the fact
that Y is naturally a locally sufﬁcient statistic for Y.
Remark 4: For tandem networks, the deﬁnition of conditional sufﬁciency is more general than global sufﬁciency. This
is because if there exist a pair of statistics (Tx (X), Ty (Y))
that are globally sufﬁcient for θ, then Tx (X) must be conditionally sufﬁcient for θ, conditioned on Y. Therefore, for the
inference problem under the HCI model, one can also obtain
a conditional sufﬁcient statistic using Theorem 1.
Similar to the deﬁnition of minimal sufﬁcient statistic [2],
we can deﬁne the notion of minimal conditional sufﬁcient
statistic as follows.
Deﬁnition 2: A conditional sufﬁcient statistic T (X) is a
minimal conditional sufﬁcient statistic if it is a function of
any other conditional sufﬁcient statistic U (X).
The following theorem provides a meaningful way to ﬁnd
minimal conditional sufﬁcient statistics.
Theorem 5: Let X, Y be distributed according to p(x, y|θ).
Suppose there exists a function T (x) such that for every two
ˆ
sample points x, x, and y, the ratio f (x,y|θ) is constant as a
f (ˆ ,y|θ)
x
function of θ if and only if T (x) = T (ˆ ). Then T (X) is a
x
minimal conditional sufﬁcient statistic for θ given Y.
The proof follows the same line of proof for Theorem 6.2.13
in [2].

III. S UFFICIENCY FOR TANDEM N ETWORK
A tandem network, as illustrated in Fig. 2, is one such that
compressed data are transmitted to a node which also has
its own observation. The second node will then make a ﬁnal
decision using its own data and the input from the ﬁrst node.
Knowing that Y is available at the fusion center even without
directly observing Y should have an impact on how node X
summarizes its own data X. A natural way of extending the

3

Xn

Yn

Enc 1

Enc 2
Fig. 3.

R1

Dec

ˆ
Xn

which is the rate region for encoding (X n , T n (Y n )) where
T n (Y n ) is the i.i.d sequence T (Yi ), i = 1, · · · , n. The
following theorem shows that encoding reduced data T n (Y n )
achieves the same rate region as encoding the original data.
Theorem 6:

R2

R=R

Source coding with side information

Proof: It is straightforward to show R ⊇ R . To show
R ⊆ R , let (R1 , R2 ) ∈ R, then there exists a U such
that X − Y − U , R1 ≥ H(X|U ), R2 ≥ I(Y ; U ). Since
(X, T (Y )) − Y − U and X − T (Y ) − Y , the Markov chain
X − T (Y ) − Y − U holds. Therefore, R1 ≥ H(X|U ), R2 ≥
I(Y ; U ) ≥ I(T (Y ); U ) by the data processing inequality.
Thus, (R1 , R2 ) ∈ R .
A direct consequence of Theorem 6 is that the corner point
of the rate region (R1 = H(X|Y ), R2 = H(T (Y )) may
be strictly smaller than (R1 = H(X|Y ), R2 = H(Y ). This
observation was ﬁrst reported in [17]. Speciﬁcally, the corner
point can be obtained by ﬁnding the smallest admissible R2
when R1 = H(X|Y ) and it was shown that [17]

Example 3: Let {Xi , Yi }, i = 1, · · · , n be independent and
identically distributed (i.i.d) according to p(x, y|θ), where
2 θ < x < θ + 1, θ < y < x,
0 otherwise.

p(x, y|θ) =

The marginal distribution of X and Y are therefore,
p(x|θ)
p(y|θ)

= 2(x − θ), θ < x < θ + 1,
= 2(θ + 1 − y), θ < y < θ + 1.

It can be easily shown that no data reduction is possible
using the marginal distribution, i.e., no meaningful locally sufﬁcient statistics can be found other than the data themselves.
Note that X is uniformly distributed on the interval (y, θ + 1),
therefore, we have
1
p(x|y, θ) = n
, yi < xi , (max{xi } − 1) < θ.
i
(θ + 1 − yi )
i=1

inf{R2 : (H(X|Y ), R2 ) ∈ R} =
=

inf

X−Y −U,X−U−Y
H(ΦX ).
Y

I(Y ; U ),

As it turns out, the quantity ΦX is precisely the minimal
Y
sufﬁcient statistic of X given Y .

Thus, maxi {Xi } is a conditional sufﬁcient statistic for θ,
conditioned on Y. Similarly, we can obtain that mini {Yi }
is a conditional sufﬁcient statistic of Y, conditioned on
the X sequence. This is consistent with the fact that
(maxi {Xi }, mini {Yi }) is globally sufﬁcient given both X and
Y.

B. Remote source coding with side information
Consider a model in Fig 4, which is the remote source
coding with side information available at both the encoder and
decoder. We will show that in this problem, the rate distortion
function will not change by encoding a conditional sufﬁcient
statistic T (X).
Let (X, Y, Z) ∼ p(x, y, z) and d(z, z ) be a given disˆ
tortion function. Let (X n , Y n , Z n ) be i.i.d sequences drawn
from (X, Y, Z). Upon receiving the sequences (X n , Y n ), the
encoder generates a description of the sources with rate R
and sends it to the decoder who has the side information
Y n and wishes to reproduce Z n with distortion D. The rate
distortion function R(D) is the inﬁmum of rate R such that
there exist maps fn : X n × Y n → {1, · · · , 2nR }, gn :
ˆ
Y n × {1, · · · , 2nR } → Z n such that

IV. S UFFICIENCY AND D ISTRIBUTED S OURCE C ODING
For the point to point remote rate distortion problem, it was
shown that sufﬁcient statistic based data reduction achieves
the same rate distortion function as the original data [14].
This section studies the connection between the sufﬁciency
principle and distributed source coding problems.
A. Source coding with side information
Consider the lossless source coding problem in Fig. 3.
An i.i.d. sequence of source pairs (X n , Y n ) are encoded
separately with rates (R1 , R2 ) and the descriptions are sent
to a decoder where only X n is to be recovered with asymptotically vanishing probability of error. A rate pair (R1 , R2 )
is achievable if there exists a lossless source code with rates
(R1 , R2 ). The rate region R is deﬁned as the closure of the
set of all achievable rate pairs and was shown to be [15], [16],

lim sup Ed(Z n , gn (Y n , fn (X n , Y n ))) ≤ D.
n→∞

It is easy to show that the rate distortion function R(D) is:
R(D) = min min I(X; U |Y ),
p(u|x,y)

f

where the minimum is taken over all p(u|x, y) and functions
R = {(R1 , R2 ) : R1 ≥ H(X|U ), R2 ≥ I(Y ; U ), X − Y − U }. z = f (u, y) such that
ˆ
Assume T (Y ) is a sufﬁcient statistic for X, i.e., X − T (Y ) −
Y . Deﬁne
R = {(R1 , R2 )

ˆ
E1 [d(Z, Z)] =

p(x, y, z)p(u|x, y)d(z, f (u, y)) ≤ D. (3)
x,y,z,u

: R1 ≥ H(X|U ), R2 ≥ I(T (Y ); U ),

Let T (X) be a conditional sufﬁcient statistic for the remote
source Z, conditioned on Y (i.e., Z − (T (X), Y ) − (X, Y )).

X − T (Y ) − U },

4

Xn
Zn

Enc

such that local sufﬁciency implies global sufﬁciency. A cooperative spectrum sensing example is given to illustrate the
usefulness of such an approach. For the tandem network, we
introduced the notion of conditional sufﬁciency and developed
related theories and tools.
The sufﬁciency principle for networked inference has applications beyond that of decentralized inference. In particular,
data reduction using suitable notions of sufﬁciency appears
to incur no penalty on the rate region for various distributed
source coding problem. There are potentially other distributed
source coding problems where sufﬁciency based data reduction
may also prove to be optimal.

R

p(x, y|z)

Dec

ˆ
Zn

Yn
Fig. 4.

Remote source coding with side information.

Deﬁne
R (D) = min min I(T (X); U |Y ),
p(u|t,y)

f

where the minimum is taken over all p(u|t, y) and functions
z = f (u, y) such that
ˆ
ˆ
E2 [d(Z, Z)] =

R EFERENCES
[1] R. A. Fisher, “On the mathematical foundations of theoretical statistics,”
in Philosophical Transactions of the Royal Society of London. Series
A, 1922, vol. 222, pp. 309–368.
[2] G. Casella and R. L. Berger, Statistical Inference, Duxbury, Belmont,
CA, 1990.
[3] E.L. Lehmann and G. Casella, Thoery of Point Estimation, springer,
New York, 2nd edition, 1998.
[4] T.M. Cover and J.A. Thomas, Elements of Information Theory, Wiley,
New York, 2nd edition, 2006.
[5] R. Viswanathan, “A note on distributed estimation and sufﬁciency,”
IEEE Trans.Inf. Theory, vol. 39, no. 5, pp. 1765–1767, Sep. 1993.
[6] E. B. Hall, A. E. Wessel, and G. L. Wise, “Some aspects of fusion
in estimation theory,” IEEE Trans. Inf. Theory, vol. 37, pp. 420–422,
1991.
[7] P. Ishwar, R. Puri, K. Ramchandran, and S. S. Pradhan, “On rateconstrained distributed estimation in unreliable sensor networks,” IEEE
Journal on Seleted Areas in Communications, pp. 765–775, April 2005.
[8] J. N. Tsitsiklis, “Decentralized detection,” in Advances in Statistical
Signal Processing, H. V. Poor and Eds. JAI Press J. B. Thomas, Eds.,
Greenwich, CT, 1993.
[9] B. Chen and P.K. Willett, “On the optimality of likelihood ratio test
for local sensor decisions in the presence of non-ideal channels,” IEEE
Trans.Inf. Theory, vol. 51, pp. 693–699, Feb. 2005.
[10] H. Chen, B. Chen, and P.K. Varshney, “Further results on the optimality of likelihood ratio quantizer for distributed detection in non-ideal
channels,” IEEE Trans.Inf. Theory, vol. 55, pp. 828–832, Feb. 2009.
[11] J.N. Tsitsiklis and M. Athans, “On the complexity of decentralized
decision making and detection problems,” IEEE Trans. on Automatic
Control, vol. 30, pp. 440–446, May 1985.
[12] H. Chen, B. Chen, and P.K. Varshney, “A new framework for distributed
detection with conditionally dependent observations,” IEEE Trans.
Signal Processing, vol. 60, no. 3, pp. 1409–1419, Mar. 2012.
[13] F. Peng, H. Chen, and B. Chen, “On energy detection for cooperative
spectrum sensing,” in Proc. of the 46th Conference on Information
Sciences and Systems, Princeton, NJ, March 2012.
[14] K. Eswaran and M. Gastpar, “Rate loss in the CEO problem,” in Proc.
of the 39th Conference on Information Sciences and Systems, Baltimore,
MD, March 2005.
[15] R. F. Ahlswede and J. K¨ ner, “Source coding with side information and
o
a converse for degraded broadcast channels ,” IEEE Trans. Inf. Theory,
vol. 21, pp. 629–637, May 1975.
[16] A. D. Wyner, “On source coding with side information at the decoder,”
IEEE Trans. Inf. Theory, vol. 21, pp. 294–300, May 1975.
[17] S. Kamath and V. Anantharam, “A new dual to the G´ cs-K¨ rner
a
o
common information deﬁned via the Gray-Wyner system,” in Proc.
Annual Allerton Conference on Communications, Control and Computing, Monticello, IL, Sep. 2010.

p(t, y, z)p(u|t, y)d(z, f (u, y)) ≤ D. (4)
t,y,z,u

R (D) is the rate distortion function when we have
(T n (X n ), Y n ) instead of (X n , Y n ) at the encoder, where
T n (X n ) is the i.i.d sequence T (Xi ), i = 1, · · · , n.
Theorem 7:
R(D) = R (D).
Proof: It is obvious that R(D) ≤ R (D).
We now show R(D) ≥ R (D). For any U that achieves
R(D), since T (X) is a function of X, we have the Markov
chain (T (X), Y ) − (X, Y ) − U , hence
I(X; U |Y ) ≥ I(T (X); U |Y ).
Given that T (X) is a conditional sufﬁcient statistic for Z,
we have the following
D

ˆ
≥ E1 [d(Z, Z)]
=

d(z, f (u, y))
y,z,u

=

⎛

d(z, f (u, y))⎝
y,z,u

=

p(z|x, y)p(x, y, u)
x

⎠
p(x, y, u) (5)

p(z|t, y)
t

d(z, f (u, y))
y,z,u

⎞
x:T (x)=t

p(z|t, y)p(t, y, u)

(6)

t

where (5) comes from the deﬁnition of conditional sufﬁciency
and (6) is true by deﬁning p(t, y, u) = x:T (x)=t p(x, y, u).
This shows that for any p(u|x, y) and f (u, y) satisfying (3)
there exist p(u|t, y) and f (u, y) such that (4) is satisﬁed. Thus,
R(D) ≥ R (D).
V. C ONCLUSION
This paper developed the sufﬁciency principle that guides
local data reduction in networked inference with dependent
observations for two classes of inference networks: parallel
network and tandem network.
For the parallel network, a previously proposed hierarchical
conditional independence model is used to obtain conditions

5

