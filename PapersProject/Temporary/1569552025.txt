Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Mon Apr 23 00:35:25 2012
ModDate:        Tue Jun 19 12:55:31 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      1725912 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569552025

1-bit Hamming Compressed Sensing
Tianyi Zhou

Dacheng Tao

Centre for Quantum Computation & Intelligent Systems
FEIT, University of Technology Sydney, Australia
Email: tianyi.zhou@student.uts.edu.au

Centre for Quantum Computation & Intelligent Systems
FEIT, University of Technology Sydney, Australia
Email: dacheng.tao@uts.edu.au

their signs. However, the signal is restricted to be sparse. Quantization
is a coarse and irreversible description of the original signal, and
to recover it is the same as recovering a box constraint. Thus it
is possible to recover the quantization of a dense signal from a
small number of measurements. In this paper, we mainly consider
the problem of fast quantized recovery of a general signal.
The primary contribution of this paper is developing 1-bit Hamming compressed sensing (HCS) to recover the quantized signal
from its 1-bit measurements with extremely small time cost and
without signal sparsity constraint. In compression, we adopt the 1bit measurements [8] to guarantee consistency (rf. longer version)
and B SE but employ them in a different way. In particular, we
introduce a bijection between each dimension of the signal and
a Bernoulli distribution. The underlying idea of 1-bit HCS is to
estimate the Bernoulli distribution for each dimension from the 1-bit
measurements, and thus each dimension of the signal can be recovered from the corresponding Bernoulli distribution. In recovery, we
propose a k-bit HCS quantizer for the signal domain, whose intervals
are the mappings of the uniform linear quantization boundaries for
the Bernoulli distribution domain. 1-bit HCS searches the nearest
neighbor of the estimated Bernoulli distribution among the boundaries
and recovers the quantization of the corresponding dimension as the
HCS quantizer interval associated with the nearest neighbor. The
main signiﬁcance of 1-bit HCS is as follows: 1) it provides a direct
and simple recovery of quantized signal for digital systems; 2) it
only requires to compute nk Kullback-Leibler (KL) divergences for
obtaining k-bit recovery of an n-dimensional signal, and is therefore
considerably more efﬁcient than CS and 1-bit CS; 3) successful
recovery can be obtained from only O(log n) measurements. Thus 1bit HCS can be applied to general signals without sparse assumption.
We theoretically study a quantized recovery error bound of 1-bit HCS
by investigating the precision of the estimation and its impact on the
KL divergence based nearest neighbor search. An HCS extension
“compressed labeling” [12] signiﬁcantly reduces the complexity of
multi-label learning problem.

Abstract—Compressed sensing (CS) and 1-bit CS cannot directly
recover quantized signals preferred in digital systems and require
time consuming recovery. In this paper, we introduce 1-bit Hamming
compressed sensing (HCS) that directly recovers a k-bit quantized signal
of dimension n from its 1-bit measurements via invoking n times of
Kullback-Leibler divergence based nearest neighbor search. Compared
to CS and 1-bit CS, 1-bit HCS allows the signal to be dense, takes
considerably less (linear and non-iterative) recovery time and requires
substantially less measurements. Moreover, 1-bit HCS can accelerate 1bit CS recover. We study a quantized recovery error bound of 1-bit HCS
for general signals. Extensive numerical simulations verify the appealing
accuracy, robustness, efﬁciency and consistency of 1-bit HCS.

I. I NTRODUCTION
Recent results in compressed sensing (CS) [1][2][3] prove that
a sparse or compressible signal can be exactly recovered from
its linear measurements, rather than uniform samplings, at a rate
signiﬁcantly lower than the Nyquist rate. The measurement matrix
is required to have the restricted isometry property (RIP) [4][5],
or to be incoherent with the bases on which the signal is sparsely
represented, for the purpose of ensuring the exact reconstruction
via an p (0 ≤ p < 2) penalized/constrained minimization of
the measurement error. Random matrix is desirable in CS for its
promising incoherence and RIP.
However, CS [6] encounters several problems when applied to
practical digital systems, where analog-to-digital converters (ADCs)
not only sample, but also quantize each measurement to a ﬁnite
number of bits. One key problem is that CS cannot handle the
quantized measurements (rounding of coarse quantization leads to
large error, while ﬁne quantization requires expensive ADCs). Thus
1-bit CS [7][8] is developed to reconstruct sparse signals from 1bit measurements, which capture the signs of the CS measurements.
The 1-bit measurements signiﬁcantly reduce the costs and strengthen
the robustness of hardware implementation. Although the 1-bit measurements lead to the loss of scale information, 1-bit CS ensures
consistent reconstructions of signals on the unit 2 sphere [9][10].
Similar to RIP in CS, the binary -stable embedding (B SE) [8] of
the 1-bit measurement guarantees the theoretical reconstruction and
robustness.
Another important problem is that digital systems prefer to use
the quantized recovery of the original signal, which they can directly
process, but the recoveries of both CS [11] and 1-bit CS are realvalued. In order to apply them to digital systems, additional quantization is required. Moreover, the time consuming optimization based
and iterative recovery in CS and 1-bit CS limits their applications
in practical systems, especially when signals are of high-dimension.
In addition, we prefer to control the trade-off between speed and
accuracy in signal recovery. Quantized recovery offers a possible
solution, because the number of bits in quantization can be adjusted
to reach different resolutions. The last problem is that CS or 1-bit CS
achieves exact recovery under the Nyquist rate due to replacement of
the previous uniform sampling with random linear measurements or

II. 1- BIT M EASUREMENTS
1-bit HCS recovers the quantized signal directly from its quantized
measurements, each of which is composed of a ﬁnite number of
bits. We consider the extreme case of 1-bit measurements of a signal
x ∈ Rn , which are given by
y = A(x) = sign (Φx) ,

(1)

where sign(·) is an element-wise sign operator and A(·) maps x from
Rn to the Boolean cube BM := {−1, 1}M . Since the scale of the
signal is lost in 1-bit measurements y (multiplying x with a positive
scalar will not change the signs of the measurements), the consistent
reconstruction can be obtained by enforcing the signal x ∈ Σ∗ :=
K
{x ∈ S n−1 : x 0 ≤ K} where S n−1 := {x ∈ Rn : x 2 = 1} is
the n-dimensional unit hyper-sphere.

1

1

KL divergence
0.5

Si

………
0

−0.5

………
−1
0

50

100

150

200

250
i

300

350

400

450

500

ˆ
Fig. 1. (a) Quantized recovery in 1-bit HCS. Bernoulli distribution P (xi ) given in Theorem 1 has estimate P (xi ) (8) from 1-bit measurements y = A(x).
ˆ
1-bit HCS searches the nearest neighbor of P ( xi ) among the k boundaries Pj (j = 0, · · · , k − 1) (4) in the Bernoulli distribution domain. The quantization
of xi , i.e., qi is recovered as the interval between the two boundaries Si−1 and Si corresponding to the nearest neighbor, wherein Si (6) is a mapping of
Pi−1 and Pi in signal domain.(b) HCS quantizer. The boundaries Si in (6) when k = 10, 30, 50, · · · , 510 and xinf = −1, xsup = 1.

arbitrary xi , the nearest neighbor of P (xi ) among the k boundaries
Pj (j = 0, · · · , k−1) indicates the interval qi that xi lies in the signal
domain. The k + 1 boundaries Sj (j = 0, · · · , k) associated with the
k intervals qj (j = 0, · · · , k) are calculated from the k boundaries
Pj (j = 0, · · · , k − 1) according to the bijection deﬁned in Theorem
ˆ
1. In 1-bit HCS recovery, P (xi ) is estimated as P (xi ) from the 1-bit
ˆ
measurements y. Then the nearest neighbor of P (xi ) among the k
boundaries Pj (j = 0, · · · , k−1) is determined by comparing the KLˆ
divergences between P (xi ) and Pj . The quantization of xi deﬁned
by HCS quantizer is recovered as the interval qi corresponding to the
nearest neighbor.
In this section, we ﬁrst introduce the HCS quantizer, which is a
mapping resulting from the uniform linear quantizer of the Bernoulli
distribution domain to the signal domain. The quantized recovery
procedure is composed of n times of KL-divergence based nearest
neighbor searches. Thus it is a linear algorithm much faster than the
conventional reconstruction algorithms of CS and 1-bit CS, which
require optimization with the p (0 ≤ p < 2) constraint/penalty, or
iterative thresholding/greedy search. We then study the upper bound
of the quantized recovery error errH .

A. Bijection
In contrast to CS and 1-bit CS, 1-bit HCS does not recover the
original signal, but reconstructs the quantized signal by recovering
each dimension in isolation. In particular, according to Lemma 3.2
in [13], we show that there exists a bijection (cf. Theorem 1)
between each dimension of the signal x and a Bernoulli distribution,
which can be uniquely estimated from the 1-bit measurements. The
underlying idea of 1-bit HCS is to estimate the Bernoulli distribution
for each dimension, and recover the quantization of the corresponding
dimension as the interval where the Bernoulli distribution’s mapping
lies in.
Theorem 1. (Bijection) For a normalized signal x ∈ Rn with
x 2 = 1 and a normalized Gaussian random vector φ that is drawn
uniformly from the unit 2 sphere in Rn (i.e., each element of φ is
ﬁrstly drawn i.i.d. from the standard Gaussian distribution N (0, 1)
and then φ is normalized as φ/ φ 2 ), given the ith dimension of
the signal xi and the corresponding coordinate unit vector ei =
{0, · · · , 0, 1, 0, · · · , 0}, where 1 appears in the ith dimension, there
exists a bijection P : R → P from xi to the Bernoulli distribution of
the binary random variable si = sign ( x, φ ) · sign ( ei , φ ):
1
Pr (si = −1) = π arccos (xi ) ,
1
Pr (si = 1) = 1 − π arccos (xi ) .

P (xi ) =

A. HCS quantizer
(2)

Since 1-bit HCS aims at recovering the quantization of the original
signal, we ﬁrstly introduce HCS quantizer, which deﬁnes the intervals
and boundaries for quantization in the signal domain. These intervals
and boundaries are uniquely derived from a predeﬁned uniform linear
quantizer in the Bernoulli distribution domain. Given a signal and the
boundaries of HCS quantizer, its k-bit quantization can be identiﬁed.
We will show HCS quantizer performs closely to the uniform linear
quantizer.
Note that in the quantized recovery of 1-bit HCS, the reconstruction
and quantization are simultaneously accomplished. Thus the HCS
quantizer will not play an explicit role in the recovery procedure.
However, it is related to and uniquely determined by the quantization
of the Bernoulli distribution domain, which plays an important role
in the recovery and explains the reconstruction q ∗ . Moreover, it will
be applied to the error bound analyses for errH .
We introduce the HCS quantizer Q(·) by deﬁning a bijective
mapping from the boundaries of the Bernoulli distribution domain to
the intervals of the signal domain according to Theorem 1. Assume
the range of a signal x is given by:

Since the mapping between xi and P (xi ) is bijective, given P (xi ),
the ith dimension of x can be uniquely identiﬁed. According to
the deﬁnition of si , P (xi ) can be estimated from the instances of
the random variable sign ( x, φ ) (sign ( ei , φ ) is also used but
does not depend on x), which are exactly the 1-bit measurements y
deﬁned in (1). Therefore, the 1-bit measurements y include sufﬁcient
information to reconstruct xi from the estimation of P (xi ), and the
recovery accuracy of xi depends on the accuracy of the estimation
to P (xi ).
Given a signal x, its quantization q = Q(x) by HCS quantizer
Q(·), and the quantized recovery q ∗ = R(y) obtained by 1-bit HCS
reconstruction R(·) from the 1-bit measurements y = A(x), errH
is the quantization error determined by the difference between q and
q ∗ . The upper bound of errH will be given in Sections 4.
III.

K - BIT

R ECONSTRUCTION

The primary contribution of this paper is the quantized recovery
in 1-bit HCS, which reconstructs the quantized signal from its 1bit measurements (1). Figure 1(a) illustrates 1-bit HCS quantized
recovery. To deﬁne the HCS quantizer, we ﬁrstly ﬁnd k boundaries
Pj (j = 0, · · · , k − 1) (4) in Bernoulli distribution domain by
imposing the uniform linear quantizer to the range of Pj− . Given an

−1 ≤ xinf ≤ xi ≤ xsup ≤ 1, ∀i, · · · , n.

(3)

By applying the uniform linear quantizer with the quantization
interval ∆ to the Bernoulli distribution domain, we get the corre-

2

The quantization of xi can then be recovered by searching the
ˆ
nearest neighbor of P (xi ) among the k boundary Bernoulli distributions Pj (j = 0, · · · , k −1) in (4). In this paper, the distance between
ˆ
Pj and P (xi ) is measured by the KL-divergence:

sponding boundaries
Pi−
Pi+

Pi =

1
π

= Pr (−1) = arccos (xinf ) − i∆,
,
= Pr (1) = 1 − Pr (−1) .

(4)

i = 0, · · · , k − 1.

1
1
1
arccos (xinf ) − arccos (xsup )
k−1 π
π
1
−
=
P − − Pk .
k−1 0

Pi−
Pi− + ∆

−
Pi +∆

1 − Pi−

−
1−Pi

1 − Pi− − ∆

−
1−Pi −∆

j

∗
∗
Sqi −1 ≤ xi ≤ Sqi .

.

C. Quantized recovery error bound
We investigate the error bound of the quantized recovery (10) by
∗
studying the difference between qi (7) and qi , which are the quantization of xi and its quantized recovery by 1-bit HCS, respectively.
The difference between q and q ∗ deﬁnes the error errH , which is
the error caused by 1-bit HCS reconstruction (10):
(errH )i =

∗
∗
 Sqi − Sqi +1 ≤ (qi − qi − 1) ∆max ,
0,

∗
∗
Sqi − Sqi +1 ≤ (qi − qi − 1) ∆max ,

(7)

∗
qi > q i ;
∗
q i = qi ;
∗
qi < q i .

(12)

The ∆max denotes the largest interval between neighboring boundaries of the HCS quantizer, i.e., ∆max = maxj=1,··· ,k (Sj − Sj−1 ).
∗
In order to investigate the difference between qi and qi , we
study the upper bound for the probability of the event that the true
quantization of xi is qi = 1 + α, while its recovery by 1-bit HCS
∗
is qi = 1 + β(β = α). According to the HCS quantizer (7) and the
1-bit HCS reconstruction (10), this probability is

The k + 1 boundaries of the k-bit HCS quantizer in (6) deﬁne k
intervals in R. Quantized recovery in 1-bit HCS reconstructs a quantized signal by estimating which interval each dimension of the signal
x lies in. The estimation is obtained by a nearest neighbor search in
the Bernoulli distribution domain. To be speciﬁc, an estimation of
P (xi ) given in (2) can be derived from the 1-bit measurements y.
For each P (xi ), we ﬁnd its nearest neighbor among the k boundaries
Pj (j = 0, · · · , k − 1) (4) in the Bernoulli distribution domain.
The interval that xi lies in is then estimated as the interval of
HCS quantizer corresponding to the nearest neighbor. KL-divergence
measures the distance between two Bernoulli distributions in the
nearest neighbor search.
According to Theorem 1, the bijection from xi to a particular
Bernoulli distribution, i.e., P (xi ) given in (2), has an unbiased
estimation from the 1-bit measurements y

−
ˆ
ˆ
= Pr (si = −1)
 P (xi )



= j : [y · sign (Φi )]j = −1 /m,
ˆ
P (xi ) =
(8)
 P (xi )+ = Pr (si = 1)
ˆ
 ˆ


ˆ
= 1 − Pr (si = −1) ,
where Φi is the i

(11)

The 1-bit HCS recovery algorithm is fully summarized in (10),
which only includes simple computations without iteration and thus
can be easily implemented in real systems. According to (10), the
quantized recovery in 1-bit HCS requires nk computations of KLdivergence between two Bernoulli distributions. This indicates the
high efﬁciency of 1-bit HCS (linear recovery time), and the trade-off
between resolution (k) and time cost (nk).

B. KL-divergence based nearest neighbor search

th

(10)

Thus the interval that xi lies in can be recovered as

Although the mapping between the boundaries of HCS quantizer Si to
the boundaries of the quantizer in the Bernoulli distribution domain
Pi is bijective, such mapping cannot be explicitly obtained. So it
is difﬁcult to derive the corresponding quantizer in the Bernoulli
distribution domain from a predeﬁned HCS quantizer. Thus HCS
quantizer cannot be ﬁxed as a uniform linear quantizer and has to be
computed from a predeﬁned quantizer in the Bernoulli distribution
domain. Fortunately, HCS quantizer performs very closely to the
uniform linear quantizer, especially when xi is not very close to
−1 or 1. Figure 1(b) shows the fact.
Given a signal x and the boundaries deﬁned in (6), its k-bit
quantization q is:
Q(x) = q, qi = {j : Sj−1 ≤ xi ≤ Sj } .

,
(9)

∀i = 1, · · · , n, ∀j = 0, · · · , k − 1.

1/∆


ˆ
P (xi )+

∗
ˆ
R(y) = q ∗ , qi = 1 + arg min DKL Pj P (xi ) ,

where
−
Pi

Pj+

The interval that xi lies in among the k intervals deﬁned by the
boundaries Sj (j = 0, · · · , k) in (6) is identiﬁed as the one whose
corresponding boundary distribution Pj is the nearest neighbor of
ˆ
P (xi ). Therefore, the quantized recovery of x, i.e., q ∗ , is given by

(5)

We deﬁne the k-bit HCS quantizer in the signal domain by
computing its k + 1 boundaries as a mapping from the k boundaries
Pi (i = 0, · · · , k − 1) to R in the Bernoulli domain:

i = 0;
 xinf ,


π
Si =
cos
, i = 1, · · · , k − 1;
(6)
−
1+f (Pi )



xsup ,
i = k.



+ Pj+ log

ˆ
P (xi )−
∀i = 1, · · · , n, ∀j = 0, · · · , k − 1.

∆=

f Pi− = 

Pj−

ˆ
DKL Pj P (xi ) = Pj− log

The interval ∆ is

ˆ
Pr β = arg min DKL Pj P (xi ) | Sα ≤ xi ≤ Sα+1 . (13)
j

In order to study the conditional probability in (13), we ﬁrst consider
ˆ
an equivalent event of β = arg min DKL Pj P (xi ) , shown in the
j

following Lemma 1.
Lemma 1. (Equivalence) The event that the nearest neighbor of
ˆ
P (xi ) among Pj (j = 0, · · · , k − 1) is Pβ equals to the event that
ˆ
P (xi ) is closer to Pβ than both Pβ−1 and Pβ+1 , where the distance
ˆ
between Pj and P (xi ) is measured by KL divergence (9), i.e.,
ˆ
β = arg min DKL Pj P (xi ) ⇐⇒
j

ˆ
ˆ
 DKL Pβ−1 P (xi ) − DKL Pβ P (xi ) > 0,
ˆ
ˆ
 DKL Pβ+1 P (xi ) − DKL Pβ P (xi ) > 0.

column of the measurement matrix Φ.

3

(14)

HCS quantized recovery error (n=1024, k=1024)

BIHT quantized recovery error (n=1024, k=1024)

1

0.4

1

0.4

0.5

0.3
K/n

K/n

0.3
0.2

0.5

0.2

0.1
0

0

0.5

1

1.5

2
m/n

2.5

3

3.5

0.1
0

0

4

0

0.5

HCS quantized recovery time (n=1024, k=1024)

1

1.5

2
m/n

2.5

3

3.5

0

4

BIHT quantized recovery time (n=1024, k=1024)

1

1

0.5

0

1
K/n

K/n

1

0.5

0

0.5

1

1.5

2
m/n

2.5

Fig. 2.
n=1024, K=819, k=256, SNR=26.0206

3

3.5

n=1024, K=819, k=256, SNR=26.0206

24

0

0.5

1

1.5

2
m/n

2.5

3

HCS
BIHT

3.5

0

4

3.5

HCS
BIHT

9

n=512, K=256, k=256, SNR=30.4576

30

HCS
BIHT

22

n=512, K=256, k=256, SNR=30.4576

n=512, K=256, k=256, SNR=30.4576
0.45

10

HCS
BIHT
0.4

0

0

4

0.5

Phase plots of 1-bit HCS and “1-bit CS+HCS quantizer” in the noiseless case.

n=1024, K=819, k=256, SNR=26.0206

0.45

0.5

HCS
BIHT

HCS
BIHT

0.4
3

8

20

25

0.35

14

12

Quantized recovery time (seconds)

0.2

16

6

5

4

Quantized recovery SNR(dB)

0.25

18

7
Quantized recovery error

0.3

Quantized recovery time (seconds)

Quantized recovery SNR(dB)

Quantized recovery error

0.35
0.3

0.25

0.2

0.15

20

15

3

2.5

2

1.5

1

0.15
10

10
0.5

0.1

0.05
0

0.1

2

8

0.5

1
m

Fig. 3.

1.5

2
4

x 10

6
0

0.05

1

0.5

1
m

1.5

2

0
0

0.5

1
m

4

x 10

1.5

0
0

2

6000

8000 10000

5
0

2000

4000

6000

8000 10000

0
0

2000

4000

m

6000

8000 10000

m

Quantized recovery error vs. number of measurements of 1-bit HCS and “1-bit CS+HCS quantizer” in the noisy case.

where f is deﬁned as

−
f Pj

Corollary 1. (Upper bounds in two cases) The conditional probability given in (13) can be upper bounded by

1−

−∆

−
1−Pj −∆

1/∆




.

ˆ
Pr β = arg min DKL Pj P (xi ) | Sα ≤ xi ≤ Sα+1 ≤
j



2 


 1
1
1
 exp −2m · 
− π arccos (xi )  ,
 2


f P−
+1

q ∗ +1

i


∗
qi > qi ;
2 




 1
1
1

 ,
 2 exp −2m ·  π arccos (xi ) −


f P − +1


q∗

i

∗
qi < qi .
(19)

| Sα ≤ xi ≤ Sα+1 ≤ Sβ ≤

2 
(16)

ˆ
ˆ
Pr DKL Pβ+1 P (xi ) − DKL Pβ P (xi ) > 0

The minimum amount of 1-bit measurements that ensures the
successful quantized recovery in 1-bit HCS is then directly obtained
from Theorem 2.

| Sβ+1 ≤ Sα ≤ xi ≤ Sα+1 ≤



+∆

−
Pj

−
1−Pj

∗
Pr [R(y)]i = qi | [Q(x)]i = qi =

ˆ
ˆ
Pr DKL Pβ−1 P (xi ) − DKL Pβ P (xi ) > 0

2 

1
1
1
exp −2m · 
− arccos (xi )  ,
−
2
π
f Pβ+1 + 1

−
Pj

−
Pj +∆

−
1 − Pj

Theorem 2. (Quantized recovery bound) Given HCS quantizer
Q(·) in (7) and 1-bit HCS reconstruction R(·) (10), the probability
of the event that the true quantization of xi is qi = 1 + α while its
∗
∗
recovery by 1-bit HCS is qi = 1 + β(qi = qi ) is upper bounded by

Proposition 1. (Two probabilistic bounds) The two conditional
probabilities in (15) are upper bounded by





=


−
Pj

By using Lemma 1, Corollary 1 and Proposition 1, we have the
following Theorem about the upper bound of the probability in (13).

Hence we bound the conditional probability in (13) by exploring
the upper bounds of the two conditional probabilities in Corollary 1.

1
1
1
 ,
exp −2m ·  arccos (xi ) −
−
2
π
f Pβ + 1

−
Pj

(18)

ˆ
Pr β = arg min DKL Pj P (xi ) | Sα ≤ xi ≤ Sα+1 ≤
j

ˆ
ˆ
 Pr
DKL Pβ−1 P (xi ) − DKL Pβ P (xi ) > 0




| Sα ≤ xi ≤ Sα+1 ≤ Sβ ) ,
(15)
ˆ
ˆ

DKL Pβ+1 P (xi ) − DKL Pβ P (xi ) > 0
 Pr



| Sβ+1 ≤ Sα ≤ xi ≤ Sα+1 ) .



4000
m

By using the equivalence in Lemma 1, the conditional probability
given in (13) can be upper bounded by two other conditional
probabilities, whose conditions are the two cases of the condition
in (13).



2000

4

x 10

(17)

Corollary 2. (Amount of measurements) 1-bit HCS successfully
reconstructs xi with probability exceeding 1 − η (0 ≤ η ≤ 1) if the

4

recovery time shows that 1-bit HCS takes substantially less time
than “1-bit CS+HCS quantizer”. Thus 1-bit HCS can signiﬁcantly
improve the efﬁciency of practical digital systems and eliminate the
hardware cost for additional quantization. So the trade-off is quite
advantageous.

number of measurements
1
1
m≥
log
,
2δi
2η

(20)

where
2

δi = min

1
−
f Pqi

1
− arccos (xi )
π
+1

B. Quantized recovery error vs. measurements in the noisy case

,

We show the trade-off between quantized recovery error and the
amount of measurements on 2500 trials for noisy signals of different
n, K, k and signal-to-noise ratio (SNR). Given ﬁxed n, K, k and
SNR, we uniformly choose 50 values of m between 0 and 16n. For
each m value, we conduct 50 trials of 1-bit HCS recovery and “1bit CS+HCS quantizer” by recovering the quantizations of 50 noisy
signals from their m 1-bit measurements. The quantized recovery
error and time cost of each trial are shown in Figure 3.
Figure 3 shows that the quantized recovery error of both 1-bit HCS
and “1-bit CS+HCS quantizer” drops drastically with an increase in
the number of measurements. For dense signals with large noise, the
two methods perform nearly the same on the recovery accuracy. This
indicates that 1-bit HCS works well on dense signals and is robust to
noise compared to CS and 1-bit CS. In addition, the time taken for
1-bit HCS increases substantially slower than that of “1-bit CS+HCS
quantizer” with an increase in the number of measurements.

2

1
1
arccos (xi ) −
−
π
f Pqi +1 + 1

.

(21)

Moreover, 1-bit HCS successfully reconstructs the signal x with
probability exceeding 1 − η if the number of measurements
m≥

1
n
log
.
2 mini δi
2η

(22)

Remark: Corollary 2 states that the quantization of an ndimensional signal x on the unit sphere can be successfully recovered
by 1-bit HCS from m = O(log n) with high probability. Thus 1-bit
HCS provides an economical sampling scheme that does not rely on
sparse or compressible assumption to the signal.
A new issue in 1-bit HCS is the trade-off between the measurement amount m and the recovery resolution k. According to the
deﬁnition of δi in (21), both the upper bound for the probability of
reconstruction failure in (19) and the least number of measurements
∗
ensuring reconstruction success in (20) will be reduced if |qi − qi |
increases. This indicates two facts: 1) the interval xi lies in is easier
to be mistakenly recovered as its nearest intervals; and 2) when we
increase the number of bits k in quantized recovery, xi will become
closer to the boundaries Sq−1 and Sq , which leads to the decreasing
of mini δi in (22). In this case, the number of measurements m has
to be increased in order to ensure a successful recovery.

V. ACKNOWLEDGEMENT
We would like to thank the reviewers for their constructive comments. This work is supported by the Australian Research Council
discovery project (ARC DP-120103730).
R EFERENCES
[1] D. L. Donoho, “Compressed sensing,” IEEE Transactions on Information
Theory, vol. 52, no. 4, pp. 1289–1306, 2006.
[2] E. J. Cand` s and T. Tao, “Near-optimal signal recovery from random
e
projections: Universal encoding strategies?” IEEE Transactions on Information Theory, vol. 52, no. 12, pp. 5406–5425, 2006.
[3] E. Cand` s, M. Rudelson, T. Tao, and R. Vershynin, “Error correction via
e
linear programming,” Foundations of Computer Science, Annual IEEE
Symposium on, pp. 295–308, 2005.
[4] E. J. Cand` s, J. K. Romberg, and T. Tao, “Robust uncertainty principles:
e
exact signal reconstruction from highly incomplete frequency information,” IEEE Transactions on Information Theory, vol. 52, no. 2, pp.
489–509, 2006.
[5] E. J. Cand` s, J. K. Romberg, and T. Tao, “Stable signal recovery from
e
incomplete and inaccurate measurements,” Communications on Pure and
Applied Mathematics, vol. 59, no. 8, pp. 1207–1223, 2006.
[6] A. M. Bruckstein, D. L. Donoho, and M. Elad, “From sparse solutions of
systems of equations to sparse modeling of signals and images,” SIAM
Review, vol. 51, no. 1, pp. 34–81, 2009.
[7] P. T. Boufounos and R. G. Baraniuk, “One-bit compressive sensing,” in
Conference on Information Sciences and Systems (CISS), 2008.
[8] L. Jacques, J. N. Laska, P. T. Boufounos, and R. G. Baraniuk, “Robust 1bit compressive sensing via binary stable embeddings of sparse vectors,”
arXiv:1104.3160, 2011.
[9] P. T. Boufounos, “Greedy sparse signal reconstruction from sign measurements,” in Proc. Asilomar Conference on Signals Systems and
Computers, 2009.
[10] J. N. Laska, Z. Wen, W. Yin, and R. G. Baraniuk, “Trust, but verify:
Fast and accurate signal recovery from 1-bit compressive measurements,”
Rice University CAAM Technical Report TR10-30, 2010.
[11] E. J. Cand` s and T. Tao, “The dantzig selector: statistical estimation
e
when p is much larger than n,” Annals of Statistics, vol. 35, no. 6, pp.
2313–2351, 2007.
[12] T. Zhou, D. Tao, and X. Wu, “Compressed labeling on distilled labelsets
for multi-label learning,” Machine Learning (Springer), 2012, Online
First.
[13] M. X. Goemans and D. P. Williamson, “Improved approximation algorithms for maximum cut and satisﬁability problems using semideﬁnite
programming,” Journal of the ACM, vol. 42, no. 6, pp. 1115–1145, 1995.

IV. E MPIRICAL STUDY
This section evaluates 1-bit HCS and compares it with “BIHT [8]
(for 1-bit CS)+HCS quantizer” on two groups of numerical experi∗
ments. We use average quantized recovery error n |qi − qi | /nk
i=1
to measure errH shown in Section 3.3. In each trial, we draw a
normalized Gaussian random matrix Φ ∈ Rm×n given in Theorem 1
and a signal of length n and cardinality K, whose K nonzero entries
drawn uniformly at random on the unit 2 sphere. Please refer to the
supplementary material for complete experiments.
A. Phase transition in the noiseless case
We ﬁrst study the phase transition properties of 1-bit HCS and
1-bit CS on quantized recovery error and on recovery time in the
noiseless case. We conduct 1-bit HCS and “BIHT+HCS quantizer”
for 105 trials. In particular, given ﬁxed n and k, we uniformly choose
100 different K/n values between 0 and 1, and 100 different m/n
values between 0 and 4. For each {K/n, m/n} pair, we conduct
10 trials, i.e., 1-bit HCS recovery and “1-bit CS+HCS quantizer”
of 10 n-dimensional signals with cardinality K from their m 1-bit
measurements. The average quantized recovery errors and average
time costs of the two methods on overall 104 {K/n, m/n} pairs are
shown in Figure 2.
In Figure 2, the phase plots of quantized recovery error show
the quantized recovery of 1-bit HCS is accurate if the 1-bit measurements are sufﬁcient. Compared to “1-bit CS+HCS quantizer”,
1-bit HCS needs slightly more measurements to reach the same
recovery precision, because 1-bit CS recovers the exact signal, while
1-bit HCS recovers its quantization. This is an unavoidable price for
direct recovery of quantization. However, the phase plots of quantized

5

