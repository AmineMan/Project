Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 14:08:30 2012
ModDate:        Tue Jun 19 12:56:31 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      333498 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569560503

On Decoding Error Exponent of Gaussian Channel
with Noisy Feedback:
Nonexponential Number of Messages
Marat V. Burnashev

Hirosuke Yamamoto

Inst. of Information Transmission Problems
Russian Academy of Sciences
Moscow, Russia
Email: burn@iitp.ru

School of Frontier Sciences
The University of Tokyo
Chiba, Japan
Email: hirosuke@ieee.org

Abstract—For information transmission a discrete time channel with independent additive Gaussian noise is used. There
is also feedback channel with independent additive Gaussian
noise, and the transmitter observes without delay all outputs
of the forward channel via that feedback channel. Transmission
of nonexponential number of messages is considered and the
achievable decoding error exponent for such a combination of
channels is investigated. It is shown that for any ﬁnite noise in
the feedback channel the achievable error exponent is better than
similar error exponent of the no-feedback channel. Method of
transmission/decoding used in the paper strengthens the earlier
method used by authors for BSC. In particular, for small
feedback noise, it allows to get the gain up to 23.6% (instead
of 14.3% earlier for BSC).

- Transm. x- AWGN(1)
6
z

(1)

where ξ = (ξ1 , . . . , ξn ) are independent N (0, 1)–Gaussian
2
random variables, i.e. Eξi = 0, Eξi = 1. It is also assumed
that there is a noisy feedback channel, and the transmitter
observes (without delay) all outputs of the forward channel
via this noisy feedback channel
zi = yi + σηi ,

i = 1, . . . , n,

(2)

where η = (η1 , . . . , ηn ) are independent (and independent of
2
ξ) N (0, 1)–Gaussian random variables, i.e. Eηi = 0, Eηi =
1, and the value σ = 0 is given. No coding is used in
the feedback channel (i.e. the receiver simply re-transmits
all received outputs to the transmitter). In other words, the
feedback channel is “passive” (see Fig. 1).
We assume that the input x satisﬁes the following constraint
n

x2 ≤ nA,
i

Channel model

where A is a given constant. For short, we denote by
AWGN(A) the channel (1) with constraint (3) without feedback, and by AWGN(A, σ) that channel with noisy feedback
(2).
Since Shannon’s paper [1] it has been known that even
noiseless feedback does not increase the capacity of the
Gaussian (or any other memoryless) channel. However, feedback can improve the decoding error probability (or simplify
effective transmission methods). In the case of noiseless feedback possibility of such improvement of the decoding error
probability with respect to no-feedback channel was shown
for a number of channels in [3]–[10].
In case of noisy feedback possibility of such improvement
with respect to no-feedback channel was ﬁrst shown in [11],
[12] (for binary symmetric channel). The purpose of this paper
is to get similar (in fact, much stronger) results for Gaussian
channel.
We consider the case when the overall transmission time
n and M = eo(n) equiprobable messages {θ1 , . . . , θM } are
ˆ
given. After the moment n, the receiver makes a decision θ on
the message transmitted. We are interested in the best possible
decoding error exponent (and whether it can exceed similar
exponent of the channel without feedback).
Such case of nonexponential (on n) number M (i.e. R = 0)
was investigated in [11], [12] (for BSC channel). Later in [13],
[14] those results were generalized to positive rates R.
Some related results or topics for channels with noiseless
feedback can be found in [2–12], and for the case of noisy
feedback – in [15], [16] (see also discussion in [12]).

We consider the discrete time channel with independent
additive Gaussian noise, i.e. if x = (x1 , . . . , xn ) is the input
codeword then the received block y = (y1 , . . . , yn ) is
i = 1, . . . , n,

-

AWGN(σ) 

Fig. 1.

I. I NTRODUCTION AND MAIN RESULTS

yi = xi + ξi ,

y Receiver

(3)

i=1

The research described in this publication was made possible in part by the
Russian Fund for Fundamental Research (project number 12-01-00905a).

1

In the paper we consider the case when (ln Mn )/n → 0 as
n → ∞ (it corresponds to zero-rate of transmission).
It is known [2] (it corresponds to a simplex code)

For comparison with the paper results we remind results obtained earlier in [11]–[14]. There the direct and feedback channels were BSC(p) and BSC(p1 ), respectively. It was shown in
[11]–[14] that there exists a critical value pcrit (p, R) > 0 such
that if p1 < pcrit (p, R) then it is possible to improve decoding
error exponent of the no-feedback channel. If, in particular,
both R and p1 are small then gain is 14.3%. In order to get
such improvement the transmission/decoding method with one
“switching” moment was developed and investigated.
The method of [11], [12] was applied to Gaussian channel
AWGN(A, σ) in [18] with similar to [11], [12] results (in
particular, with the same asymptotic gain of 14.3%).
Remark 1: The transmission method used in [11]–[14] reduces the problem to testing of two most probable (at some
ﬁxed moment) messages. It was mentioned in [12, Remark 1]
and [14, Remark 3] that such method is not optimal even for
one switching moment.
In the paper, still using one “switching” moment, we
essentially improve the transmission/decoding method from
[11]–[14]. We show that for any feedback noise intensity
σ 2 < ∞ it is possible to improve the best error exponent
of the AWGN(A) channel without feedback. The transmission/decoding method with one “switching” moment, giving
such improvement, is described in Section II. It strengthens the
method introduced by authors earlier in [11]–[14]. Of course,
if σ is not small then the improvement is small (but strictly
positive).
Remark 2: We consider the case when feedback noise intensity σ > 0 is ﬁxed and does not depend on the number
of messages M . The case when σ is such that σ 2 M is small
represents, in a sense, the noiseless feedback case (cf. [18]).
For x, y ∈ Rn denote (x, y) =

n

xi yi , x

2

E(M, A) =

A
.
(5)
2
In other words, for large (but nonexponential) M noiseless
feedback gives 100% of error exponent improvement over
similar error exponent of the no-feedback Gaussian channel.
For AWGN(A, σ) channel denote by F1 (M, A, σ) the error
exponent for the transmission method with one switching
moment, described in Section II. Clearly, F1 (M, A, σ) ≤
F (M, A, σ) for all M, A, σ.
In order to avoid bulky general formulations we present
results only in two extreme cases: small σ and large σ. Our
main results are as follows.
Theorem. Let ln M = o(n), n → ∞. Then:
a) If σ → 0 then the formula holds
F (M, A, 0) =

F1 (M, A, σ) ≥

1
AM
√
1+
4(M − 1)
2+ 5
1
+ o(1) .
−
2M

(6)

√
Since 1/(2 + 5) ≈ 0.236, then for large M formula (6)
gives 23.6% of improvement with respect to the no-feedback
channel.
b) For any σ < ∞ the following lower bound holds

= (x, x),

1
AM
1+
4(M − 1)
56(1 + σ 2 )
ln M
−O
n
> E(M, A).

d (x, y) = x − y 2 . A subset C = {x1 , x2 . . . , xM } with
xi 2 = An, i = 1, 2, . . . , M is called a (M, A, n)–code of
length n.
For a code C = {xi } denote by Pe (C) the minimal possible
decoding error probability

F1 (M, A, σ) ≥

Pe (C) = min max P (e|xi ),

(7)

In other words, for any non-exponential Mn and any σ < ∞
we get an improvement of the no-feedback error exponent.
Remark 3: Using the transmission strategy with one ﬁxed
switching moment, it is possible to get better gain than in
(6) (elaborating more intensively dependence among random
variables involved). But computations become too much bulky.
Much more interesting would be to investigate transmission
strategies with several (or increasing) number of switching
moments.

i

where P (e|xi ) – conditional decoding error probability provided xi was transmitted, and minimum is taken over all
decoding methods.
If M = Mn is the number of messages, then for AWGN(A)
channel denote by Pe (M, A, n) the minimal possible decoding
error probability for the best (M, A, n)–code. Introduce the
exponent (in n) of that function
n→∞

(4)

It is also known that if σ = 0 (i.e. noiseless feedback) then
[7] for all ﬁxed M

i=1

E(M, A) = lim sup

AM
.
4(M − 1)

1
1
ln
.
n
Pe (M, A, n)

II. T RANSMISSION / DECODING METHOD
We use the transmission strategy with one ﬁxed switching
moment at which the coding function will be changed. The
transmission method used earlier in [11]–[14], (and in [18])
reduces the problem to testing of two most probable (at some
ﬁxed moment) messages. We improve that strategy in both
transmission and decoding stages.

Similarly, for AWGN(A, σ) channel let Pe (M, A, σ, n) be
the minimal possible decoding error probability for the best
transmission method. Introduce the function
1
1
F (M, A, σ) = lim sup
ln
.
Pe (M, A, σ, n)
n→∞ n

2

For simplicity, we consider the case M ≤ (n + 2)/2. We
partition the total transmission time [1, n] on two phases:
[1, M − 1] (phase I) and [M, 2M − 2] (phase II). Thus the
total length of the code used is 2M − 2. The remaining time
[2M − 1, n] is not used. After moment 2M − 2 the receiver
makes a decision in favor of the most probable message θi
(based on all received on [1, 2M − 2] signals).
Each of M codewords {xi } of length 2M − 2 have the
form xi = (xi , xi ), where xi has length M − 1 (to be used
on phase I) and xi also has length M − 1 (to be used on
phase II). Similarly, the received block y can be represented
as y = (y , y ), where y is the block received on phase I
and y is the block received on phase II. Denote by z the
received (by the transmitter) block on phase I. The codewords
ﬁrst parts {xi } are ﬁxed, while the second parts {xi } will
depend on the block z received by the transmitter on phase
I.
We set two positive constants A1 , A2 such that
A1 + A2 = nA,
and denote
β=

A2
,
A1

A3 =

then on phase II the transmitter uses the same simplex
code of M codewords {xi } of length M − 1.
Case 2: If after phase I
d(3)t − d(2)t = τ A3 > τ0 A3 ,

then on phase II the transmitter uses another code {xk }:
a) two most probable messages θi , θj have opposite
codewords xi = −xj of length M − 1 which have nonzero coordinates only at moment M − 2;
b) remaining M − 2 messages {θk } use a simplex code
of M −2 codewords {xk } of length M −3 trailed by two
0’s at moments M −2, M −1. All those M −2 codewords
{xk } are orthogonal to the ﬁrst two codewords (xi , xj ).
This transmission method generalizes the method used in
[14]. The code used in case I resolves the case when after
(1)t
(2)t
(3)t
phase I three most probable codewords x
,x
,x
are
approximately equiprobable.
Remark 4: The assumption M ≤ (n + 2)/2 was used only
to simplify description of the codes (simplex) used on phases I
and II. In fact, all results remain valid if for a code C on phase
I we have mini=j xi − xj 2 ≥ 2A1 (1 + o(1)) as n → ∞
(and similarly for phase II). That condition can be fulﬁlled if
M = eo(n) .

(8)

M A1
.
M −1

(9)

Decoding. Due to noise in the feedback channel the re(3)t
(2)t
(1)t
,x
,x
ceiver does not know exactly codewords x
and the value τ , and therefore it does not know the code
used on phase II. But based on the received block y , the
receiver may evaluate the conditional probabilities of code(3)t
(2)t
(1)t
and of the value τ , and so get the
,x
,x
words x
probabilities with which each code was used.
As a result, based on whole received block y, the receiver
uses maximum likelihood decoding, evaluating posterior probabilities {p(y|xi )} and making decision in favor of most
probable message θi . Such full decoding is described below.

Then A = (1 + β)A1 /n.
Arrange the distances {d(xi , y ) = y − xi 2 , i =
1, . . . , M } for the receiver after phase I in the increasing order,
denoting
d(1) = min d(xi , y ) ≤ d(2) ≤ . . . ≤ d(M ) = max d(xi , y )
i

i

(1)

(2)

(case of tie has zero probability). Let also x , x , . . .,
(M )
be the corresponding ranking of codewords {x } after
x
(1)
is the closest to y codeword,
phase I for the receiver, i.e x
etc.
Similarly, arrange the distances {d(xi , z ) = z − xi 2 ,
i = 1, . . . , M } for the transmitter after phase I in the
increasing order, denoting

III. D ECODING ERROR PROBABILITY Pe . S KETCH OF
T HEOREM PROOF
Note that
1
p (y|x2 )
= (x2 − x1 , y) −
ln
p (y|x1 )
2

d(1)t = min d(xi , z ) ≤ d(2)t ≤ . . . ≤ d(M )t = max d(xi , z ).
i

i

(1)t

(2)t

(M )t

Let also x
,x
,...,x
be the corresponding ranking of codewords {x } after phase I for the transmitter, i.e
(1)t
x
is the closest to z codeword, etc.

x2

2

− x1

2

1
x2 − x1 2 . (12)
2
If xtrue is the true codeword then y = xtrue + ξ and
ξ = (ξ , ξ ) = (ξ1 , . . . , ξn ), where all {ξi } are independent
N (0, 1)–Gaussian random variables. If xtrue = x1 , then
= (x2 − x1 , y − x1 ) −

Transmission. On phase I the transmitter uses a simplex
code of M codewords {xi } of length M −1 such that xi 2 =
A1 .
For phase II we set a number τ0 > 0. Based on the
received block z the transmitter selects three most probable
(1)t
(2)t
(3)t
codewords x
,x
,x
and calculates for them the
value d(3)t − d(2)t = τ A3 ≥ 0. Depending on the value τ
the transmitter uses the following code {xk } with xk 2 =
A2 , k = 1, . . . , M :
Case 1: If after phase I
d(3)t − d(2)t = τ A3 ≤ τ0 A3 ,

(11)

ln

1
p (y|x2 )
= (x2 − x1 , ξ) − x2 − x1 2 ,
p (y|x1 )
2

where (x, ξ) is N (0, x 2 )–Gaussian random variable.
The receiver makes decision after moment n using whole
received block y. If after phase I the difference d(3) − d(2) is
rather close to τ0 A3 (see (10) and (11)) then due to noise in
the feedback link the receiver cannot be sure which code was
(1)
(2)
used by the transmitter on phase II (since lists {x , x }
(1)t
(2)t
and {x
,x
} may turn out to be different). But based on

(10)

3

where x1 , x2 depend on k (via Zk ). If θtrue = θ1 , then y =
x1 + ξ , and

y the receiver knows the probability distribution of the code
used by the transmitter on phase II. Then in the decoding it
should take into account that distribution.
Note that if θtrue = θ1 then for di = y − xi 2 we have
di − d1 = 2A3 + 2(x1 − xi , ξ ),

4

i = 2, . . . , M.

Y ≤ ln 4 − A2 + max{(x1 , x2 ) + (ξ , x2 − x1 ) + ln pk }.
k

If θtrue = θ1 , then for decoding error probability Pe we
have
Pe = P max ln
i≥2

p y y , θ2
= e−A2
pk e(x1 ,x2 )+(ξ ,x2 −x1 ) ,
p y y , θ1
k=1

eY =

Therefore
Pe ≤M P {X + Y ≥ 0 | θ1 }

p y θi
≥ 0 θ1
p y θ1

=M Ey P {X + Y ≥ 0 | y , θ1 }
4

p y θ2
≤ M P ln
≥ 0 θ1
p y θ1

Ey P {X + ln 4 − A2 + (x1 , x2 )

M
k=1

= M P {X + Y ≥ 0 | θ1 } ,

+(ξ , x2 − x1 ) + ln pk ≥ 0 y , Zk , θ1

(13)
4

e−nBk ,

≤M

where

(17)

k=1

p y θ2
p y θ1
= (y , x2 − x1 )
= −A3 + (x2 − x1 , ξ ),

X = ln

where
1
ln Eξ e−bk , k = 1, . . . , 4,
n
1
A2 + A3 − (x2 − x1 , ξ )
bk =
2 x2 − x 1 2

Bk = −
(14)

p y y , θ2
Y = ln
.
p y y , θ1

(15)

2

−(x1 , x2 ) − ln pk − ln 4]+ ,

Main difﬁculty represents investigation of random variable Y .
For that purpose denote
(t)

and introduce the following sets of random events:
(t)

Z1 = z : d23 ≤ τ0 A3 ,
Z2 = z :
Z3 = z :

(t)
d23

(1)t

> τ0 A3 , {x1 , x2 } = {x
> τ0 A3 , {x1 , x2 }

(t)

Z4 = z : d23 > τ0 A3 , {x1 , x2 }

{x
{x

,x

(1)t

(1)t

(2)t

,x

,x

min{B1 , B3 , B4 } ≥
} ,

(2)t

(2)t

A
1
1 + min β,
4
3 + 4β
+ O(σ) + o(1) ,

} =1 ,
B2 ≥

}=∅ ,

β
A
+ O(σ) + o(1)
1+
4
1+β

and therefore

where | · | represents the cardinality of a set. Sets Z2 , Z3 , Z4
describe all possible relations between pairs {x1 , x2 } and
(1)t
(2)t
{x
,x
} of most probable messages for the receiver
and the transmitter, respectively. Note that only the cardinality
(1)t
(2)t
of the intersection {x1 , x2 } {x
,x
} matters in the
evaluation of Y .
Denote
pk = P(Zk y , x1 ),

(19)

and where x1 , x2 depend on k.
Further proof of Theorem consists of evaluating probabilities {pk } and values {Bk }. It is rather computationally
involved and general formulas are quite bulky. For that reason
we present resulting formulas for {Bk } only when σ → 0 and
M → ∞. In that case we set τ0 → 0 and get (as n → ∞)

d23 = d(3)t − d(2)t = τ A3 ,

(t)
d23

(18)

min Bk ≥

k=1,...,4

A
β
1
1 + min
,
4
1 + β 3 + 4β
+ O(σ) + o(1) .

We set β such that both terms under minimization become
equal, i.e. set
√
β = ( 5 − 1)/4 ≈ 0.3090.

k = 1, . . . , 4.

Then we get

We have
p y y , θ2
p y z , x2
= Ez |y
p y y , θ1
p y z , x1

min Bk ≥

k=1,...,4

from which formula (6) (as M → ∞) follows.
A developed version of this paper with detailed proofs will
appear as [19].

4

pk e(y ,x2 −x1 ) ,

=

A
1
√ + O(σ) + o(1) ,
1+
4
2+ 5

(16)

k=1

4

ACKNOWLEDGMENT

[10] H. Yamamoto and K. Itoh, “Asymptotic performance of a modiﬁed
Schalkwijk–Barron scheme for channels with noiseless feedback,” IEEE
Trans. Inf. Theory, vol. 25, no. 6, pp. 729–733, 1979.
[11] M. V. Burnashev and H. Yamamoto, “On BSC, noisy feedback and
three messages,” in Proc. IEEE International Symposium on Information
Theory, Toronto, Canada, June 2008, pp. 164–168.
[12] M. V. Burnashev and H. Yamamoto, “On zero-rate error exponent
for BSC with noisy feedback,” Problems of Information Transmission,
vol. 44, no. 3, pp. 33–49, 2008.
[13] M. V. Burnashev, H. Yamamoto, “Noisy Feedback Improves the BSC
Reliability Function,” in Proc. IEEE International Symposium on Information Theory, Seoul, Korea. June–July 2009, pp. 886–889.
[14] M. V. Burnashev and H. Yamamoto, “On reliability function of BSC
with noisy feedback,” Problems of Information Transmission, vol. 46,
no. 2, pp. 2–23, 2010.
[15] S. C. Draper and A. Sahai, “Noisy feedback improves communication
reliability,” in Proc. IEEE International Symposium on Information
Theory, Seattle, WA, July 2006, pp. 69–73.
[16] Y.-H. Kim, A. Lapidoth and T. Weissman, “ The Gaussian channel with
noisy feedback,” in Proc. IEEE International Symposium on Information
Theory, Nice, France, June 2007, pp. 1416–1420.
[17] A. Tchamkerten and E. Telatar, “Variable length coding over an unknown
channel,” IEEE Trans. Inf. Theory, vol. 52, no. 5, pp. 2126–2145, May
2006.
[18] Yu Xiang, Young-Han Kim, “On the AWGN channel with noisy
feedback and peak energy constraint,” in Proc. IEEE International
Symposium on Information Theory, Austin, Texas, June 2010, pp. 256259.
[19] M. V. Burnashev and H. Yamamoto, “On reliability function of Gaussian
channel with noisy feedback: zero rate,” Problems of Information
Transmission, vol. 48, 2012.

The authors wish to thank the University of Tokyo for
supporting this joint research.
R EFERENCES
[1] C. E. Shannon, “The zero error capacity of a noisy channel,” IRE
Trans. Inf. Theory, vol. IT-2, no. 3, pp. 8–19, Sept. 1956.
[2] C. E. Shannon, “Probability of Error for Optimal Codes in Gaussian
Channel,” Bell System Techn. J., vol. 38, no. 3, pp. 611–656, 1959.
[3] R. L. Dobrushin, “Asymptotic bounds on error probability for message
transmission in a memoryless channel with feedback,” Probl. Kibern.,
no. 8, pp. 161–168, Fizmatgiz, Moscow, 1962.
[4] M. Horstein, “Sequential decoding using noiseless feedback,” IEEE
Trans. Inf. Theory, vol. 9, no. 3, pp.136–143, 1963.
[5] E. R. Berlekamp, Block coding with noiseless feedback, Ph. D. Thesis,
MIT, Dept. Electrical Enginering, 1964.
[6] J. P. M. Schalkwijk and T Kailath, “A coding scheme for additive
noise channels with feedback,” IEEE Trans. Inf. Theory, vol. 12, no. 3,
pp. 172–182, 183–189, April 1966.
[7] M. S. Pinsker, “The probability of error in block transmission in a
memoryless Gaussian channel with feedback,” Problems of Information
Transmission, vol. 4, no. 4, pp. 3–19, 1968.
[8] M. V. Burnashev, “Data transmission over a discrete channel with
feedback: Random transmission time,” Problems of Information Transmission, vol. 12, no. 4, pp. 10–30, 1976.
[9] M. V. Burnashev, “On a Reliability Function of Binary Symmetric
Channel with Feedback,” Problems of Information Transmission, vol. 24,
no. 1, pp. 3-10, 1988.

5

