Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May 16 09:58:52 2012
ModDate:        Tue Jun 19 12:54:43 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      339253 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565435

On Establishing the Shannon Ordering for Discrete
Memoryless Channels
Yuan Zhang and Cihan Tepedelenlioglu
School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ 85287, USA
Email: {yzhang93, cihan}@asu.edu

computational aspects of channel inclusion, by formulating
a convex optimization problem for determining if one DMC
includes another, and proving that if inclusion is established,
sparsity can be exploited to dramatically reduce the complexity
of ﬁnding the optimal code for the better DMC based on the
code for the worse one.
The rest of this paper is organized as follows. Section
II describes the notations and literature background involved
in this paper. Section III derives checkable conditions for
channel inclusion between DMCs with special structure, and
also between BEC and BSC. Computational issues regarding
channel inclusion are addressed in Section IV, and Section V
concludes the paper.
II. N OTATIONS AND P RELIMINARIES

Abstract— This paper studies several problems concerning
channel inclusion, which is a partial ordering between discrete
memoryless channels (DMCs) proposed by Shannon. Speciﬁcally,
checkable conditions are derived for channel inclusion between
DMCs with certain special structure, and these conditions are
related to the mathematical theory of majorization. The conditions for channel inclusion between binary erasure channel (BEC)
and binary symmetric channel (BSC) are also derived, which are
shown to be equivalent to those for channel output degradation.
Furthermore, the determination of channel inclusion is considered as a convex optimization problem, and the sparsity of the
weights related to the representation of the worse DMC in terms
of the better one is revealed when channel inclusion holds between
two DMCs.

I. I NTRODUCTION
The comparison between different communication channels has been a long-standing problem since the establishment
of Shannon theory. Speciﬁcally, such comparison is usually
established through partial ordering between two channels.
Channel inclusion [1] is a partial ordering deﬁned for DMCs,
when one DMC is obtained through randomization at both the
input and the output of another, and the latter is said to include
the former. Such an ordering between two DMCs implies that
for any code over the worse (included) DMC, there exists a
code of the same rate over the better one with a lower error
rate. Channel inclusion can be viewed as a generalization of
the comparisons of statistical experiments established in [2],
[3], in the sense that the latter involves output randomization
(degradation, with a condition given in [4, Proposition 4.1]) but
not input randomization. There are also other kinds of channel
ordering, e.g. more capable ordering and less noisy ordering
[5], which enable the characterization of capacity regions of
broadcast channels.
Given these kinds of ordering of DMCs, it is of interest to
know how it can be determined if two DMCs have a speciﬁc
ordering relation analytically and numerically. To the best of
our knowledge, regarding the conditions for channel inclusion,
the only results beyond Shannon’s paper [1] are provided in
[6], and there is not yet any discussion on the numerical
characterization of channel inclusion in existing literature.
In this paper, we derive checkable conditions for channel
inclusion between DMCs with certain special structure, as well
as between BEC and BSC, which complement the results in
[6] in useful ways, and relate channel inclusion to the wellestablished majorization theory. In addition, we delineate the

Throughout this paper, we use lower case letters such as
α to denote scalars, upper case letters such as K to denote
matrices, and bold letters such as a to denote row vectors,
unless otherwise speciﬁed. The entry of K with index (i, j)
and the entry of a with index i are denoted by [K](i,j) and a(i)
respectively. The i-th row and j-th column of K are denoted
by [K](i,·) and [K](·,j) respectively. Also for convenience, we
use the same letter to denote a DMC and its stochastic matrix,
and apply the terminologies “square”, “doubly stochastic” and
“circulant” for matrices directly to DMCs. We next provide a
sketch of the literature background related to this paper. We
have the following deﬁnitions of channel inclusion drawn from
[1], and also degradation based on [2], [3] which is a special
case of channel inclusion.
Deﬁnition 1: A DMC described by matrix K1 is said to
include another DMC K2 , denoted by K1 ⊇ K2 or K2 ⊆ K1 ,
if there exists a probability vector g ∈ Rβ and β pairs of
+
stochastic matrices {Rα , Tα }β such that
α=1
β

g(α) Rα K1 Tα = K2 .

(1)

α=1

Intuitively, K2 can be thought of as an input/output processed
version of K1 , with g(α) being the probability that K1 is
processed by Rα (input) and Tα (output). K1 and K2 are
said to be equivalent if K1 ⊇ K2 and K2 ⊇ K1 .
Deﬁnition 2: A DMC K2 is said to be a degraded version
of another DMC K1 , if there exists a stochastic matrix T such
that K1 T = K2 .
There are several analytical conditions for channel inclusion derived in [6]. Speciﬁcally, in [6] a DMC is treated

This work was supported in part by the National Science Foundation under
Grant CCF 1117041.

1

as an equivalence class of stochastic matrices, with any two
matrices in the class equivalent in the sense of Deﬁnition 1.
With this notion, the author considers two kinds of DMCs: an
equivalence class containing a 2×2 full-rank stochastic matrix
P , and an equivalence class containing an n × n stochastic
matrix with identical diagonal entries p and identical offdiagonal entries (1 − p)/(n − 1). Necessary and sufﬁcient
conditions for K1 ⊇ K2 are derived for the cases in which
K1 and K2 are of either of the two kinds, in terms of
(2,2)
the relations between the parameters {[P ](i,j) }(i,j)=(1,1) and
(n, p) pertaining to K1 and K2 [6, Theorems 5, 7, 8 ,9].
Channel inclusion can be equivalently deﬁned with Rα ’s
and Tα ’s in Deﬁnition 1 being stochastic matrices in which
all the entries are 0 or 1, as stated in [1], where Rα ’s and
Tα ’s of this kind are called pure channels. This is easily
corroborated based on the fact that every stochastic matrix
can be represented as a convex combination of such “pure”
matrices [7, Theorem 1]. Given this equivalent deﬁnition,
K2 ⊆ K1 has the implication that if there is a set of m code
words {wl }m of length n, such that an error rate of Pe is
l=1
achieved with the code words being used with probabilities
{pl }m under K2 , then there exists a set of m code words
l=1
of length n, such that an error rate of Pe ≤ Pe is achieved
under K1 with the code words being used with probabilities
{pl }m . In [8, p.116], this implication is stated as one DMC
l=1
being better in the Shannon sense than another (different from
the ordering itself), and it is pointed out that K1 ⊇ K2 is a
sufﬁcient but not necessary condition for K1 to be better in
the Shannon sense than K2 , with the proof provided in [9].
Channel inclusion, as deﬁned, is a partial ordering between
two DMCs: it is possible to have two DMCs K1 and K2 such
that K1
K2 and K2
K1 . Under this situation, it cannot
be asserted that there exists a code for one DMC performing
at least as well as the given code for the other. For the purpose
of making it possible to compare an arbitrary pair of DMCs,
particularly their error rate performance under the situation
that the code for one DMC is constructed based on the code for
the other, a metric based on the total variation distance, namely
Shannon deﬁciency is introduced in [10]. In our notation, the
Shannon deﬁciency of K1 with respect to K2 is deﬁned as
T

β

δS (K1 , K2 )

inf

g(α) Rα K1 Tα − K2

inf

β∈N g;Rα ,Tα

found. Furthermore, other useful deﬁciency-like quantities are
established in [10] by substituting the total variation distance
with other divergence obeying a data processing inequality
between probability distributions.
Here we describe some important mathematical concepts
involved in this paper.
Deﬁnition 3: For two vectors a, b ∈ Rn , a is said to
weakly majorize (or dominate) b, written a w b, if and
k
k
only if i=1 a↓ ≥ i=1 b↓ for k = 1, . . . , n, where a↓
(i)
(i)
(i)
and b↓ are entries of a and b sorted in decreasing order. If
(i)
n
n
a w b and i=1 a(i) = i=1 b(i) , a is said to majorize (or
dominate) b, written a b.
Deﬁnition 4: A circulant matrix is a square matrix in
which the i-th row is generated from cyclic shift of the ﬁrst
row by i − 1 positions to the right.
Deﬁnition 5: An n × n matrix P is said to be doubly stochastic if the following conditions are satisﬁed: (i)
[P ](i,j) ≥ 0 for i, j = 1, . . . , n; (ii)
i [P ](i,j) = 1 for
j = 1, . . . , n; (iii) j [P ](i,j) = 1 for i = 1, . . . , n.
Deﬁnition 6: A DMC is called symmetric if its rows are
permutations of each other, and its columns are permutations
of each other. A DMC is called weakly symmetric if its rows
are permutations of each other, and its every columns have
identical sums (drawn from [11, p.190]).
It is easy to verify that if a symmetric DMC has a square
matrix, then it must be doubly stochastic. In this paper, we
will focus mostly on square DMCs (i.e. DMCs with equal size
of input and output alphabets), and we assume this condition
unless otherwise speciﬁed.
III. A NALYTICAL C ONDITIONS FOR C HANNEL I NCLUSION
In general, given two DMCs K1 and K2 , there is no
straightforward method to check if one includes the other
based on their entries. Nevertheless, it is possible to characterize the conditions for channel inclusion between them
in terms of some checkable relations between their entries,
for the cases in which both K1 and K2 have special structure.
There are several different special properties pertaining DMCs,
and we reveal the relations between them as follows. As we
have described in Section II, the n × n channel considered
in [6] has identical off-diagonal entries, making it a special
case of circulant DMC. It is also clear that circulant DMC is
a special case of symmetric DMC as deﬁned in [11, p.190],
and symmetric DMC is a special case of weakly symmetric
DMC deﬁned therein. Furthermore, weakly symmetric DMC
is a special case of doubly stochastic DMC if they are square.
In this section, we aim to derive checkable conditions for the
cases of doubly stochastic and circulant DMCs, while the applicability of these conditions can be extended considering the
relations between DMCs with different special properties as
described above. Also, we derive the conditions for inclusion
between BEC and BSC, since they are commonly used in
information theory.

α=1

∞

(2)
where g ∈
is a probability vector, Rα ’s and Tα ’s are
stochastic matrices, A ∞ maxi [A](i,·) 1 = AT 1 , and
we impose matrix transpose since we treat channel matrices
as row-stochastic instead of column-stochastic. Intuitively, the
above Shannon deﬁciency quantiﬁes how far is K1 apart from
including K2 . With the notion of Shannon deﬁciency, it is
possible to quantify how close K1 can perform with respect to
K2 when K1 K2 and the code for K1 is constructed based
on the one for K2 . Speciﬁcally, it is shown in [10] than given
K2 achieving error rate with some code and δS (K1 , K2 ) ≤
, a code for K1 achieving error rate no more than + can be
Rβ
+

A. Doubly Stochastic and Circulant DMCs
Considering that doubly stochastic matrices have signiﬁcant theoretical importance, and doubly stochastic DMCs can

2

while K1 ( ) ⊆ K2 (p) if and only if p = 0. In addition, each
of these conditions coincides with the condition for the worse
DMC to be a degraded version of the better one.
Proof: See Appendix C.

be thought of as a generalization of square symmetric DMCs,
we ﬁrst introduce the following theorem
Theorem 1: Let K1 and K2 be n × n doubly stochastic
DMCs, with w1 and w2 being the n2 × 1 vectors containing
all the entries of K1 and K2 respectively. Then w2 w1 is
a necessary condition for K2 ⊆ K1 .
Proof: See Appendix A.
Consider the case of both K1 and K2 being n×n circulant.
This kind of channels are known to be involved in discrete
degraded interference channels [12]. We have the following
result
Theorem 2: Let K1 and K2 be n×n circulant DMCs, with
vectors v1 and v2 being the ﬁrst rows of them respectively.
Then for K2 ⊆ K1 , a necessary condition is v2
v1 . A
sufﬁcient condition is that v2 can be represented as the circular
convolution of v1 and another probability vector in Rn , i.e.
+
n
the existence of a vector x ∈ Rn such that i=1 x(i) = 1 and
+
v1 ⊗ x = v2 .
Proof: See Appendix B.
It is clear that a 2 × 2 doubly stochastic DMC is circulant
and characterized solely by the cross-over probability, thus
the condition for the inclusion between two 2 × 2 such
DMCs boils down to the comparison between their crossover probabilities. Furthermore, for n = 3, 4, it is easy to
verify that if an n × n symmetric DMC is not circulant, it
can be transformed into circulant through row and column
permutations (i.e. by multiplying permutation matrices on the
left and right), therefore we can conclude that
Corollary 1: For n = 3, 4, let K1 and K2 be n × n
symmetric DMCs, which can be transformed into circulant
DMCs K1 and K2 respectively through row and column
permutations. Let v1 and v2 be the ﬁrst rows of K1 and
K2 respectively. Then for K2 ⊆ K1 , a necessary condition
is that v2
v1 , while a sufﬁcient condition is that v2 can
be represented as the circular convolution of v1 and another
probability vector in Rn .
+

IV. C OMPUTATIONAL A SPECTS OF C HANNEL I NCLUSION
In Section III, we have established some analytical conditions for determining if a DMC K1 includes another DMC K2
or not. It is also of interest to know how this can be determined
numerically. Furthermore, once it has been determined that
K2 ⊆ K1 , as long as low complexity is sought for the
purpose of obtaining the optimal code for K1 based on the
one for K2 , it is desirable for g(α) ’s in (1) to contain as many
zeros as possible. In this section, we provide a sketch of how
these can be implemented, from both theoretical and empirical
aspects. We begin by formulating the optimization problem for
determining if channel inclusion holds between two DMCs.
For the case in which channel inclusion holds, we prove the
sparsity of g and give discussions on obtaining sparse solution
of g through compressed sensing.
We ﬁrst take a look at the solvability of determining if
K2 ⊆ K1 through convex optimization. For K1 of size n1 ×
m1 and K2 of size n2 × m2 , with g(α) ’s as variables, the
problem can be formulated as
β

g(α) Rα K1 Tα − K2

minimize
α=1
β

1

(3)

g(α) = 1, g(α) ≥ 0

subject to
α=1

where Rα ’s of size n2 × n1 and Tα ’s of size m1 × m2 are
stochastic matrices, and K2 ⊆ K1 is determined if the optimal
value is zero. As mentioned in Section II, Rα ’s and Tα ’s can
be equivalently treated as pure channels, so there are at most
nn2 mm1 different {Rα , Tα } pairs, and consequently there are
1
2
ﬁnitely many g(α) ’s involved in the problem (3) (i.e. β < ∞).
It is easy to see that (3) is a convex optimization problem, and
it can be re-formulated as the linear programming problem

B. Channel Inclusion between BEC and BSC
So far we have derived several conditions for channel
inclusion between DMCs, considering two of the same size.
Now we consider what the conditions are like between BEC
and BSC, which are commonly considered channels in information theory and are of different sizes. It is well-known
that a BEC is characterized by its erasure probability and a
BSC is characterized by its cross-over probability p, with the
stochastic matrices given as K1 ( ) = [1 − , 0, ; 0, 1 − , ]
and K2 (p) = [1 − p, p; p, 1 − p] respectively. Therefore, the
conditions for the inclusion between the two DMCs will boil
down to how and p are related. Here we assume 0 ≤ p ≤ 1/2
for convenience, since it is easy to see that two BSCs with
cross-over probabilities p and 1 − p are equivalent in the sense
of Deﬁnition 1. We have the following theorem characterizing
the inclusion between K1 ( ) and K2 (p) through and p.
Theorem 3: For a BEC K1 ( ) with erasure probability
∈ [0, 1] and a BSC K2 (p) with cross-over probability
p ∈ [0, 1/2], K2 (p) ⊆ K1 ( ) if and only if 0 ≤ ≤ 2p,

n2

c(i) (here c is n2 × 1)

minimize
i=1

β

subject to − c ≤

g(α) Rα K1 Tα − K2
α=1

≤c

(4)

(·,j)
β

g(α) = 1, g(α) ≥ 0

for j = 1, . . . , m2 , and
α=1

We also notice that (4) provides a way to evaluate the Shannon
deﬁciency of K1 with respect to K2 deﬁned by (2), thereby
implying the upper bound of the error rate performance of K1
with respect to the error rate performance of K2 .
In the above analysis, it is worth noticing that the maximum possible number of {Rα , Tα } pairs, given by nn2 mm1
1
2
(or (n!)2 if both K1 and K2 are n×n doubly stochastic), grows
very rapidly with the sizes of K1 and K2 . With K2 ⊆ K1

3

already determined, it is natural to ask if (1) can hold with
some reduced number of {Rα , Tα } pairs. In other words, we
seek to have a sparse solution of g. We have the following
theorem regarding the sparsity of g given K2 ⊆ K1 .
Theorem 4: For two DMCs K1 of size n1 × m1 and K2
of size n2 × m2 , if K2 ⊆ K1 , there exist a probability vector
g ∈ Rβ1 and β1 pairs of stochastic matrices {Rα , Tα }β1 such
+
α=1
that (1) holds with β = β1 ≤ n2 (m2 − 1) + 1. If both K1
and K2 are n × n doubly stochastic, the number of necessary
{Rα , Tα } pairs in (1) can be improved as β1 ≤ (n − 1)2 + 1.
Proof: See Appendix D.
For a numerical example, consider a DMC K1 =
[0.7, 0.3; 0.2, 0.8], and construct another DMC

for inclusion between DMCs with certain special structure
(doubly stochastic, circulant etc), and also derived necessary
and sufﬁcient conditions for inclusion between BEC and
BSC. For the issues of determining channel inclusion through
numerical computation, we formulate a linear programming
problem leading to the quantitative result on how far is one
DMC apart from including another, which has an implication
on the comparison of their error rate performance. In addition,
for the case in which one DMC includes another, we derive
the upper bound for the necessary number of pairs of pure
channels involved in the representation of the worse DMC
in terms of the better, which is signiﬁcantly less than the
maximum possible number of such pairs. This kind of sparsity
implies reduced complexity of ﬁnding the optimal code for the
better DMC based on the code for the worse one.

1 0
0.7 0.3
0.2 0.8
+ 0.05
+ 0.65
1 0
0.2 0.8
0.7 0.3
0.3 0.7
0.8 0.2
0 1
+ 0.1
+ 0.1
+ 0.05
0.8 0.2
0.3 0.7
0 1

K2 = 0.05

A PPENDIX A. P ROOF OF T HEOREM 1
We start from (1) with Rα ’s and Tα ’s being pure channels,
as equivalent to Deﬁnition 1. It is clear that every entries of K2
are linear combinations of the entries of K1 . Considering the
fact that there is a one-to-one mapping between the entries
of K1 and the entries of w1 , as well as the same situation
for K2 and w2 , it follows that there is a matrix P such that
w2 = P w1 , and the conditions for K2 ⊆ K1 are related to
certain properties the combining coefﬁcients [P ](i,j) ’s have.
Based on Birkhoff’s Theorem [14, p.30], both K1 and K2 are
inside the convex hull of n×n permutation matrices, therefore
it is sufﬁcient for Rα ’s and Tα ’s to contain only permutation
β
matrices; otherwise
α=1 g(α) Rα K1 Tα will fall out of the
convex hull of n × n permutation matrices, which contradicts
with the doubly stochastic condition. Consequently, for each
α, Rα K1 Tα gives a matrix having exactly the same set of
entries as K1 , generated by permuting the columns and rows
of K1 . In other words, Rα ’s and Tα ’s do not replace any row
of K1 with the duplicate of another row, or merge any column
into another column and then replace it with zeros.
We now consider the implications of the structure of
Rα K1 Tα on the properties of [P ](i,j) ’s. We have j [P ](i,j) =
β
2
α=1 g(α) = 1 for i = 1, . . . , n since each entry of K1
is contained in Rα K1 Tα exactly once, for α = 1, . . . , β.
On the other hand, since each entry [K2 ](i,j) of K2 is the
convex combination of the entries with the same index (i, j)
of Rα K1 Tα ’s, while each entry of Rα K1 Tα is exactly an
β
entry of K1 , it follows that
i [P ](i,j) =
α=1 g(α) = 1
2
for j = 1, . . . , n . Also, it is straightforward to see that
[P ](i,j) ≥ 0 for i, j = 1, . . . , n2 due to the non-negativeness of
g(α) ’s. Consequently, P is doubly stochastic, and w2 = P w1
implies that w2 w1 [14, p.155], completing the proof.

(5)
which is clearly included in K1 and boils down to K2 =
[0.325, 0.675; 0.625, 0.375]. On the other hand, it can be
veriﬁed that
0.2 0.8
0.3 0.7
0.8 0.2
+ 0.05
+ 0.2
0.7 0.3
0.8 0.2
0.3 0.7
(6)
from which we can see that it is sufﬁcient to represent K2 in
terms of K1 together with 2 · (2 − 1) + 1 = 3 {Rα , Tα } pairs,
thereby corroborating Theorem 4.
It is well-known that a typical approach to recover a sparse
signal vector from its linear measurements is compressed
sensing with 1 norm minimization. For applying this approach
to our problem, we can formulate it as
K2 = 0.75

β

|g(α) |

minimize
α=1
β

(7)
g(α) Rα K1 Tα = K2

subject to
α=1

with variables g ∈ Rβ . It is easy to prove that the optimal
g always comes out non-negative given K2 ⊆ K1 . However,
(7) normally does not give sparse solution of g, since the
number of independent measurements (n2 (m2 −1) or (n−1)2 )
in (7) is usually less than 2β1 (up to 2n2 (m2 − 1) + 2 or
2(n−1)2 +2), and the condition for sparse probability vector to
be solvable through 1 norm minimization [13] is not satisﬁed.
Empirically, we ﬁnd that sparse solution of g is obtainable
with non-optimal sparsity, by replacing the objective function
in (7) with α∈A |g(α) | where A ⊂ {1, . . . , β}, although there
is no established theory regarding this approach. There are also
other sparsity-inducing numerical methods such as matching
pursuit, which will be considered in our future work.

A PPENDIX B. P ROOF OF T HEOREM 2
Let w1 and w2 be the n2 × 1 vectors containing all the
entries of K1 and K2 respectively. It is easy to see that w1
and w2 contain the entries of v1 and v2 each duplicated n
times respectively. Given K2 ⊆ K1 , based on Theorem 1 we
k
k
↓
↓
know that w2
w1 , thus
i=1 n · v1(i) ≥
i=1 n · v2(i)

V. C ONCLUSIONS
In this paper, we investigate the characterization of channel
inclusion between DMCs through analytical and numerical
approaches. We have established several checkable conditions

for k = 1, . . . , n, and it follows that

4

k
i=1

↓
v1(i) ≥

k
i=1

↓
v2(i)

n

n

for k = 1, . . . , n. In addition,
i=1 v1(i) =
i=1 v2(i) =
1 as required for stochastic matrices, therefore v2
v1 is
necessary for K2 ⊆ K1 .
We next prove that the existence of a probability vector
x ∈ Rn such that v1 ⊗ x = v2 is sufﬁcient for K2 ⊆ K1 . Let
+
P be the n × n permutation matrix such that xP is cyclic
shifted to the right by 1 with respect to x, and let X be
the n × n matrix with the i-th column being P i−1 xT . It
is easy to see that both P i−1 and X are circulant. Given
v1 ⊗x = v2 , it follows that v1 X = v2 due to the deﬁnition of
circular convolution. Also, notice that P i−1 X = XP i−1 since
the multiplication of two circulant matrices are exchangeable.
Consequently, the i-th row of K1 , given by v1 P i−1 , and
the i-th row of K2 , given by v2 P i−1 , are related through
(v1 P i−1 )X = v1 XP i−1 = (v2 P i−1 ). It then follows that
K1 X = K2 with a stochastic matrix X, i.e. Deﬁnition 1 is
satisﬁed, and the proof is complete.

convex hull of the points (0, 0), (1, 1), (1−p, p) and (p, 1−p).
Clearly, this condition is satisﬁed only when p = 0, therefore
p = 0 is necessary for K1 ( ) ⊆ K2 (p). The sufﬁciency of
p = 0 for K1 ( ) ⊆ K2 (p) can be easily seen from
1−
0

0
1−

=

1
0

0
1−
·
1
0

0
1−

(10)

which implies that both Deﬁnition 1 and Deﬁnition 2 are
satisﬁed, and the proof is complete.
A PPENDIX D. P ROOF OF T HEOREM 4
Since an n2 × m2 stochastic matrix is determined by its
n2 rows and ﬁrst m2 − 1 columns, the class of all n2 × m2
stochastic matrices can be viewed as a convex polytope in
n2 (m2 −1) dimensions. We apply Carath´ odory’s theorem [15,
e
p.155], which asserts that if a subset S of Rm is k-dimensional,
then every vector in the convex hull of S can be expressed as
a convex combination of at most k + 1 vectors in S, on (1)
with K1 of size n1 × m1 and K2 of size n2 × m2 . It is clear
that Rα K1 Tα and K2 are at most n2 (m2 − 1)-dimensional.
Therefore if K2 is in the convex hull of {Rα K1 Tα }β , it can
α=1
be expressed as a convex combination of at most n2 (m2 −1)+
1 matrices in {Rα K1 Tα }β , i.e. the number of necessary
α=1
{Rα , Tα } pairs can be bounded as β = β1 ≤ n2 (m2 − 1) + 1
if (1) holds. Similar proof can follow for the case of both K1
and K2 being n × n doubly stochastic, in which they are at
most (n − 1)2 -dimensional.

A PPENDIX C. P ROOF OF T HEOREM 3
Consider for (1) in Deﬁnition 1, with Rα ’s and Tα ’s being
pure channels. Clearly Rα ’s should be 2 × 2 and Tα ’s should
be 3 × 2, and there are ﬁnitely many different cases of them.
Given these conditions, it is clear that K1 ( )Tα can only be
the following cases: [1, 0; 1, 0], [1 − , ; 0, 1], or two other
2 × 2 matrices obtained by column permutations on these two.
Consequently, Rα K1 ( )Tα can only be the following cases:
[1, 0; 1, 0], [1− , ; 0, 1], [1− , ; 1− , ], [0, 1; 1− , ], or four
other matrices obtained by column permutations on these four.
Similar to the geometrical interpretation of channel inclusion
given in [1], we can see that the point (1−p, p) ∈ R2 should be
in the convex hull of the points (0, 0), (1, 1), (1 − , 0), (1, ),
(0, 1 − ) and ( , 1). With the considerations of geometrical
symmetry and 0 ≤ p ≤ 1/2, it follows that (1 − p, p) should
have no smaller vertical coordinate than (1− /2, /2) which is
the mid-point of (1− , 0) and (1, ). Thus for K2 (p) ⊆ K1 ( ),
it is required that p ≥ /2, i.e. 0 ≤ ≤ 2p.
To prove the sufﬁciency of ≤ 2p for K2 (p) ⊆ K1 ( ), we
can see that


x
1−x
1−p
p
1−
0
x  (8)
=
· 1 − x
p
1−p
0
1−
1/2
1/2

R EFERENCES
[1] C. E. Shannon, “A note on a partial ordering for communication
channels,” Information and Control, vol. 1, pp. 390–397, 1958.
[2] D. Blackwell, “Comparison of experiments,” 2nd Berkeley Symposium
on Mathematical Statistics and Probability, pp. 93–102, 1951.
[3] ——, “Equivalent comparisons of experiments,” The Annals of Mathematical Statistics, vol. 24, pp. 265–272, 1953.
[4] J. E. Cohen, J. H. B. Kempermann, and G. Zbaganu, Comparisons of
Stochastic Matrices with Applications in Information Theory, Statistics,
Economics and Population, 1st ed. Birkhauser Boston, Sep. 1998.
[5] J. Korner and K. Marton, “Comparison of two noisy channels,” Topics
in Information Theory (edited by I. Csiszar and P. Elias), Keszthely,
Hungary, pp. 411–423, Aug. 1975.
[6] H. J. Helgert, “A partial ordering of discrete, memoryless channels,”
IEEE Transactions on Information Theory, vol. IT-3, no. 3, pp. 360–
365, Jul. 1967.
[7] W. Schaefer, “A representation theorem for stochastic kernels and its
application in comparison of experiments,” Series Statistics, vol. 13,
no. 4, pp. 531–541, 1982.
[8] I. Csiszar and J. Korner, Information theory: coding theorems for
discrete memoryless systems, 1st ed. New York: Academic Press, 1981.
[9] M. A. Karmazin, “Solution of a problem of Shannon (in Russian),”
Problemy Kibernet, vol. 11, pp. 263–266, 1964.
[10] M. Raginsky, “Shannon meets Blackwell and Le Cam: Channels, codes,
and statistical experiments ,” IEEE International Symposium on Information Theory, pp. 1220–1224, Jul. 2011.
[11] T. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd ed.
Wiley-Interscience, Jul. 2006.
[12] N. Liu and S. Ulukus, “The Capacity Region of a Class of Discrete
Degraded Interference Channels,” IEEE Transactions on Information
Theory, vol. 54, no. 9, pp. 4372–4378, Sep. 2008.
[13] A. Cohen and A. Yeredor, “On the use of sparsity for recovering discrete
probability distributions from their moments,” IEEE Statistical Signal
Processing Workshop, pp. 753–756, Jun. 2011.
[14] A. W. Marshall, I. Olkin, and B. C. Arnold, Inequalities: Theory of
Majorization and Its Applications, 2nd ed. Springer, Jan. 2009.
[15] R. T. Rockafellar, Convex Analysis. Princeton: Princeton University
Press, 1970.

holds with x = (1 − p − /2)/(1 − ). Given 0 ≤ p ≤ 1/2 and
≤ 2p, it follows that 0 ≤ x ≤ 1, thus Deﬁnition 1 is satisﬁed
and K2 (p) ⊆ K1 ( ). Also, it can be seen that Deﬁnition 2 is
satisﬁed, and K2 (p) is a degraded version of K1 ( ).
We now consider what is necessary and sufﬁcient for
K1 ( ) ⊆ K2 (p). Consider the Z-channel


1 0
1−
1−
0
K3 ( ) =
=
· 0 1 (9)
0
1
0
1−
0 1
It is clear that K3 ( ) ⊆ K1 ( ), and based on the transitivity
of channel inclusion [1], we have K3 ( ) ⊆ K2 (p). Similar
to the geometrical interpretation of channel inclusion given in
[1], we can see that the point (1 − , 0) should be inside the

5

