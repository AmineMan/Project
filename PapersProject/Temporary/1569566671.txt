Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May  3 02:52:43 2012
ModDate:        Tue Jun 19 12:55:44 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      2643071 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566671

Joint Source-Channel Coding of one Random
Variable over the Poisson Channel
Albert No

Kartik Venkat

Tsachy Weissman

Stanford University
Email: albertno@stanford.edu

Stanford University
Email: kvenkat@stanford.edu

Stanford University
Email: tsachy@stanford.edu

Abstract—We study the transmission of a single random variable across the Poisson channel, which takes a continuous-time
waveform {λt : 0 ≤ λt ≤ T } as an input, where 0 ≤ λt ≤ A, for
all 0 ≤ t ≤ T . The output of the channel is a non-homogeneous
Poisson arrival process with rate λT . We explore the class of
schemes that are optimal in the distortion exponent sense under
mean squared loss. We determine a family of optimal encoders
for this channel which achieves the minimum mean squared
error. In addition, we characterize the distortion exponent for
‘separation’ based schemes, and also provide an upper bound for
the maximal distortion exponent across all joint source channel
strategies under this setting.
















Fig. 1: Illustration of the joint source-channel coding problem.
















Note that as the time duration of channel use T increases,
we expect the loss in estimating X to converge to zero. The
“distortion exponent” of the joint source-channel coding strategy captures the rate at which this loss decays exponentially
to zero. A larger distortion exponent also indicates a lower
system complexity for achieving a target average estimation
loss. Thus, a transmission scheme that achieves the maximal
distortion exponent is highly desirable.
The rest of the paper is organized as follows. In Section
II, we provide a detailed description of the problem. Section
III deals with the optimal encoding of a random variable
for the Poisson channel under a mean squared loss criterion.
Our main result shows that a simple family of binary valued
encoders is optimal, in a manner that will be made precise.
In Section IV, we introduce the notion of separation based
schemes for the Poisson channel and explicitly characterize
the distortion exponent for such schemes. We also provide an
upper bound on the maximal distortion exponent over all joint
source channel schemes. We conclude with a summary of our
ﬁndings in Section V.

I. I NTRODUCTION
The literature abounds with results pertaining to the Poisson
channel, and its numerous applications in ﬁelds ranging from
communication theory to neuroscience. Also known as the
direct detection optical channel, it is widely studied as the
canonical model for optical communication. Several systems
based on this channel model have been described in the literature (cf. [1], [2]). The Poisson channel has also been associated
with a signiﬁcant body of results in information theory. In [3]
and [4], Wyner characterized the capacity and error exponent
for the channel, while also providing an explicit construction
of a family of exponentially optimal codes for the same.
More recently, Atar et. al. in [5], have discovered fundamental
links between classical information theoretic quantities and the
average estimation loss (under a natural loss function) for this
channel.
The Poisson channel has a fundamental role to play in
‘sensory processing’ in neuroscience, where sensory data in
the form of neural spike trains are modeled by a Poisson
arrival processes (cf. [6]). [7] discusses ‘efﬁcient recoding’
of information in sensory pathways, which has ramiﬁcations
in biological signal processing. Some of the fundamental
questions that one wishes to investigate in the biological
context are: “to what extent does optimal encoding depend
on the statistics of the stimulus?”, and “whether separation
of source and channel is optimal or not?” In this paper, we
seek to address some of these questions, from an information
theoretic angle.
We investigate the problem of transmitting a single random variable X (representing the environment in the neuroscientiﬁc context) across the Poisson channel, where our
benchmark for performance is the minimum mean squared
loss in estimating X based on the channel output process.

II. P ROBLEM D ESCRIPTION
A. Poisson Channel
We consider the Poisson Channel under a peak power
constraint. The channel input is a waveform λ(t), 0 ≤ t ≤ T
(which we shall denote throughout, using the shorthand λT )
which satisﬁes 0 ≤ λ(t) ≤ A, where the parameter A is the
peak power. λ(·) deﬁnes the rate function of the output Poisson
process, Y T . We consider the problem of transmitting a scalar
random variable1 X, under a minimum mean squared loss
criterion. Thus, we are in the setting of a joint source-channel
coding problem (see Fig. 1). Our goal is to come up with a
coding strategy that achieves the maximal distortion exponent
for the Poisson channel.
1 We

1

impose a ﬁnite second moment constraint on X








1) Distortion Exponent: The distortion exponent α is deﬁned by,
α = lim sup −
T →∞

1
ˆ
log inf E[(X − X(Y T ))2 ].
T

(2)

inf

E[(X − E[X|Y T ])2 ]







Wyner introduced a family of exponentially optimal codes.
We describe the main features of the code below, and refer
the reader to [3] for detailed discussions and implications of
the same. For ∆ > 0 deﬁne W [0, T ] as follows,

(3)

λT : λt ≡ xi , for t ∈ [(i − 1) , i ), xi ∈ {0, A}

(7)

In other words, W [0, T ] is the set of encoding waveforms, denoted by λT , which are constant on the interval
T
[(i − 1) , i ) for every i ∈ {1, 2, · · · ∆ } and takes only the
values 0 or A. If encoder maps to some λT ∈ W [0, T ],
we say that encoder employs “Wyner delta coding strategy”.
In order to be explicit, we denote the corresponding output
process of the Poisson channel as Y T , whenever the encoder
uses the Wyner delta coding strategy. This setting is illustrated
in Fig. 2
Having set up the notation for the problem, we now describe
our main result for this section below:
Theorem 1:

(4)

inf

inf

∆>0 λT ∈W∆ [0,T ]
∆

T
E[(X − E[X|Y∆ ])2 ] = βT

(8)

The above result proves that there exists a {0, A} valued
equally-spaced binary waveform, that achieves the best performance across all encoders in a rich class of encoders for
the Poisson Channel. It is striking that this class of schemes
is rich enough to guarantee minimum mean squared loss for
the continuous time Poisson channel.
Below we present two different proofs for Theorem 1, the
latter exploiting a recently uncovered information-estimation
identity for the Poisson channel.

(5)

In principle, the optimal performance benchmark would be
deﬁned as the inﬁmum of the mean squared error, over all
˜
possible encoding and decoding functions. Denote by F[0, T ]
the space of right continuous bounded (by constant A) functions on the interval [0, T ]. Deﬁne,
˜
λT ∈F [0,T ]




A. Wyner Approximation

In this scenario, we know that the Bayes estimator minimizes
the mean squared loss and is given by the conditional expectation of X given the observations Y T . I.e.,

βT =








In this section, we consider the problem of encoding a
random variable to be transmitted across the Poisson channel.
Before stating our main theorem, we describe below the family
of encoders discussed in [3]. We will use this family of codes
and argue that they are essentially optimal in the sense of
minimum expected squared loss.

In the above discussion, we presented the optimal distortion
exponent as the target of a good coding strategy. Note that a
much stricter criterion for performance would be to achieve a
minimum average loss performance, i.e. competing not only
in the limiting exponent of the mean squared error for large T ,
but in the actual expected estimation loss itself, for all values
of T .
The mean squared error for a given encoder-decoder pair
ˆ
{λ(·), X(·)} is given by

ˆ
X Bayes (Y T ) = E[X|Y T ].








III. O PTIMAL E NCODING S TRATEGY FOR P OISSON
C HANNEL

B. A stricter performance criterion

ˆ
mse = E[(X − X(Y T ))2 ].



Fig. 2: Employing the Wyner delta coding scheme.

3) Decoder: Let S(T ) denote the space of nondecreasing, nonnegative integer-valued step functions on the interval
[0, T ]. Thus the output of the channel Y T ∈ S(T ). A decoder
is a measurable mapping
ˆ
X : S(T ) → R




(1)

where the inﬁmum is over all joint source-channel coding
schemes, or more explicitly encoders and decoders, formally
deﬁned as follows.
2) Encoder: Let F[0, T ] denote the set of measurable
nonnegative waveforms that reside on the interval [0, T ] and
are bounded above by a ﬁxed constant A. The encoder is
described by a mapping
λ : R → F[0, T ]



(6)

B. Proof of Theorem 1

We now decompose the problem into two parts. We ﬁrst
attempt to describe an efﬁcient coding strategy for the Poisson
channel, that will be optimal according to the criterion in (6).
The second part of the problem is to understand the notion of
separation based schemes in this setting, and the corresponding
distortion exponent. We discuss these two problems in sections
III and IV respectively.

˜
For a given encoding strategy λT ∈ F[0, T ], we will
T
construct a Wyner delta approximation λ ∈ W [0, T ], as
follows. For i ≤ t < (i + 1) , deﬁne
λ

2

,t

=

(i+1)
0

A

if

0

otherwise

λs ds >

i
0

λ

,s ds

+A

(9)

T
T
for every {Yn }n≥1 , Y T ∈ S(T ) such that d(Yn , Y T ) → 0
T
T
and the arrival times of Y and Yn , which are denoted by
{t1 , · · · tm } and {tn,1 , · · · tn,m } respectively, satisfy ti ≤ tn,i
for all n, i.
˜
Lemma 3: For λT ∈ F[0, T ], the optimal Bayesian estimaˆ T ) = E[X|Y T = y T ] = f (y T ) is right continuous in
tor X(y
the sense of Deﬁnition 1.
Proof: Let N denote the number of arrivals in Y T process
and the corresponding arrival times be given by t1 , · · · , tN .
We note that

It follows from (9) that,
t

0≤

t

λs ds −
0

λ

,s ds

< 2A .

(10)

0

In addition, if t∆ be the solution of the equation
t

t

λs ds =
0

λ

,s ds

(11)

0

then t∆ must satisfy the inequality
0≤t −t<2 .

(12)

E[X|Y T = y T ] = E[X|{t1 = τ1 , · · · tn = τn }, N = n] (16)

1) Constructing of an Approximate Estimator: Let N denote a homogeneous Poisson process with rate 1. Let Yt be
a non-homogeneous Poisson process with rate function λT . It
is easy to observe that Yt has a same probability distribution
as N t λs ds . Thus, we can view the Y T process as a function
0
of N and λT , i.e. Y T = F (N, λT ). Similarly, we can deﬁne
Y T = F (N, λT ) so that both Y T and Y T are in the same
probability space.
We wish to argue that the estimator based on the λ∆
encoding scheme performs as well as the original estimator
based on the λ process. Before stating the result, we need
to equip the space of output processes S(T ) with a suitable
metric, which would then allow us to discuss notions of
convergence and continuity on this space. Since Y T is arrival
process, it is natural to deﬁne the metric based on arrival times.
Deﬁne a(Y T ) to be the number of arrivals of the Y T process.
Then, d : S(T ) × S(T ) → [0, ∞] be deﬁned as follows,

which can be evaluated explicitly to yield the following
expression

(

sup1≤i≤N |ti − t

lim

|| ||∞ →0
i ≥0,∀i

t

λs ds = τi =
0

T
0

λs (x) ds

dP (x)

T
n
x
x
x − 0 λs (˜) ds dP (˜)
i=1 λτi (˜)) e
n
i=1 λτi , by right continuity of

.

E[X|tn = τ n +

n

] = E[X|tn = τ n ]

ˆ
ˆ
Eλ [(X − Xλ (Y T ))2 ] < Eλ [(X − Xλ (Y T ))2 ] +

,i | otherwise

(17)

(18)

(19)

ˆ
where Eλ and Xλ (Y T ) are expectation and the Bayes optimal
estimator respectively, when the encoder employs the coding
ˆ
scheme λT . More explicitly, Xλ (Y T ) = Eλ [X|Y T ]. We
ˆ
deﬁne Eλ and Xλ in the same fashion.
Following arguments will ﬁnalize the proof of main theorem.
ˆ
ˆ
Eλ [(X − Xλ (Y T ))2 ] ≤Eλ [(X − Xλ (Y T )2 ]
(20)
ˆ λ (Y T ))2 ] + . (21)
<Eλ [(X − X
(20) is because of the optimality of Bayesian estimator and
(21) is direct consequence of Lemma 3.
C. Alternative Proof of Theorem 1

,i

λ

,s ds.

(14)

In this section, we present an alternate proof to Theorem 1
which exploits the mismatched estimation and relative entropy
equivalence presented in [5].
In order to invoke the result in [5, Theorem 4.4], we need to
˜
restrict attention to the class of encoder waveforms in F[0, T ]
that are also bounded away from zero. We note that one
can approximate any given encoding function by one that is
bounded away from zero, while preserving the mean squared
error performance of the system.
Thus, without any loss of generality, we consider the
˜
modiﬁed class of encoding functions Fδ [0, T ], which is the

0

By applying equations (11) and (12), it is clear that 0 ≤ ti −
t ,i < 2 , and therefore, d(Y T , Y T ) < 2 which converges
to 0 as
→ 0.
Armed with Lemma 2, we now wish to establish the right
continuity of the Bayes estimator.
Deﬁnition 1 (Right continuity of an Estimator): An
ˆ
estimator X : S(T ) → R is right continuous if
ˆ T
ˆ
lim X(Yn ) = X(Y T ),

n→∞

λτi (x)) e−

Therefore, E[X|Y T = y T ] is right continuous in y T in the
sense of Deﬁnition 1.
Now, we are ready to prove Theorem 1.
2) Wrapping up the proof: It is enough to show that given
˜
any λT ∈ F[0, T ] and > 0, there exist ∆ > 0 and the
corresponding encoder waveform λT , such that

(13)
where Y T has arrivals at times {ti }N and Y T has arrivals
i=1
at times {t ,i }N . It is easy to check that d(·, ·) is a metric.
i=1
We are now ready to state the following Lemma.
Lemma 2: lim →0 Y T = Y T almost surely.
Proof: Given N, recall that Yt = N t λs ds and Y ,t =
0
N t λ ,s ds . Now from (10), ∃ ∆ small enough, such that the
0
number of arrivals of the Y T and Y T processes are same, i.e.
d(Y T , Y T ) < ∞ almost surely.
T
Let N 0 λs ds have arrival times τ1 , τ2 , · · · , τn . Deﬁne ti
and t ,i such that
ti

n
i=1

Let Λ(τ n ) =
λ, Λ(τ n ) is
right continuous under the metric d(·, ·). Now, by dominated
convergence theorem, we can state that

if a(Y T ) = a(Y T )

∞

d(Y T , Y T ) =

x·(

(15)

3

collection of right continuous waveforms {λt : 0 < δ < λt ≤
A, t ∈ [0, T ]}
˜
Lemma 4: Given λT (·) ∈ Fδ , there exists a sequence of
˜ T (·)}n≥1 such that λT (x) is continuous on t
˜
waveforms {λn
n
for all x and Kullback-Leibler divergence between the joint
laws2 converges to zero, in the following manner:
lim D(PX,Y T ||PX,Y T ) = 0
˜

n→∞

n



n

n



dT V (P, Q) =

n

n

(24)

dT V (P, Q) =
(25)

˜
l(λn,t (x), λt (x)) dt dx.

PX (x)

(26)

˜
l(λn,t (x), λt (x))dt = 0

(27)

˜T
lim |VAR(X|Yn ) − VAR(X|Y T )| = 0

0

n→∞

Therefore,
∞
n→∞

˜
l(λn,t (x), λt (x)) dt dx = 0.

(28)

0

T
˜
Since we know that λt ∈ (δ, A], 0 l(λn,t (x), λt (x))dt is
bounded. By dominated convergence theorem,

lim D(PX,Y T ||PX,Y T ) = 0
˜

n→∞

(33)

(34)

n

(35)

(36)

which is direct consequence of Lemma 5.
Clearly, the source might not have bounded support in
general, for instance when X is Gaussian. However, it is a
straightforward argument to extend the result in Corrolary 6
to the full generality of a random variable with ﬁnite second
moment.

T

PX (x) lim



Proof: By an application of (32) and the fact that if X
has bounded support, then the conditional variance itself is
bounded, we get the required relationship

T

−∞

1
sup |EP [f ] − EQ [f ]|
2c ||f ||∞ ≤c

n→∞

Since λT (x) is right continuous on t, we can always ﬁnd a
˜
sequence of continuous waveforms {λT }n≥1 such that
n
n→∞

1
sup |EP [f ] − EQ [f ]|
2 ||f ||∞ ≤1

˜T
lim |VAR(X|Yn ) − VAR(X|Y T )| = 0.

0

lim




n

T

−∞



Lemma 5 tells us that if limn→0 dT V (Pn , Q) = 0, then for
any bounded function f , limn→∞ EPn [f ] − EQ [f ] = 0.
Corollary 6: If
X
has
bounded
support
and
limn→∞ D(PX,Y T ||PX,Y T ) = 0, then
˜

We now use Theorem 4.4 of [5] to argue that (25) can be
equivalently expressed as
∞



We can easily generalize the result, such that for every constant
c>0

∞

PX (x)D(PY T |λT (x) ||PY T |λT (x) ) dx.
˜ ˜



We now recall an equivalent formulation of the total variation
distance in the following Lemma.
Lemma 5: For function f , denote E[f ] = E[f (X, Y )], then

(22)

which can be equivalently written as
−∞





Fig. 3: Illustration of a separation based scheme

Proof: Let us deﬁne the loss function l : [0, ∞) ×
[0, ∞) → [0, ∞] given by
x
− x + x.
ˆ
(23)
l(x, x) = x log
ˆ
x
ˆ
We ﬁrst note that
D(PX,Y T ||PX,Y T ) = D(PY T |X ||PY T |X |PX )
˜
˜



(29)

IV. D ISTORTION E XPONENT A NALYSIS
Using the result in Lemma 4, we want to argue that difference of mean squared error between the two encoding schemes
converges to zero as well. Pinsker’s inequality [8] provides
us with a lower bound for the relative entropy distance in
terms of total variance distance between the two laws. The
total variation distance between two probability measures P
and Q is given by
dT V (P, Q) = sup{|P (A) − Q(A)| : A ∈ G}

In this section, we analyze the distortion exponent of the
joint source channel schemes for our setting. In addition we
explore and explicitly characterize the distortion exponent for
the class of ‘separation’ based schemes, which essentially tell
us how much the optimal encoding strategy depends on the
statistics of the source.
Deﬁnition 2 (Separation Based Scheme): A
separation
based scheme is characterized by a source quantizer that
performs a quantization of the input random variable X, in
a channel agnostic manner. The channel coder then encodes
each symbol of the ﬁnite quantizer output, assuming a
uniform prior across this symbol alphabet. This scenario is
depicted in Fig. 3.
Recall the deﬁnition of the distortion exponent α in (1).
Let us deﬁne αsep to be the distortion exponent, over all
separation based strategies, which we have described above.
Note trivially, that α ≥ αsep .

(30)

where G is the sigma algebra on which the probability space
is deﬁned. Thus, we have
D(P ||Q)/2 ≥ dT V (P, Q)

(31)

Using Lemma 4 and (31), it is straightforward to argue that
lim dT V (PX,Y T , PX,Y T ) = 0.
˜

n→∞
2 where

n

(32)

˜
˜T
Yn denotes the channel output when the encoder uses λT
n

4




"

!!

"#$%&'(

#!

*+,##'-

)'$%&'(

number of messages M , that is independent of T . Let EM
denote the maximum distortion exponent in this setting. In
[3], it has been shown that

!
"

M A
.
(40)
M −2 4
Therefore, for any joint source channel coding scheme that
attains an distortion exponent α, and for all M ,
˜
EM ≤

%

$

"#$%&'(

!
$

./**
*%&0#12
/$+'3'

)'$%&'(

!
%

Fig. 4: Constructing a channel code using a joint source
channel scheme

M A
(41)
M −2 4
Thus, we arrive at an upper bound for the maximum distortion
exponent α of the Poisson channel, given by
α ≤ EM ≤
˜

A. Achievability Scheme via Separation
α≤

In this section, we present the optimal separation based
strategy, which achieves the exponent αsep . We employ the
optimal source quantizer Q , followed by the optimal channel
coding strategy for the Poisson channel (cf. [3]). Let ES (R)
and EW (R) denote the maximal distortion exponent at rate R
of the source quantization scheme (i.e., maximal decay rate in
n of the mse of a quantizer for X with enR levels) and the
Poisson channel, respectively. Thus, the overall mean squared
error can be expressed as
.
mse(T ) = e−EW (R)·T + e−ES (R)·T ,
(37)
.
log aT
where aT = bT means limT →∞ log bT = 1. Therefore, the
corresponding distortion exponent will be given by αsep =
min{EW (R), ES (R)}. For example, if the source is a Gaussian random variable, and indeed much more generally for
continuous random variables under benign regularity conditions on the PDFs, it is well known that ES (R) = 2R.
Note that we can choose the appropriate R to maximize
the distortion exponent and thus, the achievable distortion
exponent is max0≤R≤CW min{EW (R), 2R}, where CW is
capacity of the Poisson channel. In [3], Wyner showed that the
distortion exponent satisifes, EW (R) = A/4−R if R ≤ Rcrit
where Rcrit = A log 2.
4
From now on, we restrict our attention to the cases when
ES (R) = 2R. Since the above scheme is clearly the optimum
over all separation schemes, we can get an exact formula for
αsep :
αsep = max min{EW (R), 2R} =
R

A
,
6

A
6

(42)

V. C ONCLUSIONS
We consider the joint source channel coding problem for
transmitting a single random variable across the Poisson channel. We establish that given any joint source channel coding
scheme λT (·) from the rich class of functions allowed, there
exists a Wyner delta coding scheme λT (·) which can perform
∆
as well as λT (·), not only in the distortion exponent sense, but
also in expected squared error for a ﬁxed value of T . Further,
we explicitly characterize the maximal distortion exponent for
the class of separation based schemes for the Poisson channel.
We also provide an upper (and lower) bound for the overall
distortion exponent in this setting. This is summarized in the
following relationship:
A
A
≤α≤ .
(43)
6
4
We are currently working towards closing the gap in our
characterization of α. In particular, answering whether the left
inequality in (43) is tight, which would imply optimality of
separation in a strong distortion-exponent sense. We conjecture
that the answer is afﬁrmative.
αsep =

R EFERENCES
[1] I. Bar-David and G. Kaplan, “Information rates of the photon-limited
overlapping pulse position modulation channels,” IEEE Trans. Inform.
Theory, vol. IT-30, pp. 455-464, 1984.
[2] J. E. Mazo and J. Salz, “On optical data communication via direct
detection of light pulses,” Bell Syst. Tech. J., vol. 55, pp. 347-369, 1976.
[3] A. D. Wyner, “Capacity and error exponent for the direct detection
photon channel,” IEEE Trans. Inform. Theory, vol. 34, pp. 1449-1461,
Nov. 1988
[4] A. D. Wyner, “Capacity and error exponent for the direct detection
photon channel,” IEEE Trans. Inform. Theory, vol. 34, pp. 1462-1471,
Nov. 1988
[5] R. Atar, T. Weissman, “Mutual Information, Relative Entropy, and
Estimation in the Poisson Channel”, IEEE Trans. Inform. Theory, vol.
58, no. 3, Mar 2012
[6] Omer Bobrowski, Ron Meir, Shy Sholam, and Yonina C. Eldar, “A
neural network implementing optimal state estimation based on dynamic
spike train decoding,” Neural Information Processing Systems, 2007.
[7] J. Atick and A.N. Redlich, “Could information theory provide an
ecological theory of sensory processing?”, Network: Computation in
Neural Systems, 5:213-251, 1992.
[8] T.M. Cover and J.A. Thomas, “Elements of Information Theory,” Wiley,
1991.

(38)

A
where the maximum is achieved by R∗ = 12 . Naturally, this
acts as a lower bound for the overall distortion exponent α:

α ≥ αsep =

A
.
4

(39)

B. Upper Bound for α
In this section we derive an upper bound for the distortion
exponent α. The idea is to show that if we have a good
joint source channel coding scheme that achieves an distortion
exponent α, one can construct a corresponding channel coding
˜
scheme with a ﬁnite message set of size M , that attains
the distortion exponent α. This is illustrated in Fig. 4. Now
˜
consider a zero rate communication regime with a ﬁxed

5

