Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 17:49:22 2012
ModDate:        Tue Jun 19 12:55:31 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      427773 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566873

Minimum Complexity Pursuit: Stability Analysis
Shirin Jalali

Arian Maleki

Richard Baraniuk

Center for Mathematics of Information Department of Electrical Engineering Department of Electrical Engineering
Rice University
Rice University
California Institute of Technology
Houston, Texas, 77005
Houston, Texas, 77005
Pasadena, California, 91125

their linear measurements without having the knowledge of
the underlying structure? Does this ignorance incur a cost in
the sampling rate?
In algorithmic information theory, Kolmogorov complexity, introduced by Solomonoff [12], Kolmogorov [13], and
Chaitin [14], deﬁnes a universal notion of complexity for
ﬁnite-alphabet sequences. Given a ﬁnite-alphabet sequence
x, the Kolmogorov complexity of x, K(x), is deﬁned as the
length of the shortest computer program that prints x and
halts. In [15], extending the notion of Kolmogorov complexity to real-valued signals1 by their proper quantization, we
addressed some of the above questions. We introduced the
minimum complexity pursuit (MCP) algorithm for recovering “structured” signals from their linear measurements. We
showed that ﬁnding the “simplest” solution satisfying the
linear measurements recovers the signal using many fewer
measurements than its ambient dimension.
In this paper, we extend the results of [15] to the case
where the measurements are noisy. We ﬁrst propose an
updated version of MCP that takes into account that the
measurements are a linear transformation of the signal plus
Gaussian noise. We prove that the proposed algorithm is
stable with respect to the noise and derive bounds on its
reconstruction error in terms of the sampling rate and the
variance of the noise.
The organization of this paper is as follows. Section II
deﬁnes the notation used throughout the paper. Section II-B
deﬁnes Kolmogorov information dimension of a real-valued
signal. Section III formally deﬁnes the MCP algorithm and
reviews and extends some of the related results proved in
[15]. Section IV considers the case of noisy measurements
and proves that MCP is stable. Section V mentions some of
the related work in the literature, and Section VI concludes
the paper. Appendix A presents two useful lemmas used in
the proofs.

Abstract— A host of problems involve the recovery of structured signals from a dimensionality reduced representation
such as a random projection; examples include sparse signals
(compressive sensing) and low-rank matrices (matrix completion). Given the wide range of different recovery algorithms
developed to date, it is natural to ask whether there exist
“universal” algorithms for recovering “structured” signals from
their linear projections. We recently answered this question in
the afﬁrmative in the noise-free setting. In this paper, we extend
our results to the case of noisy measurements.

I. I NTRODUCTION
Data compressors are ubiquitous in the digital world. They
are built based on the premise that text, images, videos,
etc. all are highly structured objects, and hence exploiting
those structures can dramatically reduce the number of bits
required for their storage. In recent years, a parallel trend has
been developing for sampling analog signals. There too, the
idea is that many analog signals of interest have some kind
of structure that enables considerably lowering the sampling
rate from the Shannon-Nyquist rate.
The ﬁrst structure that was extensively studied in this
context is sparsity. It has been observed that many natural signals have sparse representations in some domain.
The term compressed sensing (CS) refers to the process
of undersampling a high-dimensional sparse signal through
linear measurements and recovering it from those measurements using efﬁcient algorithms [1], [2]. Low-rankedness [3],
model-based compressed sensing [4]–[8], and ﬁnite rate of
innovation [9] are examples of some other structures that
have already been explored in the literature.
While in the original source coding problem introduced by
Shannon [10], the assumption was that the source distribution
is known both to the encoder and to the decoder, and hence
is used in the code design, it was later shown that this
information is not essential. In fact, universal compression
algorithms are able to code stationary ergodic processes at
their entropy rates, without knowing the source distribution
[11]. In other words, there exists a family of compression
codes that are able to code any stationary ergodic process at
its entropy rate asymptotically [11]. The same result holds
for universal lossy compression.
One can ask similar questions for the problem of undersampling “structured” signals: How to deﬁne the class
of “structured” signals? Are there sampling and recovery
algorithms for the reconstruction of “structured” signals from

II. D EFINITIONS
A. Notation
Calligraphic letters such as A and B denote sets. For
a set A, |A| and Ac denote its size and its complement,
respectively. For a sample space Ω and event set A ⊆ Ω,
1 These type of extensions are straightforward and have already been
explored in [16].

1

1A denotes the indicator function of the event A. Bold-

Theorem 1: Assume that xo = (xo,1 , xo,2 , . . .) ∈ [0, 1]∞ .
For integers m and n, let κm,n denote the Kolmogorov
information dimension of xn at resolution m. Then, for any
o
τn < 1 and t > 0, we have
√
nd−1 + t + 1 + 1 √ −2m+2
n
n
n2
P xo − xo 2 >
ˆ
τn

faced lower case letters denote vectors. For a vector x =
(x1 , x2 , . . . , xn ) ∈ Rn , its p and ∞ norms are deﬁned as
n
p
x p
maxi |xi |, respectively. For
p
i=1 |xi | and x ∞
integer n, let In denote the n × n identity matrix.
For x ∈ [0, 1], let ((x)1 , (x)2 , . . .), (x)i ∈ {0, 1}, denote
∞
−i
the binary expansion of x, i.e., x =
i=1 2 (x)i . The
m-bit approximation of x, [x]m , is deﬁned as [x]m
m
−i
n
i=1 2 (x)i . Similarly, for a vector (x1 , . . . , xn ) ∈ [0, 1] ,
n
[x ]m ([x1 ]m , . . . , [xn ]m ).
B. Kolmogorov complexity
The Kolmogorov complexity of a ﬁnite-alphabet sequence
x with respect to a universal Turing machine U is deﬁned
as the length of the shortest program on U that prints x
and halts.2 Let K(x) denote the Kolmogorov complexity of
binary string x ∈ {0, 1}∗ ∪n≥1 {0, 1}n .
Deﬁnition 1: For real-valued x = (x1 , x2 , . . . , xn ) ∈
[0, 1]n , deﬁne the Kolmogorov complexity of x at resolution
m as
K [·]m (x) = K([x1 ]m , [x2 ]m , . . . , [xn ]m ).
Deﬁnition 2: The Kolmogorov information dimension of
vector (x1 , x2 , . . . , xn ) ∈ [0, 1]n at resolution m is deﬁned
as
K [·]m (x1 , x2 , . . . , xn )
.
κm,n
m
To clarify the above deﬁnition, we derive an upper bound
for κm,n .
Lemma 1: For (x1 , x2 , . . .) ∈ [0, 1]∞ and any resolution
sequence {mn }, we have
κm,n
lim sup
≤ 1.
n
n→∞
Therefore, by Lemma 1, we call a signal compressible,
if lim supn→∞ n−1 κm,n < 1. As stated in the following
proposition, Lemma 1’s upper bound on κm,n is achievable.
iid
Proposition 1: Let {Xi }∞ ∼ Unif[0, 1]. Then,
i=1

P

s.t.

d
Axn = yo .

2
2

2κn log n

>
e

κn log n
(1−τ 2 +2 log τ )
2

≤ e1.4κn log n e−1.7κn log n + e

d 2

+ e− 2 t

− d t2
2
2
2

ˆo
which shows that as n → ∞, P( xn − xn
o

,

(3)

> ) → 0.

According to Corollary 1, if the complexity of the signal is
less than κ, then the number of linear measurements needed
for asymptotically perfect recovery is, roughly speaking, at
the order of κ log n. In other words, the number of measurements is proportional to the complexity of the signal and
only logarithmically proportional to its ambient dimension.
Corollary 2: Assume that xo = (xo,1 , xo,2 , . . .) ∈ [0, 1]∞
and m = mn = log n . Let κn
κmn ,n . Then, if d =
dn = 3κn , we have

Consider the problem of reconstructing a vector xn ∈
o
d
[0, 1]n from d < n random linear measurements yo =
Axn . The MCP algorithm proposed in [15] reconstructs xn
o
o
d
from its linear measurements yo by solving the following
optimization problem:
K

xn − xn
ˆo
o
≤ 2

III. M INIMUM COMPLEXITY PURSUIT

x

>0

Therefore, for n large enough,

in probability.

min
n

d 2

Hence, ﬁxing t > 0 and setting τn = τ = 0.1, for any
and large enough values of n we have
√
√
( nd−1 + t + 1 + 1) n2−2m+2
≤ .
τn

1
K [·]m (X1 , X2 , . . . Xn ) → 1
mn

[·]m

2

d

≤ 22κm,n m e 2 (1−τn +2 log τn ) + e− 2 t .
Theorem 1 can be proved following the steps used in the
proof of Theorem 2 in [15]. To interpret this theorem, in
the following we consider several interesting corollaries that
follow from Theorem 1. Note that in all of the results, the
logarithms are to the base of Euler’s number e.
Corollary 1: Assume that xo = (xo,1 , xo,2 , . . .) ∈ [0, 1]∞
and m = mn = log n . Let κn
κmn ,n . Then if dn =
κn log n , for any > 0, we have P ( xn − xn 2 > ) → 0,
ˆo
o
as n → ∞.
Proof: For m = mn = log n and dn = κn log n ,
√
( nd−1 + t + 1 + 1) n2−2mn +2
√
κn log n −1 + (t + 1)n−1 + n−1 . (2)
≤ 2

P

(x1 , . . . , xn )

1
√ xn − xn
ˆo
n o

2

>

→ 0,

as n → ∞, for any > 0.
Proof: Setting τn = n−0.5 , m = mn = log n , and d =
dn = 3κn in Theorem 1, it follows that

(1)

Let the elements of A ∈ Rd×n , Aij , be i.i.d. N (0, 1).3
d
Let xn = xn (yo , A) denote the output of (1) to inputs yo =
ˆo
ˆo d
n
Axo and A. Theorem 1 stated below is a generalization of
Theorem 2 proved in [15].

P

1
√ xn − xn
ˆo
n o

2

>2
−1

≤ 22κn log n e1.5κn (1−n

2 See

Chapter 14 of [11] for the exact deﬁnition of a universal computer,
and more details on the deﬁnition of Kolmogorov complexity.
3 Note that in [15] we had assumed that A
i,j ∼ N (0, 1/d).

√
d−1 + (t + 1)n−1 + 2 n−1
n
−log n)

= e−(1.5−2 log 2)κn log n+κn (1.5−1.5n

2

d 2

+ e− 2 t
−1

)

d 2

+ e− 2 t .

Since 1.5 − 2 log 2 > 0, for any
we have
2

Let en
xn − [xn ]m and en
ˆm
xn − [ˆn ]m denote the
ˆo
xo
m
o
o
quantization errors of the original and the reconstructed signals, respectively. Using these deﬁnitions, and the CauchySchwartz inequality, we ﬁnd a lower bound for A(ˆn −
xo
xn ) 2 as
o 2

> 0 and n large enough,

√
d−1 + (t + 1)n−1 + 2 n−1 < .
n

1
It follows that P( √n xn − xn
ˆo
o

2

> ) → 0, as n → ∞.

A(ˆn − xn )
xo
o

= A([ˆn ]m + en − [xn ]m − en )
xo
ˆm
o
m

In other words, if we are interested in the normalized
mean square error, or per element squared distance, then 3κn
measurements are sufﬁcient.

= A([ˆn ]m − [xn ]m ) + A(ˆn −
xo
em
o
≥ A([ˆn ]m − [xn ]m )
xo
o

IV. S TABILITY ANALYSIS OF MCP

− 2 A(ˆn − en )
em
m

d
Aˆn − yo
xo

2
2

≤ ([ˆn ]m − [xn ]m )T AT wd + en − en
xo
ˆm
o
m

d
Expanding Aˆn − yo
xo
follows that

A(ˆn − xn )
xo
o

2
2

2
2

= wd 2 .
2

= Aˆn − Axn − wd
xo
o

2
2

+ wd

wd

2
2

2
2

2
2

Note that |S| ≤ 22κm,n m . Deﬁne the event E1 as
{∀ hn ∈ S : (wd )T Ahn ≤ t1 hn 2 }.

E1

For any ﬁxed hn , Ahn is an i.i.d. zero-mean Gaussian
vector of length d and variance hn 2 . Assuming that
2
hn 2 = 1 and applying Lemma 3, we obtain
P (wd )T Ahn ≥ t1 = P

w d 2 G ≥ t1
√
= P wd 2 G ≥ t1 , wd 2 ≥ dσ(1 + t2 )
√
+ P wd 2 G ≥ t1 , wd 2 < dσ(1 + t2 )
√
≤ P wd 2 ≥ dσ(1 + t2 )
√
+ P G ≥ t1 ( dσ(1 + t2 ))−1

(6)

in (6), it

2

≤ e−dt2 /2 + e

t2

1
− 2σ2 d(1+t

2)

2
2

c
P(E1 ) ≤ 22κm,n m e−dt2 /2 + e

≤ 2 (w )

A(ˆn
xo

−

xn )
o

(11)

t2

1
− 2σ2 d(1+t

2)

.

Note that

≤ 2(wd )T A(ˆn − xn )
xo
o
d T

.

Hence, by the union bound and the fact that |S| ≤ 22κm,n m
[11], we have

− 2(wd )T A(ˆn − xn ) ≤ wd 2 .
xo
o
2
(7)

from both sides of (7), we obtain

A(ˆn − xn )
xo
o

AT w d 2 .
(9)

S = {hn − hn : K(hn ) ≤ κm,n m, K(hn ) ≤ κm,n m} .
1
2
1
2

2

Canceling wd

2

Let set S be the set of all vectors of length n that can be
written as the difference of two vectors with complexity less
than km,n m; that is,

d
≤ Axn − yo
o

= Axn −
o

(8)

For any x ∈ [0, 1], 0 ≤ x − [x]m < 2−m . Therefore,
√
(10)
en − en 2 ≤ n2−2m+2 .
ˆm
m

>

2
2
Axn −
o

.

em
xo
≤ ([ˆn ]m − [xn ]m )T AT wd + (ˆn − en )T AT wd
m
o

(4)

(2κm,n m)σ
→ 0,
(5)
ρd
√
as n → ∞, where ρ (1 − r−1 )2 /2.
According Theorem 2, as long as d > 8rκn log n the
algorithm is stable in the sense that the reconstruction
error is proportional to the variance of the input noise. By
increasing the number of measurements one may reduce the
reconstruction error.
Proof: Since by deﬁnition K [·]m (xn ) = km,n mn , xn is
o
o
also a feasible point in (4). But, by assumption, xn is the
ˆo
solution of (4). Therefore,
2
2

2

xo
(wd )T A(ˆn − xn )
o

2

xn − xn
ˆo
o

A ([ˆn ]m − [xn ]m )
xo
o

2

On the other hand, again using our deﬁnitions plus the
Cauchy-Schwartz inequality, we ﬁnd an upper bound on
|(wd )T A(ˆn − xn )| as
xo
o

Note that κm,n m is an upper bound on the Kolmogorov
complexity of xo at resolution m. The major issue of this
section is to calculate the number of measurements required
for robust recovery in noise.
Theorem 2: Assume that xo = (xo,1 , xo,2 , . . .) ∈ [0, 1]∞ .
For integers m and n, let κm,n denote the information
dimension of xn at resolution m. If m = mn = log n
o
and d = 8rκm,n m, where r > 1, then for any > 0, we
have
P

2
2

xo
ˆm
= ([ˆn ]m − [xn ]m + en − en )T AT wd
o
m

K [·]m (xn ) ≤ κm,n m.

s.t.

2
2

≥ A([ˆn ]m − [xn ]m )
xo
o

d
Axn − yo 2 ,
2

x

2
2
en ) 2
m 2

em
xo
− 2 (ˆn − en )T AT A ([ˆn ]m − [xn ]m )
m
o

In the previous section we considered the case where the
signal is exactly of low complexity and the measurements
are also noise-free. In this section, we extend the results to
d
noisy measurements, where yo = Axn + wd , with wd ∼
o
2
N (0, σ Id ). Assuming that the complexity of the signal is
known at the reconstruction stage, we consider the following
reconstruction algorithm:
arg min
n

2
2

A(ˆn − en )
em
m

.

3

2

≤ σmax (A) en − en 2 .
m
m

(12)

For t3 > 0, deﬁne the event E2 as

Inequality (17) involves a quadratic equation of ∆m 2 .
√
Finding the roots of this quadratic equation, using 1 + x ≤
1 + x/2, and replacing m = log n , we obtain

√
√
σmax (A) < (1 + t3 ) d + n .

(n)

E2

It can be proved that [17]

(2κm,n log n)d−1
√
√
√
+ (γ1 n−1 + γ2 d−1 ) + d−1 γ4 ,
(18)
√
√
where γ1 = √ 1 + t5 (1 + t3 )(1 − t4 )−1 , √ = 1 + t5 (1 −
γ2
t4 )−1 , γ3 = 1 + t2 (1−t4 )−1 and γ4 = 1 + t6 (1−t4 )−1 .
On the other hand, by the union bound,
∆m

(n),c

P E2

≤e

−dt2 /2
3

.

(13)

√
√
But if σmax (A) < (1 + t3 ) d + n, then from (10)
A(ˆn − en )
em
m

≤

2

1 + (1 + t3 )

(n)

d
n

2−m+1 n.

c
c
c
c
c
≤ P(E1 ) + P(E2 ) + P(E3 ) + P(E4 ) + P(E5 ).

(19)
√
Given d = 8rκm,n m, choosing t2 = t4 = 1/ r and
ﬁxing t1 , t3 , t5 , . . . , t8 at appropriate ﬁxed small numbers,
(12), (13), (14), (15) and (16) guarantee that (19) goes to
zero, as n → ∞. Moreover, for chosen parameters, γ3 <
√
√
2/(1 − r−1√ Finally, √ any > 0, for n large enough,
).
for
√
(γ1 n−1 + γ2 d−1 ) + d−1 γ4 < . This concludes the
proof.

(n)

d

(n),c

≤ 22κm,n m e 2 (t4 +log(1−t4 )) .
(n)

Deﬁne the event E4
(n)

(14)

as

{∀ hn ∈ S; Ahn

E4

2
2

< (1 + t5 )d hn 2 },
2

V. R ELATED WORK

Again by the union bound and Lemma 2, it follows that
− d (t5 −log(1−t5 ))
2

(n),c
E4

P

≤ 22κm,n m e

.

The MCP algorithm proposed in [15] is mainly inspired
by [18] and [19]. Consider the universal denoising problem
where θ is corrupted by additive white Gaussian noise as
X n = θ + Z n . The denoiser’s goal is to recover θ from the
noisy observation X n . The minimum Kolmogorov complexity
estimation (MKCE) approach proposed in [18] suggests
ˆ
a denoiser that looks for the sequence θ with minimum
Kolmogorov complexity among all the vectors that are within
some distance of the observation vector X n . [18] shows that
if θi are i.i.d., then under certain conditions, the average
ˆ
marginal conditional distribution of θi given Xi tends to the
actual posterior distribution of θ1 given X1 .
In [18], the authors consider the problem of recovering a
low-complexity sequence from its linear measurements. Let
S(k0 ) {xn ∈ [0, 1]n : K(xn ) ≤ k0 }. Consider measuring
d
xn ∈ S(k0 ) using a d×n binary matrix A. Let yo = Axn . To
o
o
n
d
ˆ
recover xo from measurements yo , [18] suggests ﬁnding xn
arg minxn : yo =Axn K(xn ), and proves that
as xn (yo , A)
ˆ d
d
if d ≥ 2k0 , then this algorithm is able to ﬁnd xn with high
o
probability. Clearly assuming that a real-valued sequence has
a low complexity is very restrictive, and hence S(k0 ) does
not include any of the classes that has been studied in CS
literature. For instance most of the one sparse signals have
inﬁnite Kolmogorov complexity, and hence the result of [18]
does not imply useful information.
In a recent and independent work, [20] and [21] consider a
scheme similar to MCP. For a stationary and ergodic source,
they propose an algorithm to approximate MCP. While the
empirical results are promising, no theoretical guarantees are
provided on either the performance of MCP or their ﬁnal
algorithm.
The notion of sparsity has already been generalized in
the literature in several different directions [3], [4], [9],
[22]. More recently, [22] introduced the class of simple
functions and atomic norm as a framework that uniﬁes some

(15)

Finally, for t6 > 0, deﬁne
(n)

{ AT w d

E5

2
2

≤ nd(1 + t6 )}.

Given wd , AT wd is an n dimensional i.i.d. Gaussian random
vector with variance wd 2 . Hence, by Lemma 2,
2
P

AT w d
≤e

2
2

≥ nγ 2 (1 + t7 ) wd

− n (t7 −log(1+t7 ))
2

2
2

= γ2

.

On the other hand, again by Lemma 2,
P( wd

2
2

d

< d(1 − t8 )) ≤ e 2 (t8 +log(1−t8 )) .

Choosing t6 , t7 , t8 > 0 such that t6 < t7 and 1 + t6 =
(1 − t8 )(1 + t7 ), it follows that
P

AT w d

2
2 ≥ nd(1 + t6 )
T d 2
= P A w 2 ≥ nd(1 + t6 ), wd 2 > d(1 − t8 )
2
+ P AT wd 2 ≥ nd(1 + t6 ), wd 2 < d(1 − t8 )
2
2
d
− n (t7 −log(1+t7 ))
2
2 (t8 +log(1−t8 ))

≤e

+e

.

≤ σγ3

c
c
c
c
c
P ((E1 ∩ E2 ∩ E3 ∩ E4 ∩ E5 )c ) = P(E1 ∪ E2 ∪ E3 ∪ E4 ∪ E5 )

Deﬁne the event E3 as E3
{∀ hn ∈ S : Ahn 2 >
2
n 2
(1 − t4 )d h 2 }. By the union bound and Lemma 2, it
follows that
P E3

2

(16)

Combining (8) and (9) and the upper and lower bounds
derived for the corresponding terms of (8) and (9), and
choosing t1 = 2σ d(1 + t2 )(2κm,n m), with probability
P(E1 ∩ E2 ∩ E3 ∩ E4 ∩ E5 ), the following inequality holds:
√
(1 − t4 ) d ∆m 2
2
√
√
√
√
−m+1
−2
1 + t5 2
n((1 + t3 ) d + n)) ∆m 2
√
− 2 σ 1 + t2 2κm,n m ∆m 2
√
− 2−m+1 1 + t6 n ≤ 0.
(17)

4

because

of the above observations and extends them to some other
signal classes. While all these models can be considered as
subclasses of the general model considered in this paper,
it is worth noting that even though the recovery approach
proposed here is universal, given the incomputibility of
Kolmogorov complexity, it is not useful for practical purposes. Finding practical algorithms with provable performance guarantees is left for future research.
In this paper, we have focused on deterministic signal
models. For the case of random signals, [23] considers the
problem of recovering a memoryless process from its undersampled linear measurements and establishes a connection
between the required number of measurements and the Renyi
entropy of the source. Also, our work is in the same spirit
as the minimum entropy decoder proposed by Csiszar in
[24], which is a universal decoder, for reconstructing an i.i.d.
signal from its linear measurements.

n
i=1

an ,

d
d

2
Zi > d(1 + τ )) ≤ e− 2 (τ −log(1+τ )) .
i=1

The proof of Lemma 2 is presented in [15].
Lemma 3: Let X n and Y n denote two independent Gaussian random vectors with i.i.d. elements. Further assume that
for i = 1, . . . , n, Xi ∼ N (0, 1) and Yi ∼ N (0, 1). Then the
n
distribution of (X n )T y n = i=1 Xi Yi is the same as the
n
distribution of X 2 G, where G ∼ N (0, 1) is independent
of X n 2 .
Proof: We need to show that (X n )T Y n / X n 2 is distributed as N (0, 1) and is independent of X n 2 . To prove
the ﬁrst claim, note that

i=1

On the other hand, given X n / X n
n

i=1

Xi
Xn

2

Yi .

Xi
Xn

Yi ∼ N (0, 1).
2
2

and Y n

[1] D. L. Donoho. Compressed sensing. IEEE Trans. Info. Theory,
52(4):489–509, April 2006.
[2] E. Cand` s, J. Romberg, , and T. Tao. Robust uncertainty principles:
e
Exact signal reconstruction from highly incomplete frequency information. IEEE Trans. Info. Theory, 52(2):489–509, February 2006.
[3] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed minimum rank
solutions to linear matrix equations via nuclear norm minimization.
SIAM Review, 52(3):471–501, April 2010.
[4] R. G. Baraniuk, V. Cevher, M. F. Duarte, and C. Hegde. Model-based
compressive sensing. IEEE Trans. Info. Theory, 56(4):1982 –2001,
April 2010.
[5] Y. C. Eldar, P. Kuppinger, and H. Bolcskei. Block-sparse signals:
Uncertainty relations and efﬁcient recovery. IEEE Trans. on Sig. Proc.,
58(6):3042–3054, 2010.
[6] M. Stojnic, F. Parvaresh, and B. Hassibi. On the reconstruction of
block-sparse signals with an optimal number of measurements. IEEE
Trans. on Sig. Proc., 57(8):3075–3085, 2009.
[7] M. Stojnic. Block-length dependent thresholds in block-sparse compressed sensing. Arxiv preprint arXiv:0907.3679, 2009.
[8] D. Malioutov, M. Cetin, and A.S. Willsky. A sparse signal reconstruction perspective for source localization with sensor arrays. IEEE
Trans. on Sig. Proc., 53(8):3010–3022, August 2005.
[9] M. Vetterli, P. Marziliano, and T. Blu. Sampling signals with ﬁnite
rate of innovation. IEEE Trans. on Sig. Proc., 50(6):1417–1428, June
2002.
[10] C. E. Shannon. A mathematical theory of communication: Parts I and
II. Bell Syst. Tech. J., 27:379–423 and 623–656, 1948.
[11] T. Cover and J. Thomas. Elements of Information Theory. Wiley, New
York, 2nd edition, 2006.
[12] R. J. Solomonoff. A formal theory of inductive inference. Inform.
Contr., 7:224–254, 1964.
[13] A. N. Kolmogorov. Logical basis for information theory and probability theory. IEEE Trans. Info. Theory, 14:662–664, 1968.
[14] G. J. Chaitin. On the length of program for computing binary
sequences. J. Assoc. Comput. Mach., 13:547 – 569, 1966.
[15] S. Jalali and A. Maleki. Minimum complexity pursuit. In Proc. 49th
Allerton Conference on Communication, Control, and Computation,
Monticello, IL, Sep. 2011.
[16] L. Staiger. The Kolmogorov complexity of real numbers. Theoretical
Computer Science, 284(2):455 – 466, 2002.
[17] E. Cand` s, J. Romberg, , and T. Tao. Decoding by linear programming.
e
IEEE Trans. Info. Theory, 51(12):4203–4215, Dec. 2005.
[18] D. L. Donoho. Kolmogorov sampler. Preprint, 2002.
[19] D. L. Donoho, H. Kakavand, and J. Mammen. The simplest solution
to an underdetermined system of linear equations. In Proc. IEEE Int.
Symp. Info. Theory, July 2006.
[20] D. Baron and M. Duarte. Universal MAP estimation in compressed
sensing. In Proc. of 49th Allerton Conference on Communication,
Control, and Computation, Monticello, IL, Sep. 2011.
[21] D. Baron and M. Duarte. Signal recovery in compressed sensing via
universal priors. Arxiv preprint arXiv:1204.2611, 2012.
[22] V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. Willsky. The convex
geometry of linear inverse problems. Preprint, 2010.
[23] Y. Wu and S. Verd´ . Renyi information dimension: Fundamental limits
u
of almost lossless analog compression. IEEE Trans. Info. Theory,
56(8):3721–3748, Aug. 2010.
[24] I. Csiszar. Linear codes for sources and source networks: Error
exponents, universal coding. IEEE Trans. Info. Theory, 28:585–592,
1982.

The following two lemmas are frequently used in our
proofs.
Lemma 2 (χ-square concentration): Fix τ > 0, and let
d
2
Zi ∼ N (0, 1), i = 1, 2, . . . , d. Then, P( i=1 Zi < d(1 −
d
(τ +log(1−τ ))
, and
τ )) ≤ e 2

Xi
Xn

n

R EFERENCES

A PPENDIX A

n

a2 = 1. Therefore, since the distribution of
i
Yi given X n / X n 2 = an is independent of
2

To prove the independence, note that X n / X n
are both independent of X n 2 .

In this paper, we have considered the problem of recovering structured signals from their random linear measurements. We have investigated the minimum complexity pursuit
(MCP) scheme. Our results conﬁrm that if the Kolmogorov
complexity of the signal is upper bounded by κ, then MCP
recovers the signal accurately from O(κ log n) random linear
measurements, which is much smaller than the ambient
dimension. In this paper, we have speciﬁcally proved that
MCP is stable, such that the 2 -norm of the reconstruction
error is proportional to the standard deviation of the noise.

(X n )T Y n
=
Xn 2

n
i=1

i=1

VI. C ONCLUSION

P(

Xi
Xn

(A-1)

2

= an ,

Yi ∼ N (0, 1),
2

5

