Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 20:25:21 2012
ModDate:        Tue Jun 19 12:55:02 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      399083 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565123

Sumset Inequalities for Differential Entropy
and Mutual Information
Ioannis Kontoyiannis

Mokshay Madiman

Department of Informatics
Athens University of Economics & Business
Patission 76
Athens 10434, Greece.
yiannis@aueb.gr

Department of Statistics
Yale University
24 Hillhouse Avenue
New Haven, CT 06511, USA.
mokshay.madiman@yale.edu

with the cardinalities |A|, |B| of A and B, respectively. For
example, we have the obvious bounds,

¨
Abstract—The Plunnecke-Ruzsa sumset theory gives bounds
connecting the cardinality of the sumset A + B deﬁned as
{a + b ; a ∈ A, b ∈ B} with the cardinalities of the original
sets A, B. For example, the sum-difference bound states that,
|A+B| |A| |B| ≤ |A−B|3 , where A−B = {a−b ; a ∈ A, b ∈ B}.
Interpreting the differential entropy h(X) as (the logarithm
of) the size of the effective support of X, the main results
here are a series of natural information-theoretic analogs for
these bounds. For example, the sum-difference bound becomes
the new inequality, h(X + Y ) + h(X) + h(Y ) ≤ 3h(X − Y ),
for independent X, Y . Our results include differential-entropy
¨
versions of Ruzsa’s triangle inequality, the Plunnecke-Ruzsa
inequality, and the Balog-Szemer´ di-Gowers lemma. Versions
e
of most of these results for the discrete entropy H(X) were
recently proved by Tao, relying heavily on a strong, functional
form of the submodularity property of H(X). Since differential
entropy is not functionally submodular, in the continuous case
many of the corresponding discrete proofs fail, in several cases
requiring substantially new proof strategies. The basic property
that naturally replaces functional submodularity is the data
processing property of mutual information.

max{|A|, |B|} ≤ |A + B| ≤ |A| |B|,

(1)

as well as much more subtle results, like the Ruzsa triangle
inequality [10],
|A − C| ≤

|A − B| |B − C|
,
|B|

(2)

or the sum-difference bound [10],
|A + B| ≤

|A − B|3
,
|A| |B|

(3)

all of which hold for arbitrary subsets A, B, C of the integers
or any other discrete abelian group, and where A−B = {a−b :
a ∈ A, b ∈ B}.
Recall that Shannon’s asymptotic equipartition property
(AEP) [2] says that the entropy H(X) of a discrete random variable X can be thought of as the logarithm of the
effective cardinality of the alphabet of X. This suggests
a correspondence between bounds for the cardinalities of
sumsets like, e.g., |A + B|, and corresponding bounds for the
entropy of sums of discrete random variables, e.g., H(X +Y ).
First identiﬁed by Ruzsa [11], this connection has also been
explored in the last few years in different directions by, among
others, Tao and Vu [15], Lapidoth and Pete [6], Madiman and
Kontoyiannis [8], and Madiman, Marcus and Tetali [9].
Most recently (and most extensively), this connection was
developed by Tao in [13]. The main idea is to replace sets
by (independent, discrete) random variables, and then replace
the log-cardinality, log |A|, of each set A by the (discrete,
Shannon) entropy of the corresponding random variable. Thus,
for independent discrete random variables X, Y, Z, the simple
bounds (1) become [2], respectively,

I. I NTRODUCTION
The ﬁeld of additive combinatorics [14] provides tools that
allow us to count the number of occurrences of particular
additive structures in speciﬁc subsets of a discrete group. The
prototypical example is the study of the existence of arithmetic
progressions within speciﬁc sets of integers – as opposed to
the multiplicative structure that underlies prime factorization
and much of classical combinatorics and number theory. There
have been several major developments and a lot of highproﬁle mathematical activity in connection with additive combinatorics in recent years, perhaps the most famous example
being the celebrated Green-Tao theorem on the existence of
arbitrarily long arithmetic progressions within the set of prime
numbers.
The Pl¨ nnecke-Ruzsa sumset theory offers an important
u
collection of tools, consisting primarily of sumset inequalities
[14]. The sumset A+B of two discrete sets A and B is deﬁned
as, A + B = {a + b : a ∈ A, b ∈ B}, and a sumset inequality
is an inequality connecting the cardinality |A + B| of A + B

H(X), H(Y ) ≤ H(X + Y ) ≤ H(X) + H(Y ),
and Ruzsa’s bounds (2) and (3) become [13],

0 I.K.

was supported, in part, by a Marie Curie International Outgoing Fellowship, PIOF-GA-2009-235837. M.M. was supported by the NSF CAREER
grant DMS-1056996 and by NSF grant CCF-1065494.

H(X − Z) + H(Y ) ≤ H(X − Y ) + H(Y − Z)
and H(X + Y ) + H(X) + H(Y ) ≤ 3H(X − Y ).

1

Following recent work reported in [7][8], our main motivation is to examine the extent to which this analogy can
be carried further: According to the AEP [2], the differential
entropy h(X) of a continuous random variable X can be
thought of as the logarithm of the “size of the effective
support” of X. In this work we state and prove natural
“differential entropy analogs” of various sumset bounds, many
of which were proved for the discrete Shannon entropy in
[4][15][7][11][13][9].
The main technical ingredient in the proofs of many of the
corresponding discrete bounds was a strong, functional form
of the submodularity property of the discrete Shannon entropy;
see Section II. The fact that differential entropy is not functionally submodular was the source of the main difﬁculty as well
as the excitement for the present development. Instead, the
main technical ingredient in our proofs is the data processing
property of mutual information. Indeed, most of our results can
be equivalently stated in terms of mutual information instead
of differential entropy. And since data processing is universal
in that it holds regardless of the space in which the relevant
random variables take values, these proofs offer alternative
derivations for the discrete counterparts of these results.
In view of the fact that additive noise is one of the most
common modeling assumptions in Shannon theory, it is natural
to expect that the bounds developed here may have applications in core information-theoretic problems. Preliminary
connections in this direction can be found in the recent work
[1][3][16].

gives the upper bound,
H(X + Y ) ≤ H(X) + H(Y ).

Our starting point is the recent work of Tao [13], where a
series of sumset bounds are established for H(X), beginning
with the elementary inequalities (4) and (5). The arguments in
[13] are largely based on the following important observation
[13][9]:
Lemma 2.1 (Functional submodularity of discrete entropy):
If X0 = F (X1 ) = G(X2 ) and X12 = R(X1 , X2 ), then:
H(X12 ) + H(X0 ) ≤ H(X1 ) + H(X2 ).
Proof. By data processing for mutual information and entropy, H(X1 ) + H(X2 ) − H(X12 ) ≥ H(X1 ) + H(X2 ) −
H(X1 , X2 ) = I(X1 ; X2 ) ≥ I(X0 ; X0 ) = H(X0 ).
Our main goal in this work is to examine the extent to which
the bounds in [13] and in earlier work extend to the continuous
case. The differential entropy h(X) of a continuous random
variable (or random vector) X is deﬁned in nats, and in order
to avoid uninteresting technicalities, we assume throughout
that the differential entropies in the statements of all our results
exist and are ﬁnite.
The ﬁrst important difference between H(X) and h(X)
is that the differential entropy of function of X is typically
different from that of X itself, even for linear functions [2]:
For any continuous random vector X and any nonsingular
matrix T , h(T X) = h(X) + log |det(T )|, which is different
from h(X) unless T has determinant equal to ±1.
The upper bound in (5) also fails in general for independent
continuous X, Y : Take, e.g., X, Y to be independent Gaussians, one with variance σ 2 > 2πe and the other with variance
1/σ 2 . And the functional submodularity Lemma 2.1 similarly
fails for differential entropy. For example, taking X1 = X2
an arbitrary continuous random variable with ﬁnite entropy,
F (x) = G(x) = x and R(x, x ) = ax for some a > 1,
the obvious differential-entropy analog of Lemma 2.1 yields
log a ≤ 0.
On the other hand, the simple lower bound in (5) does
generalize,

II. P RELIMINARIES
Throughout the paper, log denotes the natural logarithm
loge , the discrete entropy is deﬁned in nats (in terms of loge ),
and the alphabet of any discrete random variable X is assumed
to be a (ﬁnite or countably inﬁnite) subset A of the real line
or of an arbitrary discrete abelian group. Perhaps the simplest
bound on the entropy H(X+Y ) of the sum of two independent
random variables X, Y is,
H(X + Y ) ≥ max{H(X), H(Y )},
which easily follows from elementary properties [2],
H(X) + H(Y )

= H(X, Y ) = H(Y, X + Y )
= H(X + Y ) + H(Y |X + Y )
≤ H(X + Y ) + H(Y ),

h(X + Y ) ≥ max{h(X), h(Y )},

(4)

and similarly with the roles of X and Y interchanged. The
ﬁrst and third equalities follow from the chain rule and
independence, the second equality follows from the “data
processing” property that H(F (Z)) = H(Z) if F is a oneto-one function, and the inequality follows from the fact that
conditioning reduces entropy.
A similar argument using the nonnegativity of conditional
entropy [2],
H(X) + H(Y )

(5)

(6)

and is equivalent to the data processing inequality,
min{I(X + Y ; X), I(X + Y ; Y )} ≥ 0,
as can be easily seen using standard properties of the entropy
and mutual information.
Our overall development will be largely based on the idea
that the use of functional submodularity can be avoided
by reducing the inequalities of interest to data-processing
inequalities for appropriately deﬁned mutual informations.
This reduction is sometimes straightforward, but sometimes
far from obvious.

= H(Y, X + Y )
= H(X + Y ) + H(Y |X + Y )
≥ H(X + Y ),

2

Lemma 3.4: If X, Y, Z are independent, then,

III. D IFFERENTIAL E NTROPY S UMSET B OUNDS
Unless explicitly stated otherwise, all random variables
are assumed to be continuous (i.e., taking real values with
an absolutely continuous density with respect to Lebesgue
measure), and the differential entropy of any random variable
or random vector appearing in the statement of any of our
results is assumed to exist and be ﬁnite.

h(X + Y + Z) + h(Y ) ≤ h(X + Y ) + h(Y + Z),
which is equivalent to the data processing inequality,
I(X + Y + Z; X) ≤ I(X + Y ; X).
The proof of Lemma 3.4 is given in [7][8]. Combining the
last two lemmas, yields:

A. Ruzsa distance and the doubling and difference constants

Theorem 3.5 (Doubling-difference inequality): If X1 , X2
are independent and identically distributed (i.i.d.), then:

In analogy with the corresponding deﬁnition for discrete
random variables [13], we deﬁne the Ruzsa distance between
any two continuous random variables X and Y as,

h(X1 + X2 ) − h(X1 )
1
≤
≤ 2.
2
h(X1 − X2 ) − h(X1 )

1
1
distR (X, Y ) = h(X − Y ) − h(X ) − h(Y ),
2
2
where X ∼ X and Y ∼ Y are independent. It is obvious
that distR is symmetric, and it is nonnegative because of the
lower bound in (6). Our ﬁrst result states that it also satisﬁes
the triangle inequality:

Equivalently:
1
I(X1 + X2 ; X2 )
≤
≤ 2.
2
I(X1 − X2 ; X2 )
If we deﬁne the doubling constant and the difference constant of a random variable X as,

Theorem 3.1 (Ruzsa triangle inequality): If X, Y, Z are independent, then:

σ[X] = exp{h(X + X ) − h(X)}
and

h(X − Z) ≤ h(X − Y ) + h(Y − Z) − h(Y ).

respectively, where X is an independent copy of X, then
Theorem 1 says that:

Equivalently, for arbitrary random variables X, Y, Z:

Corollary 3.6: For any random variable X,
1
distR (X, X) ≤ distR (X, −X) ≤ 2distR (X, X),
2
equivalently,
δ[X]1/2 ≤ σ[X] ≤ δ[X]2 .

distR (X, Z) ≤ distR (X, Y ) + distR (Y, Z).
The proof of the discrete version of this result in [13] is
based on the discrete entropy analog of the bound,
h(X, Y, Z) + h(X − Z)≤h(X − Y, Y − Z) + h(X, Z),

δ[X] = exp{h(X − X ) − h(X)},

(7)

Note. As mentioned on pp. 64-65 of [14], the analog of the
above upper bound, σ[X] ≤ δ[X]2 , in additive combinatorics
is established via an application of the Pl¨ nnecke-Ruzsa inu
equalities. It is interesting to note that the entropy version
of this result (both in the discrete and continuous case) can
be deduced directly from elementary arguments. Perhaps this
is less surprising in view of the fact that strong versions of
the Pl¨ nnecke-Ruzsa inequality can also be established by
u
elementary methods in the entropy setting. See Section III-B
and the discussion in [12].

which is proved using functional submodularity. Although in
general it fails for differential entropy, we may try to adapt
the proof of Lemma 2.1 itself in this particular setting. But the
obvious modiﬁcation of the discrete proof in the continuous
case also fails; the analog of the ﬁrst inequality in the proof
of Lemma 2.1, corresponding to H(X12 ) ≤ H(X1 , X2 ), is,
h(X, Y, Z) ≤ h(X − Y, Y − Z, X, Z),
which is false, since the last term, h(X − Y, Y − Z, X, Z) =
−∞. Nevertheless, the actual inequality (7) does hold true.

The proofs of Theorem 3.5 and Corollary 3.6 are given
in [8].

Lemma 3.2: The inequality (7) holds true for any three
independent random variables X, Y, Z, and it is equivalent to
the following data processing inequality:

We now come to the ﬁrst result whose proof in the continuous case is necessarily signiﬁcantly different than its discrete
counterpart. It is also the only major result here for which we
give a complete proof. For the proofs of all other results, see
[5].

I(X; (X − Y, Y − Z)) ≥ I(X; X − Z)
The proof of Theorem 3.1 based on Lemma 3.2 and the
proof of the lemma itself are both given in [8].
Replacing Y by −Y , the triangle inequality yields:

Theorem 3.7 (Sum-difference inequality): For any two independent random variables X, Y :

Lemma 3.3: If X, Y, Z are independent, then:

h(X + Y ) ≤ 3h(X − Y ) − h(X) − h(Y ).

h(X − Z) + h(Y ) ≤ h(X + Y ) + h(Y + Z).

(8)

Equivalently, for any pair X, Y ,

In a similar vein we also have:

distR (X, −Y ) ≤ 3distR (X, Y ).

3

(9)

The equivalence of (9) and (8) follows simply from the
deﬁnition of the Ruzsa distance. Before giving the proof, we
state and prove the following simple version of the theorem
in terms of mutual information:

Lemma 3.10: Under the assumptions of Proposition 3.9:
h(Z, Y1 , Y2 ) + h(Z) − h(Y1 ) − h(Y2 ) = h(X1 ) + h(X2 ).
Proof. Expanding and using elementary properties, we have
that h(Z, Y1 , Y2 ) + h(Z) − h(Y1 ) − h(Y2 ) equals,

Corollary 3.8 (Sum-difference inequality for information):
For any pair of independent random variables X, Y , and all
0 ≤ α ≤ 1:

h(Y1 , Y2 |Z) + 2h(Z) − h(Y1 ) − h(Y2 )

αI(X + Y ; X) + (1 − α)I(X + Y ; Y )

= h(Y1 |Z) + h(Y2 |Z) + 2h(Z) − h(Y1 ) − h(Y2 )

≤ (1 + α)I(X − Y ; X) + (1 + (1 − α))I(X − Y ; Y ).

= h(Y1 , Z) + h(Y2 , Z) − h(Y1 ) − h(Y2 )
= 2h(Z) − I(Y1 ; Z) − I(Y2 ; Z)

Proof. Applying (8) with X, Y in place of X , Y , and
subtracting h(X) from both sides, yields,

= h(Z|Y1 ) + h(Z|Y2 )

h(X + Y ) − h(X) ≤ 3h(X − Y ) − 2h(X) − h(Y ),

= h(X1 − Y1 |Y1 ) + h(X2 − Y2 |Y2 )
= h(X1 ) + h(X2 ),

or equivalently,
h(X + Y ) − h(X + Y |Y ) ≤2[h(X − Y ) − h(X − Y |Y )]

as claimed.

+ [h(X − Y ) − h(X − Y |X)],
Proof of Proposition 3.9. The most important step of the
proof is the realization that the (trivial) result (13) needs to be
replaced by the following:

which, in terms of mutual information becomes,
I(X + Y ; Y ) ≤ 2I(X − Y ; Y ) + I(X − Y ; X).

(10)

Repeating the same argument, this time subtracting h(Y )
instead of h(X) from both sides, gives,
I(X + Y ; X) ≤ 2I(X − Y ; X) + I(X − Y ; Y ).

h(Z, X3 , Y1 , Y2 , Y3 ) + h(X3 + Y3 )
≤ h(X3 , Y3 ) + h(X3 − Y2 , X1 − Y3 , X2 , Y1 ).

(14)

(11)
Before establishing (14) we note that it implies,

Multiplying (10) by α, (11) by (1 − α), and adding the two
inequalities gives the stated result.

h(X3 + Y3 ) ≤ h(X3 − Y2 ) + h(X1 − Y3 )

The main result (8) of Theorem 3.7 is a simple consequence
of the following proposition.

+ h(X2 ) + h(Y1 ) − h(Z, Y1 , Y2 ),
which, combined with Lemma 3.10, gives the required result.
To establish (14) we ﬁrst note that, by construction, X1 −
Y1 = X2 − Y2 = Z, therefore,

Proposition 3.9: Suppose X, Y are independent, let Z =
X − Y , and let (X1 , Y1 ) and (X2 , Y2 ) be two conditionally
independent versions of (X, Y ) given Z. If (X3 , Y3 ) ∼ (X, Y )
are independent of (X1 , Y1 , X2 , Y2 ), then:

X3 + Y3

h(X3 + Y3 )+h(X1 ) + h(Y2 )

= X3 + Y3 + (X2 − Y2 ) − (X1 − Y1 )
=

(X3 − Y2 ) − (X1 − Y3 ) + X2 + Y1 ,

≤ h(X3 − Y2 ) + h(X1 − Y3 ) + h(Z). (12)
and hence, by data processing for mutual information,
The proof of the discrete analog of the bound (12) in [13]
contains two important steps, both of which fail for differential
entropy. First, functional submodularity is used to deduce the
discrete version of,

I(X3 ; X3 + Y3 ) ≤ I(X3 ; X3 − Y2 , X1 − Y3 , X2 , Y1 ),
or, equivalently, h(X3 + Y3 ) − h(Y3 ) equals

h(X1 , X2 , X3 , Y1 , Y2 , Y3 ) + h(X3 + Y3 )
≤ h(X3 , Y3 ) + h(X3 − Y2 , X1 − Y3 , X2 , Y1 ),

h(X3 + Y3 ) − h(X3 + Y3 |X3 )

(13)

≤ h(X3 ) + h(X3 − Y2 , X1 − Y3 , X2 , Y1 )

but (13) is trivial because the ﬁrst term above is equal
to −∞. Second, the following simple mutual information
identity (implicit in [13]) fails: If Z = F (X) and X, X
are conditionally independent versions of X given Z, then
I(X; X ) = H(Z). Instead, for continuous random variables,
Z and X are conditionally independent given X , and hence,

− h(X3 − Y2 , X1 − Y3 , X2 , Y1 , X3 )
= h(X3 ) + h(X3 − Y2 , X1 − Y3 , X2 , Y1 )
− h(Z, Y1 , Y2 , Y3 , X3 ),
where the last equality follows from the fact that the linear
map, (z, y1 , y2 , y3 , x3 ) → (x3 − y2 , y1 + z − y3 , y2 + z, y1 , x3 ),
has determinant 1. Rearranging and using the independence of
X3 and Y3 gives (14) and completes the proof.

I(X; X ) ≥ I(X; Z) = h(Z) − h(Z|X) = +∞.
Instead of this, we will use:

4

Following the corresponding development in [13] for discrete random variables, ﬁrst we establish a weaker result in
the following proposition.

B. The differential entropy Pl¨ nnecke-Ruzsa inequality
u
In additive combinatorics, the Pl¨ nnecke-Ruzsa inequality
u
for iterated sumsets is a subtle result with an involved proof
based on the theory of commutative directed graphs; see
Chapter 6 of [14]. It is interesting that its entropy version
can be proved as a simple consequence of the data processing
bound in Lemma 3.4; see [5] for details.

Proposition 3.14: (Weak Balog-Szemer´ di-Gowers lemma)
e
Under the assumptions of Theorem 3.13, we have:

Theorem 3.11 (Pl¨ nnecke-Ruzsa inequality): Suppose that
u
the random variables X, Y1 , Y2 , . . . , Yn are independent,
and that, for each i, Yi is only weakly dependent on (X +
Yi ), in that I(X + Yi ; Yi ) ≤ log Ki for ﬁnite constants
K1 , K2 , . . . , Kn . In other words,

The proofs of the last two results are both signiﬁcantly
different from the proofs of the corresponding discrete versions
in [13]. The main step, in each case, is the identiﬁcation of
the “correct” data processing bound that needs to replace the
use of functional submodularity. See [5] for details.

h(X + Yi ) ≤ h(X) + log Ki ,

h(X1 − X2 |Y ) ≤ h(X) + 4 log K.

R EFERENCES

for each i.

[1] A.S. Cohen and R. Zamir. Entropy ampliﬁcation property and the loss
for writing on dirty paper. Information Theory, IEEE Transactions on,
54(4):1477 –1487, April 2008.
[2] T.M. Cover and J.A. Thomas. Elements of Information Theory. J. Wiley,
New York, 1991.
[3] R.H. Etkin and E. Ordentlich. The degrees-of-freedom of the Kuser Gaussian interference channel is discontinuous at rational channel
coefﬁcients. Information Theory, IEEE Transactions on, 55(11):4932
–4946, Nov. 2009.
[4] V.A. Kaimanovich and A.M. Vershik. Random walks on discrete groups:
Boundary and entropy. The Annals of Probability, 11(3):pp. 457–490,
1983.
[5] I. Kontoyiannis and M. Madiman. Sumset and inverse sumset inequalities for differential entropy and mutual information. Preprint, 2012.
[6] A. Lapidoth and G. Pete. On the entropy of the sum and of the difference
of two independent random variables. Proc. IEEEI 2008, Eilat, Israel,
2008.
[7] M. Madiman. On the entropy of sums. In Information Theory Workshop,
2008. ITW ’08. IEEE, pages 303–307, May 2008.
[8] M. Madiman and I. Kontoyiannis. The entropies of the sum and
the difference of two IID random variables are not too different. In
Proceedings of the 2010 IEEE International Symposium on Information
Theory, pages 1369 –1372, June 2010.
[9] M. Madiman, A. Marcus, and P. Tetali. Entropy and set cardinality
inequalities for partition-determined functions, with applications to sumsets. Random Structures & Algorithms, 2011, DOI: 10.1002/rsa.20385.
[Online]. Available: http://arxiv.org/abs/0901.0055
[10] I.Z. Ruzsa. Sums of ﬁnite sets. In G.V. Chudnovsky D.V. Chudnovsky
and M.B. Nathanson, editors, Number Theory: New York Seminar.
Springer-Verlag, 1996.
[11] I.Z. Ruzsa. Sumsets and entropy. Random Structures & Algorithms,
34(1):1–10, 2009.
[12] T. Tao. An entropy Pl¨ nnecke-Ruzsa inequality. Blog entry, at
u
http://terrytao.wordpress.com/, October 27, 2009.
[13] T. Tao. Sumset and inverse sumset theory for Shannon entropy.
Combinatorics, Probability and Computing, 19:603–639, 2010.
[14] T. Tao and V. Vu. Additive combinatorics. Cambridge studies in
advanced mathematics. Cambridge University Press, 2006.
[15] T. Tao and V. Vu. Entropy methods. Unpublished notes, available at:
www.math.ucla.edu/˜tao/. 2006.
[16] Y. Wu, S. Shamai (Shitz), and S. Verd´ . A general formula for the
u
degrees of freedom of the interference channel. Preprint, 2011.

Then,
h(X + Y1 + Y2 + · · · + Yn ) ≤ h(X) + log K1 K2 · · · Kn ,
or, equivalently,
I(X+Y1 +Y2 +· · ·+Yn ; Y1 +Y2 +· · ·+Yn ) ≤ log K1 K2 · · · Kn .
By an application of the entropy Pl¨ nnecke-Ruzsa inequality
u
we can establish the following bound on iterated sums; see [5]
for the proof.
Theorem 3.12 (Iterated sum bound): Suppose that X and
Y are independent random variables, let (X0 , Y0 ), (X1 , Y1 ),
. . . , (Xn , Yn ) be i.i.d. copies of (X, Y ), and write Si =
Xi + Yi for the sums of the pairs, i = 0, 1, . . . , n. Then:
h(S0 +S1 +· · ·+Sn ) ≤ (2n+1)h(X +Y )−nh(X)−nh(Y ).
C. The differential entropy Balog-Szemer´ di-Gowers lemma
e
The differential entropy version of the Balog-Szemer´ die
Gowers lemma stated next says that, if X, Y are weakly
dependent and X + Y has small entropy, then there exist
conditionally independent versions of X, Y that have almost
the same entropy, and whose independent sum still has small
entropy.
Theorem 3.13: (Balog-Szemer´ di-Gowers lemma) Suppose
e
that X, Y are weakly dependent in the sense that I(X; Y ) ≤
log K, i.e.,
h(X, Y ) ≥ h(X) + h(Y ) − log K,
for some K ≥ 1, and suppose also that,
1
1
h(X) + h(Y ) + log K,
2
2
Let X1 , X2 be conditionally independent versions of X given
Y , and let Y be a conditionally independent version of Y ,
given X2 and Y ; in other words, the sequence X2 , Y, X1 , Y
forms a Markov chain. Then:
h(X + Y ) ≤

h(X2 |X1 , Y ) ≥ h(X) − log K
h(Y |X1 , Y ) ≥ h(Y ) − log K
1
1
h(X) + h(Y ) + 7 log K.
h(X2 + Y |X1 , Y ) ≤
2
2

5

