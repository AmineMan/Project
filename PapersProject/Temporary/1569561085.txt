Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Sat May  5 07:49:42 2012
ModDate:        Tue Jun 19 12:54:29 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      435246 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569561085

Information Divergence is more χ2-distributed than
the χ2-statistics
Peter Harremo¨ s
e

G´ bor Tusn´ dy
a
a

Copenhagen Business College
Copenhagen, Denmark
Email: harremoes@ieee.org

R´ nyi Institute of Mathematics
e
Budapest, Hungary
Email: tusnady@renyi.hu

Abstract—For testing goodness of ﬁt it is very popular to use
either the χ2 -statistic or G2 -statistics (information divergence).
Asymptotically both are χ2 -distributed so an obvious question
is which of the two statistics that has a distribution that is
closest to the χ2 -distribution. Surprisingly, when there is only one
degree of freedom it seems like the distribution of information
divergence is much better approximated by a χ2 -distribution
than the χ2 -statistic. For random variables we introduce a new
transformation that transform several important distributions
into new random variables that are almost Gaussian. For the
binomial distributions and the Poisson distributions we formulate
a general conjecture about how close their transform are to the
Gaussian. The conjecture is proved for Poisson distributions.

30

25

20

15

10

5

5

We consider the problem of testing goodness-of-ﬁt in
a discrete setting. Here we shall follow the classic approach to this problem as developed by Pearson, Neyman
and Fisher. The question is whether a sample with observation counts (X1 , X2 , . . . , Xk ) has been generated by
the distribution Q = (q1 , q2 , . . . , qk ) . For sample size n
the counts (X1 , X2 , . . . , Xk ) is assumed to have a multinomial distribution. We introduce the empirical distribution
ˆ
P = X1 , X2 , . . . , Xk where n denotes the sample size
n
n
n
n = X1 + X2 + · · · + Xk . Often one uses one of the Csisz´ r
a
[1] f -divergences
k

qj f
j=1

pj
ˆ
qj

.

(1)

is large. Whether

ˆ
Df P , Q is considered to be small or large depends on the
signiﬁcance level [2]. The most important cases are obtained
for the convex functions f (t) = n(t − 1)2 and f (t) = 2nt ln t
leading to the Pearson χ2 -statistic
χ2 =

(Xnj − nqnj )2
nqnj
j=1
k

(2)

or the likelihood ratio statistic
G2 = 2

k

j=1

Xnj ln

Xnj
nqnj

20

25

30

35

which is a scaled version of information divergence that we
will denote D without subscript. In this paper we shall focus
on the case where there are only two bins because this allow
us to formulate in a qualitattive manner in terms of what
we will call the intersection conjecture. With only two bins
the multinomial distribution of counts can be described by
a binomial distribution. We will also consider the limiting
case where the binomial distribution is replaced by a Poisson
distribution. This corresponds in a sense to having only one
bin.
Notation 1: Please note that we follow the notation from
[3] by denoting the likelihood ratio statistic by G2 rather than
G as done in many textbooks and articles. Our G2 should not
be confused with Getis–Ord’s G statistic [4].
One way of choosing between various statistics is by
computing their asymptotic efﬁciency. In 1985 it was proved
that the G2 -statistic is more efﬁcient in the Bahadur sense than
the χ2 -statistic, and this result has been extended in a number
of papers [5]–[9]. The asymptotic Bahadur efﬁciency of G2
implies that a much smaller sample size is needed when using
G2 than when using χ2 if a ﬁxed power should be achieved
at a very small signiﬁcance level for some alternative. Since
this type of result only holds asymptotically for large sample
sizes it may be difﬁcult to use for a speciﬁc ﬁnite sample
size. Therefore we will turn our attention to another important
property for the choice of statistic.

ˆ
The null hypothesis is accepted if the test statistic Df P , Q
ˆ
is small and rejected if Df P , Q

15

Fig. 1. Q-Q plot of a χ2 -distribution against the distribution of the G2 statistic for a symmetric binomial distribution with n = 51. The midpoint of
each step is marked. The red line marks the identity.

I. C HOICE OF STATISTIC

ˆ
Df P , Q =

10

(3)

1

Figure 1 is not satisﬁed when the G2 -statistic is plotted against
the χ2 -distribution so in the rest of this paper a different
type of plots will be used. For getting a better approximation
another strategy is Bartlett’s adjustment, see [10].
T. Dunning [12] has made a summary of what the typical
recommendations are about whether one should use the χ2 statistic or the G2 -Statistic. The short version is that the
statistic is approximately χ2 -distributed when each bin contains at least 5 observations or the calculated variance for
each bin is at least 5, and if any bin contains more than
twice the expected number observations then the G2 -statistic
is preferable to the χ2 -statistic. Our main idea is to change
the statistic into a signed version as it was introduced by
Barndorff-Nielsen as a signed likelihood ratio [13]. We call
the operation G-transform and change our orientation from
hypothesis testing to normal approximation of distributions
of sums of independent variables. Our main observation is
that the G-transform covers probabilities in the whole domain
including large deviations.
Notation 2: In the rest of this paper we will let τ denote
the circle constant 2π and let φ denote the standard Gaussian
density
2
exp − z2
.
τ 1/2
We let Φ denote the distribution function of the standard
Gaussian

35

30

25

20

15

10

5

5

10

15

20

25

30

35

Fig. 2. Q-Q plot of a χ2 -distribution against the distribution of the χ2 statistic for a symmetric binomial distribution with n = 51. The red line
marks the identity.

For the practical use of a statistic it is important to calculate
or estimate the distribution of the statistic. This can be done
by exact calculation, by approximations, or by simulations.
Exact calculations may be both time consuming and difﬁcult.
Simulation often requires statistical insight and programming
skills. Therefore most statistical tests use approximations to
calculate the distribution of the statistic. For a ﬁxed number
of bins the distribution of the χ2 -statistic becomes closer
and closer to the χ2 -distributions as the sample size tends to
inﬁnity. For a large sample size the empirical distribution will
with high probability be close to the generating distribution
and the Csisz´ r f -divergence Df can be approximated by a
a
scaled version of the χ2 -statistic

t

Φ (t) =

φ (z) dz .
−∞

II. T HE G- TRANSFORM AND ITS DISTRIBUTION

f (0) 2
Df (P, Q) ≈
· χ (P, Q) .
2

Here we shall introduce a transformation that is useful for
our understanding of the ﬁne structure of the distribution
of the likelihood ratio statistics. Consider a 1-dimensional
exponential family Pβ where

Therefore the distribution of any f -divergence may be approximated by a scaled χ2 -distribution, i.e. a Γ-distribution.
From this argument one might get the impression that the
distribution of the χ2 -statistic is closer to the χ2 -distribution.
Figure 1 and Figure 2 show that this is far from the the case.
Both ﬁgures are Q-Q plots where for each p ∈ [0, 1] a point is
plottet with the p quantile the square of a standard Gaussian
as ﬁrst coordinate and the p quantile of the distribution of
the statistic as the second coordinate. Figure 1 shows that
the G2 -statistic is almost as χ2 -distributed as it can be when
one takes into account that the likelihood ratio statistic has
a discrete distribution. Each step is intersected very close
to its midpoint. Figure 2 shows that the distribution of the
χ2 -statistic deviates systematically from the χ2 -distribution
for small signiﬁcance levels. For larger signiﬁcance levels
both statistics will give approximately the same results which
is related to the fact that the two statistics have the same
asymptotic Pitman efﬁciency. The two plots show that at least
in some cases the distribution of the G2 -statistic is much
closer to a χ2 -distribution than Pearson statistic is. The next
question is whether there are situations where the likelihood
ratio statistic is not approximately χ2 -distributed. For binomial
distributions that are very skewed the intersection property of

exp (β · x)
dPβ
(x) =
dP0
Z (β)
and Z denotes the partition function given by
Z (β) =

exp (β · x) dP0 x .

Let P µ denote the element in the exponential family with
mean value µ. Let µ0 denote the mean value of P0 . Then
D (P µ P0 ) =

ln

dP µ
(x)
dP0

dP µ x.

To verify that D (P µ P0 ) is χ2 -distributed it is sufﬁcient
to verify that the square root is a centered Gaussian. This
motivates the next deﬁnition:
Deﬁnition 3: Let X be a random variable with distribution
P0 . Then the G-transform G (X) of X is the random variable
given by
1/2

G (x) =

2

− (2D (P x P0 )) , for x < µ0 ;
1/2
(2D (P x P0 )) , for x ≥ µ0 .

6

6

4

4
2

2

−4

−2

2

4

−6

6

−4

−2

2

4

6

−2
−2
−4
−4
−6

Fig. 3.

Symmetric binomial distribution with n = 30.

Fig. 6.

6

These plots support the following conjecture:
Conjecture 4 (The intersection property): Let M denote a
binomial distributed or Poisson distributed random variable
and let G (M ) denote the G-transform of M. The quantile
transform between G (M ) and a standard Gaussian Z is a
step function and the identity function intersects each step,
i.e.

4

2

−4

−2

2

4

Poisson distribution with mean equal to 20.

6

−2

−4

P (M < m) < P (Z ≤ G (m)) < P (M ≤ m)
for all integers m.
Another way of reformulating the intersection property is
that in the stochastic ordering X should be less than a random
variable with point probabilities Φ (G (m)) − Φ (G (m − 1))
and greater than a random variable with point probabilities
Φ (G (m + 1)) − Φ (G (m)) , where G (−1) is deﬁned as −∞
and G (n + 1) is deﬁned to be ∞ for a binomial distribution
number parameter n. The conjecture is so well supported by
numerical calculations that we would not hesitate to recommend it for estimation of tail probabilities for the binomial
distributions in goodness of ﬁt tests instead of using the usual
χ2 -approximation of the χ2 -statistic.
As we see both skewed binomial distributions and Poisson
distributions have different step sizes for positive and negative
values. Although the quantile transform between G (M ) and
a standard Gaussian has the intersection property interference
between the step sizes may have the effect that the quantile
transform between the G2 -statistic and the χ2 -distribution does
not necessarily have the intersection property. We believe that
the G-transform is always closer to a standard Gaussian than
the original. We have no idea, which distributions have the
intersection property.

Fig. 4. Binomial distribution with success probability equal to 0.3 and n =
30.

Using G (x) instead of D (P x P0 ) as statistic is essentially
the difference between using a one-sided test instead of a twosided test. With this deﬁnition one easily sees that the Gtransform of a Gaussian is a standard Gaussian. In [14] it
was veriﬁed that if a random variable X satisﬁes a Cram´ r
e
n
1
condition the then with a minor correction Gn n i=1 Xi
1
is Gaussian within a factor of the order 1 + O n . In this
paper we are interested in sharp bounds rather than asymptotic
results.
Now we can make quantile plots of the Gaussian distribution
against the distribution of the G-transform of various random
variables. On Figure 3-7 the G-transform of some binomial
and Poisson distributions are compared with the standard
Gaussian via their Q-Q plot. In these plots the red lines
correspond to the bounds P (X ≤ x) ≤ exp (−D (P x P0 ))
for x ≤ µ0 and P (X ≥ x) ≤ exp (−D (P x P0 )) for x ≥ µ0 .

III. T HE LINK TO WAITING TIMES
6

Hitherto we have discussed inequalities for discrete distributions but there is an interesting link to inequalities for continuous distributions associated with waiting times. Assume
that M is Poisson distributed with mean t and T is Gamma
distributed with shape parameter m and scale parameter 1, i.e.
the distribution of the waiting time until m observations from
an Poisson process with intensity 1. Then

5
4
3
2
1

1

2

3

4

5

6

−1

P (M ≥ m) = P (T ≤ t) .
Fig. 5. Binomial distribuiton with success probability equal to 0.1 and n=30.

The Gamma distribution Γ (m, θ) has density

3

(4)

is an increasing then X1 is less than X2 in the usual stochastic
ordering.
Theorem 6: The G-transform of a Gamma distributed random variable is less than a standard Gaussian in the stochastic
ordering.
Proof: Let T be a Γ (m, 1) distributed random variable.
The distribution in the exponential family based Γ (m, 1) with
t
mean t is Γ m, m . The G-transform is

2.5

1.25

0
-2.5

-1.25

0

1.25

2.5

G (t) = ± 2D Γ m,

-1.25

t
dΓ m, m
exp (t − m)
(t) =
.
t m
dΓ (m, 1)
m

Fig. 7. Q-Q plot of a standard Gaussian against Γ(1, 1) (black), Γ(3, 1)
(yellow), Γ(20, 1) (blue), and another standard Gaussian (green). The red
curves are the large deviation bounds.

Let W = G (T ) with density f (w) . We want to prove that
φ(w)
f (w) is increasing. Now

1 m−1
t
1
t
exp −
θm Γ (m)
θ

D (Γ (m, θ1 ) Γ (m, θ2 )) = m

so that
φ (w)
φ (G (t)) G (t)
=
=
f (w)
f (t)

θ1
θ1
− 1 − ln
θ2
θ2

.

In particular

=
t
m

Γ (m, 1)

= t − m − m ln

t
.
m

G (t)
t
dΓ(m, m )
τ 1/2 dΓ(m,t)

(t) · f (t)

Γ (m)
· tG (t) .
τ 1/2 mm exp (−m)

Hence we want to prove that tG (t) is increasing.

Next we note that
t
D (P o (m) P o (t)) = D Γ m,
m

f G−1 (w)
G (G−1 (w))

f (w) =

so the divergence can be calculated as

D Γ m,

Γ (m, 1)

where ± means that we will use + when t is greater than the
mean k and use − when t is less than m. For the Gamma
distribution we have

-2.5

f (t) =

1/2

t
m

tG (t) = ±t

2D (t)
2 (2D)

Γ (m, 1) .

1/2

t
m

= ±m1/2

t
m

2

−1
1/2

t
− 1 − ln m

.

With the substitution u = t/m we have to prove that

If GP is the G-transform for P o (t) and GΓ is the Gtransform for Γ (m, 1) then GP (m) = −GΓ (t) . This shows
that if the G-transforms of the Gamma distributions are close
to a Gaussian then so are the G-transforms of the Poisson
distributions. Figure 7 shows Q-Q plots of the G-transform of
some Gamma distributions.
We see that the ﬁt with a straight line of slope 1 is extremely
good. The point (0,0) is not on the line reﬂecting the fact that
the means and the medians of the Gamma distributions do
not coincide. In the next section we shall see that the quantile
transform between a Gaussian and the G-transform of Gamma
distributions is always below the identity.

±

u−1
(2 (u − 1 − ln u))

1/2

is increasing. We have
d
du

±

u−1
1/2

(2 (u − 1 − ln u))

=±

u − 2 ln u −

1
u

(2 (u − 1 − ln u))

3/2

so we want to prove that
± u − 2 ln u −

1
u

≥ 0.

1
Now we have to prove that (u) = u − 2 ln u − u is positive
for u > 1 and negative for u < 1. Obviously (1) = 0 so it
is sufﬁcient to prove that (u) ≥ 0, but

IV. T HE INCREASING PROPERTY
In this section we shall formulate some conditions that
are stronger than the intersection property. The proof of the
following lemma is an easy exercise so we omit the proof.
Lemma 5: Let f1 and f2 be the densities of the random
variables X1 and X2 with respect to some measure µ on the
real numbers. If
f1
f2

(u) =

1−

1
u

2

≥ 0.

Next we shall formulate an even stronger conjecture and see
that it actually implies that binomial distributions and Poisson
distributions have the intersection property.

4

clear yet. Since we only discuss the cases with one or two
bins our results can be reformulated in terms of conﬁcence
intervals. We hope to cover conﬁdence intervals in a future
paper.
In the present paper the focus has been on the two bin case.
We do not know if something equivalent of the intersection
property can be formulated for more than two bins. For results
on more than two bins it may be better to try to generalize
the results on asymptotics presented in [14].

20
15
10
5
5

10

15

k

20
25

30

35

40

45

50

−5
−10
−15
−20

VI. ACKNOWLEDGEMENT
Fig. 8. Plot of the logarithm of (4) for a symmetric binomial distribution
with n = 50.

The authors want to thank Unnikrishnan Jayakrishnan for
useful discussions. We also want to thank Jen˝ Reiczigel and
o
L´dia Rejt˝ for helping with some numerical computations at
ı
o
an early stage of the developing the ideas presented in this
paper and L´ szl´ Gy¨ rﬁ who we worked in parallel on other
a o
o
aspects of the intersection conjecture. Finally we would like
to thank Sune Jakobsen for comments to this manuscript.

Conjecture 7 (The increasing property): If M is a binomially or Poisson distributed random variable with G-transform
G (M ) then
m→

P (M = m)
.
Φ (G (m + 1)) − Φ (G (m))

(5)
R EFERENCES

is increasing and

[1] I. Csisz´ r, “Eine informationstheoretische Ungleichung und ihre Anwena
dung auf den Beweis der ergodizit¨ t von Markoffschen Ketten,” Publ.
a
Math. Inst. Hungar. Acad., vol. 8, pp. 95–108, 1963.
[2] E. Lehman and G. Castella, Testing Statistical Hypotheses. New York:
Springer, 3rd ed. ed., 2005.
[3] B. S. Everitt, The Cambridge Dictionary of Statistics. Cambridge
University Press, 1998.
[4] T. Zhang, “Limiting distribution of the G statistics,” Statistics and
Probability Letters, vol. 78, p. 16561661, 2008.
[5] M. P. Quine and J. Robinson, “Efﬁciencies of chi-square and likelihood
ratio goodness-of-ﬁt tests.,” Ann. Statist., vol. 13, pp. 727–742, 1985.
[6] J. Beirlant, L. Devroye, L. Gy¨ rﬁ, and I. Vajda, “Large deviations
o
of divergence measures on partitions,” J. Statist. Planning and Infer.,
vol. 93, pp. 1–16, 2001.
[7] L. Gy¨ rﬁ, G. Morvai, and I. Vajda, “Information-theoretic methods
o
in testing the goodness-of-ﬁt,” in Proc. International Symposium on
Information Theory, Sorrento, Italy, June25-30, p. 28, 2000.
[8] P. Harremo¨ s and I. Vajda, “On the Bahadur-efﬁcient testing of unifore
mity by means of the entropy,” IEEE Trans. Inform Theory, vol. 54,
pp. 321–331, Jan. 2008.
[9] P. Harremo¨ s and I. Vajda, “Efﬁciency of entropy testing,” in Internae
tional Symposium on Information Theory, pp. 2639–2643, IEEE, July
2008.
[10] O. E. Barndorff-Nielsen and P. Hall, “On the level-error after Bartlett
adjustment of the likelihood-ratio statistic,” Biometrika, vol. 75, no. 2,
pp. 374–378, 1988.
[11] R. R. Sokal and R. J. Rohlf, Biometry: the principles and practice of
statistics in biological research. New York: Freeman, 1981. ISBN 07167-2411-1.
[12] T. Dunning, “Accurate methods for the statistics of surprise and coincidence,” Computational Linguistics, vol. 19, pp. 61–74, March 1993.
[13] O. E. Barndorff-Nielsen and D. R. Cox, Inference and asymptotics.
Chapman and Hall, 1994.
[14] L. Gy¨ rﬁ, P. Harremo¨ s, and G. Tusn´ dy, “Gaussian approximation of
o
e
a
large deviation probabilities.” Submitted for presentation at ITW, 2012.
[15] P. W. Glynn, “Upper bounds on Poisson tail probabilities,” Operations
Research Letters, vol. 6, pp. 9–14, March 1987.
[16] J. Chen and H. Rubin, “Bounds for the difference between median and
mean of Gamma and Poisson distributions,” Statistics and Probability
Letters, vol. 4, pp. 281–283, 1986.
[17] K. Hamza, “The smallest uniform upper bound on the distance between
the mean and the median of the binomial and Poisson distributions,”
Statistics and Probability Letters, vol. 23, pp. 21–25, 1995.
[18] W. Perkins, M. Tygert, and R. Ward, “χ2 and classical exact tests often
wildly misreport signiﬁcance; the remedy lies in computers.” Uploaded
to ArXiv, September 2011.

P (M = m)
m→
Φ (G (m)) − Φ (G (m − 1))
is decreasing.
The conjecture is supported by numerous numerical computations. If it holds the intersection property follows by
Lemma 5. The increasing property implies log-concavity of
the distribution but for instance the geometric distribution
is log-concave but does not satisfy the intersection property.
We have some indications that the conjecture also holds for
any distribution of a sum of independent Bernoulli random
variables.
Theorem 8: The intersection property is satisﬁed for any
Poisson random variable.
Proof: (Outline) The inequality
P (M < m) ≤ P (Z ≤ G (m))
follows from Theorem 6 combined with Equation 4. The
inequality
P (M ≤ m) ≥ P (Z ≤ G (m))
can be proved case by case for m ≤ 5. For m > 5 it is proved
using the intersection property.
Theorem (8) gives bounds on the tail probabilities for
Poisson distributions that are far better than what can be found
in the literature (see for instance [15]). At the same time the
theorem gives bounds on the median that are compatible with
the bounds in the literature [16], [17].
V. D ISCUSSION
Many goodness-of -ﬁt tests involve parameter estimation
(that is, the model is a parametric family of distributions,
not a single distribution). In such cases, the G2 -statistic may
converge slower to a χ2 -distribution than the χ2 -statistic [18].
How such results are related to the presents results is now

5

