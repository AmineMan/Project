Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 11:43:09 2012
ModDate:        Tue Jun 19 12:55:45 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      333709 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566581

Polar codes for discrete alphabets
Eren Saso˘ lu
¸ ¸ g
UCSD, La Jolla, CA, USA
esasoglu@ucsd.edu

X1 ∈ X = {0, 1} and Y1 is arbitrary. Here, the Xs can be
interpreted as i.i.d. inputs to a memoryless channel, and the
Y s as the output. Alternatively, one can think of the Xs as
the output of an i.i.d. source, and the Y s as side information
about the source. Let N = 2n for n = 1, 2, . . . and deﬁne a
sequence of transforms Gn : {0, 1}N → {0, 1}N recursively
through

Abstract—An open problem in polarization theory is whether
all memoryless channels and sources with composite (that is,
non-prime) alphabet sizes can be polarized with deterministic,
Arıkan-like methods. This paper answers the question in the
afﬁrmative by giving a method to polarize all discrete memoryless
channels and sources. The method yields codes that retain the
low encoding and decoding complexity of binary polar codes.
Index Terms—Channel polarization, source polarization, polar
codes, entropy inequality, non-binary codes.

G0 (u) = u,
I. I NTRODUCTION

Gn (u1 , u2 ) = πn Gn−1 (u1 ) ⊕ Gn−1 (u2 ), Gn−1 (u2 ) ,

Polar coding was introduced as a low-complexity coding
technique for achieving the symmetric-capacity of binary-input
discrete memoryless channels [1]. It was shown later that the
same technique can be used to obtain entropy-achieving source
codes [2].
The low-complexity properties of polar codes are due to
their nested structure, which makes them suitable for recursive
and efﬁcient encoding/decoding algorithms. Their capacityand entropy-achieving properties, on the other hand, are due
to polarization: Applying the polar code transform to a memoryless channel yields a set of (almost) noiseless and (almost)
useless channels, and (almost) no mediocre channels. Polar
channel coding consists in using the noiseless channels for
data transmission. (For the interpretation in source coding,
see [2].) One of the many appeals of this method is that the
code transform to be applied is chosen independently from
the underlying channel or source. That is, a single transform
polarizes all binary-input channels and binary sources.
A method that extends the polarization technique to channels and sources (hereafter, processes) with prime alphabet
sizes was given in [3]. There, it was shown that the technique
falls short of polarizing all processes when the alphabet size
is composite. It was also shown in the same reference that
given any i.i.d. process, there exists a set of transforms that
will yield polarization. One can also achieve optimal coding
rates by viewing a process with composite alphabet size as
a collection of processes with prime alphabets and polarizing
each process separately, although this method in general does
not polarize the original process.
In this note we describe a recursive method to polarize
all i.i.d. processes with arbitrary discrete alphabets. Codes
obtained through this method are similar to binary polar codes
in performance and complexity.

(1)

where u = (u1 , u2 ) and πn is a permutation over the set
{0, 1}N , known as the shufﬂe operator. Now if we deﬁne
N
N
U1 := Gn (X1 ),
i−1
then the entropies H(Ui | Y1N U1 ) polarize, that is, they are
all close to either 0 or 1 when N is large:

Theorem 1 ([1],[2]). For every
1
N →∞ N
1
lim
N →∞ N
lim

> 0,

i−1
i : H(Ui | Y1N U1 ) > 1 −
i−1
i : H(Ui | Y1N U1 ) <

= H(X1 | Y1 ),
= 1 − H(X1 | Y1 ).

It was later shown in [3] and [4] that Theorem 1 also holds
if X = {0, . . . , q −1} for some prime q and the addition in (1)
is carried out modulo-q.
The standard proof of Theorem 1 begins with a martingale
argument that shows the existence of the claimed limits. The
difﬁcult part is to show the values of these limits, which
amounts to proving that almost all entropies created by (1)
are close to 0 or 1. Note that in each recursion the transform
in (1) creates two entropies
H(X1 + X2 | Y12 ) and

H(X2 | Y12 , X1 + X2 )

out of H(X1 | Y1 ), where X1 and Y1 now are shorthand for Ui
i−1
and (Y1N U1 ), respectively. The following result states that
the two entropies above are strictly different if H(X1 | Y1 )
is moderate1 , and thus the only ﬁxed points of this entropy
transformation are 0 and 1, yielding the theorem (see the
mentioned references for details):
Lemma 1 ([4]). For all prime q and > 0, there exists δ > 0
such that H(X1 | Y1 ) ∈ ( , 1 − ) implies

II. P RELIMINARIES

H(X1 + X2 | Y12 ) ≥ H(X1 | Y1 ) + δ.

Let (X1 , Y1 ), (X2 , Y2 ), . . . be a sequence of independent
and identically distributed (i.i.d.) random variables, where

1 Throughout, entropies will be computed with base-q logarithms, and
therefore will be [0, 1]-valued.

1

the transform in (1), obtaining a collection of K polar codes.
It is easy to see that optimal rates in channel and source coding
can also be achieved by this method.
Our purpose here is different. We would like to see whether
there exist transforms that recursively polarize, as in Theorem 1, all i.i.d. processes with a given composite alphabet size.
In the proof of Proposition 1 we already saw a difﬁculty in
polarizing such processes: When X1 is uniformly distributed
on certain subsets of its alphabet, the transform in (3) is
ineffectual, since it also yields random variables that are also
conﬁned to the same subset. In what follows we will still
restrict our attention to transforms of the form (3), but will
choose the mapping f : X 2 → X so as to avoid the described
anomaly. In particular, we will call f a polarizing mapping if
(p.i) for all x2 ∈ X , the mapping x1 → f (x1 , x2 ) is
invertible,
(p.ii) for all x1 ∈ X , the mapping x2 → f (x1 , x2 ) is
invertible,2 and
(p.iii) for all 2 ≤ K ≤ q−1 and distinct a0 , . . . , aK−1 ∈ X ,
the matrix

Observe that the statement of the lemma holds uniformly
over all joint distributions on (X1 , Y1 ) with arbitrary Y1
and moderate H(X1 | Y1 ). A statement of such generality
appears to be necessary for proving polarization, since the
i−1
distributions of (Ui , Y1N U1 ) become arbitrarily complex as
the transformation size grows. Unfortunately, Lemma 1 does
not hold when q is composite:
Example 1. Let X1 be uniformly distributed over X =
{0, 1, 2, 3} and let Y1 ∈ {0, 1} be such that pY |X (0 | 0) =
pY |X (0 | 2) = pY |X (1 | 1) = pY |X (1 | 3) = 1. Then,
H(X1 | Y1 ) = 1/2.
Also let U1 = X1 + X2 and X2 = U2 . Then, the pairs
(X1 , Y1 ), (U1 , Y12 ), and (U2 , Y12 U1 ) are identically distributed
(after appropriate grouping and labelling), and therefore
H(U2 | Y12 U1 ) = H(X1 | Y1 ) = H(U1 | Y12 ).

(2)

That is, the transformation has no effect on the resulting
distributions. Clearly, this also implies that applying the same
transform a second time (and further) will have no effect on
the distributions or on the entropies.

Bij = f (ai , aj ),

The difﬁculty illustrated in the example above is common
to all alphabets X of composite size. It is also not peculiar
to the transform (X1 , X2 ) → (X1 + X2 , X2 ): Suppose that
f is an operation for which the pair (X , f ) is a group, and
consider the mapping (X1 , X2 ) → (U1 , U2 )
U1 = f (X1 , X2 ),

U2 = X2 .

has at least K + 1 distinct entries.
Example 2. Consider a matrix F with Fij = f (i, j), i, j =
0, . . . q−1. (That is, F is the Cayley table of f .) Then it is easy
to see that modulo-3 addition is polarizing whereas modulo-4
addition is not, since their respective Cayley tables are




0 1 2 3
0 1 2
1 2 3 0

G=
F = 1 2 0 ,
2 3 0 1 .
2 0 1
3 0 1 2

(3)

Then we have
Proposition 1. If q is composite, then there exists a > 0 and
a distribution on (X1 , Y1 ) for which H(X1 , Y1 ) ∈ ( , 1 − )
and
H(U2 |

Y12 U1 )

= H(X1 | Y1 ) = H(U1 |

Y12 ).

i, j = 0, . . . , K − 1

Note that G00 = G22 = 0 and G02 = G20 = 2, violating
(p.iii). See also Example 1.

(4)

In the next section, we will show that polarizing mappings
deﬁned through (p.i)–(p.iii) above create two strictly different
entropies out of a moderate one:

Proof: It is known [5, p. 28] that if q is composite, then
the group (X , f ) has a proper nontrivial subgroup. That is,
there exists a set S
X with |S| > 1 such that (S, f ) is
a group. A pair (X1 , Y1 ) where Y1 is constant and X1 is
uniformly distributed over S satisﬁes (4).
The argument above implies that transforms (X1 , X2 ) →
(f (X1 , X2 ), X2 ) with a group operation f fail to polarize
certain i.i.d. processes if applied recursively. On the other
hand, it is easy to show that for all (X1 , Y1 ) with moderate
H(X1 | Y1 ), there exists an invertible mapping (X1 , X2 ) →
(f (X1 , X2 ), X2 ) for which

Theorem 2. For all > 0 there exists δ( ) > 0 such that
if (X1 , Y1 ), (X2 , Y2 ) are i.i.d. random variable pairs with
H(X1 | Y1 ) ∈ ( , 1 − ), and if f : X 2 → X is a polarizing
mapping, then
H(f (X1 , X2 ) | Y12 ) ≥ H(X1 | Y1 ) + δ( ).

(5)

Theorem 2 justiﬁes our deﬁnition of a polarizing mapping, as equation (5) and the invertibility of the mapping
(X1 , X2 ) → (f (X1 , X2 ), X2 ) sufﬁce to achieve polarization
recursively. For this purpose, let us redeﬁne the sequence of
transforms Gn through

H(f (X1 , X2 ) | Y12 ) > H(X1 | Y1 ).
Therefore, all discrete i.i.d. processes can be polarized by
choosing suitable mappings at each recursion [3]. Alternatively, if one is only interested in constructing optimal polarlike channel and source codes for composite alphabet sizes,
then one can also view X1 as a collection of random variables
W1 , . . . , WK with prime alphabet sizes, and polarize the prok−1
cesses {(Wk , (Y1 W1 )}, k = 1, . . . , K separately through

G0 (u) = u,
Gn (u1 , u2 ) = πn f (Gn−1 (u1 ), Gn−1 (u2 )), Gn−1 (u2 ) ,

(6)

2 In group theory, a pair (X , f ) with f satisfying (p.i) and (p.ii) is known
as a quasigroup.

2

and Kν ≤ q − 2. The next lemma shows that a polarizing
mapping will strictly increase entropy even under such irregularities:

where f is a polarizing mapping and acts on its argument
vectors componentwise. Now letting
N
N
U1 = Gn (X1 ),

Lemma 2. For all , ν > 0, there exists δ( , ν) > 0 such
that if X1 , X2 ∈ X are independent random variables with
H(X1 ), H(X2 ) ∈ ( , 1 − ) and MpX1 ,ν = MpX2 ,ν = M for
some M with 1 ≤ |M | ≤ q − 1, and if f is a polarizing
mapping, then

the main polarization result can be proved for arbitrary alphabets through the arguments in [4]:
Theorem 3. For every

> 0,

1
i−1
= H(X1 | Y1 ),
i : H(Ui | Y1N U1 ) > 1 −
N
1
i−1
lim
i : H(Ui | Y1N U1 ) <
= 1 − H(X1 | Y1 ).
N →∞ N
It is worth mentioning that conditions (p.i)–(p.iii) are sufﬁcient for Theorems 2 and 3 to hold, but may not all be necessary: (p.i) guarantees that the one-step mapping (X1 , X2 ) →
(f (X1 , X2 ), X2 ) is one-to-one, and (p.iii) guarantees that
anomalous distributions such as the one in Example 1 are
also polarized; it turns out that this is indeed the only type of
irregularity that needs handling. Condition (p.ii) is in fact not
necessary for polarization to take place, and can be relaxed.
We include it only because it helps simplify the proofs. This
condition is also not a very restrictive one; there are several
simple families of mappings that satisfy (p.i)–(p.iii) for all
alphabet sizes. We give one example here:
lim

N →∞

H(f (X1 , X2 )) ≥ H(Xi ) + δ( , ν),

i = 1, 2.

Proof: We will prove the claim for i = 2; the claim
for i = 1 will follow by the symmetry in the assumptions.
It follows from (p.ii) that there exist q distinct permutations
πi : X → X , i = 0, . . . , q − 1 such that f (j, i) = πi (j).
Observe also that (p.i) implies
πi (x) = πj (x) for all i = j, x ∈ X .
Deﬁning probability distributions ri through ri (u)
−1
pX2 (πi (u)), we have

(7)
=

q−1

pf (X1 ,X2 ) =

pX1 (i)ri .

(8)

i=0

It sufﬁces to show that there exist a, b ∈ X for which
(i) pX1 (a), pX1 (b) ≥ η( , ν) for some η( , ν) > 0, and
(ii) ra − rb 1 ≥ ν,
since the claim will then follow immediately from (8), the
strict concavity of entropy, and that H(ri ) = H(X2 ) for all i.
First consider the case M = {a} for some a ∈ X , and
observe that H(X1 ) > implies pX1 (a) ≥ pX1 (b) ≥ η( ) for
some b = a and η( ) > 0, satisfying (i). It also follows from
(7) that ra (πa (a)) − rb (πa (a)) = pX1 (a) − pX1 (c) for some
c = a, implying (ii) since the latter difference is at least ν,
and therefore yielding the claim.
Suppose now that 2 ≤ |M | ≤ q − 1. Deﬁne, for all x ∈ X
and T ⊂ X , the sets

Proposition 2. The mapping f (x1 , x2 ) = x1 + π(x2 ), where
π : X → X is the permutation

 q/2 if x = 0

π(x) = x − 1 if 1 ≤ x ≤ q/2


x
otherwise
is polarizing for all q = |X |.
We give a proof of Proposition 2 in Section IV. The Cayley
table of f is given below for q = 6.


3 0 1 2 4 5
4 1 2 3 5 0


5 2 3 4 0 1


0 3 4 5 1 2


1 4 5 0 2 3
2 5 0 1 3 4

−1
Sx,T = {i : πx (i) ∈ T },

and observe that (p.iii) implies that
∀T ⊂ X , 2 ≤ |T | ≤ q − 1, ∃a, b ∈ T such that Sa,T = Sb,T .
(9)

III. P ROOF OF T HEOREM 2
Let us ﬁrst introduce a deﬁnition in order to capture the
anomaly described in Example 1: Given a distribution p over
X , let a0 , . . . , aq−1 be any labelling of the elements of X for
which p(a0 ) ≥ p(a1 ) ≥ . . . ≥ p(aq−1 ). For all ν > 0, let

Now let a, b ∈ M be such that Sa,M = Sb,M . It then follows
from the deﬁnition of M that there exists x ∈ X for which
|ra (x) − rb (x)| ≥ ν, satisfying (ii). That (i) is also satisﬁed
can be seen by noting that |M | ≤ q − 1 and a, b ∈ M imply
pX2 (a), pX2 (b) ≥ ν. This concludes the proof.
We are now ready to prove Theorem 2: Let H1 , H2 , and
Hu be [0, 1]-valued random variables with

Kν := min {i ≤ q − 2 : p(ai ) − p(ai+1 ) > ν} ∪ {q − 1}
and deﬁne
Mp,ν := {a0 , . . . , aKν }.

H1 = H(X1 | Y1 = y1 )

Recall that an anomaly may arise for random variables X1 and
X2 if their probability masses are conﬁned in a subset of their
alphabet. This can be stated in general as MpX1 ,ν = MpX2 ,ν

H2 = H(X2 | Y2 = y2 )
Hu = H(f (X1 , X2 ) | Y1 = y1 , Y2 = y2 )

3

whenever (Y1 , Y2 ) = (y1 , y2 ). Clearly, H1 and H2 are i.i.d.
with
E[H1 ] = E[H2 ] = H(X1 | Y1 ).
Suppose ﬁrst that Pr[H1 ≤
/2(2 − ). Then, the event

IV. P ROOF OF P ROPOSITION 2
Here we show that for all q = |X |, the function f : X 2 →
X , f (x1 , x2 ) → x1 + π(x2 ) with

 q/2
if x = 0

π(x) = x − 1 if 1 ≤ x ≤ q/2


x
otherwise

/2], Pr[H1 ≥ 1 − /2] ≥

A = y1 , y2 : H1 ≤ /2, H2 ≥ 1 − /2
2

has probability at least /2(2 − ) . Further, as both functions x1 → f (x1 , x2 ) and x2 → f (x1 , x2 ) are invertible for
all x2 and x1 respectively, we have Hu ≥ H1 , H2 for all
(Y1 , Y2 ) = (y1 , y2 ). Thus,

is polarizing. That (p.i) and (p.ii) are satisﬁed readily follows
from π being a permutation. It remains to show (p.iii), i.e.,
that for all 2 ≤ K ≤ q − 1 and a0 < a1 < . . . < aK−1 in X ,
the matrix

H(f (X1 , X2 ) | Y1 Y2 )

Bij = ai + π(aj ),

= E[Hu ]

has at least K + 1 distinct entries. We will consider two cases:
K ≥ 3: We will show, by contradiction, that the sets {Bi1 }
and {Bi(K−1) } are not identical, which leads to the claim.
For this purpose, note ﬁrst that 1 ≤ a1 < aK−1 . Also, since
Bi1 = ai + π(a1 ) and Bi(K−1) = ai + π(aK−1 ), it follows
that if {Bi1 } = {Bi(K−1) }, then there exists an L ≤ K and
distinct i1 , . . . , iL ∈ {0, 2, 3 . . . , K − 1} such that

= Pr[A] · E[Hu | A] + Pr[Ac ] · E[Hu | Ac ]
≥ Pr[A] · E[H2 | A] + Pr[Ac ] · E[H1 | Ac ]
≥ Pr[A] · E[H1 + 1 − | A]
+ Pr[Ac ] · E[H1 | Ac ]
≥ E[H1 ] +

2
2(2− )

= H(X1 | Y1 ) +

(1 − )
2

2(2− )

B1(K−1) = Bi1 1

(1 − ),

Bi1 (K−1) = Bi2 1
.
.
.

yielding the claim.
Now suppose instead that Pr[H1 ≤ /2] < 2(2− ) . Then,
since
E[H1 ]
2−2
Pr[H1 ≥ 1 − /2] ≤
≤
,
1 − /2
2−
it follows that

BiL−1 (K−1) = BiL 1
BiL (K−1) = B11 .
This implies

Pr[H1 ∈ ( /2, 1 − /2)] ≥

.
(10)
2(2 − )
A similar argument shows that the above inequality also holds
when Pr[H1 ≥ 1 − /2] < 2(2− ) . We will now show that
the conditions of Lemma 2 hold with positive probability
whenever we have (10): It can be shown that for all > 0,
there exists ν( ) > 0 for which H(V ) ≤ 1 − /2 implies
|MpV ,ν | ≤ q − 1 (see, for example, [4, Lemma 2]. Given such
a ν, let S1 ⊂ X and S2 ⊂ X be random sets with
S1 = MpX1 |Y1 =y1 ,ν

π(aK−1 ) − π(a1 ) = ai1 − a1

= a1 − aiL .
Since the terms on the right-hand side above sum to 0, we
have L[π(aK−1 ) − π(a0 )] = 0. As ai1 , . . . , aiL = a1 , this
implies that L divides q, which in turn implies

whenever Y2 = y2 .

max

i=0,...,K−1

We therefore have 1 ≤ a1 ≤ q/2 < aK−1 . It then follows
from (11) that ai1 − a1 = aK−1 − a1 + 1, i.e., ai1 = aK−1 + 1,
a contradiction.
K = 2: Suppose, contrary to the claim, that {B00 , B10 } =
{B01 , B11 }. This implies B01 = B10 , i.e.,

2

has probability at least [ /2q (2 − )] . It then follows from
Lemma 2 that Hu ≥ H1 + δ( , ν( )) for some δ( , ν( )) > 0
whenever y1 , y2 ∈ B. Therefore

a1 − a0 = π(a0 ) − π(a1 ).

E[Hu ] = Pr[B] · E[Hu | B] + Pr[B c ] · E[Hu | B c ]

(13)

A similar reasoning to the one for the case K ≥ 3 also yields
(12). Since K = 2, it follows that a1 − a0 = q/2 . On the
other hand, it follows from the deﬁnition of π that

≥ Pr[B] · E[H1 + δ( , ν( )) | B]
+ Pr[B c ] · E[H1 | B c ]
2

(12)

aK−1 − a0 ≥ q/2 .

B = {y1 , y2 : S1 = S2 = S}

/2q (2 − )

(ai − ai−1 ) ≤ q/2

(where a−1 = aK−1 ) and thus

As S1 and S2 are independent and identically distributed, it
follows from (10) and the above argument that there exists
S ⊂ X with 1 ≤ |S| ≤ q − 1 such that the event

= E[H1 ] +

(11)

= ai2 − ai1
.
.
.

whenever Y1 = y1

S2 = MpX2 |Y2 =y2 ,ν

i, j = 0, . . . , K − 1

· δ( , ν( )),

a1 − a0 = q/2

completing the proof.

implies

π(a0 ) − π(a1 ) = q/2 ,

contradicting (13). This completes the proof.

4

V. R EMARKS

We know from from previous results that Theorem 4 also
gives the asymptotic performance of the multi-layered coding
method in [3] (described here after Proposition 1). It is of
interest to analyze and compare the performance of multilayered coding with that of ‘direct polarization’ coding at ﬁnite
blocklengths. We leave this for future study.

We have seen that all discrete memoryless channels and
sources can be polarized by recursive transforms. The transforms described here depend only on the alphabet size of
the underlying process, and are independent of the probability
structure. In this sense, they retain the universality of Arıkan’s
binary transform. This universality also extends beyond memoryless processes. It can be shown through similar arguments
in [6, Chapter 5] that such transforms also polarize a large
class of processes with memory.
Codes based on the transforms described here will also
retain the low encoding and decoding complexity of binary
polar codes, due to their recursive structure. Their asymptotic
error probability behavior is also similar to that of binary polar
codes:

R EFERENCES
[1] E. Arıkan, “Channel polarization: A method for constructing capacityachieving codes for symmetric binary-input memoryless channels,” IEEE
Trans. Inform. Theory, vol. IT-55, pp. 3051–3073, July 2009.
[2] E. Arıkan, “Source polarization,” Proc. Intl. Symp. Inform. Theory, Austin,
TX, June 2010.
[3] E. Saso˘ lu, E. Telatar, E. Arıkan, “Polarization for arbitrary discrete
¸ ¸ g
memoryless channels,” Proc. IEEE Inform. Theory Workshop, Taormina,
Italy, Oct. 2009.
[4] E. Saso˘ lu, “An entropy inequality for q-ary random variables and its
¸ ¸ g
application to channel polarization,” Proc. Intl. Symp. Inform. Theory,
Austin, TX, June 2010.
[5] A. Clark, Elements of Abstract Algebra. New york: Dover, 1971.
[6] E. Saso˘ lu, Polar Coding Theorems for Discrete Systems. Ph.D. Disser¸ ¸ g
tation, EPFL, Lausanne, Switzerland, 2011.

Theorem 4 ([6, Chapter 3]). the block error probability Pe
of polar channel (respectively, source) codes based on (6)
satisﬁes
β
Pe ≤ 2−N
for all β < 1/2 and sufﬁciently large blocklength N , provided
that the code rate is below the capacity of the channel
(respectively, above the entropy of the source).

5

