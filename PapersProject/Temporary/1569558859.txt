Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 21:14:21 2012
ModDate:        Tue Jun 19 12:54:50 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      461050 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569558859

Tackling Intracell Variability in TLC Flash Through
Tensor Product Codes
Ryan Gabrys∗ , Eitan Yaakobi†‡ , Laura Grupp† , Steven Swanson† , Lara Dolecek∗
∗ University

of California, Los Angeles

† University

of California, San Diego

‡ California

Institute of Technology

magnetic recording [2], [3]. In a concatenated coding scheme,
the use of a tensor product parity code as the inner code was
shown to offer the performance advantages of a short length
parity code but without the associated rate penalty. In this
work, it is shown that generalized tensor product codes can
be used to efﬁciently correct the errors that occur within a
TLC Flash device, and in turn extend the lifetime of a memory
system. The main contributions are construction methods for
codes that correct up to t1 symbol errors with up to l1 bit
errors and t2 symbol errors with up to l2 bit errors.
In Section II, the data collected from a TLC Flash chip
is summarized. In Section III, the error model, motivated
by the experimental data, is proposed. In Section IV, code
constructions for this model are given. In Section V, these
constructions are shown through simulation to be superior to
commonly used storage codes. Section VI concludes the paper.

Abstract—Flash memory is a promising new storage technology. To fully utilize future multi-level cell Flash memories, it is
necessary to develop error correction coding schemes attuned
to the underlying physical characteristics of Flash. Based on a
careful inspection of ﬁne-grained, experimentally-collected error
patterns of TLC (three bits per cell) Flash, we propose a
mathematical model that captures the intracell variability, which
is manifested by certain patterns of bit-errors. Error correction
codes are constructed for this model based upon generalized
tensor product codes. For ﬁxed levels of redundancy, these codes
are shown to exhibit substantially lower bit error rates than
existing error correction schemes.

I. I NTRODUCTION
Flash memory devices can be found almost everywhere
today. They are lighter, faster and more shock resistant than
traditional magnetic hard drives. As this technology scales and
the storage density increases, data errors become more prevalent, making error correction coding critical for maintaining
data integrity.
The storage density of a Flash memory device is dependent
on the number of discrete voltage levels the ﬂoating gate cell
is capable of representing. In early generations, every memory
cell could represent two voltage levels and thus store a single
bit (SLC). The demand for increased storage capacity has
created the need to store more than a single bit per cell by
simply representing more than two voltage levels. In this work,
we follow the commonly adopted nomenclature and assume
that multiple level cell (MLC) chips store two bits per cell,
and that triple level cell (TLC) chips store three bits per cell.
Recently, the subject of error-correction coding for Flash
memory has received signiﬁcant attention. In [5], trellis coded
modulation techniques were applied to Flash memory. In [8],
the use of non-binary LDPC codes was investigated. In [6],
algebraic error-correction codes were used for rewriting as
well as correcting errors. In [1], [4], codes that correct limited
magnitude asymmetric errors were constructed. In [12], this
model was extended to correct graded error patterns.
The error model in this work is motivated by data collected
from a TLC Flash device. As observed in [13], if the information from each Flash cell is interpreted as a triple-bit word,
then the errors largely cause only a single bit in each word
to change. From this observation, we suggest the use of a
new class of codes derived from tensor product codes [7],
[11] in the context of Flash memory. This work generalizes
the result of [13] to correct errors that mostly have only a
small number of bits in error for each cell-error. The technique
used to address the problem is based on the generalized tensor
product (GTP) scheme proposed in [7].
Tensor product codes were ﬁrst introduced in [11] and were
generalized to produce efﬁcient binary codes in [7]. More
recently, tensor product codes were used in the context of

II. S TRUCTURE AND E RROR C HARACTERIZATION OF TLC
F LASH
In this section, we report on the observed errors measured
from a TLC chip provided by an anonymous vendor. A TLC
chip is divided into multiple planes. Each plane is divided into
a set of blocks and these blocks are further decomposed into
pages. For the particular TLC chip measured, there are 384
pages within a block and 8 kilobytes (KB) within a page. The
eight discrete voltage levels from the cell are represented as a
triple-bit word. We refer to the ﬁrst bit in the word as the most
signiﬁcant bit (MSB), the second bit in the word as the center
signiﬁcant bit (CSB), and the third bit in the word as the least
signiﬁcant bit (LSB). For more details on the structure of a
TLC chip, see [13].
The errors were measured from sixteen blocks evenly
divided across two planes. The following testing procedure
was repeatedly performed. On the ﬁrst cycle of every 100
program/erase (P/E) cycles, a block was erased, and random
data was then written and ﬁnally read back for errors. On the
other 99 cycles, the block was simply erased and the memory
was programmed to simulate the aging of the device.
In Figure 1, the Bit Error Rate (BER) is illustrated for the
TLC chip tested over the course of its lifetime. It can be seen
that over time, the BER increases dramatically but at different
rates depending on which bit is programmed. The ’Symbol
Error Rate’ plot refers to the symbol error rate when each cell
is represented as a symbol over GF (8).
The dominant trend from Figure 1 is that the ’Symbol Error
Rate’ appears to be roughly the sum of the individual BERs
of the MSB, CSB, and LSB. This suggests that whenever a
cell-error occurs, with high probability only one of the three
bits in the cell errs. More speciﬁcally, 96.17% of cell-errors
only had a single bit in error. This is a result of the special

1

−2

[t1 , t2 ;

10

−3

Error Rate

Deﬁnition 5. Let 0 < 1 < 2 ≤ m, t1 , t2 > 0. Then a 2m -ary
code C is said to be a [t1 , t2 ; 1 , 2 ]2m -bit-error-correcting
code if it can correct any [t1 , t2 ; 1 , 2 ]2m -bit-error-vector.
The next deﬁnition is useful in determining the parity-check
matrices of bit-error-correcting codes.
Deﬁnition 6. Let A ∈ GF (q)m×n , B ∈ GF (q)p×r . Then the
tensor product of A and B is deﬁned as the matrix


a0,0 B
...
a0,n−1 B


.
.
mp×nr
..
.
.
A⊗B = 
.
 ∈ GF (q)
.
.
.

−4

10

LSB (BER)
CSB (BER)
MSB (BER)
Symbol Error Rate

−5

0

1000

2000

3000

4000

5000

Program/Erase Cycles

Fig. 1.

if the following holds:

1) wt(e) = |{i : ei = 0}| ≤ t1 + t2 .
2) ∀i, wt(ei ) ≤ 2 .
3) |{i : wt(ei ) > 1 }| ≤ t2

10

10

1 , 2 ]2m -bit-error-vector

Error Rates Measured from a TLC Flash Device.

programming property of the bits where the three bits are not
programmed all at once. More details on this phenomena are
reported in [13]. Note that this error model is considerably
different than the one of asymmetric limited magnitude errors,
Student Version of MATLAB
studied in many previous works, e.g., [1] and [4].
The new codes introduced in this paper correct errors that
mostly affect a single bit within each cell-error. In addition,
these new codes also have the special property that they can
correct the remaining few cell-errors with two or three biterrors.
III. M ODEL AND D EFINITIONS

am−1,0 B

...

am−1,n−1 B

Furthermore,
rank(A ⊗ B) = rank(A) · rank(B).
IV. C ODE C ONSTRUCTIONS
In this section, code constructions are given for bit-errorcorrecting codes. The section begins by revisiting a result from
[11] that can be used to create [t; ]2m -bit-error-correcting
codes. This idea is extended to create [t1 , t2 ; 1 , 2 ]2m -biterror-correcting codes. Any error pattern a [t1 , t2 ; 1 , 2 ]2m
code can correct is also correctable by a [t1 +t2 ; 2 ]2m code. It
is shown that the former code has better redundancy whenever
t1
t2 ( 2 − 1 ) is large.
In [11], it was shown that the tensor product of two parity
check matrices results in a code that can correct a prescribed
number of errors of a pre-deﬁned type. For example, suppose
a code with a parity check matrix H1 ∈ GF (2)r1 ×m corrects
all burst errors of length 2 and a code with a parity check
matrix H2 ∈ (GF (2)r1 )r2 ×n corrects any 3 symbol errors.
Then H2 ⊗H1 is a parity check matrix of a code of length nm
bits, partitioned into n m-bit blocks. This code corrects any 3
block-errors assuming each block-error is a burst of length 2.
In Construction A, this result is stated more formally.
A. Construction A

In this section, the relevant error models as well as code
deﬁnitions are given.
Deﬁnition 1. A linear code C of length n and dimension k
over an alphabet of size q that can correct t errors is referred
to as an [n, k, t]q code.
All codes considered in this work have alphabets whose
cardinality is q = 2m where m is some positive integer. Each
cell can take on 2m possible values and is displayed as an mbit vector. Thus, a word of length n is represented as a lengthnm binary vector where bits mi, . . . , m(i + 1) − 1 represent
the ith cell for 0 ≤ i ≤ n − 1.
Accordingly, every cell-error is represented as a length-m
vector ei . For a ﬁxed , if wt(ei ) ≤ then such an error
is called an -bit-cell-error, where the Hamming weight of a
vector x is denoted by wt(x). Motivated by the nature of the
errors observed, it is useful to deﬁne the following class of
error-vectors and codes.
Deﬁnition 2. Given the parameters t and , an error-vector
e = (e0 , e1 , . . . , en−1 ) over (GF (2)m )n is called a [t; ]2m bit-error-vector if the following holds:
1) wt(e) = |{i : ei = 0}| ≤ t
2) ∀i, wt(ei ) ≤ .

We start by presenting a construction of [t; ]2m -bit-errorcorrecting codes.
Construction A. (see ﬁrst [11]) Let C1 be an [m, k1 , ]2 code
with a parity check matrix H1 . Let C2 be an [n, k2 , t]2m−k1
code with a parity check matrix H2 . Then, the code CA with
the parity matrix
HA = (H2 ⊗ H1 ) ,
(1)

Deﬁnition 3. A 2m -ary linear code C that can correct
any [t; ]2m -bit-error-vector is called a [t; ]2m -bit-errorcorrecting code.
From the data collected from the TLC ﬂash device, it was
observed that while most cell-errors suffered a single bit-error,
only a small number of cells had double or triple bit-errors.
Therefore, to correct all observed errors, it is useful to deﬁne
the following reﬁned error-vectors and corresponding codes.
Deﬁnition 4. Let 0 < 1 < 2 ≤ m, t1 , t2 > 0. Then a
vector e = (e0 , e1 , . . . , en−1 ) over (GF (2)m )n is called a

is a [t; ]2m -bit-error-correcting code of length n.
The correctness of the error-correction capability was
proved in [11]. Furthermore, since the parity check matrix
of the code CA is the tensor product of the matrices H1
and H2 , and rank(H2 ⊗ H1 ) = rank(H2 ) · rank(H1 ),
we get that the redundancy of the code CA is r1 r2 , where
r1 = m − k1 and r2 = n − k2 . An example of the encoding
of such codes was given in [10]. Suppose c ∈ CA , where
c = (c0 , . . . , cn−1 ) ∈ (GF (2)m )n . Then,

2

HA · cT = (H2 ⊗ H1 ) · cT

h0,0 H1
...
h0,n−1 H1

.
.
..
.
.
=
.
.
.
hm−1,0 H1


=

h0,0
.
.
.
hm−1,0

...

...
..
.
...

hm−1,n−1 H1
 

h0,n−1
.
.
.

 
·

hm−1,n−1

1) There exists 0 ≤ r < r such that the matrix H1
comprised of the ﬁrst r rows of H1 is a parity check
matrix for an [m, m − r , 1 ]2 code C1 .
2) H1 is an r × m matrix consisting of the last r rows
of H1 , where r = r − r .
3) H2 is a parity check matrix for an [n, k2 , t1 +t2 ]2r code
C2 , and r2 = n − k2 .
4) H3 is a parity check matrix for an [n, k3 , t2 ]2r code
C3 , and r3 = n − k3
Then, a parity check matrix for a [t1 , t2 ; 1 , 2 ]2m -bit-errorcorrecting code CB of length n is


 T
·c
H1 · c 0 T
.
.
.
H1 · cn−1 T



,

where hi,j represents the symbol in position row i, column j of
H2 . Thus, c ∈ CA if and only if (H1 · c0 T , . . . , H1 · cn−1 T ) ∈
C2 and the code CA can be expressed as follows:
CA = {c = (c0 , . . . , cn−1 ) ∈ (GF (2)m )n :
cT , . . . , H 1
0

HB =

cT )
n−1

(H1 ·
·
∈ C2 }.
For completeness and for the subsequent discussion, let us
describe here a decoder for the code CA . Let
D1 : {0, 1}r1 → {0, 1}m , D2 : ({0, 1}r1 )r2 → ({0, 1}r1 )n
be the decoder of the code C1 , C2 , respectively. Here, and
henceforth we assume that the input to the decoders of the
constituent codes is the syndrome of the received vector and
the output is the detected error vector. We also assume that
if the code can correct t errors, then the weight of the output
error vector is at most t. If the decoder ﬁnds an error vector
of weight greater than t then the all-zero vector is returned as
an output.
The decoder DA : ({0, 1}m )n → ({0, 1}m )n of the code
CA gets as an input a word of the form y = c + e, where
c ∈ CA and e ∈ (GF (2)m )n is a [t; ]2m -bit-error-vector. The
output of the decoder is the estimate of the error vector:
1) D2 (H2 · (H1 · y T , . . . , H1 · y T )T ) = (s0 , . . . , sn−1 ).
0
n−1
2) e = (D1 (s0 ), . . . , D1 (sn−1 )).
Lemma 1. The decoder output satisﬁes DA (y) = e = e
Proof: According to the deﬁnition of the code CA we
have (H1 · cT , . . . , H1 · cT ) ∈ C2 and we can write
0
n−1

H2 ⊗ H1
H3 ⊗ H1

.

(2)

Remark 1. The parity check matrix H1 of the 2 -errorcorrecting code C1 needs to satisfy the property that it can
be decomposed into two matrices, where the ﬁrst matrix is
a parity check matrix of an 1 -error-correcting code C1 . We
note this requirement is not hard to satisfy as many codes can
follow this structure, and in particular BCH codes.
We ﬁrst present an example followed by the decoder for CB
before the error-correction ability is proven.
Example 1. Suppose C1 is a triple error-correcting [3, 0, 3]2


1 0 1
code with a parity check matrix H1 =  0 1 1 . Let
0 0 1
1 0 1
r = 2 so that H1 =
where H1 is a parity check
0 1 1
matrix for a [3, 1, 1]2 Hamming code and H1 = ( 0 0 1 ).
Let C2 be a [15, 9, 2]4 code with a parity check matrix H2 .
Furthermore, let C3 be a [15, 11, 1]2 Hamming code with a
parity check matrix H3 . Then using Construction B, the code
CB has a parity check matrix
H2 ⊗ H1
HB =
.
H3 ⊗ H1

(H1 · y T , . . . , H1 · y T )
0
n−1
= (H1 · cT , . . . , H1 · cT ) + (H1 · eT , . . . , H1 · eT ).
0
n−1
0
n−1

HB is the parity check matrix for a [1, 1; 1, 3]23 -bit-errorcorrecting code. The particular choice of C1 in this example
results in the same code that was proposed in [13].
Note that c ∈ CB if and only if
H2 ⊗ H1
0 = HB · c T =
· cT
H3 ⊗ H1

The vector (H1 · eT , . . . , H1 · eT ) has weight at most t and
0
n−1
since C2 can correct t errors we get that
(s0 , . . . , sn−1 ) = (H1 · eT , . . . , H1 · eT ).
0
n−1
Next, for every 0 ≤ i ≤ n − 1
H1 · y T = H1 · (cT + eT ) = H1 · cT + H1 · eT
i
i
i
i
i

=

H1 · e T
i

and since si =
and the weight of ei is at most , we
get that D1 (si ) = ei , that is, e = e.

H2 ·
H3 ·

H1 · c0 T , . . . , H1 · cn−1 T
H1 · c0 T , . . . , H1 · cn−1 T

T
T

.

Hence, the code CB can be expressed as
CB = {c = (c0 , . . . , cn−1 ) ∈ (GF (2)m )n :

B. Construction B
The codes given in Construction A correct error patterns
according to the maximum number of bit-errors in every cell
(or m-bit symbol). Construction B extends this idea so that,
while most cells suffer a small number of bit-errors, relatively
few cell-errors may occur with a larger number of bit-errors.
We capture this property in the following construction of
[t1 , t2 ; 1 , 2 ]2m -bit-error-correcting codes.

(H1 · cT , . . . , H1 · cT ) ∈ C2 ,
0
n−1
(H1 · cT , . . . , H1 · cT ) ∈ C3 }.
0
n−1
and its redundancy is at most r r2 + r r3 .
Let us denote
D1 : {0, 1}r → {0, 1}m , D1 : {0, 1}r → {0, 1}m ,

Construction B. Let C1 be an [m, k, 2 ]2 code with a parity
check matrix H1 , and let r = m−k be such that the following
holds:

D2 : ({0, 1}r )r2 → ({0, 1}r )n ,
D3 : ({0, 1}r )r3 → ({0, 1}r )n ,

3

to be the decoder of the code C1 , C1 , C2 , C3 , respectively. As
before, the input to all these encoders is the syndrome and
the output is the error vector whose weight is no greater than
the guaranteed error-correction capability of the corresponding
code.
Before presenting the decoder’s steps, let us explain the
idea behind this construction and its decoding procedure. We
start in a similar fashion to the decoder in Construction A,
where at most t1 + t2 cell-errors each of weight at most 1 are
found. Clearly, it may not possible to correct all cell-errors this
way. If a cell-error has weight at most 1 then it is corrected.
Otherwise, it is miscorrected to a cell-error vector, with weight
at most 1 + 2 since the weight of each miscorrection has
been restricted to be 1 . This, in turn, guarantees that the new
cell-error vector is not a codeword in C1 , since its minimum
distance is at least 2 2 +1. Thus, the next step is to detect these
cells which were miscorrected. For cell-errors with more than
1 bits in error, the remaining part of the syndrome according
to the code C1 is recovered. The decoder D1 is then used to
recover the remaining errors.
The decoder DB : ({0, 1}m )n → ({0, 1}m )n of the code
CB gets as an input a word of the form y = c + e, where
c ∈ CB and e ∈ (GF (2)m )n is a [t1 , t2 ; 1 , 2 ]2m -bit-errorvector. Its output is an estimate of the error vector DB (y) = e.
The decoder DB operates as follows:
1) D2 (H2 · (H1 · y T , . . . , H1 · y T )T ) = (s0 , . . . , s0 ).
0
n−1
0
n−1
2) e∗ = (D1 (s0 ), . . . , D1 (s0 )).
0
n−1
3) y = y + e∗ .
T
4) D2 (H2 · (H1 · y 0T , . . . , H1 · y n−1 )T ) = (s0 , . . . , sn−1 ).
T
T
5) D3 (H3 · (H1 · y 0 , . . . , H1 · y n−1 )T ) = (s0 , . . . , sn−1 ).
6) Let I = {i : (si , si ) = (0, 0)}.
7) Let y satisfy: y i = y i if i ∈ I and y i = y i if i ∈ I.
/
T
8) D3 (H3 · (H1 · y 0 T , . . . , H1 · y n−1 )T ) = (s1 , . . . , s1 ).
0
n−1
9) e = (e0 , . . . , en−1 ) where ei = e∗ if i ∈ I and
/
i
otherwise ei = D1 (s0 , s1 ).
i
i
Theorem 1. The decoder output satisﬁes DB (y) = e = e.
Proof: Let y = c + e be the received word to the decoder
DB where c ∈ CB and e ∈ (GF (2)m )n is a [t1 , t2 ; 1 , 2 ]2m bit-error-vector. Then according to the deﬁnition of the code
CB , (H1 · cT , . . . , H1 · cT ) ∈ C2 and
0
n−1
(H1 · y T , . . . , H1 · y T )
0
n−1

no cell errors of weight less than 1 and all the remaining (at
most t2 ) cell-errors have weight at most 1 + 2 .
Steps 4 and 5 compute the syndrome using y as input.
Since the minimum distance of the code C1 is at least
2 2 + 1 > 1 + 2 , we get that for all 0 ≤ i ≤ n − 1, if
a miscorrection occurred, then e∗ is not a codeword in C1 .
i
Therefore, (si , si ) = (0, 0) and in step 6 the set I is the set
of all 0 ≤ i ≤ n − 1 such that 1 < wt(ei ) ≤ 2 . In step 7,
the word y is the word of y after removing all cell-errors of
weight at most 1 .
In step 8 the remaining portion of the syndrome is recovered
for all cell-errors with more than 1 bits in error. Lastly in
step 9 for every cell-error at position i, if 1 or less bit-errors
occurred then e∗ is its corresponding cell-error vector and if
i
more bit-errors occurred then the decoder D1 is used. Since
C1 can correct 2 errors and the syndrome H1 · ei T = (s0 , s1 )
i
i
is known for all cell-errors with more than 1 bits in error,
these errors are corrected as well.
It can be shown that CB requires less redundancy than
log n
CA approximately whenever log m < t1 ( 2 − 1 ). The next
t2
construction reduces the required redundancy in certain cases
when the ratio t1 is small.
t2
C. Construction C
Construction C extends Construction B by using a combination of codes whose abilities are to correct errors, correct
erasures, and detect errors. In particular, the code C1 in
Construction B is modiﬁed such that it corrects 1 errors
and detects when there are between 1 + 1 and 2 errors.
Accordingly, the code C3 in Construction B need only correct
t2 erasures instead of t2 errors.
Construction C. Let CC be a code with the following modiﬁcations with respect to the code construction of CB :
1) The code C1 remains an [m, k, 2 ]2 code as in Construction B where r = m − k.
2) The matrix H1 now consists of the ﬁrst r rows of H1
where
a) H1 is a parity check matrix for an [m, m−r , 1 ]2 bit-error-correcting code C1 ,
b) The minimum distance of C1 is at least 1 + 2 + 1
so is can detect an error vector of weight between
1 + 1 and 2 .
3) H1 is an r × m matrix consisting of the last r rows
of H1 , where r = r − r .
4) H2 is a parity check matrix for an [n, k2 , t1 +t2 ]2r code
C2 , and r2 = n − k2 .
2
5) H3 is a parity check matrix of an [n, k3 , t2 ]2r code
C3 that can correct at least t2 erasures, and r3 = n−k3 .
H2 ⊗ H1
A parity check matrix for CC is HC =
. The
H3 ⊗ H1
decoders of the codes C1 and C3 are also changed while the
decoders for C1 and C2 remain the same as in Construction B.
The decoder D1 , in addition to correcting 1 errors, also
detects if the number of errors is between 1 + 1 and 2 . The
decoder is deﬁned
r
D1 : {0, 1} → {0, 1}m ∪ {E},

= (H1 · cT , . . . , H1 · cT ) + (H1 · eT , . . . , H1 · eT ).
0
n−1
0
n−1
The vector (H1 ·eT , . . . , H1 ·eT ) now has weight at most
0
n−1
t1 + t2 and since the code C2 can correct this number of errors
we get that (s0 , . . . , s0 ) = (H1 · eT , . . . , H1 · eT ) after
0
n−1
0
n−1
step 1.
At step 2 since for every 0 ≤ i ≤ n − 1, H1 · y T =
i
H1 · cT + H1 · eT , if wt(ei ) ≤ 1 ,
i
i
e∗ = D1 (s0 ) = ei ,
i
i
as C1 corrects 1 errors. However, if the weight of ei is
between 1 + 1 and 2 then e∗ = D1 (s0 ) = ei . This
i
i
observation results from the fact that the decoder C1 can only
return a cell-error vector of weight at most 1 . In particular,
we get that wt(e∗ ) ≤ t2 and for all 0 ≤ i ≤ n − 1,
wt(e∗ ) ≤ 1 + 2 . Thus, at the end of step 3, y contains
i

4

where the symbol E indicates a detected error of weight
between 1 + 1 and 2 . Note that now the decoder D1 never
miscorrects when the number of errors in each cell is at most
2 . The input to the decoder D3 is no longer a syndrome but
a vector x = (x0 , . . . , xn−1 ) with at most t2 erasures
D3 : ({0, 1}r ∪ ?)n → ({0, 1}r )n ,
where ? is the erasure symbol. The output of the decoder
DC is the ‘erasure’ vector e = (e0 , . . . , en−1 ), where for
every ei = 0, ei is the correct value for xi . The decoder
DC : ({0, 1}m )n → ({0, 1}m )n is summarized below. Recall
the input is y = c + e.
1) D2 (H2 · (H1 · y T , . . . , H1 · y T )T ) = (s0 , . . . , s0 ).
0
n−1
0
n−1
2) e∗ = (D1 (s0 ), . . . , D1 (s0 )).
0
n−1
3) Let I = {i : e∗ = E}.
i
4) Let e satisfy: ei = 0 if i ∈ I and ei = e∗ if i ∈ I.
/
i
5) y = y + e .
T
6) Let x satisfy: xi = ? if i ∈ I and xi = H1 · y i if
i ∈ I.
/
7) D3 (x0 , . . . , xn−1 ) = (s1 , . . . , s1 ).
0
n−1
/
8) e = (e0 , . . . , en−1 ) where ei = e∗ if i ∈ I and
i
T
otherwise ei = D1 (s0 , s1 + H1 · y i ).
i
i
The proof of the decoder correctness of Construction C
is omitted due to a lack of space. It can be shown that
Construction C requires less redundancy than Construction A
log n
approximately when log m < ( t1 + 1 )( 2 − 1 ). Furthermore,
t2
2
it requires less redundancy than Construction B roughly when
log n
t1
1
log m > ( t2 − 2 )( 2 − 1 ).
V. P ERFORMANCE AND R ESULTS
In this section, the performance of various linear errorcorrecting codes with guaranteed error-correction capability
is evaluated for the TLC Flash device. The results of these
simulations are shown in Figure 2. All the known codes used
were the best known linear codes according to [9] of the
longest block length.

bits for each Flash memory cell. Next, an independent binary
[256, 240, 2]2 code was used to protect the remaining bit of
information from each cell. This scheme was designed to target
the property observed in Section II where the CSB and LSB
were more likely to err than the MSB.
The [3, 2; 1, 3]8 -bit-error-correcting code of length 256 with
rate 0.904 was generated according to Construction B. In this
case, C1 is the [3, 1, 1]2 binary repetition code. The code C2
is the same [256, 227, 5]4 code used in Scheme A and C3 is
the same [256, 240, 2]2 code used in Scheme A as well. For
reference, we included a [7; 1]23 bit-error code of length 256
and rate 0.83 (constructed using one tensor product operation).
From Fig. 2 this particular tensor product code has the ability
to delay the appearance of any errors in the system by a factor
of 4 over the naive GF (8) code. In addition, the proposed
tensor product code offers a 1.6x lifetime improvement over
the popular BCH codes.
VI. C ONCLUSION
In this work, data from a TLC Flash device demonstrated
that when errors occur within a Flash cell, the vast majority of
such errors only affect one of the 3 bits of information. This
observation was used to motivate a new error-correction model
for Flash memory. Error-correcting code constructions based
upon generalized tensor product codes were provided that
were analytically and empirically shown to offer a potentially
valuable component for future coding schemes in the context
of Flash memory.
ACKNOWLEDGEMENT
Research supported in part by SMART scholarship, and
NSF grants CCF-1029030 and CCF-1150212.
R EFERENCES
[1] Y. Cassuto et al., “Codes for asymmetric limited-magnitude errors with
application to multi-level ﬂash memories, IEEE Trans. on Inform. Theory,
vol. 56, no. 4, pp. 1582-1595, Apr. 2010.
[2] P. Chaichanavong and P. H. Siegel, “A tensor-product parity code for
magnetic recording,” IEEE Trans. on Magnetics, vol. 42, no. 2, pp. 350352, Feb. 2006.
[3] P. Chaichanavong and P. H. Siegel, “Tensor-product parity codes: combination with constrained codes and application to perpendicular recording,”
IEEE Trans. on Magnetics, vol. 42, no. 2, pp. 214-219, Feb. 2006.
[4] N. Elarief and B. Bose, “Optimal, systematic, q-ary codes correcting all
asymmetric and symmetric errors of limited magnitude,” IEEE Trans.
Inform. Theory, vol. 56, no. 3, pp. 979-983, Mar. 2010.
[5] S. Fei et al., “Multilevel Flash memory on-chip error correction based on
trellis coded modulation,” IEEE ISCS, Island of Kos, Greece, Sep. 2006.
[6] Q. Huang, S. Lin, and K. A. S. Abdel-Ghaffar, “Error-correcting codes for
ﬂash coding,” IEEE Trans. Inform. Theory, vol. 57, no. 9, pp. 6097-6108,
Apr. 2011.
[7] H. Imai and H. Fujiya, “Generalized tensor product codes,” IEEE Trans.
Inform. Theory, vol. 27, no. 2, pp. 181-187, Mar. 1981.
[8] Y. Maeda and H. Kaneko, “Error control coding for multilevel cell
ﬂash memories using nonbinary low-density parity-check codes,” IEEE
ISDFTVS, Chicago, IL, Oct. 2009.
[9] University of Sydney, “Magma Computational Algebra System,”
http://magma.maths.usyd.edu.au/magma/, 2011.
[10] J. K. Wolf, “An introduction to tensor product codes and applications to
digital storage systems,” IEEE ITW, Punta del Este, Uruguay, Oct. 2006.
[11] J. K. Wolf, “On codes derivable from the tensor product of check
matrices,” IEEE Trans. Inform. Theory, vol. 11, no. 2, pp. 281-284, Apr.
1965.
[12] E. Yaakobi et al., “On codes that correct asymmetric errors with graded
magnitude distribution,” IEEE ISIT, St. Petersburgh, Russia, Aug. 2011.
[13] E. Yaakobi et al., “Characterization and error-correcting codes for TLC
ﬂash memories,” IEEE ICNC, Maui, HI, Jan.-Feb. 2012.

Error Rates of Codes Applied to TLC Flash

−2

10

[128,116,3]8
[255,231,3]2

−3

Scheme A
[3,2;1,3]23 Code

Bit Error Rate

10

[7;1]23 Code
−4

10

−5

10

−6

10

−7

10

0

500

1000

1500

2000

2500

3000

3500

4000

4500

5000

P/E Cycles

Fig. 2.

Bit Error Rates of Codes Applied to TLC Flash

The ﬁrst code shown in the legend of Fig. 2 is a non-binary
[128, 116, 3]8 code with rate 0.906 where each 8-ary symbol
corresponds to an 8-ary cell. The second code in the ﬁgure is
the result of applying a [255, 232, 3]2 BCH code three times
to each of the 3 bits in each cell.
The rate 0.904 code labeled ‘Scheme A’ is comprised of a
non-binary [256, 227, 5]4 code applied to the LSB and CSB
Student Version of MATLAB

5

