Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May  2 09:51:35 2012
ModDate:        Tue Jun 19 12:54:54 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      482740 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565421

Bounds on estimated Markov orders of individual
sequences
∗

Luciana Vitale

´
Alvaro Mart´n
ı

Gadiel Seroussi

Instituto de Computaci´ n
o
Universidad de la Rep´ blica
u
Montevideo, Uruguay
Email: lvitale@ﬁng.edu.uy

Instituto de Computaci´ n
o
Universidad de la Rep´ blica
u
Montevideo, Uruguay
Email: almartin@ﬁng.edu.uy

HP Laboratories, Palo Alto, CA, USA, and
U. de la Rep´ blica, Montevideo, Uruguay
u
Email: gseroussi@ieee.org

is usually regarded as an asymptotic approximation of a
Minimum Description Length (MDL) estimator of the Markov
order. We will also be interested in the latter type of order
estimators, and, in particular, in the variant based on the
Krichevskii-Troﬁmov (KT) probability assignment [2]. The
cost function in this case does not include an explicit penalization term; instead, the contribution of the model size to the
cost is amortized across actual occurrences of model states in
the sequence under evaluation.
ˆ
The range of values of the estimate k has played an
important role in the theoretical analysis of the above mentioned estimators. The ﬁrst consistency results for the BIC
estimator [3], for example, assumed a known bound on the
Markov order. This assumption was removed in [1], where
it is also shown that, if no bound is assumed, pure MDL
Markov order estimators, be it in the KT or the normalized
maximum likelihood (NML) versions, are not consistent. The
consistency of the latter two was shown in [4], when the
range of the Markov order k for the minimization in the
estimation is bounded by o(log n) and c log n, respectively,
with c < 1/ log α. Similarly, in the case of the estimation of
context trees [5], the consistency of BIC and (KT-based) MDL
estimators was proved in [6], under the assumption of an upper
bound of o(log n) on the depth of the candidate context trees
considered for the minimization.
Imposing a-priori bounds on the estimated order may be
useful in some cases to guarantee consistency, but might not
be desirable in other applications. For example, in universal
lossless data compression, we are interested in choosing the
estimated order that yields the shortest description length
for the given input sequence, regardless of where the input
originated from. Similarly, in the universal simulation results
ˆ
of [7], given a sequence xn , a Markov order k is estimated
using a PML estimator, and a “simulated” sequence y n is
obtained by drawing uniformly from the set of sequences in
ˆ
the kth order Markov type class of xn that also estimate order
ˆ
ˆ
k. No assumptions are made on the range of k, and, in the
individual sequence setting, the choice of penalization function
f (n) governs a trade-off between the statistical similarity of y n
to xn , and the “richness” (entropy) of the space from which y n
is drawn. On the other hand, aside from the theoretical interest,
obtaining inherent bounds on the possible outcome of the

Abstract—We study the maximal values estimated by commonly used Markov model order estimators on individual sequences. We start with penalized maximum likelihood (PML)
ˆ
estimators with cost functions of the form − log Pk (xn )+f (n)αk ,
ˆ
where Pk (xn ) is the ML probability of the input sequence xn
under a Markov model of order k, α is the size of the input
alphabet, and f (n) is an increasing (penalization) function of n
(the popular BIC estimator corresponds to f (n) = α−1 log n).
2
Comparison with a memoryless model yields a known upper
bound k(n) on the maximum order that xn can estimate.
We show that, under mild conditions on f that are satisﬁed
by commonly used penalization functions, this simple bound
is not far from tight, in the following sense: for sufﬁciently
large n, and any k<k(n), there are sequences xn that estimate
order k; moreover, for all but a vanishing fraction of the
values of n such that k = k(n), there are sequences xn that
estimate order k. We also study KT-based MDL Markov order
estimators, and show that in this case, there are sequences xn that
estimate order n1/2− , which is much larger than the maximum
log n/ log α(1 + o(1)) attainable by BIC, or the order o(log n)
required for consistency of the KT estimator. In fact, for these
sequences, limiting the allowed estimated order might incur in
a signiﬁcant asymptotic penalty in description length. All the
results are constructive, and in each case we exhibit explicit
sequences that attain the claimed estimated orders.

I. I NTRODUCTION
Initially, we consider penalized maximum likelihood (PML)
Markov model order estimators, where, given a sequence xn
over a ﬁnite alphabet A, of size α = |A|, and a candidate
Markov order k, we deﬁne a cost1
ˆ
Ck (xn ) = − log Pk (xn ) + f (n)αk .

(1)

ˆ
Here, Pk (xn ) is the maximum likelihood (ML) probability
n
of x under a kth order Markov model (with appropriate
conventions on the initial states), and f (n) is a positive
penalization function satisfying some mild conditions to be
detailed later. The order estimated for xn is
ˆ
k(xn ) = arg min Ck (xn ) .
k≥0

(2)

Different variants of PML estimators have been extensively
studied (see, e.g., [1] and citations therein). When f (n) =
1
2 (α − 1) log n, we obtain the popular BIC estimator, which
∗ Work
1 All

supported in part by grant I+D CSIC-UdelaR.
logarithms are taken to base 2 unless speciﬁed otherwise.

1

that xn is preceded by an arbitrary ﬁxed semi-inﬁnite string
x0 . This convention uniquely determines a state selected by
−∞
xi , for each i, 0 ≤ i ≤ n, and for any order k. If xi selects
state s, 0 ≤ i < n, we say that xi+1 is emitted in state s and
that s occurs (in position i) in xn . We denote by ns (xn ) the
number of occurrences of s in xn , and, for a ∈ A, we denote
(a)
by ns (xn ) the number of times a symbol xi = a is emitted
(a)
in state s. We omit the dependence on xn of ns , ns , and
other notations, when clear from the context.
The kth order ML probability of a sequence xn is determined by the ﬁxed initial state and the empirical probabilities
ˆ
Pk (·|s) conditioned on k-states s,

estimation procedure has practical computational implications,
as a bound on the estimated order translates to a bound on the
memory requirements of an algorithmic implementation of the
estimator. Thus, in this paper, we study the maximum possible
ˆ
value of k(xn ), for any sequence xn , when no a-priori bounds
are imposed on the candidate orders.
ˆ
For k = k(x), writing C0 (xn ) ≥ Ck (xn ), trivially bounding
the ML probability, and rearranging terms, yields
n log α
≥ αk − 1,
f (n)

(3)

from which a uniform upper bound (log n− log f (n))/ log α+
ˆ
O(1) on k(xn ), which we denote by k(n), was obtained in [7].
We will show that, under mild conditions on f (n), which are
satisﬁed by commonly used penalization functions, the bound
k(n) is not far from tight, in the following sense: for any
sufﬁciently large n, and any k < k(n), there are sequences of
length n that estimate order k; moreover, for all sufﬁciently
large k and all but a vanishing fraction of the values of n such
that k = k(n), there are sequences of length n that estimate
order k. After some preliminaries in Section II, these results
are presented in Section III, by showing explicit constructions
of sequences that attain the claimed estimated orders. The
constructions rely on properties of de Bruijn sequences [8].
We initially present results for arbitrary values of α, and then
show that these results can be tightened in the case α = 2
by exploiting properties of a special kind of binary de Bruijn
sequence, the so-called Ford sequence [9]. In Section IV, we
extend our study to the MDL estimator based on the KT
probability assignment. We show that in this case, there exist
1
sequences xn that estimate order n1/2− for any ∈ (0, 2 ).
This order is much larger than the maximum possible order,
k(n) = log n/ log α + o(log n), attainable by a BIC estimator,
and also of the order o(log n) required for consistency of the
KT-based estimator [4]. In fact, we show that, in a universal
lossless compression setting, for the constructed sequences,
imposing an artiﬁcial upper-bound on the allowed estimated
order could incur a signiﬁcant asymptotic penalty in overall
description length. Similar results (with the same individual
sequences) are obtained when a context tree [5], [6], rather
than a plain Markov order, is estimated, using either the KT
or the (tree) BIC estimator.

(a)

so that

ns
ˆ
,
Pk (a|s) =
ns

s ∈ Ak , a ∈ A ,

(4)

(a)

ˆ
− log Pk (xn ) = −

n(a) log
s
s∈Ak ,a∈A

ns
.
ns

(5)

The class of PML estimators of interest is deﬁned by (1)–(2),
where we assume that f (n) is positive and nondecreasing, with
n
n
→
f (n)−
→∞ and f (n) − 0.2 We refer to the ﬁrst and second
n
terms on the right-hand side of (1), respectively, as the ML
term (speciﬁed in (5)) and the penalty term of order k.
ˆ
The upper bound k(n) on k is deﬁned as the largest value
of k satisfying (3) for a given n. Reciprocally, given k, the
smallest integer n satisfying (3), denoted n(k), is a lower
bound on the length of sequences that can estimate order k.
In particular, from the deﬁnition of n(k), we have
αk − 1
αk − 1
f (n(k)) ≤ n(k) <
f (n(k) − 1) + 1 .
log α
log α

(6)

The following lemma follows readily from the foregoing
deﬁnitions, and from (6).
Lemma 1. Given a value of n, the inequality (3) holds for
n
k
all k, 0 ≤ k ≤ k(n). We have k(n)−
→∞, n(k)−
→∞, n(k) is
nondecreasing, and, moreover, for sufﬁciently large k,
αk
αk+1
f (n(k + 1))−1 < n(k + 1)−n(k) <
f (n(k + 1)) .
log α
log α
(7)
III. S EQUENCES THAT MAXIMIZE PML- ESTIMATED ORDER
In this section we exhibit sequences of length n that get
very close to, or even precisely attain, the bound k(n) of the
previous section. The constructions will be based on de Bruijn
sequences, whose properties we review next.
A kth order de Bruijn sequence [8] is a sequence
k
b α , of length αk , k ≥ 0, such that the sliding window
bi+1 bi+2 . . . bi+k , with indices taken modulo αk , exhausts all
distinct k-tuples over A. De Bruijn sequences exist for every
order k, and every cyclic permutation of a de Bruijn sequence
is itself a de Bruijn sequence of the same order. We denote
by Bk the (nonempty) set of de Bruijn sequences of order k
that have x0
−k+1 as a sufﬁx (i.e., they match, cyclically, the

II. P RELIMINARIES AND PROPERTIES OF THE UPPER
BOUND

uj
i

We denote by
the string ui ui+1 . . . uj over A, with uj =
i
λ, the empty string, when i > j. We omit the subscript when
i = 1. We let |u| denote the length of a string u, and uv
the concatenation of strings u and v. The terms string and
sequence are used interchangeably.
We model a sequence xn as the realization of a generic
kth order Markov process, where k is unknown. We regard a
string s ∈ Ak as a state of the Markov process and we say
that a sequence y selects state s whenever s is a sufﬁx of y.
When k is not clear from the context, we explicitly refer to
s as a k-state. For the purpose of selecting states, we assume

2 In the case of penalization functions such as f (n) = c log n or f (n) =
c log log n, we assume n is large enough so that f (n) is positive.

2

over (˜, ∞) for some z ∈ R, and
z
˜
α
zf (z) < f (z) −
for all z ∈ (˜, ∞) .
z
(11)
2
It is readily veriﬁed that commonly used penalization functions
are nice. In particular, this includes functions of the form
f (n) = c log n, f (n) = c log log n, and f (n) = cnβ for
positive constants c and β < 1. The following lemma is an
immediate consequence of (11).

assumed ﬁxed initial condition). For a sequence u, we denote
by (u)∗ the concatenation of an inﬁnite number of copies of
u, and, when |u| ≥ n, we denote by [ u ]n the truncation of u
to length n. Let Bn denote the set of sequences
k
Bn =
k

k

(bα )∗

k

n

bα ∈ Bk .

(8)

The following lemma follows immediately from the deﬁnition (8) and the properties of de Bruijn sequences.

Lemma 3. If f is nice, then n/f (n) is strictly increasing with
n in (˜, ∞).
z

Lemma 2. Let xn ∈ Bn . If k ≥ k, then a k -state, when
k
it occurs in xn , always emits the same symbol. In particular,
when n = mαk for some integer m ≥ 0, then
(i) each k-state s occurs m times in xn , and we have m =
(a)
ns = ns for some a∈A (which depends on s);
(ii) if j < k, each possible j-state s occurs mαk−j times
in xn and each symbol of A is emitted mαk−j−1 times
(a)
in s, i.e., ns = mαk−j−1 and ns = mαk−j for all
a ∈ A and all s ∈ Aj .

In the sequel, for a real number z and a positive integer N ,
we write z N as shorthand for N z/N , i.e., the smallest
multiple of N that is not smaller than z.
Theorem 2. Assume f is nice. Then, for sufﬁciently large k,
ˆ
if n > n(k) αk and xn ∈ Bn , then k(xn ) = k.
k
Remark: To interpret Theorem 2, we observe that for a given
value of k, by Lemma 3, the set of integers n such that k(n) =
k is given by the range n(k) ≤ n < n(k + 1). The fraction
of values of n in this range for which the theorem does not
provide a sequence of length n that estimates order k(n) is
upper-bounded by

Theorem 1. For sufﬁciently large n, if k < k(n) and xn ∈
ˆ
Bn , then k(xn ) = k.
k
Proof: By Lemma 2, if k ≥ k, then the ML term of order
k of xn is zero. Thus, since the penalty term grows with the
n
ˆ
order, we must have k(xn ) ≤ k. Let m = αk . If j < k,
then by Lemma 2 (ii), we have, for all a ∈ A and all s ∈ Aj ,
(a)
ns (xn ) ≤ (m + 1)αk−j−1 and ns (xn ) ≥ mαk−j , where at
n(a)
s
least one inequality is strict, which implies that ns < m+1 .
mα
Therefore, by (5), we have

log α
αk
<
n(k + 1) − n(k)
f (n(k + 1)) −

−−→ 0 ,
−−

where the inequality follows from the leftmost inequality
in (7), and the limit follows from the unboundedness of n(k)
and f (n). Thus, Theorem 2 guarantees that for all but a
vanishing fraction of values of n such that k(n) = k, there
are sequences of length n that estimate order k.
To prove Theorem 2, we rely on a series of lemmas.
Lemma 4 below follows immediately from [4, Lemma 4].
For a probability vector P = (p1 , . . . , pα ), we denote by
α
H(P ) = − i=1 pi log pi the entropy of P .

m+1
mα
= n log
.
mα
m+1
s∈Aj ,a∈A
(9)
ˆ
Using (9) and recalling that − log Pk (xn ) = 0, we obtain, for
0 ≤ j < k,
mα
+ f (n)αj − f (n)αk
Cj (xn ) − Ck (xn ) > n log
m+1
mα
f (n)(1 − αk+1 )
> n log
+
m+1
α
mα
n log α
≥ n log
−
m+1
α
√ m
1
m
1− α
= n log α
≥ n log
2
, (10)
m+1
m+1
ˆ
− log Pj (xn ) > −

k→∞

log α
αk

n(a) log
s

Lemma 4. If P = (p1 , . . . , pα ) is a probability vector
1
2
satisfying 2α ≤ pi ≤ α for all i, 1 ≤ i ≤ α, then
α

H(P ) ≥ log α − α

pi −
i=1

1
α

2

.

Lemma 5. Let xn ∈Bn , where n and k satisfy (3), let m =
k
n
, r = n − αk m, and assume m≥1. Then,
αk
rα
ˆ
− log Pj (xn ) ≥ n log α −
, 0 ≤ j < k.
(12)
4m
ˆ
Proof: Since − log Pj (xn ) is non-increasing with j, it
sufﬁces to consider j = k−1. For a (k−1)-state s, let Rs
be the set of symbols of A that are emitted in state s in
xn k +1 , the truncated (possibly empty) copy of a sequence
mα
from Bk at the end of xn . Let rs = |Rs |, and deﬁne
T = { s ∈ Ak−1 | rs > 0 }. Clearly, s rs = r, and |T | ≤ r.
(a)
By Lemma 2 (ii), we have ns = m + 1 if a ∈ Rs and
(a)
ns = m otherwise, so that ns = mα+rs . Thus, with m ≥ 1,
we have, for all a ∈ A,

where the third inequality follows from the ﬁrst claim of
Lemma 1 and the fact that k+1 ≤ k(n), and the last inequality
holds since α ≥ 2. It follows from (10) that Cj (xn ) > Ck (xn )
when m ≥ 3. The latter condition, in turn, holds for all
sufﬁciently large n, since, by (3), and with k > 0, we have
n/αk ≥ (1 − α−k )f (n)/ log α ≥ f (n)/(2 log α), which is
unbounded by our assumptions on f .
Theorem 1 shows that for sufﬁciently large n, we can
construct sequences that estimate any order k up to k(n)−1.
We next show that, with additional mild assumptions on f (n),
for most values of n we can construct sequences that estimate
precisely order k(n). We say that the function f is nice if it is
deﬁned over the positive reals, f is concave and differentiable

(a)

m
ns
m+1
2
1
≤
≤
≤
≤ ,
2α
mα + rs
ns
mα + rs
α

3

ˆ
ˆ
and Lemma 4 applied to Ps = Pk−1 (·|s) yields, together with
some algebraic manipulations,
ˆ
H(Ps ) ≥ log α −α

2

m+1
1
mα+rs − α

+

a∈Rs

1
m
mα+rs − α

Since µ≥˜, by (11), the right-hand side of (17) is positive
z
if r > 0. If r = 0, since n > n(k) αk ≥ n(k), by (3)
and Lemma 3, the right-hand side of (15) is positive. Hence,
ˆ
Cj (xn )>Ck (xn ) for j<k, and, thus, k(xn )≥k. Furthermore,
ˆk (xn )=0 and the penalty term increases with k,
since − log P
ˆ
we must have, in fact, k(xn ) = k.
Let n(k) denote the least integer n in the interval n(k) ≤
n < n(k+1) such that for all n ≥ n(k) in that interval, there
are sequences of length n that estimate order k. By Theorem 2,
we have n(k) − n(k) ≤ αk . We next show that, in the special
case α = 2, we can exploit known properties of special binary
de Bruijn sequences to reduce this gap to n(k)−n(k) = o(2k ).
Speciﬁcally, the Ford sequence of order k ≥ 0, which will
k
be denoted Fk = a2 , is constructed as follows: start with
1
ak−1 = 0k−1 , and extend the sequence using the least-ﬁrst
1
greedy algorithm [9], where, for k < i ≤ 2k , given s =
ai−1 , we set ai = 0 if ns = 0 and ai = 1 otherwise
i−k+1
(i.e., of the sibling k-tuples s0 and s1 we always choose s0
ﬁrst). It is readily veriﬁed that a de Bruijn sequence of order
k is indeed constructed this way, and that the sequence is
lexicographically ﬁrst among all binary de Bruijn sequence of
n
order k. We denote by Fk the sequence [(Fk )∗ ]n .
The following lemma is an immediate consequence of [10,
Theorem 1] (n0 and n1 are interpreted as special cases of ns ).

2

a∈A\Rs

(α − rs )rs
.
(13)
= log α −
(mα + rs )2
Now, writing the ML term of order k−1 in terms of stateconditioned empirical entropies, and applying (13), we obtain
ˆ
− log Pk−1 (xn ) =

ˆ
ns H(Ps )
s∈Ak−1

≥

ns log α −

ns

(α − rs )rs
(mα + rs )2

s∈Ak−1

s∈T

= n log α −

(α − rs )rs
,
(mα + rs )

s∈T

(14)

where we recall that rs = 0 for s∈T , and, for the last equality,
∆
that ns = mα + rs . We claim that g(rs ) = (α−rs )rs is uppermα+rs
α
bounded by 4m for all s, which, by (14), would sufﬁce to
prove (12). Indeed, elementary analysis of the function g(ρ)
for ρ ≥ 0 reveals that it has a global maximum at ρ∗ =
α( m(m + 1) − m), with
√
√
α
g(ρ∗ ) = α( m + 1 − m)2 ≤
,
4m
where the inequality is readily veriﬁed for m ≥ 1.
Proof of Theorem 2: Let k be large enough so that
n(k) ≥ z . By Lemma 3, n and k satisfy (3) for all n ≥ n(k).
˜
Now, for xn ∈ Bn , with n > n(k) αk , Lemma 5 and (1)
k
yield
rα
Cj (xn ) ≥ n log α −
+ f (n) , 0 ≤ j < k ,
4m
with m and r as deﬁned in the lemma. Thus, for 0 ≤ j < k,
ˆ
recalling that − log Pk (xn ) = 0, we have,
rα
− f (n)(αk − 1). (15)
Cj (xn ) − Ck (xn ) ≥ n log α −
4m
Write µ = mαk = n − r. Since µ ≥ n(k) αk , µ and k
satisfy (3) with µ in the role of n, i.e., we have αk − 1 ≤
µ
f (µ) log α. Thus, from (15), we have, for 0 ≤ j < k,

n
Lemma 6. Let xn = Fk . We have n0 (xj ) − n1 (xj ) =
k
O 2 log k for all j, 1 ≤ j ≤ n.
k
n
Theorem 3. Assume f is nice and xn = Fk . For sufﬁciently
large k and a well-characterized function g(k) = O(1), if
2k log k
,
(18)
n ≥ n(k) + g(k)
k
n
ˆ
then k(x ) = k.

Remark: It will turn out in the proof of Theorem 3 (given in
the full paper) that for some penalization functions of interest
we have, in fact, g(k) = o(1). In particular, if f (z) = c log z
with c > 0, the second term on the right-hand side of (18) is
k
log
O 2 k2 k , whereas for f (z) = cz β , with 0 < β < 1, it is
of the form O

2(1−β)k log k
k

.

IV. S EQUENCES WITH LARGE MDL- ESTIMATED ORDER
In this section, we consider the MDL Markov order estimator based on the KT probability assignment. We construct
sequences xn that estimate orders that are much larger than
those attainable by a BIC estimator, or than the bound on the
order required for consistency of the KT estimator. For notational simplicity, we focus on the case α = 2 (A = {0, 1}).
The KT probability [2] of order 0 of a binary sequence xn
is deﬁned as KT0 (λ) = 1 (for n = 0), and

rα
f (n)
−
µ log α
4m f (µ)
f (n) − f (µ)
rα
=−
µ log α −
+ r log α
f (µ)
4m
rf (µ)
rα
≥−
µ log α −
+ r log α ,
(16)
f (µ)
4m
where the last inequality follows from the fact that f is
˜
concave in (˜, ∞) and µ ≥ n(k) αk ≥ z . Now, since n
z
and k satisfy (3), for k > 0 we have m ≥ f (n)/(2 log α), so,
recalling also the monotonicity of f , it follows from (16) that
rα log α
rf (µ)
µ log α −
+ r log α
Cj (xn )−Ck (xn ) ≥ −
f (µ)
2f (µ)
µf (µ) + α/2
= −
+ 1 r log α . (17)
f (µ)
Cj (xn )−Ck (xn ) ≥ (µ+r) log α −

Γ n0 (xn )+ 1 Γ n1 (xn )+ 1
2
2
, n > 0, (19)
Γ(n + 1)
where Γ is the Gamma function. The KT probability of order
k ≥ 0, in turn, is deﬁned as
KT0 (xn ) =

KTk (xn ) =

KT0 (xn [s]) ,
s∈Ak

4

(20)

where xn [s] denotes the subsequence of symbols from xn that
occur in state s. Using this distribution, one can construct a
lossless description of xn of length
CKT,k (xn ) = − log KTk (xn ) + c(k) ,

where the second inequality follows by applying the uniform
bound h(p)≥4p(1 − p), recalling that 0≤j<k, and dropping
some nonnegative terms. Now, by (21), (25), and (26), recalling that m = n/(k + 1) and c(j) > 0, we obtain

(21)

4n(δ − 1) δ−1
n
−
log
2
(k + 1)
2
k+1
(2k+1)(k+1)γ
−(δ−1)ν −
− c(k).
n
Recalling that 1 < δ ≤ k+1, and factoring out δ−1, we verify
that, for 0 ≤ j < k, we have CKT,j (xn ) > CKT,k (xn )
whenever (24) holds. It is readily shown in the full paper that
CKT,j (xn ) > CKT,k (xn ) also when j > k. Therefore, under the
ˆ
conditions of the theorem, we have kKT (xn ) = k.
CKT,j (xn ) − CKT,k (xn ) >

where c(k)=O(log k) is the (non-decreasing) length of an
efﬁcient encoding of k, and we ignore integer constraints on
code lengths. The estimated Markov order for a sequence xn
is the value of k that yields the shortest description, namely,
ˆ
kKT (xn ) = arg min CKT,k (xn ) .
(22)
k≥0

We will make use of the following relation, which follows
from Stirling’s approximation (see, e.g., [11]). For every
sequence xn , we have
ˆ
− log KT0 (xn ) + log P0 (xn ) − 1 log n − ν ≤ γn−1 , (23)
2
√
where ν = log 2π, and γ is a positive constant.
n
Deﬁne the sequence Uk = [(10k )∗ ]n , where 10k is a string
consisting of a 1 followed by k 0’s. For simplicity, we assume
that the sequence x0 used to determine initial states is all
−∞
0’s, and that n is a multiple of k + 1. These constraints can
be easily removed.

Remark. Consider the case where an upper bound K(n) =
o(n1/2− ) is imposed on the allowed estimated order of the
KT-based MDL estimator (or the tree BIC estimator). Choose
n
k = (1 + ξ)K(n) for some ξ > 0, and xn = Uk . By (25),
recalling that m = n/(k + 1), we obtain CKT,k (xn ) =
O(K(n) log n) for this choice of k. On the other hand, if
j ≤ K(n), then, by (26), we have CKT,j (xn ) = Ω(n/K(n)),
n
and, hence CKT,k (xn )/CKT,j (xn ) − 0. We conclude that
→
limiting the allowed estimated order as assumed incurs a
signiﬁcant asymptotic penalty in the description length of
the individual sequence xn . The gap is more pronounced
the smaller the upper bound K(n) is. In particular, with
K(n) = O(log n) (as required for consistency of the MDL
or tree estimator [4]), we obtain a code length Ω(n/ log n)
with the restricted estimator, as compared to O(log2 n) with
an unrestricted one.

n
Theorem 4. Let n = (k+1)m for some m ≥ 1, and xn = Uk .
If k and n satisfy
4n
1
n
(2k+1)(k+1)γ
≥ log
+ν+
+ c(k) , (24)
2
(k+1)
2
k+1
n
ˆ
then kKT (xn ) = k.

Remark. It is readily veriﬁed that, for sufﬁciently large n, the
condition (24) is satisﬁed by values of k as large as k = n1/2−
1
for any ∈(0, 2 ). Notice that, since the KT cost penalizes only
states that do occur in xn , the estimated order for the sequence
n
xn =Uk would be the same if the model under estimation was
a context tree [5]. In fact, it will be shown in the full paper
that a similar result (with the same sequence xn ) holds also
for context trees under the BIC estimator.
Proof of Theorem 4 (outline): For 0 ≤ j ≤ k, xn
contains occurrences of exactly j + 1 states, namely 0j and
0 10j−1− , 0 ≤
< j. When j = k, each such state
occurs exactly m times, always followed by the same symbol.
Thus, the conditional distribution for each occurring k-state is
deterministic, and, from (20) and (23), we obtain
k+1
γ
− log KTk (xn ) ≤
log m + (k+1) ν +
. (25)
2
m
When j < k, states of the form 0 10j−1− occur m times
each, always followed by a 0. The state 0j occurs (k −j +1)m
times, m of them followed by a 1, and the rest followed by a
0. From (20) and (23), writing δ = k−j+1, and denoting the
binary entropy function by h(·), we obtain
j
1
− logKTj (xn ) ≥ δm h(δ −1 ) + log m + log(δm)
2
2
γ
γ
+ (j+1)ν−j −
m δm
j+1
γ
−1
≥ 4m(δ−1)δ +
log m + (j + 1)ν − k , (26)
2
m

R EFERENCES
[1] I. Csisz´ r and P. C. Shields, “The consistency of the BIC Markov order
a
estimator.” Annals of Statistics, vol. 28, no. 6, pp. 1601–1619, 2000.
[2] R. E. Krichevskii and V. K. Troﬁmov, “The performance of universal
encoding,” IEEE Trans. Inform. Theory, vol. 27, pp. 199–207, Mar 1981.
[3] L. Finesso, “Estimation of the order of a ﬁnite markov chain,” in Recent
Advances in Mathematical Theory of Systems, Control, Networks and
Signal Processing, H. Kimura and S. Kodama, Eds. Mita Press, 1992,
pp. 643–645.
[4] I. Csisz´ r, “Large-scale typicality of Markov sample paths and cona
sistency of MDL order estimators.” IEEE Transactions on Information
Theory, vol. 48, no. 6, 2002.
[5] J. Rissanen, “A universal data compression system,” IEEE Trans. Inform.
Theory, vol. 29, pp. 656–664, Sep. 1983.
[6] I. Csisz´ r and Z. Talata, “Context tree estimation for not necessarily
a
ﬁnite memory processes, via BIC and MDL,” IEEE Trans. Inform.
Theory, vol. 52, no. 3, pp. 1007–1016, March 2006.
´
[7] A. Mart´n, N. Merhav, G. Seroussi, and M. J. Weinberger, “Twiceı
universal simulation of Markov sources and individual sequences,” IEEE
Trans. Inform. Theory, vol. 56, no. 9, pp. 4245–4255, Sep. 2010.
[8] N. G. de Bruijn, “A combinatorial problem,” Koninklijke Nederlands
Akademie van Wetenschappen, Proceedings, vol. 49 Part 2, pp. 758–
764, 1946.
[9] H. Fredricksen, “A survey of full length nonlinear shift register cycle
algorithms,” SIAM Review, vol. 24, no. 2, pp. pp. 195–221.
[10] “The discrepancy of the lex-least de Bruijn sequence,” Discrete Mathematics, vol. 310, no. 6-7, pp. 1152 – 1159, 2010.
[11] N. Cesa-Bianchi and G. Lugosi, Prediction, learning, and games.
Cambridge: Cambridge Univ. Press, 2006.

5

