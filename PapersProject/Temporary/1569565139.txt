Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May 16 18:06:19 2012
ModDate:        Tue Jun 19 12:56:13 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      410443 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565139

Network Coded Gossip with Correlated Data
Bernhard Haeupler∗

Asaf Cohen∗

Chen Avin

Muriel M´ dard
e

MIT
haeupler@mit.edu

Ben-Gurion University
coasaf@cse.bgu.ac.il

Ben-Gurion University
avin@cse.bgu.ac.il

MIT
medard@mit.edu

In [2] and [3] the problem of compression with a rate-limited
helper is considered. In [4], Ho et al. considered the multicast
problem with correlated sources which can be viewed as
extending the Slepian-Wolf problem to arbitrary networks
through network coding. Further extensions appeared in [5]
and [6]. In all these studies the goal is to characterize the rate
region for (ﬁxed) static and memory-free networks, that is, the
set of required capacities needed for a multicast.
Gossip schemes were ﬁrst introduced in [7] as a simple and
decentralized way to disseminate a piece of information in a
network. A detailed analysis of a class of these algorithms
is given in [8]. In these schemes nodes communicate by
continuously picking communication partners in a randomized
fashion and then forwarding the information. The main ﬁgure
of merit is the stopping time – the time needed for all nodes
to be informed. Such randomized gossip-based protocols are
attractive due to their locality, simplicity, and structure-free
nature, and have been offered in the literature for various tasks.
For the task of distributing multiple messages [9] introduced
algebraic gossip, a network coding-based gossip protocol in
which nodes exchange linear combinations of their available
messages. This idea was extended to arbitrary networks in
[10] and [11]. Haeupler [12] proved tight bounds for the
stopping time of algebraic gossip for various models, including
(adversarial) dynamically changing networks [13] and nodes
with limited memory [14]. Improved bounds for non-uniform
gossip were given in [15]. The projection analysis developed
in [12] will play a key role in this paper.

Abstract—We design and analyze gossip algorithms for networks with correlated data. In these networks, either the data to
be distributed, the data already available at the nodes, or both,
are correlated. Although coding schemes for correlated data have
been studied extensively, the focus has been on characterizing
the rate region in static memory-free networks. In a gossipbased scheme, however, nodes communicate among each other by
continuously exchanging packets according to some underlying
communication model. The main ﬁgure of merit in this setting is
the stopping time – the time required until nodes can successfully
decode. While Gossip schemes are practical, distributed and
scalable, they have only been studied for uncorrelated data.
We wish to close this gap by providing techniques to analyze
network coded gossip in (dynamic) networks with correlated data.
We give a clean framework for oblivious network models that
applies to a multitude of network and communication scenarios,
specify a general setting for distributed correlated data, and give
tight bounds on the stopping times of network coded protocols
in this wide range of scenarios.

I. I NTRODUCTION
In this paper, we design and analyze information dissemination algorithms in communication networks with correlated
data. In these networks, either the data to be distributed, the
data already available at the nodes, or both, are correlated.
This problem arises in a many networking applications, such
as sensor, peer-to-peer or content distribution networks. One
such example is a large set of distributed temperature sensors
with a clock at the receiver. Both the temperatures at different
sensor locations and the time at which a measurement is taken
have high correlations among each other.
While the current information theory literature includes
several coding schemes for correlated data, the focus in these
works is mainly on characterizing the rate region - the set of
achievable rates. On the other hand, recent work in the networking literature offers a multitude of efﬁcient, decentralized
and address-oblivious schemes for information dissemination
(e.g., randomized gossip). Unfortunately these schemes treat
the data as uncorrelated and neglect any available information
at the receivers. The focus of this paper is to close this gap and
give tools for analyzing gossip-based algorithms in networks
with correlated data.
1) Related Work: Distributed compression has been studied
in information theory mainly through small, canonical problems. In [1], Slepian and Wolf considered the problem of
separately encoding two correlated sources and joint decoding.

A. Our Contributions
To our knowledge this paper is the ﬁrst to combine these
two strains of research and analyze gossip based protocols in
networks with correlated data. Our contributions are manifold:
First, we give a clean and general framework for oblivious network models in Section III and deﬁne a setting for a correlated
environment in Section II. In this general setting we extend the
projection analysis of [12] by making a connection between
the coefﬁcient vectors a node knows and the amount and type
of information it has learned. This results in simple, direct and
self-contained proofs of tight bounds on the stopping time in
the canonical models of one source and side information at the
receivers, as well as two correlated sources. In Section V we
then give results for the general scenario of multiple sources
and side information. We do this by providing tight bounds
on the time required for any set of (fractional) capacities
to be induced by the (random) packet exchanges generated

This work is partially supported by DARPA under Contract No. N6600111-C-4003 and by RESCUE, the Israeli Chief Scientist.
∗ A. Cohen and B. Haeupler contributed equally to this work.

1

the number of coefﬁcients. This renders the overhead of the
header negligible leaving a packet size of s bits as desired.
At each node, independent linear equations on the blocks
are collected for decoding. We denote with Sv the subspace
spanned by all coefﬁcient vectors received at node v. We also
use the following notion of knowledge from [12]:

in an oblivious network model. This allows to transform
results on the rate region of static memory-free networks (e.g.,
[4], [6]) into bounds on the stopping times of gossip-based
algorithms. These capacity bounds are interesting in their own
right and have the potential to be useful in other information
dissemination problems. Due to space limitation, the complete
proofs are given in [16].

Deﬁnition 1. Node v knows a coefﬁcient vector µ iff Sv is not
orthogonal to µ, i.e, there exists c ∈ Sv such that c, µ = 0.

II. N ETWORK AND I NFORMATION M ODEL

Lastly, we will make use of the following lemma.

In this section we state the broadcast problem, deﬁne the
network and communication model and specify the nature of
the source and side information.
1) Network and Communication Model: For simplicity, we
will assume that the network consists of a ﬁxed set V of n
nodes. Communication takes place in synchronous rounds. In
each round, each node v decides on a packet pv of s bits to be
sent out (possibly using randomness). Given the current state
of the network, the network model then speciﬁes (possibly
using randomness) which packet will be received by which
node in the current round. This corresponds to a probability
distribution over directed edge sets where a directed edge
(v, u) means that the packet pv is successfully received by
node u. Nodes are assumed to have unlimited memory (for
schemes with limited buffers see [14]). We denote the active
edge set of directed edges chosen for round t with Et .
2) Source and Side Information: We assume random variables {Xi }k ∪ {Yv }v∈V that are arbitrarily correlated aci=1
cording to some known memoryless joint distribution. To
generate the messages and side information we take l i.i.d.
samples from these variables and denote the resulting message
vectors with x1 , . . . , xk and the side information vectors with
yv . We assume that the k messages are initially distributed
to nodes (i.e., sources) such that each vector is given to at
least one node. We also assume that each node v ∈ V (or
terminal) is given the vector yv as side information. Given
this initial information distribution, we are interested in the
time when nodes are able to decode x1 , . . . , xk based on their
side information and the packets exchanged with other nodes.
3) The Encoding and Decoding Schemes: For a given ﬁeld
size q and slack δ > 0 we assume throughout that nodes
employ the following coding scheme: Prior to communication,
source nodes perform random binning, that is, for every 1 ≤
i ≤ k each node receiving the message vector xi applies the
same random mapping into 2l(H(Xi )+δ) bins. The resulting
bin indices (the same for every node initialized with xi ) are
l
interpreted as vectors of length h = log q (H(Xi ) + δ) over

Lemma 1 ([6], [3]). Let X ∈ X and Y ∈ Y be two arbitrarily
correlated random variables and let x, y be two vectors that
are created by taking l i.i.d. samples from their joint distribution. Suppose, for some > 0 and for some constant δ > 0,
all possible sequences in X l are randomly and uniformly
divided to at least 2l(H(X|Y )+δ) bins. Then joint typicality
decoding correctly decodes x with probability at least 1 −
for l = Ω(log −1 ) using y and any (H(X|Y ) + δ)l bits of
information on the bin index of the bin in which the true x
resides.
In particular, Lemma 1 asserts that having access to the side
information y, the message vector x can be decoded with high
l
probability using any s (H(X|Y ) + δ) linearly independent
equations on the blocks describing the bin index of x.
III. O BLIVIOUS N ETWORK M ODELS
In this section we introduce the deﬁnition of an oblivious
network model. This gives a clean and very general framework
capturing a wide variety of communication and (dynamic)
network settings. While this was already somewhat implicit
in [12] we restrict ourselves to networks without adaptive
adversarial behavior. This greatly facilitates the much cleaner
framework presented in this section.
Deﬁnition 2. A network model is oblivious if the active edge
set Et of time t only depends on t, Et for any t < t and
some randomness. An oblivious network model is i.i.d. if the
active edge set Et is sampled independently for every t from
the same distribution.
The following are examples of oblivious (and i.i.d.) network
models: In the (Uniform) Gossip Model [10], [11], [12] one
has an underlying (directed) graph G and in each round each
node picks a random neighbor as a communication partner. A
node then sends (PUSH) or requests (PULL) a packet from
its partner or both (EXCHANGE). The Rumor Mongering [9]
or Random Phone Calls Model [8] is a well-studied special
case of this in which G is complete, that is, nodes pick a
random node as a communication partner. It is easy to include
more sophisticated features in an oblivious network model.
Random packet losses in wired networks, or characteristics
of radio networks like half-duplex transmission, collisions,
packet loss rates depending on SNR and more can be easily
modeled by removing edges according to (randomized) rules.
An interesting example of non-i.i.d. oblivious network models
are (edge-)markovian evolving graphs [17].

the ﬁnite ﬁeld Fq . These vectors are split into h log q blocks
s
s
of log q symbols in Fq each, for a total of s bits per block.
During the communication phase nodes sends out random
linear combinations (over Fq ) of these blocks as packets. To
keep track of the linear combination contained in a packet
one coefﬁcient for each block of each message is introduced
and sent in the header of each packet. As in all prior works
on distributed network coding (e.g., [9], [10], [11], [12], [14],
s
[15]), we assume that log q is sufﬁciently large compared to

2

For any oblivious network model M we can deﬁne a
random ﬂooding process F (M, p, S). Informally, this process
describes which nodes are informed over time if initially only
nodes in S are informed and from there on every informed
node informs all its communication partners (as speciﬁed by
M ). The only modiﬁcation to this standard infectious process
is the parameter p which adds an independent probability of
p for each transmission to be overheard.

S of nodes. It is easy to check that the probability that a node
v does not know µ after it has received a packet from a node
that knows µ is at most 1/q. This implies that knowledge
of µ spreads through the network exactly as the ﬂooding
process FM,1/q,S . Using the assumption, Deﬁnition 4 and the
monotonicity of SF (M,1/q,S) in S we get that the probability
k
that a vector µ ∈ Fq is not known to all vectors after T steps
−(k+log −1 )
is at most q
. A union bound over all q k vectors
−1
shows that with probability at least 1 − q − log
≥ 1 − all
node will know about all vectors and it is easy to see that this
implies that all nodes are able to decode the messages.

Deﬁnition 3. Let M be an oblivious network model, p be
a probability of fault and S ⊆ V be a starting set of
nodes. We deﬁne the ﬂooding process F (M, p, S) to be the
random process S1 ⊆ S2 ⊆ . . . that is characterized by
S1 = S and for every time t we deﬁne St+1 by taking
each of the (directed) edges Et speciﬁed by M for time t
independently with probability 1 − p to obtain Et and then we
set St+1 = {v ∈ V | ∃u ∈ St : (u, v) ∈ Et ∨ v = u}.

IV. S IMPLE , D IRECT P ROOFS FOR T IGHT S TOPPING T IMES
In this section we give a simple, direct derivation of tight
stopping time bounds for gossip with one source and side
information at the nodes and gossip with two correlated
sources. Our two main results in this section are the following.

Note, that Deﬁnition 3 is only well deﬁned if M is oblivious.
Furthermore, F becomes a time-homogeneous Markov chain if
M is also i.i.d. Also, as long as for every time t the union over
the edges in M from t to inﬁnity is almost surely connected
then F is absorbing with the unique absorbing state V . We
say the ﬂooding process F stops if it reaches this absorbing
state and we denote the time this happens with the random
variable SF . The next deﬁnition pairs this ﬂooding time with
a throughput parameter α that corresponds to the exponent
of the ﬂooding process tail probability. The reason for this
deﬁnition and its connection to the multi-message throughput
behavior of network coding becomes apparent in the statement
and proof of Theorem 1 below.

Theorem 2. Suppose M is an oblivious network that ﬂoods
in time T with throughput α. For any error probability > 0,
any constant δ > 0, l = Ω(log −1 ), any packet size s
and any distributions X, {Yv }v∈V suppose the network is
initialized with a single message x generated from X and
side information yw generated from Yw at every node w. Then,
every node v will correctly decode x with probability at least
l
1
1 − after T = T + α s (H(X|Yv ) + δ) + log −1 + 3
rounds.
Theorem 3. Suppose M is an oblivious network that ﬂoods
in time T with throughput α. For any error probability
> 0, any constant δ > 0, l = Ω(log −1 ), any packet
size s and any distributions X1 , X2 suppose the network is
initialized with two messages x1 , x2 generated from X1 , X2
and nodes have no side information. Then, every node v
will correctly decode x with probability at least 1 − after
1
l
T + α s (H(X1 , X2 ) + 2δ) + log −1 + 3 rounds.

Deﬁnition 4. We say an oblivious network model M on a
node set V ﬂoods in time T with throughput α if there exists
a prime power q such that for every v ∈ V and every k > 0
we have P [SF (M,1/q,{v}) ≥ T + k] < q −αk .
To give a few illustrating examples of ﬂooding times we
note that the random phone call network model on n nodes
ﬂoods in Θ(log n) time with constant throughput. The uniform
gossip model on a connected degree bounded graph G ﬂoods
in time Θ(D) and with constant throughput where D is the
diameter of G. In many oblivious network models it is easy
to give tight bounds on the ﬂooding time and throughput.
With this framework for oblivious network models in place
we can give a cleaner restatement of Theorem 3 in [12]. We
also provide a sketch of the proof since we will later expand
on the ideas used therein.

The idea for proving these theorems is to generalize the
observation of [12] that the question of when a node can
decode is equivalent to determining when this node knows
(see Deﬁnition 1) enough coefﬁcient vectors. The proof of
Theorem 1 shows that ﬂooding or spreading of knowledge of
vectors can be analyzed using a union bound. This implies
that only the number of vectors needed is of importance. In
the case with uncorrelated sources and no side information
essentially knowledge of all coefﬁcient vectors is necessary.
In the correlated scenario, however, we want to relate the
number of vectors a node v needs to know to the conditional
entropy H(X|Yv ). Lemma 1 helps in this respect. It asserts
that in order to decode, a node with side information Y does
not necessarily need i = (H(X|Y ) + δ)l speciﬁc bits, but
rather, assuming joint typicality decoding, it requires only
any sufﬁcient amount of information about the index of the
bin in which x resides. This is achieved by any i/s packets
containing independent equations on the bin index. We can
thus focus on the knowledge a node is required to obtain in
order for its coefﬁcient matrix to have rank at least i/s.

Theorem 1 (Theorem 3 of [12]). Suppose M is an oblivious
network model that ﬂoods in time T with throughput α. Then,
for any k, random linear network coding in the network model
M spreads k arbitrarily distributed messages to all nodes with
1
probability 1 − after T = T + α (k + log −1 ) rounds.
Proof: The random linear network coding protocol we
analyze will use the same ﬁeld size that achieves the parameters T and α for M in Deﬁnition 4. We ﬁx a coefﬁcient vector
k
µ ∈ Fq . This vector is initially known to a non-empty subset

3

Unfortunately it is possible that a node knows many vectors
without having a large rank. In fact, upon reception of the ﬁrst
packet a node gets to know all but a 1/q fraction of all vectors.
On the other hand, in order to prove faster stopping times we
want to argue that the knowledge of only an exponentially
small fraction of all vectors sufﬁces for decoding. This is
achieve by the following lemma which shows that indeed only
q l speciﬁc coefﬁcient vectors sufﬁce to guarantee that at least
l independent coefﬁcient vectors were received:

accurate measure of the actual capacities achievable between
sets of nodes. Namely, to analyze the capacities induced by
the gossip packet exchange process. This is an interesting
question in its own right, and, in particular, can give a “blackbox” method to transfer any results of prior works that bound
the rates or capacities needed between sources and sinks in
the static memory-free setting to stopping times in oblivious
network models.
Herein, we develop such a characterization, and apply it to
the results of [4] and [6] to obtain stopping times for gossip
protocols with an arbitrary number of correlated sources and
side-information, generalizing the results from the last section.
We ﬁrst introduce the required notation.

Lemma 2. Let V be a ﬁnite dimensional vector space over
Fq . For every 0 ≤ h < dim V there exist w = q h + 1 vectors
v1 , . . . , vw ∈ V such that any (subspace) K ⊂ V for which
K ⊥ does not contain vi for any i has dimension at least h+1.

Deﬁnition 5. Let T > 0, node set V and active edges
E1 , E2 , . . . , ET be given. A path P from s to d is a sequence
of nodes P = (v0 , v1 , . . . , vT ) such that v0 = s, vT = d and
for every t ≤ T we have vt−1 = vt or (vt−1 , vt ) ∈ Et .

Using only random binning (Lemma 1) and Lemma 2, it is
possible to sketch the proofs of the main results in this section.
Proof Sketch of Theorem 2: We use the ﬁeld size
q that achieves the parameters T and α in Deﬁnition 4.
We furthermore choose l large enough so that the decoding
probability in Lemma 1 is at most /2. By Lemma 1, any
node v with access to the side information vector yv and
l
s (H(X|Yv ) + δ) independent equations on the blocks describing the bin index of x assigned by the random binning
procedure, can decode x with probability at least 1 − /2.
It thus remains to show that with probability 1 − /2 we
l
have dim Sv ≥ s (H(X|Yv ) + δ) after T rounds. To prove
this we apply Lemma 2 and get that there exists a set Z
l
of 2 s (H(X|Yv )+δ) coefﬁcient vectors such that if v has
knowledge of these vectors, it indeed has sufﬁciently many
independent equations. Furthermore, we refer to the proof of
Theorem 1 for the fact that knowledge of any coefﬁcient vector
(in Z) spreads through the network like a ﬂooding process. As
before we thus get the fact that in the assumed network model
M the probability that any of the coefﬁcient vectors (in Z) is
1
not known after T + α (k + (log −1 + 1)) rounds is smaller
−k
than /2 · q . Setting k = log |Z| and using a union bound
over all coefﬁcient vectors in Z the result follows.
While the proof of Theorem 3 is similar in nature to that of
Theorem 2, there is delicate point when considering multiple
sources. In a single source scenario, for each terminal there is
only one equation governing the rate. That is, r ≥ H(X|Yv )+
δ. Using Lemma 2, this rate constraint is translated into a rank
l
constraint, namely, dim(Sv ) ≥ s (H(X|Yv ) + δ) . For more
than one source, however, the rate region is given by multiple
rate constraints, and one has to make sure all are satisﬁed.
Indeed, for two sources and no side information at the nodes
this can be done using a single rank constraint. For more than
two sources, or when additional side information is available,
a more reﬁned analysis is required. This is the subject of the
next section.

Deﬁnition 6. A set of m paths with weights w1 , . . . , wm is
valid if for every t < T and every (u, v) ∈ Et the weights of
paths using (u, v) sum to at most one. We say a set of valid
weighted paths achieves a capacity of c between two nodes s
and d if the weights of paths from s to d sum up to c.
Quite intuitively, these paths correspond to an information
ﬂow through the network from the sources to the sinks; or,
alternatively, to a (fractional) network ﬂow in a time-expanded
(hyper-)graph or trellis. This intuition was made formal in [18]
which proves an explicit equivalence between the algebraic
gossip protocol and random linear network coding in the
classical memory-free model (e.g., [4]). The directed timeexpanded hypergraph GP N C that corresponds to a sequence
of active edges can also be found in [18]. We omit the details
of this equivalence and instead only recall the facts needed in
this paper:
Lemma 3. Let node set V , the active edges E1 , . . . , ET ,
destination d ∈ V and sources s1 , s2 , . . . , sk ∈ V be given.
Algebraic gossip on {Ei }T is equivalent to classical random
i=1
linear network coding in the transformed hypergraph GP N C
described in [18]. In particular, if for some integers c1 , . . . , ck ,
it is possible for every si to transmit ci packets to d, then there
exists a sequence of valid paths of weight one and a rate
ci between si and d. Conversely, if for some positive reals
c1 , . . . , ck there is a set of valid weighted paths that achieve
a capacity ci between si and d, then the capacities ci lie in
the min-cut region of GP N C .
Given this setup, we show the ﬁrst result in this direction:
Lemma 4. Let M be a network on a node set V that ﬂoods
in time T with throughput α. For any T , > 0, destination
d ∈ V and set of k source nodes s1 , s2 , . . . , sk ∈ V with
integral capacities c1 , c2 , . . . , ck ≥ 1, suppose E1 , . . . , ET is
a sequence of active edges on V sampled from M . If T ≥
1
T + α ( i ci + log −1 ) then with probability at least 1 −
there exists a selection of valid weighted paths that achieve
the capacity ci between si and t for every i.

V. C APACITIES IN O BLIVIOUS N ETWORK M ODELS
To date, analysis of gossip schemes focused only on the
dissemination time - the number of rounds required to gain the
complete knowledge in the network. However, especially when
dynamic networks are analyzed, it is interesting to gain a more

4

sufﬁcient for v, node v will correctly decoding x1 , . . . , xk with
1
l
probability at least 1 − after T + α ( s i∈[k] ci + δ +
log k + log −1 + δ) rounds.

Note that the above lemma requires the capacities to be
integral and thus essentially asks for the time until a certain
number of mutually disjoint paths occur. While this is sufﬁcient and optimal in the uncorrelated information spreading
setting this requirement can be a severe restriction.
One setting where this makes a drastic difference is when
we have k sources and the total capacities needed sum up to
less then one. This corresponds to asking for the time until
there is a path from each of the sources to the sink – without
these paths having to be disjoint. If one considers for example
the random phone call model with n nodes and k sources it
takes in expectation log n+k time until a disjoint path between
a node and each source appears while merely log n + log k
rounds are sufﬁcient to get this for non-disjoint paths.
The following lemma generalizes this observation and
strengthens Lemma 4 in this direction to give order optimal
bounds for any set of fractional capacities:

VI. C ONCLUSION
In this work, we designed and analyzed gossip schemes
for networks with correlated data. We deﬁned the concepts
of oblivious networks, partial knowledge required to decode
using side information and network capacities in the context
of gossip schemes. This allowed us to give tight bounds on
stopping times of these protocols. The suggested solution is
based on joint network-source coding [19]. While efﬁcient
joint codes exist in certain scenarios [20], [21], the general
case is computationally expensive and remains a fascinating
future research direction.
R EFERENCES
[1] D. Slepian and J. Wolf, “Noiseless coding of correlated information
sources,” IEEE Trans. Inform. Theory, vol. 19, no. 4, pp. 471–480,
1973.
[2] R. Ahlswede and J. K¨ rner, “Source coding with side information and a
o
converse for degraded broadcast channel,” IEEE Trans. Inform. Theory,
vol. 21, no. 6, pp. 629–637, 1975.
[3] A. Cohen, S. Avestimehr, and M. Effros, “On networks with side
information,” in ISIT, 2009, pp. 1343–1347.
[4] T. Ho, M. M´ dard, R. Koetter, D. R. Karger, M. Effros, J. Shi, and
e
B. Leong, “A random linear network coding approach to multicast,”
IEEE Trans. Inform. Theory, vol. 52, no. 10, pp. 4413–4430, 2006.
[5] J. Barros and S. D. Servetto, “Network information ﬂow with correlated
sources,” IEEE Trans. Inform. Theory, vol. 52, no. 1, pp. 155–170, 2006.
[6] M. Bakshi and M. Effros, “On achievable rates for multicast in the
presence of side information,” in ISIT, 2008, pp. 1661–1665.
[7] A. Demers, D. Greene, C. Hauser, W. Irish, J. Larson, S. Shenker,
H. Sturgis, D. Swinehart, and D. Terry, “Epidemic algorithms for
replicated database maintenance,” in PODC, 1987, p. 12.
[8] R. Karp, C. Schindelhauer, S. Shenker, and B. Vocking, “Randomized
rumor spreading,” in FOCS, 2000, vol. 41, pp. 565–574.
[9] S. Deb, M. M´ dard, and C. Choute, “Algebraic gossip: a network coding
e
approach to optimal multiple rumor mongering,” Trans. Networking, vol.
14, pp. 2486–2507, 2006.
[10] D. Mosk-Aoyamam and D. Shah, “Information dissemination via
network coding,” in ISIT, 2006, pp. 1748–1752.
[11] M. Borokhovich, C. Avin, and Z. Lotker, “Tight bounds for algebraic
gossip on graphs,” in ISIT, 2010, pp. 1758 –1762.
[12] B. Haeupler, “Analyzing network coding gossip made easy,” in STOC,
2011, pp. 293–302.
[13] B. Haeupler and D. Karger, “Faster information dissemination in
dynamic networks via network coding,” in PODC, 2011, pp. 381–390.
[14] B. Haeupler and M. Medard, “One Packet Sufﬁces - Highly Efﬁcient
Packetized Network Coding With Finite Memory,” in ISIT, 2011, pp.
1151 –1155.
[15] C. Avin, M. Borokhovich, K. Censor-Hillel, and Z. Lotker, “Order
optimal information spreading using algebraic gossip,” in PODC, 2011,
pp. 363–372.
[16] B. Haeupler, A. Cohen, C. Avin, and M. M´ dard, “Network coded
e
gossip with correlated data,” Arxiv preprint arXiv:1202.1801, 2012.
[17] A. Clementi, C. Macci, A. Monti, F. Pasquale, and R. Silvestri, “Flooding time in edge-markovian dynamic graphs,” in PODC, 2008, pp.
213–222.
[18] B. Haeupler, M. Kim, and M. Medard, “Optimality of Network Coding
with Buffers,” in ITW, 2011, pp. 533–537.
[19] A. Ramamoorthy, K. Jain, P. A. Chou, and M. Effros, “Separating
distributed source coding from network coding,” IEEE Trans. Inform.
Theory, vol. 52, pp. 2785–2795, 2006.
[20] G. Maierbacher, J. Barros, and M. M´ dard, “Practical source-network
e
decoding,” in ISWCS 2009, 2009, pp. 283–287.
[21] C. Avin, M. Borokhovich, A. Cohen, and Z. Lotker, “Efﬁcient Distributed Source Coding for Multiple Receivers Via Matrix Sparsiﬁcation,” in ISIT, 2011, pp. 2045–2049.

Lemma 5. Let M be a network model on a node set V that
ﬂoods in time T with throughput α. For any T , any > 0, any
sink d ∈ V and any set of k source nodes s1 , s2 , . . . , sk ∈ V
1
with rates c1 , c2 , . . . , ck > 0, if T ≥ T + α ( i ci + log k +
−1
log
) then with probability at least 1 − there exists a
selection of valid weighted paths that achieve a capacity of ci
between si and d for every i.
Proof Sketch: The idea is to combine k applications of
Lemma 4 using a union bound and capacity sharing. Set the
failure probability to /k and in the ith application of Lemma 4
set the ci to
i ci while setting all other capacities to zero.
We get that for every i with probability 1 − /k the number
of disjoint paths from si to d is at least
i ci . Via a union
bound, with probability of 1 − all these paths are there. Yet,
while the paths from each source are disjoint, the paths starting
at different sources may not be disjoint. We now take the union
of these paths while choosing the weight of a paths starting at
ci
. This gives capacity of ci between si
source si to be
c
j

j

and d. Furthermore, since any edge e is used by at most one
path going out from each source, we get that the total weight
on e summed over all paths is at most i cic ≤ 1.
j j
We will use Lemma 5 to prove our main result about
information dissemination with correlated data in oblivious
networks. To state our result we need the following deﬁnition:
Deﬁnition 7 (Slepian-Wolf region [1]). A capacity vector c =
(c1 , . . . , ck ) is sufﬁcient for v ∈ V if and only if for every
index subset S ∈ [k] we have i∈S ci ≥ H(XS |XS , Yv ).
Putting together Lemma 5, Lemma 3 and applying the results on network coding with correlated data from [4] and [6],
we can now directly state our main result which generalizes
and encompasses Theorem 3 and Theorem 2:
Theorem 4. Suppose M is an oblivious network model that
ﬂoods in time T with throughput α. For any constant δ > 0,
any error probability > 0, any l = Ω(log −1 ), any joint
distribution of X1 , . . . , Xk and the Yv ’s, any packet size s >
0, any node v and any capacity vector (c1 , . . . , ck ) that is

5

