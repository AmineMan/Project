Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 22:43:31 2012
ModDate:        Tue Jun 19 12:56:25 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      398520 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565611

A New Outer Bound on the Capacity Region of
Gaussian Interference Channels
Junyoung Nam

Giuseppe Caire

Mobile Communications, ETRI, Daejeon, Korea
Email: jynam@etri.re.kr

Electrical Engineering, USC, CA
Email: caire@usc.edu

using a more general genie signal, the successive works in
[3], [4], [5] achieved an enhancement of the ETW bound,
which will be called “enhanced ETW outer bound” in this
paper. Since the enhanced ETW bound contains the Kramer
and ETW outer bounds as special cases, the best known outer
bound is given by the enhanced ETW outer bound whose sumrate upper bound was further tightened later by Etkin [6].
In this paper, we turn our attention to a classical approach
for the proof of converse based on auxiliary random variables
in order to achieve a new outer bound on the capacity region
of the two-user Gaussian IC in the weak interference regime.
In particular, we do not construct a genie-aided channel to
outer-bound the capacity region and instead introduce appropriate auxiliary random variables and a conditional version of
the worst additive noise lemma [7]. Thus, we can obtain a
potentially tighter outer bound than those with the aid of a
genie (explicit side information to the receivers). One major
difﬁculty in upper-bounding the differences of conditional
entropies is the fact that the interfering signal terms are
no longer Gaussian distributed but just with average power
constraints. The role of the auxiliary random variables is
to replace arbitrarily distributed interfering signal with i.i.d.
Gaussian random sequences to avoid some difﬁcult terms.
Then, we can apply the conditional worst additive noise lemma
to some differences of conditional entropies to achieve a new
outer bound. Finally, we show by numerical examples that the
new outer bound is tighter than the previously known outer
bounds for certain range of parameters.
Notations: Cov(X) is the variance of X and Cov(X) is the
covariance matrix of X.

Abstract—The best known outer bound on the capacity region
of the two-user Gaussian interference channel is given as the
intersection of regions of genie-aided outer bounds, in which a
genie provides extra side information to the receivers. In this
paper, we present a new outer bound that does not resort to a
genie-aided channel but make use of auxiliary random variables.
In order to obtain such bound, we introduce a conditional version
of the worst additive noise lemma. The new bound is shown to be
tighter than genie-aided bounds for certain range of parameters.

I. I NTRODUCTION
In the past few years, the Gaussian interference channel (IC)
received great attention as a primitive model for a wireless
network with mutually interfering links. While the IC has been
studied over 30 years, the capacity region of the simplest twouser Gaussian IC is not fully characterized yet. We know the
capacity region only in some special cases such as the very
strong and strong interferences.
The capacity region for the more generally encountered
weak interference situation in wireless networks is still an open
problem. Since the works of Kramer [1] and Etkin, Tse, and
Wang [2], however, there has been a signiﬁcant progress to the
characterization of the sum capacity of the two-user Gaussian
IC in the weak interference regime. Following that work, the
sum capacity of the Gaussian IC in the very weak interference
regime was obtained independently and simultaneously by [3],
[4], [5], in which treating interference as noise with Gaussian
inputs was shown to be optimal. The sum-rate upper bounds
(converses) in all these works are based on a genie-aided
channel, in which a genie provides some side information
to either one or both receivers in order to cast the original
interference channel into a mathematically more tractable one.
The characterization of the entire capacity region in the
weak interference regime is less known. In the past decade, one
successful approach was to use a genie-aided channel to obtain
a tight outer bound to the capacity region. Kramer [1] deﬁned
a general genie signal and selected a genie that provides the
exact information of the interference to one of the receivers
such that the resulting genie-aided channel becomes the onesided Gaussian IC, for which an outer bound is known. Etkin
et al. [2] adopted a different genie signal that provides some
noisy observation of the intended signals to the receivers and
derived an outer bound (so-called “ETW outer bound”) tighter
than the Kramer’s one for certain ranges of parameters. By

II. G ENIE -A IDED O UTER B OUNDS
The two-user real Gaussian IC is deﬁned by
Y1 = X1 + h12 X2 + Z1
Y2 = h21 X1 + X2 + Z2

(1)

where the inputs X1 and X2 have the average power constraints of P1 and P2 , respectively, and Z1 and Z2 are Gaussian
random variables of unit variance that are i.i.d in time and
independent of inputs.
Suppose a sequence of (2nR1 , 2nR2 ) codes with
(n)
limn→∞ Pe
= 0. Let M1 and M2 be independent,
uniformly distributed messages over [1 : 2nR1 ] and [1 : 2nR2 ],

1

n
n
and let X1 ∈ X 1 , X2 ∈ X 2 , Y1n ∈ Y 1 , Y2n ∈ Y 2
be the random sequences induced by encoders
enci : [1 : 2nRi ] → X i , i = 1, 2, and the channel,
respectively.
The main contribution of the work by Etkin, Tse, and Wang
[2] is to devise a genie in a sophisticated manner to get a
tighter outer bound on the channel than that in [1]. The authors
designed the genie signal such that

III. N EW O UTER B OUND
In this section, we provide a new outer bound on the
capacity region of the two-user Gaussian IC. To do so, we
propose a different approach using auxiliary random variables,
without the aid of a genie.
n
n
Given the channel inputs X1 and X2 in the previous
section, we introduce auxiliary random variables (U1 , U2 )
deﬁned by

S1 = h21 X1 + Z2
S2 = h12 X2 + Z1
n
n
so as to obtain the useful properties, h(Y1n |S1 , X1 )
n n
n
n
n
and h(Y2 |S2 , X2 ) = h(S1 ), where Xi denotes the

U1 = h12 X2 + W1
(2)

U2 = h21 X1 + W2

n
h(S2 )

=
sequence
of length n of the input Xi and Si denotes side information
to receiver i for i = 1, 2. Using the genie signal, the authors
provided a meaningful outer bound in [2, Thm. 3] and showed
that a simpliﬁed Han-Kobayashi scheme can achieve a rate
region within one bit to the capacity region of the two-user
Gaussian IC.
Throughout this paper, let Xig ∼ N (0, Pi ) and Yig = Xig +
Zi for all i and the corresponding i.i.d. random sequences are
n
n
denoted by Xig and Yig , respectively. The known outer bounds
on the capacity region of the IC deﬁned by (1) in the weak
interference regime (i.e., h12 ≤ 1 and h21 ≤ 1) are as follows.
Theorem 1 (ETW bound [2]): The capacity region of a
two-user Gaussian IC in the weak interference regime is
contained in the set of rate pairs (R1 , R2 ) satisfying
R1 ≤ I(X1g ; Y1g |X2g )

where Wi is a zero mean i.i.d. Gaussian random sequence
2
n
n
with variance σWi , independent of (X1 , X2 ) and correlated
to Zi with correlation coefﬁcient ρi , for i = 1, 2. The main
n
role of the corresponding auxiliary random sequence U1 is
n
to replace the arbitrary random sequence h12 X2 (interference
with an average power constraint) by the Gaussian random
n
n
sequence h12 W1 whose components are i.i.d., and so U2 is.
We provide several lemmas here to make this paper selfcontained, some of which have appeared in different forms.
The following lemma is a generalization of the well-known
result by Thomas [8] that Gaussian distribution maximizes the
conditional distribution for a given covariance constraint.
n
Lemma 1: Let Xi , i = 1, 2, · · · , m, denote a random
¯
sequence, let S be a subset of [1 : m] and S be its
n
n
n
n
n
complement and let XS = (XS(1) , XS(2) , · · · , XS(s) ), XS =
¯
n
n
n
¯
(XS(1) , XS(2) , · · · , XS(m−s) ), where S(i) and S(i) are the ith
¯
¯
¯
¯
elements of S and S, respectively, and |S| = s. The random
n
sequences {Xi : i = 1, · · · , m} are mutually, arbitrarily
correlated for each time j with covariance matrix given
by K j = Cov(XS(1)j , · · · , XS(s)j , XS(1)j , · · · , XS(m−s)j ),
¯
¯
n
1
K X . Also let (X Sg , X Sg ) be
such that n j=1 K j
¯
a jointly Gaussian random vector with covariance K X ,
where X Sg = (XS(1)g , XS(2)g , · · · , XS(s)g ) and X Sg =
¯
(XS(1)g , XS(2)g , · · · , XS(m−s)g ). Then, we have
¯
¯
¯

(3a)

R2 ≤ I(X2g ; Y2g |X1g )

(3b)

R1 + R2 ≤ I(X1g ; Y1g |X2g ) + I(X2g ; Y2g )

(3c)

R1 + R2 ≤ I(X1g ; Y1g ) + I(X2g ; Y2g |X1g )

(3d)

R1 + R2 ≤ I(X1g ; Y1g , S1g ) + I(X2g ; Y2g , S2g )

(3e)

2R1 + R2 ≤ I(X1g ; Y1g |X2g ) + I(X1g ; Y1g )
+ I(X2g ; Y2g , S2g )

(3f)

R1 + 2R2 ≤ I(X1g ; Y1g , S1g ) + I(X2g ; Y2g |X1g )
+ I(X2g ; Y2g )

n
n
h(XS |XS ) ≤ nh(X Sg |X Sg ).
¯
¯

(3g)

(6)

Proof: We use the standard time sharing argument. Withn
1
out loss of generality, we assume that n j=1 K j = K X . Let
n
Q ∼ Unif[1 : n] be independent of Xi for all i. Then, we
have

The ETW outer bound was improved by choosing a genie
more elaborately in some subsequent works [3], [4], [5], which
used the following natural generalization of the genie signal
in (2):

n
n
h(XS |XS )
¯

S1 = h21 X1 + h21 W1
S2 = h12 X2 + h12 W2

(5)

(a)

n
= h((XS(1) , · · · , XS(s) )n |XS )
¯

(4)

n
j−1
j−1
n
h(XS(1)j , · · · , XS(s)j |XS(1) , · · · , XS(s) , XS )
¯

=

where Wi is the zero mean Gaussian random variable with
2
variance σWi , independent of (X1 , X2 ). Here we allow Wi to
be correlated to Zi with correlation coefﬁcient ρi , for i = 1, 2.
Theorem 2 (Enhanced ETW bound [3], [5]): The capacity
region of a two-user Gaussian IC in the weak interference
regime is outer-bounded by the set of rate pairs (R1 , R2 )
satisfying (3) in which (3e), (3f), and (3g) are tightened by
using the genie signal (4) and the extremal inequality for [3]
(or the entropy power inequality (EPI) for [5]).

j=1
n

≤

h(XS(1)j , · · · , XS(s)j |XS(1)j , · · · , XS(m−s)j )
¯
¯
j=1

= nh(XS(1)Q , · · · , XS(s)Q |XS(1)Q , · · · , XS(m−s)Q , Q)
¯
¯
≤ nh(XS(1)Q , · · · , XS(s)Q |XS(1)Q , · · · , XS(m−s)Q )
¯
¯
(b)

≤ nh(X Sg |X Sg )
¯

2

(7)

where (a) follows from a permutation of the rann
dom sequence XS into the random vector sequence
n
(XS(1) , · · · , XS(s) ) and (b) follows from [8, Lem.1],
given the covariance constraint on the random vector
(XS(1)Q , · · · , XS(s)Q , XS(1)Q , · · · , XS(m−s)Q ).
¯
¯
The inequality (6) may be also viewed as a generalization
of [5, Lem. 1], for which m = 2 and Yin are not arbitrary.
Moreover, our lemma upper-bounds the conditional entropy
of a random sequence with that of a Gaussian random vector,
given the particular covariance constraint that should differ
from [5, Lem. 1] to apply Lemma 1 in our main results.
The following two lemmas are with respect to the worst
additive noise lemma in [7]. The ﬁrst one is a rather straightforward generalization and the second is a conditional version
of the well-known worst additive noise lemma. The latter is
a key inequality in deriving a new outer bound based on
auxiliary random variables in this paper.
Lemma 2: Let X n denote a random sequence with an
2
average power constraint, W n and Z n be i.i.d. N (0, σW )
2
and N (0, σZ ), respectively, correlated with each other but
2
2
independent of X n . If σZ − σW ≥ 0, then
n

n

n

Note that the above lemma has a different covariance constraint from [5, Lem. 4] and it is analogous to the conditional
extremal inequality in [10, Thm. 8]. While the conditional
extremal inequality is conditioned on a scalar random variable
U since its proof relies on the classical conditional EPI [9],
our inequality (9) is on a random sequence U n .
By replacing the arbitrarily distributed interfering signals
n
n
h12 X2 and h21 X1 in (1) with the i.i.d. Gaussian sequences
n
n
W1 and W2 , respectively, and then using Lemma 3, the
following inequalities are readily given.
n
n
Lemma 4: For Xi , Yin , Zi , and Uin deﬁned in (1) and (5),
n
2
respectively, let Vi be i.i.d. N (0, σVi ) independent of both
n
n
Xk and Uk , where k = 1 if i = 2, otherwise k = 2 and
2
σVi = Cov(Zi |Wi − Zi ) for i = 1, 2. If the conditions
2
σV2 ≥ h2 Cov(W1 − Z1 )
21

˜
nh(Y1g |U1g ) − nh(h21 Y1g + V2 |U1g )

n
Xg .

where the equality holds if X =
The proof of Lemma 2 can be given either by the worst
additive noise lemma or by an extremal inequality in [10,
Corrolary 4].
Lemma 3 (Conditional Worst Additive Noise Lemma):
Let X n denote a random sequence with an average power
2
constraint, Z n be i.i.d. N (0, σZ ) independent of X n and
n
let U denote another random sequence with an average
power constraint, correlated with X n but independent of
Z n . Suppose that a corresponding random vector sequence
(Z, X + Z, U )n has an average covariance constraint such that
n
1
K . Also let (Z, Xg +Z, Ug )
j=1 Cov(Zj , Xj +Zj , Uj )
n
be a zero-mean Gaussian random vector with covariance K .
Then, we have
n

n

n

n

n

(12a)

≤

˜
nh(Y2g |U2g ) − nh(h12 Y2g + V1 |U2g )

(12b)

−2
˜
1 − σV2 h2 Cov(W1 − Z1 )V2 and V1 =
21

Then, we have
n
n
h(Y1n |U1 ) − h(h21 X1 + V2n )
n
n
n
n
n
= h(h21 X1 − h21 W1 + h21 Z1 |U1 ) − h(h21 X1 + V2n )

(9)

− n log |h21 |
(a)

n
n
n
n
n
n
= h(h21 X1 − h21 W1 + h21 Z1 |U1 ) − h h21 X1 − h21 W1
n
+ h21 Z1 +

−2
1 − σV2 h2 Cov(W1 − Z1 )V2n − n log |h21 |
21

(b)

n
n
n
n
≤ h(h21 X1 − h21 W1 + h21 Z1 |U1 )

n

h(X |U )−h(X + Z |U )

n
n
n
n
˜
− h h21 X1 − h21 W1 + h21 Z1 + V2n |U1 − n log |h21 |

= −I(Z n ; X n + Z n |U n )
= −h(Z n |U n ) + h(Z n |X n + Z n , U n )

(c)

≤ nh(X1g − W1 + Z1 |U1g )
˜
− nh h21 X1g − h21 W1 + h21 Z1 + V2 |U1g

(a)

≤ −nh(Z|Ug ) + nh(Z|Xg + Z, Ug )
= nh(Xg |Ug ) − nh(Xg + Z|Ug )

+

V1n )

n
n
n
n
n
h(Y1n |U1 ) = h(h21 X1 − h21 W1 + h21 Z1 |U1 ) − n log |h21 |.

n
n
where the equality holds if X n = Xg and U n = Ug .
Proof: Without loss of generality, we assume as before
n
1
that n j=1 Cov(Zj , Xj + Zj , Uj ) = K . Similar to the proof
in [5, Lem. 4], we have
n

−

n
h(h12 X2

−2
1 − σV1 h2 Cov(W2 − Z2 )V1 .
12
Proof: It sufﬁces to prove (12a) since we can immediately
obtain (12b) by swapping the user indices of (12a). We ﬁrst
n
n
observe that the average power constraints on X1 and X2 can
be naturally translated into an average covariance constraint on
˜
a random vector sequence (h21 Y1 , V2 , U1 )n by construction.
It is easy to see

h(X |U )−h(X + Z |U )

n

n
h(Y2n |U2 )

˜
where V2 =

n

≤ nh(Xg |Ug ) − nh(Xg + Z|Ug )

− Z2 )

n
n
h(Y1n |U1 ) − h(h21 X1 + V2n ) ≤

h(X + W ) − h(X + Z ) ≤ nh(Xg + W ) − nh(Xg + Z)
(8)

n

(11b)

≥

h2 Cov(W2
12

hold, then

n

n

(11a)

2
σV1

where (a) follows from the fact that the condition in (11a)
−2
satisﬁes 1 − σV2 h2 Cov(W1 − Z1 ) ≥ 0, (b) follows from
21
the fact that conditioning reduces entropy, and (c) follows by
n
using the fact that U1 is independent of V2n and by applying

(10)

where (a) follows from the fact that Z n is independent of U n
and from Lemma 1.

3

Using (14) - (17), we can bound R1 + R2 as follows:

Lemma 3 for a given average covariance constraint on the
˜
random vector sequence (h21 Y1 , V2 , U1 )n .
Using the above lemmas, we present the following theorem
to upper-bound the sum rate R1 + R2 .
Theorem 3: The capacity region of a two-user Gaussian IC
in the weak interference regime is contained in the following
region:

n
n
n(2R1 + 2R2 − 4 n ) ≤ nh(Y1g ) − h(Y1n |X1 ) + h(U1 )
n
n
− h(h12 X2 + V1n ) + h(Y1n |U1 ) − nh(W1 − Z1 )
n
n
n
+ nh(Y2g ) − h(Y2n |X2 ) + h(U2 ) − h(h21 X1 + V2n )
n
+ h(Y2n |U2 ) − nh(W2 − Z2 )
(a)

n
≤ nh(Y1g ) − nh(Y1g |X1g ) + nh(U1g ) + h(Y2n |U2 )

1
R1 + R2 ≤ h(Y1g ) − h(Y1g |X1g ) + h(U1g )
2
˜
− h(h12 Y2g + V1 |U2g ) + h(Y1g |U1g ) − h(W1 − Z1 )
˜
+ h(Y2g ) − h(Y2g |X2g ) + h(U2g ) − h(h21 Y1g + V2 |U1g )
+ h(Y2g |U2g ) − h(W2 − Z2 )

n
− h(h12 X2 + V1n ) − nh(W1 − Z1 ) + nh(Y2g ) − nh(Y2g |X2g )
n
n
+ nh(U2g ) + h(Y1n |U1 ) − h(h21 X1 + V2n ) − nh(W2 − Z2 )
n
where (a) follows from applying Lemma 2 to −h(Y1n |X1 ) +
n
n
n
n
n
h(U1 ) = −h(h12 X2 + Z1 ) + h(h12 X2 + W1 ) and
n
n
n
n
n
n
−h(Y2n |X2 )+h(U2 ) = −h(h21 X1 +Z2 )+h(h21 X1 +W2 ),
2
2
respectively, since σWi ≤ σZi = 1 for i = 1, 2. Then, (13)
immediately follows from Lemma 4.
Now, we get

(13)

2
2
for all (W1 , W2 ) satisfying (11a), (11b), σW1 ≤ 1, σW2 ≤ 1.
Proof: Using the auxiliary random variables in (5) and
the Fano’s inequality, we can bound R1 as follows:
n
n
n ) ≤ I(X1 ; Y1 )
n
n
n
= I(U1 ; Y1n ) + h(Y1n |U1 ) − h(Y1n |X1 )
(a)
n
n
n
n
≤ I(U1 ; Y1n |X1 ) + h(Y1n |U1 ) − h(Y1n |X1 )
n
n
n
n
n
= h(U1 ) − h(h12 X2 + W1 |h12 X2 + Z1 )
n
n
n
− h(h12 X2 + Z1 ) + h(Y1n |U1 )
(b)
n
n
= h(U1 ) − h(h12 X2 + V1n ) − nh(W1 − Z1 )

n(R1 −

˜
h(Y1g ) − h(Y1g |X1g ) + h(U1g ) − h(h12 Y2g + V1 |U2g )
+ h(Y1g |U1g ) − h(W1 − Z1 )
P1
1
= log 1 + 2
2
h12 P2 + 1

+

1


log 

4
h2 σW
12
2
2 P + σ2 −
1
h12 2
2
V1
h2 P2 +σW
12
1


(h2 P2 −ρ1 σW1 )2
2
12
P1 + h12 P2 + 1 − h2 P2 +σ2
1
12


W1
+ log 
 . (18)
2
2
σW1 + 1 − 2ρ1 σW1

+

n
h(Y1n |U1 )

(14)
where n → 0 as n → ∞, (a) follows from the well-known
fact that if p(x, y, z) = p(x)p(z)p(y|x, z), then I(X; Y |Z) ≥
I(X; Y ) and (b) follows from the identity

Interchanging the user indices, we have another equation.
Using a known bounding technique by Kramer [1], we can
also obtain upper bounds for both R1 + 2R2 and 2R1 + R2 .
Theorem 4: The capacity region of a two-user Gaussian IC
in the weak interference regime is contained in the following
region:

n
n
n
n
n
n
h(h12 X2 + W1 |h12 X2 + Z1 ) + h(h12 X2 + Z1 )
n
n
n
n
n
n
= h(W1 − Z1 |h12 X2 + Z1 ) + h(h12 X2 + Z1 )
n
n
n
n
n
= h(h12 X2 + Z1 |W1 ) + h(W1 − Z1 )
n
= h(h12 X2 + V1n ) + nh(W1 − Z1 )

where V1n is the minimum mean-squared error (MMSE)
n
n
estimation error of Z1 given W1 and hence V1n is i.i.d.
n
n
N (0, Cov(Z1 |W1 − Z1 )) independent of (W1 − Z1 ) as well
n
as X1 . Likewise, we have
n(R2 −

n)



2
h2 P2 + σW1
12

˜
R1 ≤ h(U1g ) − h(h12 Y2g + V1 |U2g ) + h(Y1g |U1g ) − h(W1 − Z1 )
˜
− h(h21 Y1g + V2 |U1g ) + h(Y2g |U2g ) − h(W2 − Z2 )
+

1
2
log 2πe (P2 + h2 P1 + 1)2−2R2 − 1 + σW2
21
2

n
n
≤ h(U2 ) − h(h21 X1 + V2n )
n
+ h(Y2n |U2 ) − nh(W2 − Z2 )

(15)

n)

˜
R2 ≤ h(U2g ) − h(h21 Y1g + V2 |U1g ) + h(Y2g |U2g ) − h(W2 − Z2 )
˜
− h(h12 Y2g + V1 |U2g ) + h(Y1g |U1g ) − h(W1 − Z1 )

n
≤ h(Y1n ) − h(Y1n |X1 )
n
≤ nh(Y1g ) − h(Y1n |X1 )

(16)

+

and, similarly, R2 by
n(R2 −

n
n
n ) ≤ nh(Y2g ) − h(Y2 |X2 ).

(19)

2
for all (W1 , W2 ) satisfying (11a), (11b) and σW2 ≤ 1.

We can also bound R1 by
n(R1 −

− R2

1
2
log 2πe (P1 + h2 P2 + 1)2−2R1 − 1 + σW1
12
2

− R1
(20)

2
for all (W1 , W2 ) satisfying (11a), (11b) and σW1 ≤ 1.
Proof: Similar to the proof of Theorem 3, we can ﬁrst

(17)

4

1.8
2
1.6

1.4
1.5

R2

R

2

1.2

1

1
0.8

0.6
0.5

0.4
Enhanced ETW outer bound
New outer bound
Simplified HK inner bound
Time division inner bound

0

0

0.5

Enhanced ETW outer bound
New outer bound
Simplified HK inner bound
Time division inner bound

0.2

1

1.5

0

2

R1

0

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

R1

Fig. 1. Bounds on the capacity region of two-user symmetric Gaussian
interference channel: P = 15 and h2 = 0.2.

Fig. 2. Bounds on the capacity region of two-user symmetric Gaussian
interference channel: P = 10 and h2 = 0.3.

bound R1 + R2 as follows:

therein, our sum-rate upper bound in Theorem 3 is shown
to be tighter in a moderately weak interference regime. For
example, this is the case when 0.42 ≤ h2 < 0.52 for the same
parameters of Fig. 1 in [6], where P = 6. Moreover, such
range of h2 is shown to be gradually widened as P increases.

˜
R1 + R2 ≤ h(U1g ) − h(h12 Y2g + V1 |U2g ) + h(Y1g |U1g )
1
n
˜
− h(W1 − Z1 ) + h(U2 ) − h(h21 Y1g + V2 |U1g )
n
+ h(Y2g |U2g ) − h(W2 − Z2 ).
(21)

R EFERENCES

Following the EPI-based bounding technique in [1, Thm. 2]
(see also [5, Lem. 11]) based on known outer bound results
for the degraded Gaussian IC [11] and the one-sided Gaussian
IC [12], we have

[1] G. Kramer, “Outer bounds on the capacity region of Gaussian interference
channels,” IEEE Trans. Inform. Theory, vol. 50, no. 3, pp. 581-586, Mar.
2004.
[2] R. H. Etkin, D. N. C. Tse, and H. Wang, “Gaussian interference channel
capacity to within one bit,” IEEE Trans. Inform. Theory, vol. 54, no. 12,
pp. 5534-5562, Dec. 2008.
[3] A. S. Motahari and A. K. Khandani, “Capacity bounds for the Gaussian
interference channel, IEEE Trans. Inform. Theory, vol. 55, no. 2, pp.
620-643, Feb. 2009.
[4] X. Shang, G. Kramer, and B. Chen, “A new outer bound and the noisyinterference sum-rate capacity for the Gaussian interference channels,
IEEE Trans. Inform. Theory, vol. 55, no. 2, pp. 689-699, Feb. 2009.
[5] V. S. Annapureddy and V. V. Veeravalli, “Gaussian interference networks:
Sum capacity in the low-interference regime and new outer bounds on
the capacity region,” IEEE Trans. Inform. Theory, vol. 55, no. 7, pp.
3032-3050, Jul. 2009.
[6] R. H. Etkin, “New sum-rate upper bound for the two-user Gaussian
interference channel,” in Proc. IEEE Int. Symp. Inform. Theory, Seoul,
Korea, Jun. 2009.
[7] S. Diggavi and T. M. Cover, “Worst additive noise under covariance
constraints, IEEE Trans. Inform. Theory, vol. 47, no. 7, pp. 3072-3081,
Nov. 2001.
[8] J. A. Thomas, “Feedback can at most double Gaussian multiple access
channel capacity, IEEE Trans. Inform. Theory, vol. IT-33, no. 5, pp. 711716, Sep. 1987.
[9] P. P. Bergman, “A simple converse for broadcast channels with additive
white Gaussian noise,” IEEE Trans. Inf. Theory, vol. IT-20, no. 2, pp.
279-280, Mar. 1974.
[10] T. Liu and P. Viswanath, “An extremal inequality motivated by multiterminal information theoretic problems, IEEE Trans. Inform. Theory, vol.
53, no. 5, pp. 1839-1851, May 2007.
[11] H. Sato, “On degraded Gaussian two-user channels,” IEEE Trans.
Inform. Theory, vol. IT-24, no. 5, pp. 638-640, Sep. 1978.
[12] M. H. M. Costa, “On the Gaussian interference channel,” IEEE Trans.
Inform. Theory, vol. IT-31, pp. 607-615, Sep. 1985.

n
n
n
h(U2 ) = h(h21 X1 + W2 )
(a)

n
n
2
n
2
log 2πe 2 n h(h21 X1 +Z2 ) − (1 − σW2 )
2
(b) n
2
≥ log 2πe (P2 + h2 P1 + 1)2−2R2 − 1 + σW2
21
2
(22)

≥

where (a) follows from EPI and (b) follows from (17).
Substituting (22) into (21) gives the upper bound (19) on
R1 + 2R2 . Interchanging the user indices, we have (20) for
2R1 + R2 .
IV. N UMERICAL E XAMPLES
Fig. 1 and Fig. 2 show the tightness of the new outer bound
in Section III on the capacity region of the two-user symmetric
Gaussian IC with two channel parameter sets (P = 15, h2 =
0.2) and (P = 10, h2 = 0.3), over the best known genie-aided
outer bound. To compare with the outer bounds that we have
discussed here, we also plot a time division inner bound and a
simpliﬁed case of the HK inner bound that does not consider
time sharing and is limited to only Gaussian distributions for
the private and common messages. Furthermore, compared to
the sum-rate upper bound given in [6] and all other bounds

5

