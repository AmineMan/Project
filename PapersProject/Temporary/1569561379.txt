Creator:         TeX output 2012.05.16:1034
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May 16 10:36:24 2012
ModDate:        Tue Jun 19 12:55:54 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      431642 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569561379

An Information-Spectrum Approach to the Capacity
Region of General Interference Channel
∗ Department

† Department

Lei Lin∗ , Xiao Ma† , Xiujie Huang† and Baoming Bai‡

of Applied Mathematics, Sun Yat-sen University, Guangzhou 510275, Guangdong, China
of Electronics and Communication Engineering, Sun Yat-sen University, Guangzhou 510006, GD, China
‡ State Lab. of ISN, Xidian University, Xi’an 710071, Shaanxi, China
Email: linlei2@mail2.sysu.edu.cn, maxiao@mail.sysu.edu.cn
i

Abstract—This paper is concerned with general interference
channels characterized by a sequence of transition (conditional)
probabilities. We present a general formula for the capacity
region of the interference channel with two pairs of users.
The formula shows that the capacity region is the union of
a family of rectangles, where each rectangle is determined by
a pair of spectral inf-mutual information rates. Although the
presented formula is usually difﬁcult to compute, it provides us
useful insights into the interference channels. For example, the
formula suggests us that the simplest inner bounds (obtained by
treating the interference as noise) could be improved by taking
into account the structure of the interference processes. This is
veriﬁed numerically by computing the mutual information rates
for Gaussian interference channels with embedded convolutional
codes.

x1 (i)

Encoder1

y1

Decoder1

ˆ
i

Decoder2

ˆ
j

Wn

j
Encoder2

y2

x 2 (j)

Fig. 1.

General interference channel W.

to compute the accessible capacity [9], the primary link is
allowed to have a non-neglected error probability. This makes
the model unattractive when the capacity region is considered.
For this reason, we will remove the ﬁxed-code constraints
on the primary users in this paper.1 In other words, we will
compute a pair of transmission rates at which both links can
be asymptotically error-free.
To justify the computational results, we consider a more
general interference channel which is characterized by a
sequence of transition probabilities W = {W n }∞ . Similar
n=1
to [12], we utilize the information spectrum approach [13][14].
The capacity region can be described in terms of the spectral
inf-mutual information rates.
The rest of the paper is structured as follows. Sec. II
introduces the main deﬁnitions, including general IC and
spectral inf-mutual information rate. In Sec. III, a formula is
proved for the capacity region of the general IC. In Sec. IV
we present numerical results for GIFC with binary-phase shiftkeying (BPSK) modulation. Sec. V concludes this paper.

I. I NTRODUCTION
The interference channel (IC) is a communication model
with multiple pairs of senders and receivers, in which each
sender has an independent message intended only for the
corresponding receiver. This model was ﬁrst mentioned by
Shannon [1] in 1961 and further studied by Ahlswede [2]
in 1974. A basic problem for the IC is to determine the
capacity region, which is currently one of long-standing open
problems in information theory. Only in some special cases,
the capacity regions are known, such as strong interference
channels and deterministic interference channels [3–6]. On
the other hand, some inner and outer bounds of the capacity
region are obtained, for example, see [4, 7, 8]. In recent years,
Etkin, Tse and Wang [8] introduced the idea of approximation
to show that Han and Kobayashi region (HK region) [4] is
within one bit of the capacity region for Gaussian interference
channel (GIFC).
In [9], the authors proposed a new computational model
for the two-user GIFC, in which one pair of users (called
primary users) are constrained to use a ﬁxed encoder and
the other pair of users (called secondary users) are allowed
to optimize their codes. The maximum rate at which the
secondary users can communicate reliably without degrading
the performance of the primary users is called the accessible
capacity of the secondary users. Since the structure of the
interference from the primary link has been taken into account
in the computation, the accessible capacity is usually higher
than the maximum rate when treating the interference as
noise, as is consistent with the spirit of [10][11]. However,

II. BASIC D EFINITIONS A ND P ROBLEM S TATEMENT
A. General IC
Let X1 , X2 be two ﬁnite input alphabets and Y1 , Y2 be two
ﬁnite output alphabets. A general interference channel W (see
Fig. 1) is characterized by a sequence W = {W n (·, ·|·, ·)}∞ ,
n=1
n
n
n
n
where W n : X1 × X2 → Y1 × Y2 is a probability transition
matrix. That is, for all n,
∑

W n (y1 , y2 |x1 , x2 )

≥ 0

W (y1 , y2 |x1 , x2 )

=

n

1.

n
n
y1 ∈Y1 ,y2 ∈Y2

1 The authors are grateful to Prof. Tse who inspired us to continue the
research in this direction when our previous work was presented in ISIT’2011.

1

n
n
The marginal distributions W1 , W2 of the W n are given by
∑
n
W1 (y1 |x1 , x2 ) =
W n (y1 , y2 |x1 , x2 ),
(1)

B. Preliminaries of Information-Spectrum Approach
The following notions can be found in [14].
Deﬁnition 4 (liminf in probability): For a sequence of random variables {Zn }∞ ,
n=1

n
y2 ∈Y2

n
W2 (y2 |x1 , x2 )

∑

=

W n (y1 , y2 |x1 , x2 ).

(2)

n
y1 ∈Y1

p- lim inf Zn ≡ sup{β| lim Pr{Zn < β} = 0}.
n→∞

(1)
(2) (1) (2)
(n, Mn , Mn , εn , εn )

Deﬁnition 1: An
code for the
interference channel W consists of the following essentials,
a) message sets:
(1)
M(1) = {1, 2, . . . , Mn },
n
(2)
M(2) = {1, 2, . . . , Mn },
n

Deﬁnition 5: If two random variables sequences X1 =
n
n
{X1 }∞ and X2 = {X2 }∞ satisfy that
n=1
n=1
n
n
n
n
PX1 X2 (x1 , x2 ) = PX1 (x1 )PX2 (x2 )

for sender 1
for sender 2

(1)

for encoder 1
for encoder 2

For sender 1 to transmit message i, encoder 1 outputs
the codeword x1 (i). Similarly, for sender 2 to transmit
message i, encoder 2 outputs the codeword x2 (j).
c) collections of decoding sets:

n
PY n |X n (Y1n |X1 )
1
log 1 1
(4)
n)
n→∞ n
PY1n (Y1
n
PY n |X n (Y2n |X2 )
1
, (5)
I(X2 ; Y2 ) ≡ p- lim inf log 2 2
n→∞ n
PY2n (Y2n )

I(X1 ; Y1 ) ≡ p- lim inf

n
{B1i ⊆ Y1 }i=1,...,M (1) ,
for decoder 1
n
n
{B2j ⊆ Y2 }j=1,...,M (2) , for decoder 2
n
∩
∩
where B1i B1i′ = ∅ for i ̸= i′ and B2j B2j ′ = ∅ for
j ̸= j ′ . After receiving y1 , decoder 1 outputs ˆ whenever
i
y1 ∈ B1ˆ. Similarly, after receiving y2 , decoder 2 outputs
i
ˆ whenever y2 ∈ B ˆ .
j
2j
d) probabilities of decoding errors:
(1)

εn =
(2)

εn =

(1)
Mn

(2)
Mn

∑ ∑

1
(1)
(2)
Mn Mn

i=1 j=1
(1)
(2)
Mn Mn
∑ ∑

1
(1)
(2)
Mn Mn
i=1 j=1

where

n
PX1 (x1 )W n (y1 , y2 |x1 , x2 ).(7)

x1 ,y1

c
n
W1 (B1i |x1 (i), x2 (j))

III. T HE C APACITY R EGION OF G ENERAL IC
In this section, we derive a formula for the capacity region
C(W) of the general IC.
Theorem 1: The capacity region C(W) of the interference
channel W is given by
∪
C(W) =
RW (X1 , X2 ),
(8)

c
n
W2 (B2j |x1 (i), x2 (j)),

(X1 ,X2 )∈SI

where RW (X1 , X2 ) is deﬁned as the collection of all
(R1 , R2 ) satisfying that
0 ≤ R1

j

≤ I(X1 ; Y1 )

(9)

0 ≤ R2

ˆ = arg max Pr{y2 |x2 (j)}
j

≤ I(X2 ; Y2 ).

(10)

To prove Theorem 1, we need the following lemmas.
Lemma 1: Let

as the transmitted messages.
Deﬁnition 2: A rate pair (R1 , R2 ) is achievable if there
(1)
(2) (1) (2)
exists a sequence of (n, Mn , Mn , εn , εn ) codes such that

n
n
(X1 = {X1 }∞ , X2 = {X2 }∞ )
n=1
n=1

lim ε(2) = 0,
n

be any channel input such that (X1 , X2 ) ∈ SI . The
corresponding output via an interference channel W =
{W n } is denoted by (Y1 = {Y1n }∞ , Y2 = {Y2n }∞ ).
n=1
n=1
(1)
(2)
Then, for any ﬁxed Mn and Mn , there exists an
(1)
(2) (1) (2)
(n, Mn , Mn , εn , εn ) code satisfying that

n→∞

(2)
log Mn

≥ R1 and lim inf
≥ R2 .
n→∞
n
n
Deﬁnition 3: The set of all achievable rates is called the
capacity region of the interference channel W, which is
denoted by C(W).
lim inf

∑

n
PY2n |X2 (y2 |x2 ) =

and

and

n
PX2 (x2 )W n (y1 , y2 |x1 , x2 ),(6)

x2 ,y2

i

=0

∑

n
PY1n |X1 (y1 |x1 ) =

where “c” denotes the complement of a set. Here we have
(2)
(1)
assumed that each message of i ∈ Mn and j ∈ Mn
is produced with equal probability.
Remark: The optimal decoding to minimize the probability
of errors is deﬁning the decoding sets B1i and B2j according
to the the maximum likelihood decoding [15]. That is, the two
receivers choose, respectively,
ˆ = arg max Pr{y1 |x1 (i)}
i

lim ε(1)
n
n→∞
(1)
log Mn

(3)

n
n
for all x1 ∈ X1 , x2 ∈ X2 and n, they are called independent
and denoted by X1 ⊥X2 .
Similar to [13], we have
△
Deﬁnition 6: Set SI = {(X1 , X2 )|X1 ⊥X2 }. Given an
(X1 , X2 ) ∈ SI , for the interference channel W, we deﬁne
the spectral inf-mutual information rate by

b) sets of codewords:
n
{x1 (1), x1 (2), . . . , x1 (Mn )} ⊂ X1 ,
(2)
n
{x2 (1), x2 (2), . . . , x2 (Mn )} ⊂ X2 ,

n→∞

n→∞

c
c
ε(1) + ε(2) ≤ Pr{Tn (1)} + Pr{Tn (2)} + 2e−nγ ,
n
n

2

(11)

(1)

1
Tn (1) = {(x1 , y1 )| n log
1
Tn (2) = {(x2 , y2 )| n log

PY n |X n (y1 |x1 )
1
1
PY n (y1 )
1
PY n |X n (y2 |x2 )
2
2
PY n (y2 )
2

(1)

>

1
n

>

(2)
1
n log Mn

log Mn + γ}

(1)

1
εn ≥ Pr{ n log

+ γ}.

n
n
PY n |X n (Y1 |X1 )
1
1
n
PY n (Y1 )
1
n
n
PY n |X n (Y2 |X2 )
2
2
n
PY n (Y2 )

≤
≤

∑

−nγ
(1)

(14)

In order to prove this inequality, we set
n
n
Ai = {y1 ∈ Y1 |PX1 |Y1n (x1 (i)|y1 ) ≤ e−nγ }.

It can be seen that
Pr{Ln } =

(1)
Mn
∑

i=1

=
≤
=

(1)
Mn
∑

i=1
(1)
Mn
∑
i=1
(1)
Mn
∑

=

n
PX1 Y1n (x1 (i), Ai

∑

(1)
Mn
∑

−nγ

≤e

(1)
Mn
∑

∩

B1i ) +
B1i ) +

(1)
Mn
∑

i=1
(1)
Mn
∑
i=1

n
PX1 Y1n (x1 (i), Ai

∩

c
B1i )

c
n
PX1 Y1n (x1 (i), B1i )

B1i

n
PX1 Y1n (x1 (i), y1 ) + εn

(1)

∩

B1i

n
PX1 |Y1n (x1 (i)|y1 )PY1n (y1 ) + εn

∑

(1)

i=1 y1 ∈B1i
(1)
Mn
∪

= e−nγ PY1n (

≤ e−nγ ,

∩

(1)

∩

∑

i=1 y1 ∈Ai
a

n
PX1 Y1n (x1 (i), Ai )

n
PX1 Y1n (x1 (i), Ai

i=1 y1 ∈Ai

−nγ
n
n
PX1 (x1 )PY1n |X1 (y1 |x1 ) e (1)
Mn

Mn

(2)

1
n

Pr{Ln } ≤ ε(1) + e−nγ .
n

j̸=1

− 1) e

≤

2

the ﬁrst inequality of (13) can be expressed as

(2)

i̸=1 (x1 ,y1 )∈Tn (1)
∑ e−nγ
(1)
(1) = (Mn
Mn
i̸=1

(1)

n
Ln = {(x1 , y1 )|PX1 |Y1n (x1 |y1 ) ≤ e−nγ },

i̸=1 (x1 ,y1 )∈Tn (1)

∑

log Mn − γ} − e−nγ

By setting

It can be seen that
∑
∪
Pr{E1i }
Pr{ E1i } ≤
i̸=
i̸=1
∑1
=
Pr{(x1 (i), y1 ) ∈ Tn (1)}
i̸=1
∑
a ∑
n
=
PX1 (x1 )PY1n (y1 )
b

1
n

n
n
Pr{PX1 |Y1n (X1 |Y1n ) ≤ e−nγ }.

εn + εn = εn + εn
∪
∪
c
c
E2j }
≤ Pr{E11 } + Pr{ E1i } + Pr{E21 } + Pr{
i̸=1

≤

n
and noticing PX1 (x1 ) = 1 , we can rewrite the ﬁrst term
(1)
Mn
on the right-hand side of the ﬁrst inequality of (13) as

For receiver 1, an error occurs if (x1 (1), y1 ) ∈ Tn (1) or
/
(x1 (i), y1 ) ∈ Tn (1) for some i ̸= 1. Similarly, for receiver 2,
an error occurs if (x2 (1), y2 ) ∈ Tn (2) or (x2 (j), y2 ) ∈ Tn (2)
/
for some j ̸= 1. So the ensemble average of the error
probabilities of decoder 1 and decoder 2 can be upper-bounded
as follows:
(1)

(2)

n
n
PY1n |X1 (y1 |x1 )
PX1 |Y1n (x1 |y1 )
=
n (y1 )
n
PY1
PX1 (x1 )

E1i = {(x1 (i), y1 ) ∈ Tn (1)}, E2j = {(x2 (j), y2 ) ∈ Tn (2)},

(2)

(1)

log Mn − γ} − e−nγ ,
(13)
n
n
for every γ > 0, where X1 (resp., X2 ) places probability
(1)
(2)
mass 1/Mn (resp., 1/Mn ) on each codeword for encoder
1 (resp., encoder 2) and (3), (6), (7) hold.
Proof of Lemma 2: The proof is similar to that of [13,
Lemma 4]. By using the relation
(2)

1
εn ≥ Pr{ n log

Proof of Lemma 1: The proof is similar to that of [13,
Lemma 3].
(1)
Codebook generation. Generate Mn independent code(1)
n
words x1 (1), ..., x1 (Mn ) ∈ X1 subject to the probability
(2)
n
distribution PX1 . Similarly, generate Mn independent code(2)
n
words x2 (1), ..., x2 (Mn ) ∈ X2 subject to the probability
n
distribution PX2 .
Encoding. To send message i, sender 1 sends the codeword
x1 (i). Similarly, to send message j, sender 2 sends x2 (j).
Decoding. The receiver 1 chooses the i such that
(x1 (i), y1 ) ∈ Tn (1) if such i exists and is unique. Similarly,
the receiver 2 chooses the j such that (x2 (j), y2 ) ∈ Tn (2) if
such j exists and is unique. Otherwise, an error is declared.
Analysis of the error probability. By the symmetry of the
random code construction, we can assume that (1, 1) was sent.
Deﬁne

(1)

(2)

Lemma 2: For all n, any (n, Mn , Mn , εn , εn ) code
satisﬁes that

where

PY1n (y1 ) + εn

B1i ) + εn ≤ e−nγ + εn ,
(1)

(1)

i=1

(15)
where B1i is the decoding region corresponding to codeword
x1 (i) and “a” follows from the deﬁnition of Ai . Therefore,
the ﬁrst inequality of (13) is proved. Similarly, we can obtain
the second inequality of (13).
Now we prove Theorem 1.
Proof of Theorem 1:
1) To prove that an arbitrary (R1 , R2 ) satisfying (9) and
(10) is achievable, we deﬁne

where “a” follows from the independence of x1 (i) (i ̸= 1)
and y1 and “b” follows from the deﬁnition of Tn (1). Similarly,
we obtain
∪
E2j } ≤ e−nγ .
(12)
Pr{
j̸=1

Combining all inequalities above, we can see that there
(1)
(2) (1) (2)
must exist at least one (n, Mn , Mn , εn , εn ) code satisfying (11).

(1)
(2)
Mn = en(R1 −2γ) and Mn = en(R2 −2γ)

3

n1

i
Encoder1

y1

x1 (i)

Decoder1

Decoder2

into the interference channels, as illustrated by the following
example.
The considered example has the model as shown in Fig. 2,
where the channel inputs x1 (i) and x2 (j) are BPSK sequences
with power constraints P1 and P2 , respectively; the additive
noise n1 and n2 are sequences of independent and identically
distributed (i.i.d.) Gaussian random variables of variance one;
the channel outputs y1 and y2 are
√
y1 = x1 (i) + ax2 (j) + n1 ,
(20)
√
y2 = x2 (j) + ax1 (i) + n2 .
(21)

ˆ
i

ˆ
j

a
a

j
Encoder2

y2

x 2 (j)
n2

Fig. 2.

Symmetric Gaussian interference channel.

For any two arbitrary input processes x1 and x2 , the deﬁned
pair of rates given in Theorem 1 are infeasible to compute.
Therefore, we assume that x1 and x2 are the outputs from
two (possibly different) generalized trellis encoders driven by
independent uniformly distributed (i.u.d.) input sequences, as
proposed in [9]. In this case, both x1 and x2 are block-wise
stationary, and (hence)

for an arbitrarily small constant γ > 0. Then, Lemma 1
(1)
(2) (1) (2)
guarantees the existence of an (n, Mn , Mn , εn , εn ) code
satisfying
(1)

n
n
PY n |X n (Y1 |X1 )
1
1
n
PY n (Y1 )
1
n
n
PY n |X n (Y2 |X2 )
2
2
≤ R2 − γ}
n)
PY n (Y

(2)

1
εn + εn ≤ Pr{ n log

≤ R1 − γ}

1
+Pr{ n log

+ 2e−nγ

1
≤ Pr{ n log
1
+Pr{ n

2
2
n
n
PY n |X n (Y1 |X1 )
1
1
n
PY n (Y1 )
1
n
n
PY n |X n (Y2 |X2 )
2
2
n)
PY n (Y2
2

≤ I(X1 ; Y1 ) − γ}

I(X1 ; Y1 ) =
−nγ

≤ I(X2 ; Y2 ) − γ} + 2e

.
(16)
From the deﬁnition of the spectral inf-mutual information rate,
we have
lim ε(1) = 0 and lim ε(2) = 0.
n
n
log

n→∞

I(X2 ; Y2 ) =

2) Suppose that a rate pair (R1 , R2 ) is achievable. Then, for
(2) (1) (2)
(1)
any constant γ > 0, there exists an (n, Mn , Mn , εn , εn )
code satisfying
(1)

(2)

lim ε(1) = 0 and
n

n→∞

≥ R2 − γ

(17)

G(D) = [1 + D + D2 1 + D2 ].

(1)

n→∞

(2)

1
εn ≥ Pr{ n log

n
n
PY n |X n (Y1 |X1 )
1
1
n)
PY n (Y1
1
n
n
PY n |X n (Y2 |X2 )
2
2
n)
PY n (Y2

≤ R1 − 2γ} − e−nγ
≤ R2 − 2γ} − e−nγ

2

.
(18)

Taking the limits as n → ∞ on both sides, we have
1
lim Pr{ n log

n→∞

1
lim Pr{ n log

n→∞

n
n
PY n |X n (Y1 |X1 )
1
1
n
PY n (Y1 )
1
n
n
PY n |X n (Y2 |X2 )
2
2
n
PY n (Y2 )

≤ R1 − 2γ} = 0
≤ R2 − 2γ} = 0

,

(23)

(24)

Fig. 3 shows the trellis representation of the signal model when
sender 1 uses CcBPSK and sender 2 uses UnBPSK. Fig. 4
shows the numerical results. The point “A” can be achieved by
a coding scheme, in which sender 1 uses a binary linear (coset)
code concatenated with the convolutional code and sender 2
uses a binary linear code, and the point “B” can be achieved
similarly; while the points on the line “AB” can be achieved
by time-sharing scheme. The point “C” represents the limits
when the two users use binary linear codes but regard the
interference as an i.u.d. additive (BPSK) noise. It can be seen
that knowing the structure of the interference can be used to
improve potentially the bandwidth-efﬁciency.

lim ε(2) = 0.
n

From Lemma 2, we get
1
εn ≥ Pr{ n log

(22)

Since x1 , x2 and y1 , y2 can be viewed as the Markov chains
and the hidden Markov chains, respectively, the right-hand
sides of (22) and (23) can be estimated by performing the
BCJR algorithm 2 [16][17].
We consider two schemes, UnBPSK and CcBPSK, where
UnBPSK means that x1 (resp. x2 ) is an i.u.d. BPSK sequence
and CcBPSK means that x1 (resp. x2 ) is an output from the
convolutional encoder with the generator matrix

n→∞

log Mn
log Mn
≥ R1 − γ and
n
n
for all sufﬁciently large n and

1
n
I(X1 ; Y1n ),
n
1
n
lim I(X2 ; Y2n ).
n→∞ n
lim

n→∞

(19)

2

V. C ONCLUSIONS

implying by deﬁnition that R1 − 2γ ≤ I(X1 ; Y1 ) and R2 −
2γ ≤ I(X2 ; Y2 ). This completes the proof since γ is arbitrary.

In this paper, we have proved that the capacity region of
the two-user interference channel is the union of a family
of rectangles, each of which is deﬁned by a pair of spectral
inf-mutual information rates associated with two independent
input processes. For special cases, the deﬁned pair of rates
can be computed, which provide us useful insights into the
interference channels.

IV. N UMERICAL R ESULTS
We have obtained a formula of the capacity region for the
general IC, which shows that any pair of independent input
processes deﬁne a pair of achievable rates. Although it is not
computable in general, the formula provides us useful insights

2 Only

4

forward recursion is required.

s-(b) u1(b) x1(b) x2(b)
0
0 P P P P
2
2
1
1
0
0 P P P P
1
1
2
2
0
0  P  P  P2  P2
1
1
0
0  P  P  P2  P2
1
1
0
1 P P P P
2
2
1
1
0
1 P P P P
2
2
1
1
0
1  P  P  P2  P2
1
1
0
1  P  P  P2  P2
1
1
1
0 P P P P
2
2
1
1
1
0 P P P P
2
2
1
1
1
0  P  P  P2  P2
1
1
1
0  P  P  P2  P2
1
1
1
1 P P P P
1
1
2
2
1
1 P P P P
1
1
2
2
1
1  P  P  P2  P2
1
1
1
1  P  P  P2  P2
1
1

s+(b) s-(b) u1(b) x1(b)
0
2
0  P  P
1
1
0
2
0  P  P
1
1
0
2
0  P  P
1
1
0
2
0  P  P
1
1
2
2
1 P P
1
1
2
2
1 P P
1
1
2
2
1 P P
1
1
2
2
1 P P
1
1
0
3
0 P P
1
1
1
1
0
3
0 P P
0
3
0 P P
1
1
0
3
0 P P
1
1
2
3
1  P  P
1
1
2
3
1  P  P
1
1
2
3
1  P  P
1
1
2
3
1  P  P
1
1

x2(b)
 P P
2
2
P P
2
2
 P  P
2
2
 P  P
2
2

 P P
2
2
P P
2
2
 P  P
2
2
 P  P
2
2

 P P
2
2
P P
2
2
 P  P
2
2
 P  P
2
2

 P P
2
2
P P
2
2
 P  P
2
2
 P  P
2
2

s+(b)
1
1
1
1
3
3
3
3
1
1
1
1
3
3
3
3

[4]

[5]

[6]

[7]

Fig. 3. The trellis section of (CcBPSK, UnBPSK) with 32 branches. For
each branch b, s− (b) and s+ (b) are the starting state and the ending state,
respectively; while the associated labeling x1 (b) and x2 (b) are the transmitted
signals at sender 1 and sender 2, respectively.

[8]

1

[9]

A

0.9

0.8

0.7

I(X2;Y2)

0.6

[10]

B

0.5

0.4

[11]

0.3

0.2

(UnBPSK, CovBPSK)
(CovBPSK, UnBPSK)

0.1

(UnBPSK, CovBPSK)
0
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

[12]

I(X1;Y1)

Fig. 4.
The evaluated achievable rate pairs of a speciﬁc GIFC, where
P1 = P2 = 7.0 dB and a = 0.5. “(UnBPSK, UnBPSK)” states that both
senders adopt UnBPSK; “(CovBPSK, UnBPSK)” states that sender 1 adopts
CovBPSK and sender 2 adopts UnBPSK; “(UnBPSK, CoBPSK)” states that
sender 1 adopts UnBPSK and sender 2 adopts CovBPSK.

[13]

[14]
ACKNOWLEDGMENT
This work was supported by the 973 Program (No.2012CB316100) and the NSF under Grants
61172082 and 60972046 of China.
R EFERENCES

[15]

[1] C. E. Shannon, “Two-way communication channels,”
in Forth Berkeley Symp. on Math. Statist. and Prob.,
J. Neyman, Ed., vol. 1. Statist. Lab. of the University of
California, Berkely: University of California Press, Jun.
20 - Jul. 30 1961, pp. 611–644.
[2] R. Ahlswede, “The capacity region of a channel with two
senders and two receivers,” The Annals of Probability,
vol. 2, no. 5, pp. 805–814, Oct. 1974.
[3] A. B. Carleial, “A case where interference does not

[16]

[17]

5

reduce capacity,” IEEE Trans. Inform. Theory, vol. 21,
no. 5, pp. 569–570, 1975.
T. S. Han and K. Kobayashi, “A new achievable rate
region for the interference channel,” IEEE Trans. Inform.
Theory, vol. IT-27, no. 1, pp. 49–60, Jan. 1981.
A. A. El Gamal and M. H. M. Costa, “The capacity
region of a class of determinsitic interference channels,”
IEEE Trans. Inform. Theory, vol. IT-28, no. 2, pp. 343–
346, Mar. 1982.
M. H. M. Costa and A. A. El Gamal, “The capacity
region of the discrete memoryless interference channel
with strong interference.” IEEE Trans. Inform. Theory,
vol. IT-33, no. 5, pp. 710–711, Sep. 1987.
G. Kramer, “Outer bounds on the capacity of Gaussian interference channels,” IEEE Trans. Inform. Theory,
vol. 50, no. 3, pp. 581–586, Mar. 2004.
R. H. Etkin, D. N. C. Tse, and H. Wang, “Gaussian
interference channel capacity to within one bit,” IEEE
Trans. Inform. Theory, vol. 54, no. 12, pp. 5534–5562,
Dec. 2008.
X. Ma, X. Huang, L. Lin, and B. Bai, “Accessible
capacity of secondary users,” Dec. 2010, submitted
to IEEE Trans. Inform. Theory, available on the web
http://arxiv.org/abs/1012.5197, also presented in ISIT, St.
Petersburg, Russia, Aug.2011, PP.814-818.
F. Baccelli, A. A. El Gamal, and D. Tse, “Interference
networks with point-to-point codes,” IEEE Trans. Inform.
theory, vol. 57, no. 5, pp. 2582–2596, May 2011.
K. Moshksar, A. Ghasemi, and A. K. Khandani, “An alternative to decoding interference or treating interference
as gaussian noise,” in IEEE International Symposium on
Information Theory,, St. Petersburg, Russia, Aug. 2011,
pp. 1176–1180.
T. S. Han, “An information-spetrum approach to capacity
theorems for the general multiple-access channel,” IEEE
Trans. Inform. Theory, vol. 44, no. 7, pp. 2773–2795,
Nov. 1998.
T. S. Han and S. Verd´ , “Approximation theory of output
u
statistics,” IEEE Trans. Inform. Theory, vol. 39, no. 3, pp.
752–772, May 1993.
T. S. Han, Information-spectrum methods in information
theory. New York: Springer-Verlag Berlin Heidelberg,
2003.
R. H. Etkin, N. Merhav, and E. Ordentlich, “Error exponents of optimum decoding for the interference channel,”
IEEE Trans. Inform. Theory, vol. 56, no. 1, pp. 40–56,
Jan. 2010.
D. M. Arnold, H.-A. Loeliger, P. O. Vontobel, A. Kavˇ i´ ,
cc
and W. Zeng, “Simulation-based computation of information rates for channels with memory,” IEEE Trans.
Inform. Theory, vol. 52, no. 8, pp. 3498–3508, Aug.
2006.
L. R. Bahl, J. Cocke, F. Jelinek, and J. Raviv, “Optimal
decoding of linear codes for minimizing symbol error
rate,” IEEE Trans. Inform. Theory, vol. IT-20, no. 2, pp.
284–287, Mar. 1974.

