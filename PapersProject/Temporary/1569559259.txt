Creator:        LaTeX with hyperref package
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 21:31:30 2012
ModDate:        Tue Jun 19 12:55:13 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      274564 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569559259

Matroidal Undirected Network
Chung Chan
Institute of Network Coding,
The Chinese University of Hong Kong
Email: cchan@inc.cuhk.edu.hk, chungc@alum.mit.edu
the bound is not tight in general as shown by the minimal
counter-example in [11] and the connection with steiner-tree
packing is not exact. The identity in [7] provides a way to
resolve this disparity and better understand the connection
between the different problems.
The main result of this work is a generalization of the graphical undirected network model referred to as the matroidal
undirected network. It can be viewed as the counterpart of
the deterministic network model in [12] since the network
may contain undirected broadcast links, interference links,
and more general ﬁnite linear channels. It is also inspired
by the more abstract view of a network as a matroid or
linking system in [13, 14], for which the max-ﬂow min-cut
theorem takes on a more general form. In addition to studying
the problem of multicasting independent messages, we will
also consider multicasting a distributed source or function of
the source just like [15, 16] for the directed network. By
relating the problem to the secure source coding problem in
[17, 18], polynomial-time solution or partial solution can be
obtained by exploiting the underlying matroidal structure. The
concept can also be extended to a network with its direction
partially ﬁxed, with some directed and undirected links. This
creates a continuum between the directed and undirected
network models. In the sequel, we will describe the model
more precisely and introduce the problem of multicasting
information over the network. Interested readers can also refer
to the paper in [1], which contains the extended results.

Abstract—The undirected graphical model is generalized to a
linear matroid. The optimal direction for multicasting can be
found in polynomial time with respect to the size of the network.
A more general problem is also considered where certain function
of a distributed source is to be computed at multiple nodes. The
converse results are derived, not from the usual cut-set bound
but through the related problem of secret key agreement and
secure source coding by public discussion. A unifying model of
partly directed network can also be formulated, covering both
the directed and undirected networks as special cases.

I. I NTRODUCTION
The problem of undirected network coding was ﬁrst studied
in [2]. The network consists of point-to-point communication
links, where information can ﬂow in two different directions,
up to a total rate below the capacity of the link. Independent
messages are generated at a source node and then multicast to
a set of sink nodes. This is done by choosing the directions
of the links and performing coding at the source and intermediate nodes. Although the number of possible directions
is exponential in the number of undirected links, efﬁcient
polynomial-time algorithm exists for computing the optimal
direction [3]. Given the optimal direction, the network coding
scheme can also be obtained in polynomial time [4]. There
is also a symmetry in the undirectedness of the network that
allows the same code to be used for different choices of the
source from the same multicast group [5].
The purpose of this work is to identify the more general
structure that makes this undirected network coding problem
polynomial-time solvable. It is motivated by the previous work
in [6, 7], which pointed out a common notion of mutual
dependence underlying the undirected network coding problem
and the seemingly different problem of secret key agreement [8]. More precisely, the capacities of the two problems
are the same and can be computed in polynomial time by
exploiting the underlying structure called matroid [9]. This
structure also leads to an information-theoretically appealing
characterization of the capacity, which can be viewed as a
natural generalization of Shannon’s mutual information to the
multivariate case and the combinatorial notion of partition
connectivity [9] originated from the tree packing problem.
A similar divergence upper bound on the capacity was ﬁrst
pointed out in [8], and the connection with steiner-tree packing
was also observed independently by [2] and [10] for the
network coding and secrecy problems respectively. However,

II. S YSTEM MODEL
A. Matroidal undirected ﬁnite linear network
The main idea is to deﬁne an undirected network not by the
interconnections of a graph but by the more general statistical
dependence of a distributed source. To illustrate this, consider
the graphical network in Fig. 1(a), which consists of three
users and two undirected links. One of the links is between
user 1 and 2 and the other is between user 1 and 3. There
are altogether 22 = 4 different ways to direct the network as
shown in Fig. 1(c). Consider the ﬁrst direction in Fig. 1(c).
If we set the input symbols (X1a , X1b ) to the independent
uniformly random (Z1a , Z1b ), then the statistics of the inputs
and outputs are captured by the distributed source in Fig. 1(b).
If we instead choose the second direction in Fig. 1(b) and set
the input symbols (X2 , X1b ) to the independent uniformly random (Z2 , Z1b ), we have the same source in Fig. 1(c). n.b. the
statistics of the channel inputs and outputs are always captured
by the same distributed source regardless of the direction. The

This work was partially supported by a grant from the University Grants
Committee of the Hong Kong Special Administrative Region, China (Project
No. AoE/E-02/08).

1

1
.

.

Z2

(a) 1-bit undirected links between
user 1 and 2, and between 1 and 3.

Y1a

.
Y2

X1b

X2

2

(b) Z1a = Z2 , Z1b = Z3 independent and uniformly random.

X1a

.
Y3

.

3

Z3

Z3

2

X1b

Z1

.
3

X1a

1

Z1b

Z1a

Y1b

Y1a

.
Y3

Y2

X1

Y1b

X2

X1
.

.
X3

Z2
(b) Z1 ⊕ Z2 ⊕ Z3 = 0 and (Zi , Zj )
uniformly random for any i ̸= j.

(a) 1-bit undirected interference link
among user 1, 2 and 3.

Y1
.

Y3

.

X3

X3

X3
X2

Y2

X2

(c) Yk = Xi ⊕ Xj with different permutations (i, j, k) of (1, 2, 3).

(c) Inputs denoted by Xi ’s and outputs by Yi ’s.
Fig. 1. A graphical undirected network: (a) undirected network; (b) emulated
source; (c) possible directions.

Fig. 2. A matroidal undirected network: (a) undirected network; (b) emulated
source; (c) possible directions.

source obtained this way by sending independent and uniformly random inputs over the network is called the emulated
source. Indeed, the emulated source completely characterizes
the undirected network in the sense every possible direction
corresponds to a different maximal subset of uniformly random
source components, referred to as a base of the emulated
source. e.g. (Z1a , Z1b ) is a base that corresponds to the ﬁrst
directed network in Fig. 1(c) with (X1a , X1b ) as the inputs.
(Z2 , Z1b ), (Z1a , Z3 ) and (Z2 , Z3 ) are the remaining bases,
each corresponding to one of the remaining directions. In
general, any graphical undirected network can be characterized
by an emulated source that is obtained by ﬁxing an arbitrary
direction and sending independent uniformly random input
symbols. The set of possible bases of the emulated source
corresponds to the set of possible ways to direct the network.
We call this kind of undirected network matroidal because
the emulated source form a matroid [9] with the rank function
being the entropy function [19] of the source. This is along the
same idea as in [13] where a channel viewed more generally
as a linking system can be regarded as a matroid with a ﬁxed
base being the set of input variables.
Matroidal undirected network is a more general concept
than the graphical network. e.g. it covers the network in
Fig. 2(a) with an undirected interference edge. There are three
possible directions as shown in Fig. 2(c). In the ﬁrst conﬁguration, user 1 and 2 choose the input bits X1 and X2 respectively,
while user 3 observes the xor bit Y3 = X1 ⊕ X2 . With X1 and
X2 chosen uniformly randomly and independently, the channel
turns into the emulated source in Fig. 2(b). There are three
possible choices of the bases, namely (Z1 , Z2 ), (Z1 , Z3 ) and
(Z2 , Z3 ). Each of them corresponds to a different direction in
Fig. 2(c). The network is characterized by the emulated source,
and so it is matroidal. Similarly, an undirected broadcast link
among the three users can be represented by the emulated
source with Z1 = Z2 = Z3 being a uniformly random bit.
Any Zi for i = 1, 2, 3 is a base and can therefore be chosen

as the input of the link. We will consider more generally the
undirected version of the following ﬁnite linear network.
Directed ﬁnite linear network:
Denote the ﬁnite ﬁeld of order q by Fq and have all logarithms taken with base q for convenience. Let V := [m] :=
{1, . . . , m} be the ﬁnite set of users in the network. User i ∈ V
˙
chooses the channel input xi as a column vector of elements in
⊺
˙1
˙m
˙
Fq . After the entire input vector xV := [ x⊺ ... x⊺ ] is speciﬁed,
user i ∈ V observes the output vector
˙
˙
zi := H i xV

i∈V

(1)

where H i is a transfer matrix with entries in Fq . We will
˙
˙
impose the additional requirement that xi is a subvector of zi ,
˙
˙
i.e. xi ⊆ zi . This does not lose generality as user i observes
his channel input xi trivially.
If the inputs of a ﬁnite linear network are generated uniformly
randomly and independently, then the channel outputs form a
ﬁnite linear source.
Finite linear source / undirected network:
A source ZV := (Zi : i ∈ V ) is called ﬁnite linear if the
component source Zi for user i ∈ V can be written as a vector
zi over Fq satisfying
zi = H i xV

i∈V

(2)

for some uniformly random subvector xV of zV , i.e.
xi ⊆ zi
∀i ∈ V
H(xV ) = ℓ(xV ) = H(zV )

(3a)
(3b)

where ℓ(xV ) denotes the length of xV , and H(·) is the entropy [19] with all logarithms taken base q. xV is called a base
⊺
of ZV and H V := [ H ⊺ ... H ⊺ ] is called a representation.
m
1
The set of all possible bases of ZV is denoted as B(ZV ).

2

A base xV satisfying (3) is a perfect compression of the source
because xV ⊆ zV and H(xV ) = H(zV ) means that there is a
bijection between xV and zV , while H(xV ) = ℓ(xV ) means
that xV cannot be compressed further. A ﬁnite linear source
deﬁnes an undirected network because every base corresponds
to a different representation which can be viewed as a directed
ﬁnite linear network. e.g. if ¯V is another base satisfying (3),
x
then it is a subvector of zV by (3a) and so can be written
as ¯V = M zV for some boolean matrix M . Since zV =
x
H V xV by (2), we have ¯V = M H V xV . M H V must be
x
invertible since the bases are uniformly distributed with the
same length by (3b). Thus, xV = (M H V )−1 ¯V and so zi =
x
H i (M H V )−1 ¯V for all i ∈ V . H V (M H V )−1 is therefore
x
the representation of ZV corresponding to ¯V . The set of all
x
bases then corresponds to a collection of ﬁnite linear networks,
which deﬁnes the undirected ﬁnite linear network.

correlated memoryless sources. Let Wi be the message from
ˆ
user i ∈ V and Wj be the estimate of the entire message
WV by user j ∈ A. The encoding and decoding proceed in
ˆ
the same way as before with Un , Gn and Gj replaced by Wi ,
i
ˆ j respectively. εn is also deﬁned as in (4) with
WV and W
the corresponding modiﬁcations. Each Wi takes values from
a message set Wi that is growing exponentially in n at rate
Ri := lim sup
n→∞

log|Wi |
.
n

(5)

The rate tuple RV := (Ri : i ∈ V ) is said to be achievable
if lim supn→∞ εn = 0 assuming the messages are uniformly
distributed. The maximum throughput or simply the capacity
of the network is deﬁned as the maximum achievable sum rate
∑ log|Wi |
sup R(V ) = sup lim sup
(6)
n
n→∞
i∈V

B. Multicasting over the undirected network

where the supremums on the left and right are taken over all
achievable rate vectors and block codes respectively.

We will consider the problem of multicasting correlated
sources or some function of the sources like [15, 16] but
over a matroidal undirected ﬁnite linear network characterized
by a ﬁnite linear source ZV as described in §II-A. Let
UV := (Ui : i ∈ V ) be the distributed source where Ui is
the component observed privately by user i ∈ V . A subset
A ⊆ V of |A| ≥ 1 sink nodes or active users want to recover
some function G of UV by the following block coding scheme.

III. M AIN RESULTS
We ﬁrst derive a necessary condition for transmissibility.
Such converse results for network coding are often derived
using the cut-set bound. It is rather tricky to apply the same
technique here because the directions of the network can vary
in time and adapt to the channel outputs. We will obtain the
desired condition in a different way through the closely related
problem of secure source coding by public discussion [17].

Block code:
A positive integer n ∈ P is chosen as the block length. UV
and ZV are extended n times into the i.i.d. sequences Un :=
V
(UV t : t ∈ [n]) and similarly Zn , with ZV t represented by the
V
vector zV t over Fq . Each user i ∈ V observes Un and then
i
transmits over the undirected network as follows.
Encoding: At time t from 1 to n, user i ∈ V speciﬁes the
˙
direction xit ⊆ zit and the corresponding channel input xit
˙
with ℓ(xit ) = ℓ(xit ) as a function of his private source Un
i
˙
and previous channel outputs denoted by zi[t−1] . An encoding
error occurs if the direction is invalid, i.e. xV ̸∈ B(ZV ).
˙
Otherwise, the channel return to user i ∈ V his output zit :=
˙
H it xV t as in (1), where H V t is the representation of ZV t
corresponding to the base xV t , i.e. zit := H it xV t as in (2).
Decoding: After time n, each active user j ∈ A attempts to
ˆ
recover Gn as a function Gj of his private source Un and his
j
˙
entire channel output sequence zi[n] . A decoding error occurs
ˆ
if Gn ̸= Gj for any j ∈ A.

Secure source coding:
The secure source coding problem involves a wiretapper in
addition to the set V of users, a subset A of which is also
identiﬁed as the active users. The users observe n iid samples
˜
of some private distributed source say UV . They want to
discuss in public until some given function G of the source,
called the secret source, can be computed by the active users
but not the wiretapper. Unlike the original network coding
problem where the communication is over a given undirected
network, there is no restriction on the public discussion. The
users can broadcast messages to all other users noiselessly
with unlimited rates for multiple rounds. The only catch is
that the public messages and the discussion scheme are known
to the wiretapper. The secret source G is said to be securely
computable if the discussion can be chosen such that the error
probability in recovering secret source sequence Gn by the
active users and the amount of information about Gn leaked
to the wiretapper decay to zero in n.

Let εn be the probability of encoding or decoding error, i.e.
{
}
ˆ
εn := Pr ∃t ∈ [n], xV t ̸∈ B(ZV ) or ∃j ∈ A, Gn ̸= Gj (4)

This problem was ﬁrst proposed in [17] as an extension
to the secret key agreement problem in [8]. It was further
extended in [18, 20] in the study of imperfect secrecy, the
achievable exponents and the admissible choices of key functions. The problem of secure source coding can be mapped to
that of undirected network coding as follows such that G is
securely computable by public discussion if G is transmissible
over the undirected network.

Gn is said to be transmissible to A if εn decays to zero,
i.e. lim supn→∞ εn = 0. The objective is to ﬁnd an easily
computable condition for transmissibility.
A particular case of interest is to attain omniscience of the
distributed source, i.e. G = UV . Another case of interest is
when the users want to send independent messages instead of

3

Intuitively, it should be equal to H(G) for G to be securely
computable since no information about G should be leaked
to the wiretapper. With the previous argument that G being
transmissible implies it is securely computable, this is the
desired necessary condition.
The polynomial time complexity in computing the optimal
zV is not straightforward because the number of constraints in
(9) is exponential in m. The idea is to exploit the underlying
matroidal structure by solving the LP using the ellipsoid
method [9]. The complexity of the ellipsoid method is equivalent to that of the separation oracle that determines whether a
solution is feasible. From (9), zV is feasible iff for all j ∈ A

From secure source coding to undirected network coding:
For the secure source coding problem, we set (UV , ZV ) as the
multiple source, where UV is independent of ZV , and (Ui , Zi )
is the component source observed privately by user i ∈ V . The
secret source G is a function of UV but not ZV . The additional
source ZV allows the users to simulate the undirected network
by public discussion as follows. At time t from 1 to n, user i ∈
V broadcasts the public message
˙
f it := xit + xit

(7)

˙
where xV t is the base for ZV t and xit is the corresponding
channel input (1) for the undirected network coding problem.
˙
Recall that xit has to be computed from Un and the previous
i
˙
channel outputs zi[t−1] but there is no undirected network
in the secure source coding problem to generate the channel
outputs. Instead, the user simulate the network by computing

0≤
0≤

where H V t is the representation of ZV t corresponding to the
ˆ
base xV t . After time n, user j ∈ A compute the estimate Gj
n
n
˙
of G from Uj and zi[n] .

B⊆V :A⊆B

[z(B) − H(UB |UB c G) − H(ZB |ZB c )]

by the fact that entropy function is submodular [19]. The same
argument applies to the last minimization. Since there are only
|A| + 1 ≤ m + 1 submodular function minimizations, the
overall complexity is polynomial in m as desired.
■
The necessary condition may not be sufﬁcient as illustrated
in [1]. The problem of function computation is difﬁcult in
general even for directed networks [16]. In the omniscience
case G = UV , however, closely matching necessary and
sufﬁcient conditions have been derived for directed networks
using the cut-set bound and random coding scheme [15].
It turns out that the necessary condition above can also be
expressed in a similar form of the cut-set bound for which
a random coding scheme can also give a closely matching
sufﬁcient condition for undirected networks.

Theorem 1 A function G of UV is transmissible to A ⊆ V :
|A| ≥ 1 over a matroidal undirected network ZV only if

Theorem 2 UV is transmissible to A over a matroidal undirected network ZV only if

(8)

H(ZV ) = min z(V )

with zV := (zi ∈ R : i ∈ V ) subject to the linear constraints
z(B) ≥ H(UB |UB c ) + H(ZB |ZB c )

, and

f (B1 ) + f (B2 ) ≥ f (B1 ∩ B2 ) + f (B1 ∪ B2 )

Using the above mapping, the overall error probability is ϵn
in (4), which decays to zero by the assumption that G is
transmissible. It remains to argue that the public discussion
reveals no information about Gn . From (7), f V [n] is uniformly
distributed because xV [n] is not only uniformly distributed by
˙
˙
(3b) but also independent of xV [n] since xV [n] is a function
˙
of Un and the channel output zV [n] , which is ultimately a
V
function of Un independent of Zn . Since f V [n] is uniformly
V
V
distributed regardless of the realization of Un , we have Un
V
V
independent of f V [n] . Hence, G is securely computable and so
the necessary condition in [17] and [18, Theorem 7] for G to
be securely computable is also the necessary condition for G
to be transmissible.

zV

min

[z(B) − H(UB |UB c ) − H(ZB |ZB c )]

These are submodular function minimizations over lattice
families, and so can be solved in polynomial time [9]. More
precisely, the ﬁrst constraint set {B ⊆ V : j ∈ B} is a lattice
/
family because, for every B1 and B2 in the family, B1 ∩ B2
and B1 ∪B2 are also in the family. The ﬁrst objective function
f (B) := z(B) − H(UB |UB c ) − H(ZB |ZB c ) is submodular

˙
H it f it − zit = H it (xit + xit ) − H it xit
˙
˙
= H it xit = zit

H(G) = H(UV ) + H(ZV ) − min z(V )

min

B⊆V :j ∈B
/

zV

where

(10)

z(B) ≥ H(UB |UB c ) + H(ZB |ZB c ) ∀B ⊆ V :B ̸⊇ A (11a)
z(B) ≥ H(ZB |ZB c )
∀B ⊆ V :B ⊇ A. (11b)

∀B ⊆ V : B ̸⊇ A (9a)

z(B) ≥ H(UB |UB c G) + H(ZB |ZB c ) ∀B ⊆ V : B ⊇ A (9b)
∑
where z(B) := i∈B zi and B c denotes V \ B. The optimal
zV can be computed in polynomial time with respect to the size
|V | = m of the network, assuming that H(UB ) and H(ZB )
can be evaluated in polynomial time for B ⊆ V .
2

This necessary condition holds only if (and if)
0 = max

min

xV ∈B B⊆V :B̸⊇A

[H(ZB c ) − x(B c ) − H(UB |UB c )] (12)

with B being the set of xV satisfying

P ROOF For the secure source coding problem in [18], the
R.H.S. of (8) is a linear program (LP) that characterizes
the maximum amount of information about G that can be
kept secret from the wiretapper under the requirement that
G is recoverable by all active users after public discussion.

x(B) ≤ H(ZB ) ∀B ⊊ V
x(V ) = H(ZV ).

(13a)
(13b)

Furthermore, xV = zV is an optimal solution to (13) if zV is
an optimal solution satisfying (10).
2

4

(12) is in the form of the cut-set bound. To see this, consider
some subset B ⊆ V : B ̸⊇ A. Since the source UV needs to
be recovered by some users in B c , namely the active users in
A \ B ̸= ∅, there must be an information ﬂow of rate at least
H(UB |UB c ) collectively from B to B c . Suppose, for simplicity, that xV t ∈ B(ZV ) is chosen as the direction for time
t prior to observing any channel outputs. Then, using some
standard information-theoretic argument, it can be argued that
∑
1
the network supports a ﬂow rate of at most n t∈[n] I(xBt ∧
ZB c |xB c t ) = H(ZB c ) − x(B c ) where I(·) denotes Shannon’s
∑
1
mutual information [19], xi := n t∈[n] ℓ(xit ), and the last
equality is by (3). Furthermore, it can be shown [9] that the
set of all possible xV forms the base B of a polymatroid.
(12) simply asserts that there is a way to direct the network
according to some xV ∈ B such that any subset B c of users
containing an active user in A can obtain information at the
required rate H(UB |UB c ) ≤ H(ZB c ) − x(B c ). The formal
proof of Theorem 2 is given in [1] using the identity in [7].
The following closely matching sufﬁcient condition is derived
using the random coding scheme by adapting the error analysis
in [12] to the current undirected network model.

IV. E XTENSION
The previous results can be extended to undirected network
models where the matroidal structure can be identiﬁed. In
particular, a more general matroidal partly directed ﬁnite
linear network can be deﬁned as in [1] by specifying the
base partially so that the remaining choices of direction still
forms the base of a matroid. The directed and undirected
networks are covered as extreme special cases. The mapping
from the secret key agreement problem to the network coding
problem holds also for other scenarios such as agreeing on
multiple keys among different groups of users and multicasting
independent messages among different multicast groups.
R EFERENCES
[1] C. Chan, publications. http://goo.gl/4YZLT, http://chungc.net63.net/pub.
[2] Z. Li and B. Li, “Network coding in undirected networks,” in Proceedings of 38th Annual Conference on Information Sciences and Systems
(CISS), 2004.
[3] Z. Li, B. Li, and L. C. Lau, “On achieving maximum multicast throughput in undirected networks,” Information Theory, IEEE Transactions on,
vol. 52, no. 6, pp. 2467 – 2485, jun. 2006.
[4] S. Jaggi, P. Sanders, P. A. Chou, M. Effros, S. Egner, K. Jain, and
L. Tolhiuzen, “Polynomial time algorithms for multicast network code
construction,” Information Theory, IEEE Transactions on, vol. 51, no. 6,
pp. 1973–1982, jun. 2005.
[5] J. Goseling, C. Fragouli, and S. N. Diggavi, “Network coding for undirected information exchange,” IEEE Communications Letters, vol. 13,
no. 1, January 2009.
[6] C. Chan, “Generating secret in a network,” Ph.D. dissertation, Massachusetts Institute of Technology, 2010, see [1].
[7] ——, “The hidden ﬂow of information,” in 2011 IEEE International
Symposium on Information Theory Proceedings (ISIT2011), St. Petersburg, Russia, Jul. 2011, see [1].
[8] I. Csisz´ r and P. Narayan, “Secrecy capacities for multiple terminals,”
a
IEEE Transactions on Information Theory, vol. 50, no. 12, Dec 2004.
[9] A. Schrijver, Combinatorial Optimization: Polyhedra and Efﬁciency.
Springer, 2002.
[10] C. Y. S. Nitinawarat and A. Reznik, “Secret key generation for a
pairwise independent network model,” in IEEE International Symposium
on Information Theory, 2008. ISIT 2008., July 2008, pp. 1015–1019.
[11] C. Chan and L. Zheng, “Mutual dependence for secret key agreement,”
in Proceedings of 44th Annual Conference on Information Sciences and
Systems, 2010, see [1].
[12] A. S. Avestimehr, S. N. Diggavi, and D. N. C. Tse, “Wireless network information ﬂow: A deterministic approach,” CoRR, vol. abs/cs/0906.5394,
2009.
[13] A. Schrijver, “Matroids and linking systems,” Journal of Combinatorial
Theory, Series B, vol. 26, no. 3, pp. 349 – 369, 1979.
[14] M. Goemans, S. Iwata, and R. Zenklusen, “An algorithmic framework
for wireless information ﬂow,” in Communication, Control, and Computing, 2009. Allerton 2009. 47th Annual Allerton Conference on, 30
2009-oct. 2 2009, pp. 294 –300.
[15] T. S. Han, “Multicasting multiple correlated sources to multiple sinks
over a noisy channel network,” Information Theory, IEEE Transactions
on, vol. 57, no. 1, pp. 4 –13, jan. 2011.
[16] R. Appuswamy, M. Franceschetti, N. Karamchandani, and K. Zeger,
“Network coding for computing: Cut-set bounds,” IEEE Transactions of
Information Theory, vol. 57, no. 2, pp. 1015–1030, Feb 2011.
[17] H. Tyagi, P. Narayan, and P. Gupta, “When is a function securely computable?” Information Theory, IEEE Transactions on, vol. 57, no. 10,
pp. 6337 –6350, oct. 2011.
[18] C. Chan, “Multiterminal secure source coding for a common secret
source,” in Forty-Ninth Annual Allerton Conference on Communication,
Control, and Computing, Allerton Retreat Center, Monticello, Illinois,
sept 2011.
[19] R. W. Yeung, Information Theory and Network Coding. Springer, 2008.
[20] C. Chan, “Agreement of a restricted secret key,” in 2012 IEEE International Symposium on Information Theory Proceedings (ISIT2012),
Cambridge, MA, Jul. 2012, see [1].

Theorem 3 UV is transmissible to A if
0 < max

min

[H(ZB c ) − x(B c ) − H(UB |UB c )] . (14)

xV ∈B B⊆V :∅̸=B̸⊇A

(14) is necessary if < is replaced by ≤ as it becomes (12). 2
As detailed in [1], the optimal direction of the network can
be obtained from the optimal xV to (14), which again can
be computed in polynomial time. In the case of multicasting
independent messages, the sufﬁcient and necessary conditions
match precisely and give the achievable rate region below.
Theorem 4 The set of achievable RV for the undirected
network ZV is the set of non-negative tuples deﬁned by
G := {xV − zV ≥ 0 : xV ∈ B, zV ∈ Q }

(15)

where B is deﬁned by (13), and Q is the set of zV satisfying
z(B) ≥ H(ZB |ZB c ) ∀B ⊆ V : B ̸⊇ A.

(16)

The capacity or maximum achievable sum rate R(V ) is
C := H(ZV ) − min z(V ).
zV ∈Q

(17)

The projection of G onto the coordinates in A is
{yA : yV ∈ G} = {yA ≥ 0 : y(A) ≤ C}
which is completely characterized by C.

(18)
2

The capacity has the alternative form of partition connectivity in [7]. It can again be computed using the ellipsoid method
in polynomial time and so as the projection (18) of G onto A.

5

