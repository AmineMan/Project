Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 15:52:38 2012
ModDate:        Tue Jun 19 12:55:54 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      401483 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569566043

On the Sum Capacity of the Discrete Memoryless Interference Channel
with One-Sided Weak Interference and Mixed Interference
Fangfang Zhu and Biao Chen
Syracuse University
Department of EECS
Syracuse, NY 13244
Email: fazhu{bichen}@syr.edu

interference are motivated by properties associated with the
corresponding Gaussian channels. Some of those deﬁnitions
are intimately related to those introduced in [15] which studies
the capacity region of the discrete memoryless Z-channel.
The rest of the paper is organized as follows. Section II
presents the channel model and relevant previous results.
Section III deﬁnes DMICs with one-sided weak interference
and derives their sum capacities. We refer to those DMICs with
one-sided interference as DMZIC (i.e., discrete memoryless Z
interference channel) for ease of presentation. The equivalence
between the DMZIC with weak interference and the discrete
degraded interference channel (DMDIC) is established which
allows one to construct a capacity outer-bound for the DMZIC
using the capacity region of the associated degraded broadcast
channel. Section IV deﬁnes DMICs with mixed interference
and derives the sum capapcity for this class of channels.
Section V concludes this paper.

Abstract—The sum capacity of a class of discrete memoryless
interference channels is determined. This class of channels is
deﬁned analogous to the Gaussian Z-interference channel with
weak interference; as a result, the sum capacity is achieved by
letting the transceiver pair subject to the interference communicates at a rate such that its message can be decoded at the
unintended receiver using single user detection. Moreover, this
class of discrete memoryless interference channels is equivalent
in capacity region to certain discrete degraded interference
channels. This allows the construction of a capacity outerbound using the capacity region of associated degraded broadcast
channels. The same technique is then used to determine the sum
capacity of the discrete memoryless interference channel with
mixed interference. The above results allow one to determine sum
capacities or capacity regions of several new discrete memoryless
interference channels.

I. I NTRODUCTION
The interference channel (IC) models the situation where
the transmitters communicate with their intended receivers
while generating interference to unintended receivers. Despite
decades of intense research, the capacity region of IC remains
unknown except for a few special cases. These include interference channels with strong and very strong interference
[1]–[5]; classes of deterministic and semi-deterministic ICs
[6], [7]; and classes of discrete degraded ICs [8], [9].
Parallel capacity results exist for the discrete memoryless
IC (DMIC) and the Gaussian IC (GIC). Carleial ﬁrst obtained
capacity region for GIC with very strong interference [1]. This
result was subsequently extended by Sato [2] to that of DMICs
with very strong interference. Note that the deﬁnition of DMIC
with very strong interference can actually be broadened to
be more consistent with its Gaussian counterpart [10]. Sato
[3] and Han and Kobayashi [4] independently established in
1981 the capacity region of GIC with strong interference,
where the capacity is the same as that of a compound multiple
access channel. In [3] Sato also conjectured the conditions of
DMICs under strong interfernce, which was eventually proved
by Costa and El Gamal [5] in 1987.
While the capacity region for the general GIC remains
unknown, there have been recent progress in characterizing
the sum capacity of certain GICs, including: GICs with onesided weak interference [11], noisy interference [12]–[14], and
mixed interference [13]. This paper attempts to derive parallel
sum capacity results for DMICs with weak one-sided and
mixed inference. Our deﬁnitions of one-sided, weak, or mixed

II. P RELIMINARIES
A. Discrete Memoryless Interference Channels
A DMIC is speciﬁed by its input alphabets X 1 and X2 , output alphabets Y1 and Y2 , and the channel transition matrices:
p(y1 |x1 x2 ) =
p(y2 |x1 x2 ) =

y2 ∈Y2
y1 ∈Y1

p(y1 y2 |x1 x2 ),

(1)

p(y1 y2 |x1 x2 ).

(2)

The DMIC is said to be memoryless if
n
n n
p(y1 y2 |xn xn ) =
1 2

i=1

p(y1i y2i |x1i x2i ).

(3)

A (n, 2nR1 , 2nR2 , λ1 , λ2 ) code for a DMIC with independent information consists of two message sets M 1 =
{1, 2, · · · , 2nR1 } and M2 = {1, 2, · · · , 2nR2 } for senders 1
and 2 respectively, two encoding functions:
n
n
f1 : M1 → X1 , f2 : M2 → X2 ,

two decoding functions:
n
n
ϕ1 : Y1 → M1 , ϕ2 : Y2 → M2 ,

1

and the average probabilities of error:

forms a Markov chain, this DMIC is said to have one-sided
interference.
2
2
1
We refer to such DMIC as simply DMZIC. The deﬁnition is
λ1 =
P r{ϕ1 (y1 ) = w1 |W1 = w1 , W2 = w2 },
|M1 ||M2 | w =1 w =1
a natural extension of that for Gaussian ZIC where X 2 causes
1
2
interference on Y 1 . From the deﬁnition, it follows that X 1 and
2nR1 2nR2
1
λ2 =
P r{ϕ2 (y2 ) = w2 |W1 = w1 , W2 = w2 }. Y2 are independent for all input distribution p(x 1 )p(x2 ).
|M1 ||M2 | w =1 w =1
1
2
To deﬁne DMZIC with weak interference, we ﬁrst revisit
some properties of Gaussian ZIC with weak interference.
A rate pair (R1 , R2 ) is said to be achievable for the DMZIC
if and only if there exist a sequence of (2 nR1 , 2nR2 , n, λ1 , λ2 ) Similar to that established in [15], it is straightforward to show
codes such that λ1 , λ2 → 0 as n → ∞. The capacity region of that a Gaussian ZIC with weak interference is equivalent in
a DMZIC is deﬁned as the closure of the set of all achievable its capacity region to a degraded Gaussian ZIC satisfying the
Markov chain
rate pairs.
nR1

nR2

X2 − (X1 , Y2 ) − Y1 .

B. Existing Results for GICs
The received signals of a GIC in its standard form are
√
(4)
Y1 = X1 + aX2 + Z1 ,
√
Y2 =
bX1 + X2 + Z2 ,
(5)

This is referred in [15] as degraded Gaussian Z channel
of type-I. This motivates us to deﬁne DMZIC with weak
interference as follows.
Deﬁnition 2: A DMZIC is said to have weak interference
if the channel transition probability factorizes as

where a and b are the channel coefﬁcients corresponding to the
interference links, X i and Yi are the transmitted and received
signals, and the channel input sequence X i1 , Xi2 , · · · , Xin is

p(y1 y2 |x1 x2 ) = p(y2 |x2 )p (y1 |x1 y2 ),

n

subject to the power constraint
j=1

and Z2 are Gaussian noises with zero mean and unit variance,
independent of X 1 , X2 .
Sason in [11] proved that the sum capacity for GICs with
one-sided weak interference (a < 1 and b = 0) is
.

I(U ; Y2 ) ≥ I(U ; Y1 |X1 ),

Motahari and Khandani in [13] established that the sum
capacity for GICs with mixed interference (a ≤ 1 and b ≥ 1)
is
min

1
P1
log 1 +
2
1 + aP2

,

bP1
1
log 1 +
2
1 + P2

+

1
log(1 + P2 ).
2

p(y1 y2 |x1 x2 ) =

=

p(y2 |x1 x2 )p(y1 |x1 x2 y2 )
p(y2 |x2 )p(y1 |x1 y2 ).

The above deﬁnition of weak interference leads to the
following sum capacity result.
Theorem 1: The sum capacity of a DMZIC with weak
interference as deﬁned above is

C. Useful Properties of Markov Chains
The following properties of Markov chains are useful
throughout the paper:
• Decomposition: X − Y − ZW =⇒ X − Y − Z;
• Weak Union: X − Y − ZW =⇒ X − Y W − Z;
• Contraction: (X − Y − Z) and (X − Y Z − W ) =⇒
X − Y − ZW .

Csum =

max {I(X1 ; Y1 ) + I(X2 ; Y2 )}.

p(x1 )p(x2 )

(11)

Proof: This sum rate is achieved by two receivers decoding their own messages while treating any interference, if
present, as noise.
For the converse,

III. T HE DMZIC WITH W EAK I NTERFERENCE
A. Discrete Memoryless Z-Interference Channel
Deﬁnition 1: For the DMIC deﬁned in Section II-A, if

(a)

n(R1 + R2 ) − n

n
n
≤ I(X1 ; Y1n ) + I(X2 ; Y2n )

(6)

(b)

n

=

for all x1 , x2 , y2 , or equivalently,
X1 − X2 − Y2

(10)

for all input distributions p(x 1 )p(u)p(x2 |u).
The channel transition probability p(y 1 y2 |x1 x2 ) becomes

We attempt to extend these results to DMICs with appropriately deﬁned one-sided weak interference and mixed interference.

p(y2 |x2 ) = p(y2 |x1 x2 ),

(9)

for some p (y1 |x1 y2 ), or, equivalently, the channel is stochastically degraded.
In the absence of receiver cooperation, a stochastically
degraded interference channel is equivalent in its capacity to a
physically degraded interference channel. As such, we will assume in the following that the channel is physically degraded,
i.e., the DMZIC admits the Markov chain X 2 − (X1 , Y2 ) − Y1 .
As a consequence, the following inequality holds

2
Xij ≤ nPi , i = 1, 2, Z1

P1
1
1
log(1 + P2 ) + log 1 +
2
2
1 + aP2

(8)

i=1

n
H(Y1i |Y1i−1 ) − H(Y1i |Y1i−1 X1 ) + H(Y2i |Y2i−1 )

n
−H(Y2i |Y2i−1 X2 )

(7)

2

(c) n

≤

(d)

i=1

n

−H(Y2i |Y2i−1 X2i )

=

(e)

i=1
n

=

i=1
n

=
i=1
n

=
i=1
(f ) n

≤

i=1
n

≤

n
H(Y1i ) − H(Y1i |X1 Y2i−1 ) + I(X2i ; Y2i |Ui )

max {I(X1 ; Y1 ) + I(X2 ; Y2 )} + .

p(x1 )p(x2 )

I(X2 ; Y1 |X1 ) ≤ I(X2 ; Y2 ),

(I(Ui X1i ; Y1i ) + I(X2i ; Y2i |Ui ))

(12)

for all product input distribution on X 1 × X2 . One can ﬁnd
examples such that the mutual information condition holds but
the Markov chain is not valid. This is different from that of
the Gaussian case; it can be shown that the coefﬁcient a ≤ 1
in a Gaussian ZIC is a sufﬁcient and necessary condition for
(12) to hold. It is yet unknown if condition (12) is sufﬁcient
for the sum capacity result (11) to hold for DMZIC with weak
interference.

(I(X1i ; Y1i ) + I(Ui ; Y1i |X1i ) + I(X2i ; Y2i |Ui ))
(I(X1i ; Y1i ) + I(Ui ; Y2i ) + I(X2i ; Y2i |Ui ))

B. Capacity Outer-bound for DMZIC with Weak Interference
For Gaussian ZICs with weak interference, Sato [2] obtained an outer-bound using the capacity region of a related
Gaussian broadcast channel constructed due to the equivalence
in capacity between a GZIC with weak interference and a
degraded GIC. The same technique can be used to obtain a
capacity outer-bound for DMZIC with weak interference, i.e.,
that satisﬁes the Markov chain X 2 −(X1 , Y2 )−Y1 . Speciﬁcally,
for any such DMZIC with weak interference, one can ﬁnd
an equivalent (in capacity region) DMDIC whose capacity
region is bounded by that of an associated degraded broadcast
channel.
Theorem 2: For a DMZIC that satisﬁes the Markov chain
X2 − X1 Y2 − Y1 , the capacity region is outer-bounded by

(I(X1i ; Y1i ) + I(X2i ; Y2i )),
i=1

where Ui
Y2i−1 for all i, (a) follows the Fano’s Inequality, (b) is from the chain rule and the deﬁnition of
mutual information, (c) is because of the fact that conditioning reduces entropy, and that Y 2i is independent of
any other random variables given X 2i , (d) is due to the
memoryless property of the channel and the fact that Y 1i
is independent of any other random variables given X 1i
i−1
n
and Y2i , then (X1,i , Y1i ) − (X1 , Y2i−1 ) − Y1i−1 forms a
Markov chain. By the weak union property, the Markov chain
n
Y1i − (X1 , Y2i−1 ) − Y1i−1 holds; (e) is because of the Markov
i−1
n
chain (X1 , X1,i+1 ) − (X1i , Y2i−1 ) − Y1i . The easiest way to
prove it is using the Independence Graph. Alternatively, we
ﬁrst note that the Markov chain
i−1
n
(X1 , X1,i+1 , Y2i−1 )

I(X1 ; Y1 |Q) + I(X2 ; Y2 |Q) +

Remark 1: The Markov chain (8) is a sufﬁcient, but not
necessary, condition for the mutual information condition

H(Y1i ) − H(Y1i |X1i Y2i−1 ) + I(X2i ; Y2i |Ui )

i=1
n

=

≤

R1 + R2

(I(X1i ; Y1i ) + I(Ui X2i ; Y2i ))

=
(g)

(g) follows from the Markov chain U i − X2i − Y2i . At last,
by introducing a time-sharing random variable Q, one obtains

n
H(Y1i ) − H(Y1i |Y1i−1 X1 Y2i−1 ) + H(Y2i |Y2i−1 )

ROB = co

− (X1i , Y2i ) − Y1i





(R1 , R2 )

p(u)p(x1 x2 |u)




R1 ≤ I(U ; Y1 ),
,
R2 ≤ I(X1 X2 ; Y2 |U ) 

where U − X1 X2 − Y2 − Y1 forms a Markov chain and U =
min{ Y1 , Y2 , X1 · X2 }.
Proof: Suppose that the DMZIC with weak interference
has inputs and outputs X 1 , X2 and Y1 , Y2 respectively. Let
us denote by X 1 , X2 and Y1 , Y2 the inputs and outputs of
another DMIC. Set X 1 = X1 , X2 = X2 , and Y1 = Y1 but
deﬁne Y2 to be a bijection of X 1 and Y2 , denoted as Y2 =
f (X1 , Y2 ). As the Markov chain (X 1 , X2 ) − Y2 − Y1 holds,
the DMIC speciﬁed by the input pair (X 1 , X2 ), and the output
pair (Y1 , Y2 ) is indeed a DMDIC.
The proof that this DMDIC has the same capacity region
as the speciﬁed DMZIC, and hence is outer-bounded by the
associated broadcast channel follows in exactly the same
fashion as Costa’s proof for the Gaussian case [16], hence
is omitted here.
Remark 2: The output Y 2 need not necessarily be a bijection
function of X 1 and Y2 ; instead, depending on the transition
probability p(y 1 |x1 y2 ), other Y2 can be constructed. However,
the associated broadcast channels would have the same the
capacity region. It will become clear in the following example.

holds, since given X 1i and Y2i , Y1i is independent of
i−1
n
X1 , X1,i+1 , Y2i−1 . By the weak union property, the following Markov chain is obtained:
i−1
n
(X1 , X1,i+1 ) − (X1i , Y2i ) − Y1i .

Together with the Markov chain
i−1
n
(X1 , X1,i+1 ) − X1i − Y2i
n
because of the independence between Y 2n and X1 , we get the
Markov chain:
i−1
n
(X1 , X1,i+1 ) − X1i − (Y1i , Y2i )

by the contraction property. Again, using the weak union
property and then the decomposition property, we obtain the
Markov chain
i−1
n
(X1 , X1,i+1 ) − (X1i , Y2i−1 ) − Y1i

as desired. Since Ui and X1i are independent, then
p(x1 x2 u) = p(x1 )p(u, x2 ), thus (f ) comes from (10). Finally,

3

C. Numerical Example

0.7
Broadcast channel outerbound
Previous outer bound

Example 1: Let X 1 = X2 = Y1 = Y2 = 2 and
the channel transition probability be given by

0.6

p(y1 y2 |x1 x2 ) = p(y2 |x2 )p(y1 |x1 y2 ),

0.5

where p(y2 |x2 ) and p(y1 |x1 y2 ) are speciﬁed in Table I.

R2

0.4

TABLE I
C HANNEL T RANSITION P ROBABILITIES
p(y2 |x2 )
x2 = 0
x2 = 1

y2 = 0
.1
.9

y2 = 1
.9
.1

p(y1 |x1 y2 )
x1 y2 = 00 or 11
x1 y2 = 01 or 10

y1 = 0
.75
0

0.3

y1 = 1
.25
1

0.2

0.1

By Theorem 1, the sum capacity is

0

0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

R1

Csum =

max

p(x1 )p(x2 )

I(X1 ; Y1 ) + I(X2 ; Y2 ) ≈ .531.
Fig. 1.

Comparison of the outer-bounds.

Moreover, one can construct Y 2 as follows:
0, if x1 y2 = 00 or 11,
1, otherwise.

Y2 =

IV. T HE DMIC WITH M IXED I NTERFERENCE
Deﬁnition 3: A DMIC is said to have mixed interference if
it satisﬁes the Markov chain

Then p(y2 |x1 x2 ) is given in Table II.

p(y2 |x1 x2 )
x1 x2 = 00
x1 x2 = 01
x1 x2 = 10
x1 x2 = 11

y2 = 0
.1
.9
.9
.1

y2 = 1
.9
.1
.1
.9

p(u)p(x1 x2 |u)

is




,

If one takes the bijection function to construct Y 2 , it will lead
to the same outer-bound. If we ﬁx R 2 to be x, then
R2 =x

=
≤

max

H(Y2 |U)=x+h2 (.1)

max

{I(X2 ; Y2 |X1 ) + min{I(X1 ; Y1 ), I(X1 ; Y2 )}} .

(15)

Proof: In order to achieve this sum rate, user 1 transmits
its message at a rate such that both receivers can decode it
by treating the signal from user 2 as noise; user 2 transmits
at the interference-free rate since receiver 2 is able to subtract
the interference from user X 1 .
For the converse, we prove the following two sum rate
bounds separately:

H(Y1 ) − H(Y1 |U )

log(|Y1 |) − fT (x + h2 (.1)),

ROB = {(R1 , R2 )|R1 ≤ log |Y1 | − fT (x + h2 (.1)), R2 ≤ x} .
This new outer-bound signiﬁcantly improves upon the following bound

R1 + R2

(14)

for all possible product distributions on X 1 × X2 .
This deﬁnition is motivated by GIC with mixed interference,
which can be shown to be equivalent in capacity region to
a degraded GIC satisfying (13) by setting p(y 1 y2 |x1 x2 ) =
p(y2 |x1 x2 )p (y1 |x1 y2 ), where p (y1 |x1 y2 ) is normal distribu√
√
tion with mean (1 − ab)x1 + ay2 and variance 1 − a. The
sum capacity for GIC with mixed interference was established
in [13]. We obtain a parallel result for the DMIC with mixed
interference as deﬁned above.
Theorem 3: The sum capacity of the DMIC with mixed
interference satisfying the two conditions (13) and (14) is
p(x1 )p(x2 )

where fT (·) is a function deﬁned by Witsenhausen and Wyner
[17]. Fig. 1 depicts the new outer-bound speciﬁed by

R1
R2

(13)

and

Using Theorem 2, the capacity region of the DMZIC
outer-bounded by that of the associated DMDBC:


R1 ≤ I(U ; Y1 ),
ROB = co
(R1 , R2 )
R2 ≤ I(X1 X2 ; Y2 |U )

max R1

X2 − (X1 , Y2 ) − Y1
I(X1 ; Y1 |X2 ) ≤ I(X1 ; Y2 |X2 )

TABLE II
P (Y2 |X1 X2 )

n

n(R1 + R2 )
n(R1 + R2 )

≤ I(X1 ; Y1 |X2 ),
≤ I(X2 ; Y2 ),

≤
≤

I(X1i X2i ; Y2i ),

(16)

i=1
n

i=1

I(X1i ; Y1i ) + I(X2i ; Y2i |X1i ). (17)

For (16), the derivation follows the same steps as Costa and
El Gamal’s result [5]. For (17), we use similar techniques
for establishing the sum capacity of the DMZIC with weak
interference in Section III. First, notice that (13) implies

≤ I(X1 ; Y1 ) + I(X2 ; Y2 ).

I(U ; Y1 |X1 ) ≤ I(U ; Y2 |X1 )

4

(18)

for any U whose joint distribution with X 1 , X2 , Y1 , Y2 is
p(u, x1 , x2 , y1 , y2 ) = p(u)p(x1 x2 |u)p(y1 y2 |x1 x2 ).

V. C ONCLUSION
In this paper, we derived the sum capacity for the DMZICs
with weak interference where weak interference is deﬁned
using a Markov condition. Similar techniques are then applied
to derive the sum capacity for DMIC with mixed interference.
Both results are analogous to the sum capacity results for the
corresponding Gaussian channel, both in the expression of the
capacity and in the encoding schemes that achieve the capacity.
The weak interference condition is deﬁned using a Markov
chain, as opposed to that using the mutual information inequality. While it appears to be somewhat restrictive, it is not
known whether the deﬁnition using the mutual information
condition will lead to the same sum capacity result.

(19)

Then,

(a)

n(R1 + R2 ) − n

n
n
n
≤ I(X1 ; Y1n ) + I(X2 ; Y2n |X1 )
n

=
i=1

(b) n

≤

n n
−H(Y2i |Y2i−1 X2 X1 )

i=1
n

n
n
H(Y1i |Y1i−1 ) − H(Y1i |Y1i−1 X1 ) + H(Y2i |Y2i−1 X1 )

n
H(Y1i ) − H(Y1i |Y1i−1 X1 Y2i−1 ) + H(Y2i |Ui X1i )

−H(Y2i |X2i X1i Ui ))

=

R EFERENCES

(I(Ui X1i ; Y1i ) + I(X2i ; Y2i |Ui X1i ))

[1] A. B. Carleial, “A case where interference does not reduce capacity,”
IEEE Trans. Inf. Theory, vol. 21, no. 5, pp. 569-570, Sep. 1975.
[2] H. Sato, “On the capacity region of a discrete two-user channel for
(I(X1i ; Y1i ) + I(Ui ; Y1i |X1i ) + I(X2i ; Y2i |Ui X1i ))
=
strong interference,” IEEE Trans. Inf. Theory, vol. 24, no. 3, pp. 377i=1
379, May 1978.
n
(c)
[3] H. Sato, “The capacity of the Gaussian interference channel under strong
≤
(I(X1i ; Y1i ) + I(Ui ; Y2i |X1i ) + I(X2i ; Y2i |Ui X1i ))
interference,” IEEE Trans. Inf. Theory, vol. 27, pp. 786-788, Nov. 1981.
[4] T. S. Han and K. Kobayashi, “A new achievable rate region for the
i=1
n
interference channel,” IEEE Trans. Inf. Theory, vol. 27, pp. 49-60, Jan.
(d)
1981.
=
(I(X1i ; Y1i ) + I(X2i ; Y2i |X1i )),
[5] M. H. M. Costa and A. El Gamal, “The capacity region of the discrete
i=1
memoryless interference channel with strong interference,” IEEE Trans.
n
where (a) is because of the independence between X 1 and
Inf. Theory, vol. 33, pp. 710-711, Sep. 1987.
n
X2 ; (b) is from the fact that conditioning reduces entropy
[6] A. El Gamal and M. H. M. Costa, “The capacity regio of a class of
i−1 n
deterministic interference channels,” IEEE Trans. Inf. Theory, vol. 28,
(X1 X1,i+1 , Y2i−1 ); (c) is from (18); and (d)
and Ui
no. 2, pp. 343-346, Mar. 1982.
is because the memoryless property of the channel and (19).
[7] H. F. Chong and M. Motani, “The capacity region of a class of
Finally, from (16) and (17), we have
semideterministic interference channels,” IEEE Trans. Inf. Theory, vol.
55, no.2 ,pp. 598-603, Feb. 2009.
R1 + R2 ≤
max I(X1 X2 ; Y2 ),
p(x1 )p(x2 )
[8] R. Benzel, “The capacity region of a class of discrete additive degraded
interference channels,” IEEE Trans. Inf. Theory, vol. 25, no. 2, pp.
max I(X1 ; Y1 ) + I(X2 ; Y2 |X1 ),
R1 + R2 ≤
p(x1 )p(x2 )
228-231, Mar. 1979.
[9] N. Liu and S. Ulukus, “The capacity region of a class of discrete
respectively.
degraded interference channels,” IEEE Trans. Inf. Theory, vol. 54, no.
We give the following example where the obtained sum
9, pp. 4372-4378, Sep. 2008.
[10] J. Xu, H. Chen and B. Chen, “New observation on interference
capacity helps determine the capacity region of a DMIC.
channels under strong/very strong interference,” in Proc. IEEE Global
Example 2: Consider the following deterministic channel:
Communications Conference (Clobecom’2010), Miami, FL, December
Y1 = X1 · X2 ,
2010.
[11] I. Sason, “On achievable rate regions for the Gaussian interference
Y2 = X1 ⊕ X2 ,
channels,” IEEE Trans. Inf. Theory, vol. 50, no. 6, pp. 1345-1356, Jun.
where the input and output alphabets X 1 , X2 , Y1 and Y2 =
2004.
{0, 1}. Notice that this channel does not satisfy the condition [12] X. Shang, G. Kramer and B. Chen, “A new outer bound and the noisyinterference sum-rate capacity for Gaussian interference channels,” IEEE
of the deterministic interference channel in [6]. Obviously, the
Trans. Inf. Theory, vol. 55, no. 2, pp. 689-699, Feb. 2009.
Markov chain (13) holds. Moreover,
[13] A. S. Motahari and A. K. Khandani, “Capacity bounds for the Gaussian
interference channel,” IEEE Trans. Inf. Theory, vol. 55, no. 2, pp. 620I(X1 ; Y1 |X2 )=H(Y1 |X2 ) = p(x2 = 1)H(X1 ),
643, Feb. 2009.
I(X1 ; Y2 |X2 )=H(Y2 |X2 ) = H(X1 ).
[14] V. S. Annapureddy and V. V. Veeravalli, “Gaussian interference
networks: sum capacity in the low-interference regime and new outer
Therefore,
bounds on the capacity region,” IEEE Trans. Inf. Theory, vol. 55, no.
I(X1 ; Y1 |X2 ) ≤ I(X1 ; Y2 |X2 ),
7, pp. 3032-3250, Jun. 2009.
H. Chong,
for all possible input product distributions on X 1 × X2 . [15] “Z”F.channel,” M. Motani, and H. K. Garg, “Capacity4,theorems for the
IEEE Trans. Inf. Theory, vol. 53, no.
pp. 1348-1365,
Therefore, this is a DMIC with mixed interference. Apply
Apr. 2007.
Theorem 3, we compute the sum capacity is
[16] M. H. M. Costa, “On the Gaussian interference channel,” IEEE Trans.
Inf. Thoery, vol. IT-31, no.5, pp. 607-615, Sep. 1985.
Csum =
max [min(I(X1 X2 ; Y2 ), I(X1 ; Y1 ) + I(X2 ; Y2 |X1 ))]
[17] H. S. Witsenhausen and A. D. Wyner, “A conditional entropy bound
p(x1 )p(x2 )
for a pair of discrete random variables,” IEEE Trans. Inf. Theory, vol.
= 1.
21, no. 5, pp. 493-501, Sep. 1975.
i=1
n

Given that (1, 0) and (0, 1) are both trivially achievable, the
above sum capacity leads to the capacity region for this DMIC
to be {(R1 , R2 ) : R1 + R2 ≤ 1}.

5

