Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May 16 16:56:51 2012
ModDate:        Tue Jun 19 12:55:29 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      368560 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566253

Adaptive Group Testing as
Channel Coding with Feedback
Matthew Aldridge
Heilbronn Institute for Mathematical Research
School of Mathematics
University of Bristol, UK
m.aldridge@bristol.ac.uk

Adaptive (or sequential) group testing, where the
outcomes of previous tests can be used to inﬂuence the
makeup of future pools.
Recently, new results on nonadaptive group testing with
arbitrarily small probability of error have been derived using
information-theoretic techniques. A recent paper of Atia and
Saligrama [3] proves bounds on T using techniques similar to
the proof of Shannon’s channel coding theorem [4].
In this paper, we study adaptive group testing using
information-theoretic techniques. Clearly adaptive group testing cannot be more difﬁcult than nonadaptive testing. We show
that it is not much easier either.
Speciﬁcally, Theorem 2 shows that the number of tests required for adaptive group testing is no more than that required
for nonadaptive testing, but is still greater than the Atia–
Saligrama lower bound. The result is obtained by techniques
similar to Shannon’s proof that feedback does not improve
capacity for channel coding [5]. As far as we are aware, this is
the ﬁrst information-theoretic result for adaptive group testing.
In combinatorial zero-error group testing using the noisefree model, adaptive testing certainly is an improvement. Only
O(K log N ) are needed for adaptive testing, whereas at least
Ω(K 2 log N/ log K) are required for nonadaptive testing [2].
We note that this is similar to the case of zero-error channel
coding, where feedback may improve the zero-error capacity
[5].
The structure of this paper is as follows. In Section II
we outline the information-theoretic approach to nonadaptive
group testing, ﬁxing notation, and reviewing the work of
Atia and Saligrama [3] and others. In Section III we brieﬂy
review Shannon’s result on channel coding with feedback,
before stating and proving our main theorem (Theorem 2).
We conclude with Section IV and discuss our results.

Abstract—Group testing is the combinatorial problem of identifying the defective items in a population by grouping items
into test pools. Recently, nonadaptive group testing – where all
the test pools must be decided on at the start – has been studied
from an information theory point of view. Using techniques from
channel coding, upper and lower bounds have been given on the
number of tests required to accurately recover the defective set,
even when the test outcomes can be noisy.
In this paper, we give the ﬁrst information-theoretic result on
adaptive group testing – where the outcome of previous tests
can inﬂuence the makeup of future tests. We show that adaptive
testing does not help much, as the number of tests required obeys
the same lower bound as nonadaptive testing. Our proof uses
similar techniques to the proof that feedback does not improve
channel capacity.

•

I. I NTRODUCTION
The problem of group testing concerns detecting the defective members of a set of items through the means of
pooled tests. Group testing as a subject dates back to the work
of Dorfman [1] in 1940s studying practical ways of testing
soldiers’ blood for syphilis, and has received much attention
from combinatorialists and probabilists since.
The setup is as follows: Suppose we have a number of items,
of which some are defective. To identify the defective items
we could test each of the items individually for defectiveness.
However, when the proportion of defective items is small, most
of the tests will give negative results. A less wasteful method
is to test pools of many items together at the same time. In
the noise-free model, a pool gives a negative test outcome if
it contains no defective items, and gives a positive outcome
if it contains at least one defective item. (In Section II of this
paper we consider models with noise.) After a number T of
such pooled tests, it should be possible to deduce which items
were defective.
Traditionally, group testing has been seen as a combinatorial
problem. One aims to ﬁnd a pooling strategy such that each
possible defective set gives a different sequence of outcomes.
This gives a zero error probability, and one is interested in
how small T can be made. (See, for example, the textbook
of Du and Hwang [2] for more details on the combinatorial
approach to group testing.)
Group testing splits into two main types:
• Nonadaptive group testing, where the entire pooling
strategy is decided on beforehand;

II. T HE INFORMATION - THEORETIC APPROACH
TO NONADAPTIVE GROUP TESTING

First we ﬁx some notation. We have N items, of which
a subset K of size K is defective. We wish to accurately
estimate the defective set from T tests. A pooling strategy
can be deﬁned by a testing matrix X = (xit ) ∈ {0, 1}N ×T ,
where xit = 1 denotes that item i is in the pool for test t, and
xit = 0 denotes that it is not. Test t gives an output Yt in some
output alphabet Y (which is usually {0, 1} also). Then, given

1

Channel coding:
Message

- Codeword
x

m

channel Received
T uses
signal Y

- Message
estimate m
ˆ

Group testing:
Defective
set K
Fig. 1.

-

Testing
matrix X

model T tests

0
1

if kt ≥ 1,
•

Yt =

is the same for all t and, given kt , the distribution of Yt is
independent of all past and future tests.
Group testing can be considered as being similar to channel
coding. Here, the defective set takes the place of the message,
the testing matrix is like the codeword (or perhaps the codebook), and the test outcomes are like the received signal. Then,
like channel coding, we want to estimate the message/defective
set using as few channel uses/tests as possible while keeping
the error probability low. Figure 1 illustrates this.
Atia and Saligrama’s main result was the following bounds
on the number of tests required to accurately detect the
defective set [3].
Theorem 1: Consider a group testing model where only
∗
∗
defects matter. Let TNA = TNA (N, K, ) be the minimum
number of tests necessary to identify K defects among N
items with error probability at most ∈ (0, 1). Then for any
p,
∗
T 1 − o(1) ≤ TNA ≤ T 1 + o(1)

with probability 1 − q,
with probability q;

Yt = 1.

Dilution model, where false negatives occur with probability ukt . That is,
if kt = 0,
if kt ≥ 1,

Yt = 0;
Yt =

- Defective
ˆ
set estm. K

A diagram illustrating the similarities between channel coding and group testing.

the test outcomes Y = (Yt ) ∈ Y T , we make an estimate
ˆ
ˆ
K = K(Y) of the defective set. The average probability of
error is .
Let kt = |{i ∈ K : xit = 1}| denote the number of defective
items in test t. In the main noise-free case, Yt = 0 (denoting
a negative test outcome) if kt = 0, and Yt = 1 (denoting a
positive test outcome) if kt ≥ 1.
We can also consider group testing with noise. Atia and
Saligrama [3] consider two noise models:
• Addition model, where false positives occur with probability q. That is,
if kt = 0,

Test
outcomes Y

as N → ∞, where
0
1

with probability ukt ,
with probability 1 − ukt .

log
T = max
L⊂K

Sejdinovic and Johnson [6] considered a model where both
the addition and the dilution errors can occur. Chan et al [11]
consider another error model:
• Symmetric error model, where false positives and false
negatives both occur with the same probability q. That is,
if kt = 0,

Yt =

0
1

Yt =

0
1

T = max
L⊂K

K
|L|

I(XK\L ; XL , Y )
log

N −|L|
K−|L|

I(XK\L ; XL , Y )

,

(1)

.

(2)

Here, the Xi are IID Bern(p), Y is related to X through the
channel model, and I denotes mutual information. We have
used the notation XL := (Xi : i ∈ L) and similar.
Atia and Saligrama [3] proved the theorem for the noisefree, addition and dilution models. Aldridge [7, Section 6.4]
pointed out that their analysis extends to any model where
only defects matter. Atia and Saligrama [3] also extended their
result to the K = o(N ) asymptotic regime.
The proof of the upper bound is similar to Gallager’s
proof [8] of the direct part Shannon’s channel coding theorem
[4]. Test pools are designed at random, with Xit = 1 with
probability p and Xit = 0 with probability 1 − p, IID over i
and t. Estimation of the defective set is done on a maximum
ˆ
likelihood basis, in that K is chosen to maximise the probabilty
of the outcome Y given the testing matrix X.

with probability 1 − q,
with probability q;

if kt ≥ 1,

N −K
|L|

with probability q,
with probability 1 − q.

Aldridge [7, Chapter 6] considered a wider class of models
where ‘only defects matter’, in that the distribution of Yt
depends only on kt (and not on how many nondefective items
are in a test pool). All the models above are examples of where
only defects matter.
We will assume throughout that the group testing model is
static and memoryless, in that the distribution of Yt given kt

2

The proof of the lower bound resembles the converse part of
Shannon’s theorem (see for example [9, Section 7.9]), where
Fano’s inequality bounds the error probability. Unfortunately,
unlike in Shannon’s theorem, we are not so lucky that the
upper and lower bounds asymptotically coincide, although
they are close up to a logarithmic factor in N [3].
There has been other recent work on nonadaptive
information-theoretic group testing. Sejdinovic and Johnson
[6] gave accurate asymptotic expressions for T for the noisefree, addition and dilution models. Cheraghchi et al [10]
considered group testing when the makeup of the pools is
constrained by a graphical structure. Numerous authors [6],
[12], [13], [11] have used modern decoding algorithms on
nonadaptive group testing simulations.
Some similar work has occured in the compressed sensing
community; see the survey of Malyutov [14].

To prove the ﬁrst inequality, we adapt Atia and Saligrama’s
proof of converse part of Theorem 1 [3], and Shannon’s
proof that feedback fails to improve channel capacity [5], as
exposited by Cover and Thomas [9, Theorem 7.12.1].
Choose a set of items L of size |L| uniformly at random
from {1, 2, . . . , N }, and choose K of size K uniformly at
random from sets containing L.
Suppose a genie reveals to us the |L| defective items L ⊂
K, leaving us to work out the remaining K − |L| defective
N −|L|
items. Given L, there are K−|L| equally likely choices of
the random K, so
H(K | L) = log

log

In adaptive group testing, the makeup of a testing pool can
depend on the outcomes of earlier tests, so

This is similar to channel coding with feedback, where
future inputs to the channel can depend on past outputs.
Shannon proved that feedback does not improve the capacity
of a single-user channel [5]. Since a transmitter could choose
not to use the feedback, it’s clear that the capacity with
feedback is at least as high as the capacity without. However
by being more careful with Fano’s inequality in the proof of
the converse, it can apply to the case of feedback also. See
[9, Section 7.12], for example, for a detailed proof.
Our result proceeds similarly. Due to the non-tightness of
the bounds on testing in the nonadaptive case, we will not
be able to show that adaptive group testing requires the same
number of tests as nonadaptive testing, but we will be able
to show that it obeys the same lower bound and requires no
more tests than the nonadaptive case.
The lack of much improvement due to adaptive testing may
initially seem surprising. However, the analogy with Shannon’s
feedback result explains why we should in fact expect this.
We emphasise that our theorem holds not only for the noisefree model, but also for the dilution and addition models, and
any model where only defects matter.
Theorem 2: Consider a group testing model where only
∗
∗
defects matter. Let TNA and TA (dependent on N , K and ) be
the minimum number of tests necessary to identify K defects
among N items with error probability at most ∈ (0, 1) for
nonadaptive and adaptive group testing respectively. Then, as
N → ∞, for any p we have the inequalities
T 1 − o(1) ≤

≤

N − |L|
K − |L|

ˆ
ˆ
= H(K | K, L) + I(K; K | L).

(4)

We can now use Fano’s inequality (see for example [9,
Theorem 2.10.1]) to bound the conditional entropy term in
(4) in terms of the error probability . Speciﬁcally, we have

xit = xit (Y1 , . . . , Yt−1 ).

∗
TNA

(3)

Using a standard identity we can rewrite (3) as

III. A DAPTIVE GROUP TESTING

∗
TA

N − |L|
.
K − |L|

ˆ
H(K | K, L) ≤ 1 + log

N − |L|
,
K − |L|

(5)

N −|L|
since there are again K−|L| choices for K [3, Section IV].
Substituting (5) into (4) gives

log

N − |L|
K − |L|

≤ 1 + log

N − |L|
ˆ
+ I(K; K | L). (6)
K − |L|

A series of standard information theory inequalities and
identities show that the mutual information term in (6) can
be bounded by
ˆ
I(K; K | L) ≤ T I(XK\L ; XL , Y ).

(7)

We relegate the elementary (but slightly long-winded) veriﬁcation of (7) to the Appendix.
Substituting (7) into (6) gives
log

N − |L|
K − |L|

≤ 1 + log

Rearranging (8) to make
≥1−T

N − |L|
+ T I(XK\L ; XL , Y ).
K − |L|
(8)
the subject gives

I(XK\L ; XL , Y )
log

N −|L|
K−|L|

−

1
log

N −|L|
K−|L|

.

(9)

Sending N → ∞ in (9), it is clear that we require

≤ T 1 + o(1)
∗
TA ≥

where T and T are as in (1) and (2).
Proof: The third inequality is part of Theorem 1. The
second inequality is trivial, as nonadaptive group testing is
merely a special case of adaptive group testing where the tester
chooses to ignore the information of previous test results.

log

N −|L|
K−|L|

I(XK\L ; XL , Y )

1 + o(1)

(10)

to force the error probability to be arbitrarily small.
Since (10) has to be true for all L ⊂ K, and all test inclusion
parameters p, this gives the result.

3

where we have used the notation XLt := (Xit : i ∈ L) for
ﬁxed t and similar. We justify the above steps as follows:
(12) is from applying the chain rule to the right hand side of
(11);
(13) is because XLt is a function of Y1 , . . . , Yt−1 and L, and
the same for K;
(14) is because conditioning reduces entropy, so removing
conditioning increases it;
(15) is because, conditional on XKt , we know Yt is independent of the previous outcomes Y1 , . . . , Yt−1 and the
defective set K.
But the term in the summand of (15) is precisely the mutual
information

IV. D ISCUSSION AND C ONCLUSION
We have considered adaptive group testing for models where
only defects matter with arbitrarily low probability of error. We
have shown that adaptive testing requires no more tests than
nonadaptive testing and, since it still obeys the Atia–Saligrama
lower bound, cannot reduce the number of tests very much.
Even given the comparison of adaptive group testing with
channel coding with feedback, our result may still seem surprising. We emphasise that the result is only for the N → ∞
asymptotic regime, where a vanishingly small proportion of
items are defective. Also, our result is not about the zeroerror = 0 case. With the noisy channel models, heuristics
such as ‘stop testing items that we are certain are defective’ are
irrelevant, as we can never be certain that an item is defective.
∗
∗
It remains an open question whether or not TA = TNA
(either exactly or in an asymptotic sense), or whether, as
with zero-error testing for the noise-free model, there is a gap
between TA and TNA . Thus our result does not guarantee that
adaptivity is of no gain, even in the asymptotic regime.
∗
∗
Even if it were the case that TA = TNA , it still would not
mean that the ability to construct tests adaptively is of no
use in group testing with errors. For modest values of N , for
example, adaptivity may reduce the number of tests necessary,
with the beneﬁt vanishing as N → ∞. Also, adaptive testing
may make optimal decoding algorithms simpler, or may make
simple suboptimal decoding algorithms more effective.

H(Yt | XLt ) − H(Yt | XKt ) = I(XK\L t ; Yt | XLt ), (16)
Since the group testing model is static and memoryless, (16)
is independent of t. Hence substituting (16) into (15) gives
T

ˆ
I(K; K | L) ≤

= T I(XK\L ; Y | XL ).

I(XK\L ; Y | XL ) = I(XK\L ; XL , Y ) − I(XK\L ; XL )
= I(XK\L ; XL , Y ),

ˆ
I(K; K | L) ≤ T I(XK\L ; XL , Y ),

which is required in the proof of Theorem 2.
We use the data processing inequality left-hand side to write
ˆ
I(K; K | L) ≤ I(K; Y | L) = H(Y | L) − H(Y | K) (11)

thus verifying the claim (7).
ACKNOWLEDGMENTS
The author was supported by the Heilbronn Institute for
Mathematical Research and by the Engineering and Physical Sciences Research Council, via the University of Bristol Bridging the Gaps cross-disciplinary feasibility account
(EP/H024786/1). The author thanks Oliver Johnson and Dino
Sejdinovic for helpful discussions.

where the second equality in (11) is standard identity and we
have used that L ∪ K = K.
We now unwrap the conditional entropy terms in (11) using
the chain rule for entropy (see for example [9, Theorem 2.5.1])
and standard identities and inequalities. This gives
T

H(Yt | Y1 , . . . , Yt−1 , L)

R EFERENCES

(12)

[1] R. Dorfman, “The detection of defective members of large populations,”
Ann. Math. Statist., vol. 14, no. 4 pp. 436–440, 1943.
[2] D.-Z. Du and F. K. Hwang, Combinatorial Group Testing and Its
Applications, 2nd edition, Series on Applied Mathematics, vol. 18,
World Scientiﬁc, 2000.
[3] G. Atia and V. Saligrama, “Boolean compressed sensing and noisy group
testing,” submitted to IEEE Trans. Inform. Theory, arXiv:0907.1061v4
[cs.IT], 2010.
[4] C. E. Shannon, “A mathematical theory of communication,” Bell Syst.
Tech. J., vol. 27, pp. 379-423 and 623–656, 1948.
[5] C. E. Shannon, “The zero error capacity of a noisy channel,” IRE Trans.
Inform. Theory, vol. 2, no. 3, pp. 8–19, 1956.
[6] D. Sejdinovic and O. Johnson, “Note on noisy group testing: asymptotic
bounds and belief propagation reconstruction,” Proc. Allerton Conf. on
Commun., Control and Computing, pp. 998–1003, 2010.
[7] M. Aldridge, “Interference mitigation in large random wireless networks,” PhD thesis, University of Bristol, UK, 2011.

− H(Yt | Y1 , . . . , Yt−1 , K)
T

H(Yt | Y1 , . . . , Yt−1 , L, XLt )

=
t=1

(13)

− H(Yt | Y1 , . . . , Yt−1 , K, XKt )
T

≤

H(Yt | XLt )
t=1

(14)

− H(Yt | Y1 , . . . , Yt−1 , K, XKt )
T

H(Yt | XLt ) − H(Yt | XKt ) ,

=

(18)

since, because we have chosen the Xi independently, we know
that XK\L and XL are independent.
Substituting (18) into (17) gives

A N INEQUALITY ABOUT MUTUAL INFORMATION
In this appendix we verify the claim (7), that
ˆ
I(K; K | L) ≤ T I(XK\L ; XL , Y ),

t=1

(17)

The mutual information term in (17) can alternatively be
written as

A PPENDIX .

ˆ
I(K; K | L) ≤

I(XK\L t ; Yt | XLt )
t=1

(15)

t=1

4

[8] R. Gallager, “A simple derivation of the coding theorem and applications,” IEEE Trans. Inform. Theory, vol. 11, no. 1, pp. 3–18, 1965.
[9] T. M. Cover and J. A. Thomas, Elements of Information Theory, second
edition, Wiley–Interscience, 2006.
[10] M. Cheraghchi, A. Karbasi, S. Mohajer and V. Saligrama, “Graphconstrained group testing,” IEEE Trans. Inform. Theory, vol. 58, no. 1,
pp. 248–262, 2012.
[11] C. L. Chan, S. Jaggi, V. Saligrama and S. Agnihotri, “Non-adaptive
group testing: explicit bounds and novel algorithms,” arXiv:1202.0206v1
[cs.IT], 2012.

[12] E. Porat and A. Rothschild, “Explicit nonadaptive combinatorial group
testing schemes,” IEEE Trans. Inform. Theory, vol. 57, no. 12, pp. 7982–
7989, 2011.
[13] M. Cheraghchi, A. Hormati, A. Karbasi and M. Vetterli, “Group testing
With probabilistic tests: theory, design and application,” IEEE Trans.
Inform. Theory, vol. 57, no. 10, pp. 7057–7067, 2011.
[14] M. Malyutov, “Recovery of sparse active inputs in general systems: a
review,” IEEE Region 8 SIBIRCON-2010, pp. 15–22, 2010.

5

