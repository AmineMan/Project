Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 10:47:48 2012
ModDate:        Tue Jun 19 12:56:40 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      530160 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569563411

Ampliﬁcation of the Hidden Gaussian
Channel States
Chao Tian
AT&T Labs-Research
Florham Park, NJ 07932, USA.
tian@research.att.com
Un

Abstract—We consider the problem of amplifying the channel
states in a state-dependent Gaussian channel, where the encoder
knows (non-causally) a noisy version of the channel states, i.e., the
channel states are hidden under the noise. We provide a complete
characterization of the minimum state reconstruction distortion
at the decoder under a power constraint at the encoder, and show
that a simple analog scheme with power control is optimal. More
precisely, if the power available to the encoder is below certain
threshold, the analog scheme using full power is optimal, however
when the power available to the encoder is above that threshold,
analog transmission using only a ﬁxed amount of the available
power is optimal. This is in contrast to the state ampliﬁcation
problem considered by Sutivong et al., when the channel states
are known perfectly at the encoder for which the full power is
always used in the optimal scheme. The converse proof of our
result relies on a channel decomposition argument which was
not necessary for the simpler case when the channel states are
known perfectly.

Vn
Encoder

Fig. 1.

Sn

Zn

+

+

+

Y n Decoder

ˆ
Sn

Ampliﬁcation of the hidden Gaussian channel states.

that threshold, though the analog scheme is still optimal, only
a ﬁxed amount of the available power should be used. The
optimality of the analog scheme implies that for this problem,
there is no loss when the noisy channel states are only known
non-causally symbol-wise instead of block-wise. Though the
achievability is relatively straightforward, the converse proof
is more involved. A channel decomposition is introduced to
provide the necessary bound, which did not appear necessary
for the simpler setting of [1].
The problem in consideration is applicable when the encoder is in fact a helper transmitter and the channel states
are the analog signal sent by the main transmitter. The
problem can also be understood as modeling the original state
ampliﬁcation system with certain less than perfect processing
components, and in this case, our result is reassuring in the
sense that as long as the amount of noise is not excessive, the
optimal analog transmission scheme using full power remains
optimal even with such noise contamination. The problem
is also closely related to the Gaussian CEO problem on the
Gaussian multiple access channel considered in [3], and we
shall return to this point later with more details.
The rest of the paper is organized as follows. The problem
deﬁnition is given in Section II, the main theorem is given
in Section III, and the proof of the main theorem is given in
Section IV. Some discussions of the result are given in Section
V, and Section VI concludes the paper.

I. I NTRODUCTION
The problem of channel state ampliﬁcation was ﬁrst considered by Sutivong et al. in [1], where the encoder, knowing
perfectly the channel states non-causally, wishes to minimize
the reconstruction distortion of the channel states at the
decoder. It was shown that for the Gaussian channel and under
the mean squared distortion measure, directly transmitting
the channel states analogously is optimal. Furthermore, the
problem was extended to simultaneously transmitting a message and amplifying the channel states, for which a complete
characterization of the optimal tradeoff between the message
rate and reconstruction distortion was also provided. Kim
et al. [2] later considered the general non-Gaussian setting,
and by measuring the state reconstruction accuracy using
mutual information instead of distortion, they were also able
to provide a complete characterization of this tradeoff.
In this work, we consider a variation of the Gaussian
channel state ampliﬁcation problem, where the encoder only
knows a noisy version of the Gaussian channel states, and
wishes to amplify these hidden states so that the receiver
can most accurately reconstruct the channel states under the
mean squared distortion measure; see Fig. 1. Our main result
is a complete characterization of the minimum achievable
distortion under an encoder power constraint. It is shown
that the analog scheme with power control is optimal. More
precisely, if the power available to the encoder is below certain
threshold, the analog scheme using full power is optimal,
however when the power available to the encoder is above

II. P ROBLEM D EFINITION
The channel in consideration is given by
Y = X + S + Z,
where X is the channel input, S
N(0, Q) is the Gaussian
channel state, and Z
N(0, N ) is the additive Gaussian
channel noise; S and Z are mutually independent. We use Si
to denote the channel state at time i, and write S1 , S2 , . . . , Sj

1

as S j ; similar notation is used for other random variables.
The encoder also has a noisy version of the channel state
non-causally (in a block-wise manner)

where
2
(N + γ 2 σu )Q
P + Q + N + 2γQ
2
σu N Q


2
2
N Q + N σu + Qσu





∗

D (P )

V = S + U,
2
N(0, σu )

where U
is Gaussian random noise independent
of (S, Z). The random vector (Si , Zi , Ui ), i = 1, 2, . . . , n, are
i.i.d. across time. Since S and V are jointly Gaussian, S can
be alternatively written as
S=

Q
V +W
2
Q + σu

λV + W,

IV. P ROOF OF T HEOREM 1

(1)

In this section, we prove Theorem 1. The achievability part
is rather simple, which is based on an analog scheme with
power control. The converse part is however less straightforward, which involves the separate treatments of two regimes,
and in one of the regimes, a novel channel decomposition
technique is introduced.

f : Rn → Rn ,

A. The Forward Part

which maps the vector V n into the channel input X n = f (V n )
satisfying an average channel power constraint

Proposition 1: The distortion power function D(P ) is upper bounded by D∗ (P ).
Proof: Consider an analog scheme where the encoder
simply sends the noisy version of the channel state Vi on the
channel after certain scaling, i.e., by letting Xi = γ ∗ Vi for
P∗
∗
i = 1, 2, . . . , n, where γ ∗
2
Q+σu for some P ∈ [0, P ].
The decoder then estimates Si single-letter-wise using the
channel output

n

E(Xi )2 ≤ P,
i=1

and a decoding function
g : Rn → R n ,
which maps the channel output Y n into a reconstruction of
ˆ
the channel state vector S n such that
1
n

otherwise

P
and γ
2
Q+σu .
As we shall show in the proof in the next section, the
optimal scheme is the pure analog scheme with power control.

where W is a zero-mean Gaussian random variable indepen2
dent of V with variance λσu .
Deﬁnition 1: An (n, d, P ) state-amplifying code consists
of an encoding function

1
n

2
−4
if P < N 2 (Q + σu )σu

Yi = Xi + Si + Zi = γ ∗ (Si + Ui ) + Si + Zi ,

n

for i = 1, 2, . . . , n. A linear least mean squared error calculation gives that the following distortion is achievable.

ˆ
E(S − S)2 = d.
i=1

D∗ (P )

Deﬁnition 2: A distortion-power pair (D, P ) is said to be
achievable if for any > 0 and sufﬁciently large n there exists
an (n, D + , P ) state-amplifying code. The collection of all
achievable (D, P ) pairs is called the achievable rate-power
region, denoted as DP. The distortion-power function is the
minimum of all distortion D such that (D, P ) ∈ DP.
It is worth noting that in the problem being considered here,
the receiver is interested in reconstructing the channel state S.
If the receiver were interested instead in reconstructing the
noise-contaminated channel state V , the problem would be
much simpler. To see this, observe that S = λV + W , and we
can alternatively view the channel as Y = X +λV +(Z +W ),
where now λV is the “real” channel state known perfectly at
the encoder and Z + W is the additive noise in the channel,
and the result in [1] would directly apply in this setting.

P

2
N + γ ∗2 σu
Q.
∈[0,P ] P ∗ + Q + N + 2γ ∗ Q

min
∗

(2)

This optimization problem can be solved explicitly, and the
optimizer is given by
P∗ =

P
2
−4
N 2 (Q + σu )σu

2
−4
if P < N 2 (Q + σu )σu
otherwise

substituting which and the corresponding γ ∗ into (2) gives the
desired result.
The optimizer P ∗ given above implies that when the
available power P is below the given threshold, the optimal
strategy is to use full power in the analog scheme; above that
threshold, the analog scheme is still optimal, however only a
ﬁxed amount in the total available power should be used.
B. The Converse Part
We ﬁrst provide a simple lower bound of D(P ), which is
tight for the high power regime.
Proposition 2: The distortion power function satisﬁes

III. M AIN R ESULT
The following theorem provides a complete characterization
of the distortion-power function D(P ).
Theorem 1: The distortion power function D(P ) is given
by

D(P ) ≥

2
σu N Q
.
2
2
N Q + N σu + Qσu

(3)

Proof: Consider the system depicted in Fig. 2, and denote
the minimum distortion obtained by the estimator in this

D(P ) = D∗ (P ),

2

Un

Zn
S

+
estimator

ˆ
S

+

Encoder
n

+ Vn

Fig. 3.

Sn

Zn

Zn

+

Vn
n

+

n
+ Y

Decoder

ˆ
Sn

The equivalent system used in Proposition 3.

Un
we can write
Fig. 2.

The system used in the proof of Proposition 2.

EYi2 = E(Xi + Si + Zi )2
= E(Xi + λVi + Wi + Zi )2
≤ Pi + Q + N + 2γi Q,

system as D. It is clear that D ≤ D(P ), because otherwise we
can always form the channel output Y in the original system
using the available information at the estimator in this new
system, and then use the original decoder g(Y ) to form a
better reconstruction. It is easily seen that

Pi
2
where Pi = EXi and γi =
2
Q+σu , and without loss
of generality, we have assumed EXi = 0; furthermore, the
following simple fact is used in the last step
2
Pi (Q + σu ),

EXi Vi ≤

2
σu N Q
,
D = E(S − E(S|U, S + Z))2 =
2
2
N Q + N σu + Qσu

(6)

with equality holds only when the correlation coefﬁcient
between Xi and Vi is 1. It follows that

which gives the desired result.

n

2
Pi + Q + N + 2 γ i Q
1
log
.
¯
2 i=1
N −N
√
The function log(x + α y + β) is concave in both x and y
when α ≥ 0, because it is a composite function of a concave
√
function x + α y + β and a concave and non-decreasing
function log(·) (see [4] for operations that preserve convexity),
√
and furthermore log(x + α y + β) is also monotonically nondecreasing, it follows by Jensen’s inequality that

¯
I(V n , S n + Z n ; Y n ) ≤

The next lower bound is more interesting, and before
introducing this bound, let us consider a decomposition of
the channel noise. Clearly, the channel noise Z can be written
as follows
¯ ¯
Z = Z + Z,
(4)

¯
¯
where Z and Z are independent zero-mean Gaussian random
¯
¯
¯
variables, with variances N and N = N − N , respectively.
This essentially provides an equivalent channel, as illustrated
n
P + Q + N + 2γQ
¯
I(V n , S n + Z n ; Y n ) ≤ log
,
(7)
in Fig. 3. The following proposition provides a set of lower
¯
2
N −N
bounds to the minimum achievable distortion.
n
n
since i=1 Pi ≤ nP , which also implies i=1 (γi )2 ≤ nγ 2 .
¯
Proposition 3: Let N ∈ [0, N ), then the distortion-power
¯ ˆ
The quantity I(V n , S n + Z n ; S n ) on the right hand side of
function satisﬁes
(5) can be lower bounded as follows
2
¯ 2
¯ ¯
(N − N )(N + σu )Q + (N + P + Q + 2γQ)N σu
D(P ) ≥
Q.
¯ ˆ
I(V n , S n + Z n ; S n )
¯
¯
(Q(N + σ 2 ) + N σ 2 )(N + P + Q + 2γQ)
u

u

n

Proof: For the system depicted in Fig. 3, let us write by
data processing inequality that
¯
¯ ˆ
I(V n , S n + Z n ; Y n ) ≥ I(V n , S n + Z n ; S n ).

¯
¯ ˆ
h(Vi , Si + Zi ) − h(V n , S n + Z n |S n )

=
=

i=1
n

(5)

¯
h(Vi , Si + Zi )
i=1
n

It is easy to see that
¯
¯
I(V , S + Z n ; Y n ) = h(Y n ) − h(Z n )
n

n

i=1
n

n

n
¯
≤
h(Yi ) − log 2πe(N − N )
2
i=1
≤

1
2

n

log 2πeEYi2 −
i=1

¯ ˆ
¯
h(Vi , Si + Zi |S n , V i−1 , S i−1 + Z i−1 )

−

i=1
n

n
¯
log 2πe(N − N ),
2

n

¯
h(Vi , Si + Zi ) −

≥

¯ ˆ
h(Vi , Si + Zi |Si )
i=1

¯ ˆ
I(Vi , Si + Zi ; Si )

=
i=1

¯
Note that Si ↔ (Vi , Si + Zi ) ↔ Y n is a Markov string,
¯
ˆ
which implies that Si ↔ (Vi , Si + Zi ) ↔ Si is also a Markov
n
ˆ
string. By the distortion constraint i=1 E(Si − Si )2 ≤ nD,

where the last inequality is because the Gaussian distribution
maximizes the differential entropy with the same variance. By
the fact that (Xi , Vi ), Wi , and Zi are mutually independent,

3

D(P) function

and the convexity of the mutual information in the conditional
probability distribution, we can write using Jensen’s inequality
that
¯ ˆ
I(V n , S n + Z n ; S n ) ≥

min

¯
ˆ
ˆ S↔(V,S+Z)↔S
S:
ˆ
E(S−S)2 ≤D

3.4
3.2

¯ ˆ
I(V, S + Z; S). (8)

3
2.8

Rr (D) =

min

¯
ˆ
ˆ S↔(V,S+Z)↔S
S:
ˆ
E(S−S)2 ≤D

Distortion

The right hand side of (8) is in fact the remote source
coding rate distortion function Rr (D) (see e.g., [5] or [6])
¯
for source S with noisy observations (V, S + Z) under the
mean squared error distortion measure. For the Gaussian case
it has an explicit solution (see. e.g., [7] or [8]), which for our
case gives

2
1.8
1.6
1.4
−20

2

for
D>

20

40
Power in dB

60

80

2
Fig. 4. A typical D(P ) function where Q = 10, N = 5 and σu = 2. The
circle gives the power threshold in Theorem 1.

Combining (5), (7), (8) and (9) gives the desired result after
some simple algebra, and the proof is complete.
The minimization problem in the remote rate distortion
function Rr (D) has a simple and intuitive solution, which can
¯
be described as follows (see. e.g., [8]). Since (S, V, S + Z) are
jointly Gaussian, we can write

outer bounding steps in the proof of Proposition 3 using the
achievable scheme input Xi = γVi , in a single-letter manner.
Clearly in this case
¯ ˆ
¯
I(V, S + Z; S) = I(V, S + Z; Y )
¯
= I(V, S + Z; γVi + S + Z)
1
P + Q + N + 2γQ
log
.
¯
2
N −N
¯
Thus as long as there exists Z such that

¯
¯
E[S|V, S + Z] = aV + b(S + Z),

b=

0

(9)

¯ 2
QN σu
2
¯
¯ 2.
Q(N + σu ) + N σu

where a and b are given as
¯
NQ
a= ¯
2 )Q + N σ 2
¯ u,
( N + σu

2.4
2.2

¯ ˆ
I(V, S + Z; S)

2
¯
Q ( N + σu )
1
= log
2 ) + N σ 2 )D − QN σ 2
¯
¯ u
¯ u,
2
(Q(N + σu

2.6

=

2
σu Q
2
¯
¯ 2.
(N + σu )Q + N σu

¯ ˆ
I(V, S + Z; S) = Rr (D),

(10)

where

ˆ
The optimal S can then be chosen as

D = E(S − E(S|γV + S + Z))2 ,

ˆ
¯
S = aV + b(S + Z) + Ws ,

then the inner bound and the outer bound match. Recall the
ˆ
optimal S in Rr (D) discussed in the previous subsection, and
¯
it is clear that if γV + (S + Z) is statistically equivalent to
¯ then (10) indeed holds. This requirement then
aV + b(S + Z),
reduces to
a
2
¯
¯
= γ ⇒ N = N ∗ γσu .
(11)
b
¯
¯
Thus by choosing N = N ∗ , the inner bound and the outer
bound match, which can always be done since in the low
power regime it is always true that

where Ws is a zero-mean Gaussian random variable indepen¯
dent of (V, S, Z) with a variance determined by
¯
D = E(S − E(S|aV + b(S + Z) + Ws ))2 .
The lower bound in Proposition 2 can also be obtained using
a limiting argument from the lower bound given in Proposition
3. However we gave Proposition 2 separately since it is rather
straightforward and directly provides insight into the problem.
C. The Matching Condition
It is clear that in the high power regime, the inner bound
and the outer bound indeed match, and it remains to show
that this is also the case in the low power regime. We could
¯
optimize over N ∈ [0, N ) using Proposition 3 to derive the
strongest lower bound on D(P ), and compare it with the
distortion achieved using the analog scheme, which however
offers little insight. Instead, an indirect approach is taken next
by analyzing the conditions in the low power regime.
Proof of Theorem 1: Let us begin by repeating the

2
¯
N ∗ = γσu =

P
σ 2 < N.
2
Q + σu u

The proof is thus complete.
Readers less comfortable with the indirect proof approach
¯
¯
given above can substitute N = N ∗ into the expression in
Proposition 3, and it is straightforward to show that it indeed
matches the upper bound given in Theorem 1. A typical D(P )
function is given in Fig. 4 for the parameters Q = 10, N = 5

4

U1n

+

amount of noise contamination, and all sensor encoders have
the same amount of power to use. However this symmetry requirement is in fact unnecessarily restrictive. Using an analysis
similar to that used in this work, it can be shown that the un4 2
2
coded scheme is optimal as long as Pi σi (σs + σi )−1 remains
2
2
constant across all the sensor encoders, where σs , Pi , σi are
the variance of the hidden Gaussian source S, the transmission
power constraint and the variance of the sensor noise Ui
at sensor i, respectively (see Fig. 5). We leave this simple
calculation to interested readers.

Encoder 1

n
U2

+

Encoder 2

Zn
Sn

M M

M

M

+

Decoder

ˆ
Sn

U kn

+

Encoder k

VI. C ONCLUSION
Fig. 5.

The sensor network problem considered in [3].

We considered the problem of amplifying the hidden Gaussian channel states, and provided a complete solution for the
minimum mean squared error distortion achievable under an
encoder power constraint. The optimal scheme in this problem
is analog transmission of the noisy channel states with power
control. This is different from the case when the channel states
are known perfectly at the encoder, for which full power is
always used in the optimal scheme. The optimality of the
analog scheme implies that for this problem, there is no
loss when the noisy channel states are known non-causally
symbol-wise instead of block-wise. Though the inner bound
is straightforward, the outer bound is derived using a novel
channel decomposition technique which was not required in
the setting when the channel states are known without noise
contamination.
Possible future work includes extension to the problem
of simultaneously transmitting a message and amplifying the
hidden Gaussian channel states, as well as to the non-Gaussian
settings.

2
and σu = 2; note that the horizontal axis is in log scale, i.e.,
in dB.

V. D ISCUSSIONS
In this section, we give a few additional remarks on the
converse proof and the connection to other problems.
A. Capacity-Limited Regime vs. Information-Limited Regime
In the low power regime, the converse proof essentially
relies on ﬁnding a good cut-set choice between the information
sources having information regarding the channel states S
and the decoder, as illustrated in Fig. 3. The lower bound of
D(P ) given in Proposition 3 is then derived by analyzing the
information transfer between the two cut-sets. As the available
power at the encoder increases, the cut moves closer to the
decoder. This increases the amount of information transfer,
¯
i.e., the value of I(S + Z, V ; Y ), between the two cut-sets, but
at the same time, the information that can be directly derived
about the channel state S from the two information sources
¯
S + Z and V is in fact decreasing. This trend continues until
the available power reaches the given threshold and it moves
into the high power regime.
In the high power regime, the cut is chosen in such a way
that the information transfer between the two cut-sets is no
longer constrained (i.e., the case in Fig. 2), however in this
case, the overall system is limited by the information that can
be derived directly from the information sources S + Z and
V for the channel states S, and thus an encoder-decoder pair
does not appear in Fig. 2, but only an estimator exists.
Perhaps not surprisingly, the low power regime can thus be
understood as channel capacity-limited, and the high power
regime as channel-state-information-limited.

ACKNOWLEDGMENT
The author wishes to thank Shlomo Shamai and Bernd
Bandemer for reading an early draft and providing feedbacks.
R EFERENCES
[1] A. Sutivong, M. Chiang, T. M. Cover, and Y.-H. Kim, “Channel capacity
and state estimation for state-dependent Gaussian channels,” IEEE
Trans. Information Theory, vol. 51, no. 4, pp. 1486–1459, Apr. 2005.
[2] Y.-H. Kim, A. Sutivong, and T. M. Cover, “State ampliﬁcation,” IEEE
Trans. Information Theory, vol. 54, no. 5, pp. 1850–1859, May 2008.
[3] M. Gastpar, “Uncoded transmission is exactly optimal for a simple
Gaussian sensor network,” IEEE Trans. Information Theory, vol. 54,
no. 11, pp. 5247-5251, Nov. 2008.
[4] S. Boyd and L. Vandenberghe, Convex optimization, Cambridge
University Press, 2004.
[5] T. Berger, Rate distortion theory: mathematical basis for data compression, Prentice Hall, 1971.
[6] H. Yamamoto and K. Itoh, “Source coding theory for multi-terminal
communication systems with a remote source,” Trans. of the IECE of
Japan, vol. E63, no. 10, pp. 700-706, 1980.
[7] M. Gastpar, “A lower bound to the AWGN remote rate-distortion
function,” Proc. IEEE Statist. Signal Process Workshop, pp. 1176-1181,
Bordeaux, France, Jul. 2005.
[8] C. Tian and J. Chen, “Remote vector Gaussian source coding with
decoder side information under mutual information and distortion constraints,” IEEE Trans. Information Theory, vol. 55, no. 10, pp. 46764680, Oct. 2009.

B. Connection to the Gaussian CEO Problem on MAC
In the converse proof, the remote source rate-distortion
function plays an instrumental role. The channel output in the
optimal scheme is essentially chosen to match the optimal
remote source coding solution in a statistical manner, and this
observation can be useful in other problems.
Particularly, it was shown in [3] that the analog scheme
using full power is optimal for the complete symmetric case,
where the sensors observe the true source S under the same

5

