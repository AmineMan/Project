Title:          scdelay_isit_6.pdf
Author:         Behzadmina
Creator:         TeX output 2012.05.09:1233
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May  9 17:13:24 2012
ModDate:        Tue Jun 19 12:54:07 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      460313 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569553537

Source Coding With Delayed Side Information
Osvaldo Simeone

Haim H. Permuter

CWCSPR, ECE Dept.
New Jersey Institute of Technology
Newark, NJ, 07102, USA
Email: osvaldo.simeone@njit.edu

ECE Dept.
Ben-Gurion University of the Negev
Beer-Sheva 84105, Israel
Email: haimp@bgu.ac.il

Abstract—For memoryless sources, delayed side information
at the decoder does not improve the rate-distortion function.
However, this is not the case for more general sources with
memory, as demonstrated by a number of works focusing on
the special case of (delayed) feedforward. In this paper, a
setting is studied in which the side information is delayed and
the encoder is informed about the side information sequence.
Assuming a hidden Markov model for the sources, at ﬁrst, a
single-letter characterization is given for the set-up where the
side information delay is arbitrary and known at the encoder,
and the reconstruction at the destination is required to be (near)
lossless. Then, with delay equal to zero or one source symbol,
a single-letter characterization is given of the rate-distortion
function for the case where side information may be delayed
or not, unbeknownst to the encoder. Finally, an example for a
binary source is provided.

Figure 1. Lossy source coding with delayed side information at the decoder.
The side information is fully available at the encoder.

Prior work: If sequences X n and Y n are memoryless, from
available results [2][3], it can be inferred that: (i) for zero
delay, i.e., d = 0, the performance of the systems in Fig. 1-2
would remain unchanged even if the decoder(s) had access to
non-causal side information, in which case the decision about
Zji , j = 1, 2, at each time i, could be based on the entire
sequence Y n , rather than only Y i ; and (ii) for strictly positive
delay d > 0, delayed side information does not improve
performance. However, these conclusions do not generally
hold if the sources have memory.
For sources with memory, a number of works have focused
on the scenario of Fig. 1 where Xi = Yi , which entails
that the decoder observes sequence X n itself with a delay
of d symbols. This setting is typically referred to as source
coding with feedforward, as introduced in [5]. Reference [1]
derives the rate-distortion function for this problem (i.e., Fig.
1 with Xi = Yi ) for ergodic and stationary sources in terms
of multi-letter mutual informations2 . This function is explicitly
evaluated for some special cases in [2][4] (see also [6]), while
an algorithm for its numerical calculation is also proposed in
[4]. The more general case of Fig. 1 with Xi = Yi is studied in
[7] assuming stationary and ergodic sources X n and Y n . The
rate-distortion function is expressed in terms of multi-letter
mutual informations, and no speciﬁc examples are provided
for which the function is explicitly computable. Moreover,
extensions of the characterization of achievable rate-distortion
trade-offs to the setting of Fig. 2 for sources with memory has
not, to the best of the authors’ knowledge, been studied. We
ﬁnally remark that for more complex networks than the ones
studied here, strictly delayed side information may be useful

I. I NTRODUCTION
Consider a sensor network in which a sensor measures a
certain physical quantity Yi over time i = 1, 2, ...n. The aim
of the sensor is communicating a processed version X n =
(X1 , ..., Xn ) of the measured sequence Y n = (Y1 , ..., Yn )
to a receiver. As an example, each element Xi could be
obtained by quantizing Yi , for i = 1, 2, ...n. To this end,
the sensor communicates a message M of nR bits to the
receiver, based on the observation of X n and Y n (R is
the message rate in bits per source symbol). The receiver is
endowed with sensing capabilities and hence it can measure
the physical quantity Y n as well. However, due to the fact
that the receiver is located further away from the physical
source, such measure may come with a delay of d symbols.
In other words, when estimating Xi , the receiver has available
not only the message M received from the sensor, but also
the sequence Y i−d = (Y1 , ..., Y i−d ), so that the estimate Zi
is a function of M and Y i−d . Delay d may or may not be
known at the sensor1 . The situation described above can be
illustrated schematically as in Fig. 1 and in Fig. 2, where Fig.
1 models the case where the delay d is known to the sensor
(i.e., the encoder), while 2 accounts for a setting where the
side information at the decoder, unbeknownst to the encoder,
may be delayed by d or not delayed.
1 In order to ensure that the sensor can produce the message M based on
the entire sequences X n and Y n , as in the example at hand, the delay at
the receiver corresponding to the given decoding rule (Xi (M, Y i−d )) can be
seen to be more precisely n + d and not d (see, e.g., [1]). Nevertheless, we
will refer to the delay at the receiver as d for simplicity.

2 Extensions are also given for arbitrary sources using information-spectrum
methods.

1

jointly distributed with Y n as
n

p(xn , y n ) = π(y1 )q(x1 |y1 )

p(xi , yi |xi−1 , y i−1 )
i=2
n

= π(y1 )q(x1 |y1 )

w1 (yi |yi−1 )q(xi |yi ).

(1)

i=2

In other words, process Xi ∈ X , i ∈ {..., −1, 0, 1, ...} corresponds to a hidden Markov model with underlying Markov
process given by Y n .
An (d, n, R, D1 , D2 ) code, with delay d ≥ 0, is deﬁned by:
(i) An encoder function

Figure 2. Lossy source coding where side information at the decoder may
be delayed. The side information is fully available at the encoder.

f: (X n × Y n ) → [1, 2nR ],

(2)

which maps sequences X n and Y n into message M ∈
[1, 2nR ]; (ii) a sequence of decoding functions for decoder
1
g1i : [1, 2nR ] × Y i−d → Z1 ,
(3)

also in the presence of memoryless sources. This is illustrated
in [9] for a multiple description problem with feedforward.
Contributions: In this work, we assume that the source Y n
is a Markov chain, and X n is such that Xi is obtained by passing Yi through a memoryless channel q(x|y) for i = 1, ..., n,
i.e., X n corresponds to a hidden Markov model. Note that the
latter may model a symbol-by-symbol processing of source Y n
as per the initial example. We derive a single-letter characterization of the minimal rate (bits/source symbol) required for
(near) lossless compression in the scenario of Fig. 1 for any delay d ≥ 0 (Sec. III). Achievability is based on a novel scheme
that consists of simple multiplexing/demultiplexing operations
along with standard entropy coding techniques. Furthermore,
we derive a single-letter characterization of the minimal rate
(bits/source symbol) required for lossy compression in the
scenarios of Fig. 1 and Fig. 2 for delays d = 0 and d = 1
(Sec. IV). Finally, we study the speciﬁc example of a binaryalphabet source with Hamming distortion (Sec. V).
Notation: For a, b integer with a ≥ b, we deﬁne [a, b] as the
interval [a, a + 1, ..., b] and xb = (xa , ..., xb ); if instead a < b
a
we set [a, b] = ∅ and xb = ∅. We will also write xb for xb
a
1
for simplicity of notation. Given a sequence xn = [x1 , ..., xn ]
and a set I = {i1 , ..., i|I| } ⊆ [1, n], we deﬁne sequence xI
as xI = [xi1 , xi2 , ..., xi|I| ] where i1 ≤ ... ≤ i|I| .

for i ∈ [1, n], which, at each time i, map message M, or
rate R [bits/source symbol], and the delayed side information
Y i−d into the estimate Z1i ∈ Z1 ; (iii) a sequence of decoding
function for decoder 2
g2i : [1, 2nR ] × Y i → Z2

(4)

for i ∈ [1, n], which, at each time i, map messages M and the
non-delayed side information Y i into the estimate Z2i ∈ Z2 .
Encoding/decoding functions (2)-(4) must satisfy the distortion
constraints
1
n

n

E[dj (Xi , Yi , Zji )] ≤ Dj , for j = 1, 2,

(5)

i=1

where the distortion metrics dj (x, y, zj ): X × Y × Zj →
[0, dmax ] are such that 0 ≤ dj (x, y, zj ) ≤ dmax < ∞ for
all (x, y, z) ∈ X × Y × Zj for j = 1, 2. Note that these
constraints are fairly general in that they allow to impose
not only requirements on the lossy reconstruction of Xi or
Yi (obtained by setting dj (x, y, zj ) independent of y or x,
respectively), but also on some function of both Xi and Yi
(by setting dj (x, y, zj ) to be dependent on such function of
(x, y)).
Given a delay d ≥ 0, for a distortion pair (D1 , D2 ), we say
that rate R is achievable if, for every > 0 and sufﬁciently
large n, there exists a (d, n, R, D1 + , D2 + ) code. We refer
to the inﬁmum of all achievable rates for a given distortion
pair (D1 , D2 ) and delay d as the rate-distortion function
Rd (D1 , D2 ). For the setting of Fig. 1, we similarly deﬁne
the rate-distortion function Rd (D1 ).

II. S YSTEM M ODEL
We present the system model for the scenario of Fig. 2,
as the scenarios of Fig. 1 follows as a special case. The
random process Yi ∈ Y, i ∈ {..., −1, 0, 1, ...}, measured at
the encoder, and, possibly with delay, at the decoders, is a
stationary and ergodic Markov chain with transition probability Pr[Yi = a|Yi−1 = b] = w1 (a|b). We deﬁne the probability
Pr[Yi = a] π(a) and also the k-step transition probability
Pr[Yi = ai |Yi−k = b]
wk (a|b), which are both independent of i by stationarity of Yi . We also set, for notational
convenience, w0 (a|b) = π(a). Sequence Y n = (Y1 , ..., Yn )
n
is thus distributed as p(y n ) = π(y1 ) i=2 w1 (yi |y i−1 ) for
any integer n > 0. The random process Xi ∈ X , i ∈
{..., −1, 0, 1, ...}, measured only at the encoder, is such that
vector X n = (X1 , ..., Xn ) ∈ X n , for any integer n > 0, is

III. L OSSLESS S OURCE C ODING WITH D ELAYED S IDE
I NFORMATION
Here we consider the setting of Fig. 1 and we characterize
the rate-distortion function Rd (D1 ) for any delay d ≥ 0 under
the Hamming distortion metric (i.e., d1 (x, y, z1 ) = 1(x =
z1 ), where 1(a) = 1 if a is true and 1(a) = 0 otherwise)
for D1 = 0. In other words, we impose that the sequence

2

X n be recovered with vanishingly small average symbol error
probability as n → ∞ at the decoder. We refer to this scenario
as (near) lossless. We have the following characterization of
Rd (0).

Enc
Xi

Proposition 1. For any delay d ≥ 0, the rate-distortion
function for the set-up in Fig. 1 under Hamming distortion
is given at D1 = 0 by
d
Rd (0) = H(Xd+1 |X2 , Y1 ),

Enc

Demux

Enc

(a)

(6)

Dec
M

(7)
(8)

d+1

·

M

Mux

Enc

X ii−d +1 , Yi − d

where the conditional entropy is calculated with respect to the
distribution
p(y1 , x1 ) = π(y1 )q(x1 |y1 ) for d = 0,
and p(y1 , x2 , ..., xd+1 ) = π(y1 )

.
.
.

Demux

Dec
Dec
Dec

w1 (yi |yi−1 )q(xi |yi ),

.
.
.

Mux

ˆ
Xi

ˆ
X ii−d +1 , Yi −d

yi ∈Y i=2
i∈[2,d+1]

(b)

for d ≥ 1.
The proof of achievability is sketched below. Details can be
found in [10], along with the proof of the converse.

Figure 3. A block diagram for encoder (a) and decoder (b) used in the proof
of achievability of Proposition 1.

Remark 2. Proposition 1 provides a “single-letter” characterization of Rd (0) for the setting of Fig. 1, since it only involves
a ﬁnite number of variables. This contrasts with the general
characterization for stationary ergodic processes of Rd (D)
(in the general lossy case D ≥ 0) given in [7], which is
a “multi-letter” expression, whose computation can generally
only attempted numerically using approaches such as the ones
proposed in [4]. Note that a multi-letter expression is also
given in [2] to characterize Rd (D) for negative delays d < 0.

described as follows. A block diagram is shown in Fig. 3
for encoder (Fig. 3-(a)) and decoder (Fig. 3-(b)). We ﬁrst
describe the encoder, which is illustrated in Fig. 3-(a). To
encode sequences (xn , y n ) ∈ (X n ×Y n ), we ﬁrst partition the
interval [1, n] into |X |d−1 |Y| subintervals, which we denote as
I(˜d−1 , y ) ⊆ [1, n], for all xd−1 ∈ X d−1 and y ∈ Y. Every
x
˜
˜
˜
such subinterval I(˜d−1 , y ) is deﬁned as
x
˜
˜
˜ i−d+1 ˜
I(˜d−1 , y ) = {i: i ∈ [1, n] and yi−d = y , xi−1 = xd−1 }.
x
(9)
In words, the subinterval I(˜d−1 , y ) contains all symbol
x
˜
indices i such that the corresponding delayed side information
available at the decoder is yi−d = y and the previous d−1 sam˜
ples in xn are xi−1 = xd−1 . For the out-of-range indices
˜
i−d+1
i ∈ [−d + 1, 0], one can assume arbitrary values for xi ∈ X
and yi ∈ Y, which are also shared with the decoder once and
for all. Note that xd−1 ∈X d−1 , y∈Y I(˜d−1 , y ) = [1, n]. Fig.
x
˜
˜
˜
4 illustrates the deﬁnitions at hand for d = 2.
As a result of the partition described above, the encoder “demultiplexes” sequence xn into |X |d−1 |Y| sequences
xd−1 y
x
˜
xI(˜ ,˜) , one for each tuple (˜d−1 , y ) ∈ X d−1 ×Y. This
demultiplexing operation, which is controlled by the previous
values of source and side information, is performed in Fig.
3-(a) by the block labelled as “Demux”, and an example of
its operation is shown in Fig. 4. By the ergodicity of process
Xi and Yi , for every > 0 and all sufﬁciently large n, the
xd−1 y
length of any sequence xI(˜ ,˜1 ) is guaranteed to be less
d−1
than npY1 X2 ,...,Xd (˜, x ) + symbols with abitrarily large
y ˜
probability.
The entropy encoder can be implemented in different
ways, e.g., using typicality or Huffman coding. Here we
consider a typicality-based encoder. Note that the entries
xd−1 y
Xi of each sequence X I(˜ ,˜) are i.i.d. with distribution

Remark 3. By setting d = 0 in (6) we obtain R0 (0) =
H(X1 |Y1 ). This result generalizes [2, Remark 3, p. 5227]
from i.i.d. sources (X n , Y n ) to the hidden Markov model
(1) considered here. Note that, for d = 1, we instead obtain
R1 (0) = H(X2 |Y1 ). As another notable special case, if side
information is absent, or equivalently if d → ∞, in accordance
to well-known results, we obtain that R∞ (0) equals the
entropy rate H(X ).
Remark 4. Is delayed side information useful (when known
also at the encoder)? That this is generally the case follows
d
from the inequality Rd (0) = H(Xd+1 |X2 , Y1 ) ≤ R∞ (0) =
H(X ), since R∞ (0) is the required rate without side information. However, the inequality above may not be strict, and
thus side information may not be useful. This is the case
for instance if Xi is an i.i.d. process or in the setting of
source coding with feedforward [5], [1], i.e., Xi = Yi , with
a Markov source X n . We will see below that the conclusion
that feedforward is not useful for Markov sources need not
hold for lossy compression (i.e., for D1 > 0).
A. Proof of Achievability for Proposition 1
Proof: (Achievability) Here we propose a coding scheme
that achieves rate (6). The basic idea is a non-trivial extension
of the approach discussed in [2, Remark 3, p. 5227] and is

3

with mutual informations evaluated with respect to the joint
distribution
p(x, y, yd , z1 , z2 ) = π(yd )wd (y|yd )q(x|y)p(z1 , z2 |x, y, yd ),
(12)
and where minimization is done over all conditional distributions p(z1 , z2 |x, y, yd ) such that
E[dj (X, Y, Zj )] ≤ Dj , for j = 1, 2.
Figure 4.
An example that illustrates the operations of the “Demux”
block of the encoder used for the achievability proof of Proposition 1,
as shown in Fig. 3, for sequences xn = (0, 0, 1, 0, 1, 0, 1, 0, 1, 1) and
y n = (0, 1, 1, 0, 1, 1, 0, 0, 1, 1), n = 10 and d = 2 (symbols corresponding
to out-of-range indices are set to zero).

(13)

Moreover, rate (10)-(11) is the rate-distortion function, i.e.,
(a)
Rd (D1 , D2 ) = Rd (D1 , D2 ), for d = 0 and d = 1.
Remark 6. Rate (10) can be easily interpreted in terms of
achievability. To this end, we remark that variable Yd plays
the role of the delayed side information Y i−d at decoder
1. The coding scheme achieving rate (10) operates in two
successive phases. In the ﬁrst phase, the encoder encodes the
n
reconstruction sequence Z1 for decoder 1. Since decoder 1 has
available delayed side information, using a strategy similar
to the one discussed in Sec. III-A, this operation requires
I(XY ; Z1 |Yd ) bits per source sample. Note that decoder 2
n
is able to recover Z1 as well, since decoder 2 has available
i
side information Y , and thus also the delayed side information
n
Y i−d . In the second phase, the reconstruction sequence Z2 for
decoder 2 is encoded. Given the side information available at
decoder 2, this operation requires rate I(X; Z2 |Y Yd Z1 ), using
again an approach similar to the one discussed in Sec. III-A.
Details can be found in [10], along with the converse proof.

pXd+1 |Y1 X2 ,...,Xd (·|˜, xd−1 ), since conditioning on the event
y ˜
˜ i−d+1 ˜
{yi−d = y , xi−1 = xd−1 } makes the random variables Xi
independent. Therefore, a rate in bits per source symbol of
d
H(Xd+1 |X2 = xd−1 , Y1 = y ) + is sufﬁcient for the entropy
˜
˜
encoder to label all -typical sequences.
We now describe the decoder, which is illustrated in Fig.
3-(b). By undoing the multiplexing operation just described,
the decoder, from the message M , can recover the individual
xd−1 y
sequences xI(˜ ,˜) through a simple demultiplexing operad−1
tion for all x
˜
∈ X d−1 and xd−1 ∈ X d−1 . This operation is
˜
represented by block “Demux” in Fig. 3-(b). However, while
xd−1 y
the individual sequences xI(˜ ,˜) can be recovered through
the discussed demultiplexing operation, this does not imply
that the decoder is also able to reorder the symbols in the
sequences so as to obtain the original sequence xn . However,
note that at time i, the decoder knows Yi−d and the previously
decoded X i−1 and can thus identify the subinterval I(˜d−1 , y )
x
˜
to which the current symbol Xi belongs. This symbol can be
then immediately read as the next yet-to-be-read symbol from
xd−1 y
the corresponding sequence xI(˜ ,˜) . Note that for the ﬁrst
d symbols, the decoder uses the values for xi and yi at the outof-range indices i that were agreed upon with the encoder (see
above). A more detailed description, including the analysis of
the impact of errors, can be found in [10].

Remark 7. For memoryless sources X n and Y n , by comparison with the results in [3], it can be concluded that delayed
side information is not useful for memoryless sources. This
conclusion generalizes the result of [2], which applies for
the setting of Fig. 1 in the special case of feedforward (i.e.,
Xi = Yi ).
(a)

Note that, by setting D2 = dmax in Rd (D1 , D2 ), we
(a)
obtain an achievable rate Rd (D1 ) for the setting of Fig. 1
(see [10] for details).
V. E XAMPLE : B INARY H IDDEN M ARKOV M ODEL

IV. L OSSY S OURCE C ODING W HERE S IDE I NFORMATION
M AY B E D ELAYED

In this section, we assume that Yi is a binary Markov chain
with symmetric transition probabilities w1 (1|0) = w1 (0|1)
ε and we assume that Xi = Yi ⊕ Ni , with “⊕” being the
modulo-2 sum and Ni being i.i.d. binary variables, independent of Y n , with pNi (1)
q, q ≤ 1/2. Note that we have
the k-step transition probabilities wk (1|0) = wk (0|1) ε(k) ,
which can be obtained recursively as ε(1) = ε and ε(k) =
2ε(k−1) (1 − ε(k−1) ) for k ≥ 2. We adopt the Hamming
distortion d1 (x, z1 ) = x ⊕ z1 .
We start by showing in Fig. 5 the rate Rd (0) obtained from
Proposition 1 corresponding to zero distortion (D1 = 0) versus
the delay d for different values of ε and for q = 0.1. For d = 0,
we have R0 (0) = H(X1 |Y1 ) = Hb (q) = 0.589, irrespective
of the value of ε, where we have deﬁned the binary entropy
function Hb (a) = −a log2 a − (1 − a) log2 (1 − a). Instead, for
d increasingly large, the rate Rd (0) tends to the entropy rate

In this section, we consider the general problem of lossy
compression for the set-up of Fig. 2, and we obtain an
(a)
achievable rate Rd (D1 , D2 ) ≥ Rd (D1 , D2 ) for all delays
d ≥ 0 and prove that such rate equals the rate-distortion
(a)
function, i.e., Rd (D1 , D2 ) = Rd (D1 , D2 ), for d = 0 and
d = 1.
Proposition 5. For any delay d ≥ 0 and distortion pair
(D1 , D2 ), the following rate is achievable for the setting of
Fig. 2
(a)

Rd (D1 , D2 ) = min I(XY ; Z1 |Yd ) + I(X; Z2 |Y Yd Z1 )
(10)
= min I(Y ; Z1 |Yd ) + I(X; Z1 Z2 |Y Yd ), (11)

4

Figure 6. Minimum required rate Rd (0) for lossless reconstruction for the
set-up of Fig. 1 with binary sources versus parameter q (ε = 0.1).

Figure 5. Minimum required rate Rd (0) for lossless reconstruction for the
set-up of Fig. 1 with binary sources versus delay d (q = 0.1).

according to hidden Markov models, and derived singleletter characterizations of the rate-distortion trade-off. Such
characterizations are established based on simple achievable
scheme that are based on standard “off-the-shelf” compression
techniques. Moreover, we have extended the analysis to a more
general set-up in which side information may or may not be
delayed.

R∞ (0) = H(X ). Note that a larger memory, i.e., a smaller ε,
leads to smaller required rate Rd (0) for all values of d.
Fig. 6 shows the rate Rd (0) for ε = 0.1 versus q for different
values of delay d. For reference, we also show the performance
with no side information, i.e., R∞ (0) = H(X ). For q = 1/2,
the source X n is i.i.d. and delayed side information is useless
in the sense that Rd (0) = R∞ (0) = H(X1 ) = 1 (Remark 4).
Moreover, for q = 0, we have Xi = Yi , so that Xi is a Markov
chain and the problem becomes one of lossless source coding
with feedforward. From Remark 4, we know that delayed side
information is useless also in this case, as Rd (0) = R∞ (0) =
H(X ) = Hb (ε) = 0.469.3 For intermediate values of q, side
information is generally useful, unless the delay d is too large.
Finally, we evaluate the achievable rate of Proposition 2 for
a general non-zero distortion D1 (see details in [10]), obtaining
(a)
Rd (D1 )

= Hb (ε(d) ∗ q) − Hb (D1 )

VII. ACKNOWLEDGEMENT
The work of O. Simeone was supported in part by the
U.S. National Science Foundation under Grant No. 0914899.
H. H. Permuter was supported in part by the Marie Curie
Reintegration fellowship.
R EFERENCES
[1] R. Venkataramanan and S. S. Pradhan, “Source coding with feedforward: Rate-distortion theorems and error exponents for a general
source,” IEEE Trans. Inform. Theory, vol. 53, no. 6, pp. 2154-2179,
Jun. 2007.
[2] T. Weissman and A. El Gamal, “Source coding with limited-look-ahead
side information at the decoder,” IEEE Trans. Inform. Theory, vol. 52,
no. 12, pp. 5218-5239, Dec. 2006.
[3] A. H. Kaspi, “Rate-distortion function when side-information may be
present at the decoder,” IEEE Trans. Inform. Theory, vol. 40, no. 6, pp.
2031-2034, Nov 1994.
[4] I. Naiss and H. Permuter, “Computable bounds for rate distortion with
feed-forward for stationary and ergodic sources,” arXiv:1106.0895v1.
[5] T. Weissman and N. Merhav, “On competitive prediction and its relation
to rate-distortion theory,” IEEE Trans. Inform. Theory, vol. 49, no. 12,
pp. 3185- 3194, Dec. 2003.
[6] R. Venkataramanan and S. S. Pradhan, “On computing the feedback
capacity of channels and the feed-forward rate-distortion function of
sources,” IEEE Trans. Commun., vol. 58, no. 7, pp. 1889–1896, July
2010.
[7] R. Venkataramanan and S. S. Pradhan, “Directed information for communication problems with side-information and feedback/feed-forward,”
in Proc. of the 43rd Annual Allerton Conference, Monticello, IL, 2005.
[8] R. Gray, “Information rates of autoregressive processes,” IEEE Trans.
Inform. Theory, vol. 16, no. 4, pp. 412- 421, Jul. 1970.
[9] R. Venkataramanan and S. S. Pradhan, “Achievable rates for multiple descriptions with feed-forward," Information Theory, IEEE Trans. Inform.
Theory, vol. 57, no. 4, pp. 2270-2277, Apr. 2011.
[10] O. Simeone and H. H. Permuter, “Source coding when the side information may be delayed,” arXiv:1109.1293.

(14)
(a)

for 0 ≤ D1 ≤ min{ε(d) ∗ q, 1 − ε(d) ∗ q} and Rd (D1 ) = 0
otherwise, where p ∗ q
p(1 − q) + (1 − p)q. This result
with q = 0 (i.e., with feedforward) and d = 1 recovers the
calculation in [5, Example 2] (see also [4]). We remark that
the rate-distortion function of a Markov source X n without
feedforward, i.e., R∞ (D1 ), is equal to Hb (ε) − H(D1 ) only
for D1 smaller than a critical value, but is otherwise larger
[8]. This demonstrates that feedforward, unlike in the lossless
setting discussed above, can be useful in the lossy case for
distortion levels D1 sufﬁciently large.
VI. C ONCLUDING R EMARKS
A general information-theoretic characterization of the
trade-off between rate and distortion for the problem of
compressing information sources in the presence of delayed
side information can be generally given in terms of multiletter expressions, as done in [7]. In this work, we have
instead focused on a speciﬁc class of sources, which evolve
3 We use the conventional deﬁnition of the binary entropy as H(x)
−x log2 x − (1 − x) log2 (1 − x).

5

