Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May  9 17:30:52 2012
ModDate:        Tue Jun 19 12:55:06 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      443268 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569555367

Gaussian Multiple Descriptions with Common and
Constrained Reconstruction Constraints
Ravi Tandon1 , Behzad Ahmadi2 , Osvaldo Simeone2 , and H. Vincent Poor1
1

Department of Electrical Engineering, Princeton University, Princeton, NJ
2
CWCSPR, New Jersey Institute of Technology, Newark, NJ
Yn

Abstract—The problem of multiple descriptions for a Gaussian
source is considered, in which all the decoders have access to correlated Gaussian side-information. Two variations of this problem
are studied. First, the rate-distortion tradeoff is characterized
under the assumption of a common reconstruction constraint,
in which the estimate produced at each of the decoders is also
exactly recoverable at the encoder. Secondly, a generalization of
this setup is studied in which possibly different distortions are
tolerable between the estimates at the encoder and the respective
estimates at each of the decoders.

ψ0 (X n )

Decoder 1

R1
Xn

Encoder

ψ2 (X n )

Yn

Decoder 0

R2
ψ1 (X n )

ˆn
X1

Decoder 2

ˆn
X0

ˆn
X2

Yn

I. I NTRODUCTION
The rate distortion function for lossy transmission of a
memoryless source X to a decoder that has access to correlated
side information Y was characterized by Wyner and Ziv
in [1]. There are two ways in which the side-information
is useful in the Wyner-Ziv problem. First, since the side
information Y is correlated with the source X, the encoder
can reduce the communication rate via binning; and secondly,
ˆ
the reconstruction X at the decoder depends on both the
digital information received from the encoder and the sideinformation Y . The second property, i.e., the dependence of
ˆ
the decoder’s estimate X on the side-information Y (which
is unavailable at the encoder) may be undesirable for certain
applications. As an example, consider the transmission of
sensitive medical records, for which it is of utmost importance
that the decoder and the encoder agree upon the estimates of
the records.
To take such scenarios into account, the problem of common
reconstruction (CR) was proposed by Steinberg in [2], in
which the rate-distortion function is characterized for a pointto-point system under a common reconstruction constraint.
Formally, a common reconstruction constraint refers to a
restriction that the encoder should also be able to reconstruct
the estimate at the decoder. The common reconstruction constraint can be perhaps too restrictive since it precludes the
decoder to use its side-information to create the estimate. A
relaxed version of the common reconstruction constraint in
which some distortion is tolerable between the estimate at the
encoder and the estimate at the decoder was studied in [3].
The corresponding rate-distortion function was characterized
in [3] (henceforth referred to as the rate-distortion function
under constrained reconstruction (ConR)).
Some generalizations and extensions of [2] to multi-user
settings have been studied recently. In particular, the ratedistortion function for the Heegard-Berger problem with CR

Fig. 1.

Multiple descriptions with side-information and CR constraints.

constraints has been recently obtained in [4]. Furthermore, the
rate-distortion tradeoff for a class of cascade source coding
problems under the CR constraint has also been obtained
in [4]. It is noted that in the latter case, the problem of
determining the rate-distortion tradeoff is open when one does
not impose the CR constraint. This demonstrates that the
CR constraint may simplify the design problem by limiting
the space of possible strategies, as already discussed in [2]
and [5]. A source coding problem with complementary side
information under a CR constraint has been studied in [6].
Joint source-channel coding for a Gaussian source over a slow
fading Gaussian broadcast channel is studied in [5], where
it is shown that the natural broadcast strategy coupled with
successive reﬁnement source coding is in fact optimal under
the CR constraint (also see [2, Sec. IV]).
In this paper, we generalize the point-to-point commonreconstruction problem of [2], and the point-to-point constrained reconstruction problem of [3] to the problem of
multiple descriptions (MD) with decoder side information. We
focus on the MD problem with three decoders, in which all the
decoders have access to the same correlated side-information
Y (see Figure 1). We focus on Gaussian source and side
information. Speciﬁcally, an encoder wishes to convey a memoryless Gaussian scalar source X to three decoders via two
rate-limited orthogonal links. The distortion constraints at all
the decoders are assumed to be measured by the mean-squared
error (MSE). The correlated side information Y , which is
assumed to be jointly Gaussian with X is available at all
the three decoders. In the absence of side-information, this
problem was solved by Ozarow [7]. Instead, the more general
case in which correlated Gaussian side-information is available
at all decoders was solved by Diggavi and Vaishampayan [8].
In this paper, we impose an additional CR constraint. We

1

ﬁrst characterize the set of achievable rates and distortion
tuples for the Gaussian MD problem under the CR constraint.
We next consider the ConR generalization of this setup,
in which some pre-speciﬁed MSE distortions are tolerable
between the estimates at the encoder and the corresponding
estimates at the decoders. It is shown that the optimal strategy
is based on separation, in the sense that a combination of
coding for the MD problem, followed by binning with respect
to side-information Y , is optimal.

(1) and
1
n

The encoder observes a memoryless source X n and produces two indices
(n)

(n)
{fj

and

dd (a, b)

(6)

(a − b)2 and de (a, b)

(a − b)2 .

(7)

Before presenting our results, we recall the rate-distortion
region without side-information [7]:

(n)
ˆn
X0 = g0 (J1 , J2 , Y n ),

(n)

where gj is the reconstruction function at decoder j, for
j = 0, 1, 2. We denote the estimates at the encoder as follows:
ˆn
Xe,1 = ψ1 (X n ),

(5)

where Z is a zero-mean, Gaussian random variable with
2
variance σZ , and independent of X. The distortion measures
are quadratic and are deﬁned as follows:

where
: X → Jj }j=1,2 are the encoding functions.
The index J1 (resp. J2 ) is received at decoder 1 (resp. decoder
2). Both the indices (J1 , J2 ) are received at decoder 0. The
decoders form their estimates as follows:
j = 1, 2, and

j = 0, 1, 2.

Y = X + Z,

(n)

J2 = f2 (X n ),

n

(n)
ˆn
Xj = gj (J, Y n ),

ˆ
ˆ
E[de (Xj,t , Xe,j,t )] ≤ De,j ,
t=1

The non-negative tuple (R1 , R2 , D0 , D1 , D2 , De,0 , De,1 , De,2 )
is achievable if for every
> 0 and sufﬁciently large
n, there exists an (n, R1 + , R2 + , D0 + , D1 +
, D2 + , De,0 + , De,1 + , De,2 + )-code. We denote
RDConR as the closure of the set of all achievable
(R1 , R2 , D0 , D1 , D2 , De,0 , De,1 , De,2 ) tuples.
This paper considers the case in which the source X is
2
Gaussian with variance σX and the side information Y is
jointly Gaussian with X. Under this model, we can write

II. P ROBLEM S TATEMENT

J1 = f1 (X n )

n

No−SI
D1
≥ exp(−2R1 )
No−SI
D2

ˆn
ˆn
Xe,2 = ψ2 (X n ) and Xe,0 = ψ0 (X n ).

(8)

≥ exp(−2R2 )

(9)

No−SI
D0
≥ exp(−2(R1 + R2 ))

A. Common Reconstruction (CR)

1

√
√
,
1 − ( ΠNo−SI − ∆No−SI )2
(10)

(n)

n
n
We call the set of ({fj }j=1,2 , {gj }j=0,1,2 , {ψj }j=0,1,2 )
functions an (n, R1 , R2 , D0 , D1 , D2 )-code if |J1 | ≤ 2nR1 ,
|J2 | ≤ 2nR2 and the reconstruction sequences satisfy

1
n

where
No−SI
Dj

ΠNo−SI

n

ˆ
E[dd (Xt , Xj,t )] ≤ Dj ,

j = 0, 1, 2,

(1)

Dj
2 ,
σX

No−SI
No−SI
(1 − D1
)(1 − D2
)

(12)

No−SI No−SI
D1
D2

(13)

∆

t=1

and

No−SI

j = 0, 1, 2,

(11)

− exp(−2(R1 + R2 )).

The rate-distortion region with side-information (without any
constraints) [8] is given as

n

n

(2)

n

n

(3)

SI
D1 ≥ exp(−2R1 )

(4)

SI
D2

Pr (ψ1 (X ) = g1 (J1 , Y )) ≤
Pr (ψ2 (X ) = g2 (J2 , Y )) ≤
n

n

Pr (ψ0 (X ) = g0 (J1 , J2 , Y )) ≤ .

The non-negative tuple (R1 , R2 , D0 , D1 , D2 ) is achievable
if for every
> 0 and sufﬁciently large n, there exists
an (n, R1 + , R2 + , D0 + , D1 + , D2 + )-code. We
denote RDCR as the closure of the set of all achievable
(R1 , R2 , D0 , D1 , D2 ) tuples.

(14)

≥ exp(−2R2 )

(15)

SI
D0 ≥ exp(−2(R1 + R2 ))

1

√
√
,
1 − ( ΠSI − ∆SI )2

(16)

SI
with Dj being deﬁned as
SI
Dj

B. Constrained Reconstruction (ConR)

Dj ·

2
2
σX + σZ
2 2
σX σZ

=

Dj
,
VX|Y

j = 0, 1, 2,

(17)

where the quantities (ΠSI , ∆SI ) are deﬁned as in (12) and (13)
No−SI
No−SI
SI
SI
with (D1
, D2
), replaced by (D1 , D2 ) and VX|Y =
2
2
σ σ
Var(X|Y ) = σ2X Z2 denotes the variance of X given Y .
+σ

We next extend the CR constraint to the ConR
constraint. In this setting, some distortion is permissible between the estimate at the decoder(s) and the respective estimate(s) at the encoder. We call the set
(n)
n
n
of ({fj }j=1,2 , {gj }j=0,1,2 , {ψj }j=0,1,2 ) functions an
(n, R1 , R2 , D0 , D1 , D2 , De,0 , De,1 , De,2 )-code if |J1 | ≤
2nR1 , |J2 | ≤ 2nR2 and the reconstruction sequences satisfy

X

Z

III. M AIN R ESULTS
Theorem 1: The set RDCR
of all achievable
(R1 , R2 , D0 , D1 , D2 )-tuples for the Gaussian MD problem
with decoder side-information and CR constraints is given as

2

follows:

CR
D0

4

≥ exp(−2R1 )

No Side Information
Side Information
Side Information, CR
Side Information, ConR, De,sym = 0.05

(18)
3.5

≥ exp(−2R2 )

(19)

≥ exp(−2(R1 + R2 ))

1

√

√

1 − ( ΠCR −

∆CR )2

3

, (20)

2.5
D1=D2=D0=Dsym

CR
D1
CR
D2

where we have deﬁned
2
2
σX + σZ
2 2
σX σZ

CR
Dj

2
D j σZ
2
Dj + σZ

,

j = 0, 1, 2.

(21)

2
SI with CR
1.5
No SI

ΠCR

CR
CR
(1 − D1 )(1 − D2 )

(22)

∆CR

CR CR
D1 D2 − exp(−2(R1 + R2 )).

(23)

1
SI with ConR, De,sym=0.05
0.5

The proof of Theorem 1 is given in the appendix.
Theorem 2: The set RDConR of all achievable
(R1 , R2 , D0 , D1 , D2 , De,0 , De,1 , De,2 )-tuples
for
the
Gaussian MD problem with decoder side-information
and ConR constraints is given as follows:
ConR
D1
≥ exp(−2R1 )

SI, No constraints
0

1




2
2
σX +σZ
2
2
σX σZ
2
2
σX +σZ
2
2
σX σZ

Dj ,

σ2 σ2
X

√

2
σZ +Dj −2

2
σZ De,j

,

Z

otherwise,
(27)

and
ΠConR
∆

ConR

ConR
ConR
(1 − D1
)(1 − D2
)

(28)

ConR ConR
D1
D2

(29)

− exp(−2(R1 + R2 )).

0.4

0.5
R1=R2=Rsym

0.6

0.7

0.8

0.9

1

This paper has focused on the generalization of the Gaussian multiple-description problem with side-information at the
decoders by imposing a common reconstruction constraint as
in [2] and also constrained reconstruction constraints as in [3].
These additional distortion constraints effectively control the
amount of side-information to be used in creating the source
estimate at each decoder. The complete rate-distortion tradeoff
has been characterized for the case of common-reconstruction
constraints. This result is then generalized to the constrainedreconstruction setting with MSE distortion measures at each
of the decoders. These tradeoffs reﬂect the penalty in terms
of additional communication rates of the encoder, if it seeks
to agree with the estimates at the decoders.

2
σZ De,j ≥ min Dj , σ2X Z2 ;
+σ
2
σZ (Dj −De,j )

0.3

IV. C ONCLUSIONS

√
√
,
1 − ( ΠConR − ∆ConR )2
(26)

where for j = 0, 1, 2, we have deﬁned
ConR
Dj




0.2

of Dsym below a certain threshold, the ConR constraints
are automatically satisﬁed, and hence the minimal rate under
ConR coincides with the minimal rate without any constraints.

(25)

ConR
D0
≥ exp(−2(R1 + R2 ))

0.1

2
2
Fig. 2. Illustration of symmetric rate-distortion tradeoffs, σX = 4, σZ = 1.

(24)

ConR
D2
≥ exp(−2R2 )

0

V. ACKNOWLEDGEMENT

The proof of Theorem 2 is omitted due to space limitations
and can be found in [9].
Remark 1: The optimal coding schemes achieving the
tradeoffs in Theorems 1 and 2 are based on a separation based
strategy, i.e., a combination of coding for the MD problem,
followed by binning with respect to side-information Y . This
is evident by comparing the expressions in Theorems 1 and
2 to the corresponding tradeoff regions in (8)-(10) and (14)(16). It is interesting to note that all four regions are described
by a similar set of constraints, except that the deﬁnition of
the effective distortions DNo−SI , DSI , DCR and DConR differ
depending on the problem under consideration. We also note
ConR
CR
that as De,j → 0, for j = 0, 1, 2, we have Dj
→ Dj ,
and the region in Theorem 2 collapses to the one in Theorem
1.
We illustrate these tradeoffs in Figure 2 for the symmetric
case in which D1 = D2 = D0 = Dsym , and R1 = R2 =
2
Rsym , De,1 = De,2 = De,0 = De,sym = 0.05, with σX = 4
2
and σZ = 1. From Figure 2, it is worth noting that for values

The work of H. V. Poor and R. Tandon was supported in
part by the Air Force Ofﬁce of Scientiﬁc Research MURI
Grant FA-9550-09-1-0643 and in part by the National Science
Foundation Grant CNS-09-05398. The work of O. Simeone
was supported by the U.S. National Science Foundation under
Grant CCF-0914899.
VI. A PPENDIX
A. Proof of Theorem 1
1) Coding Scheme with CR: The following region is
achievable for the MD problem with a CR constraint:
R1 ≥ I(X; U1 |Y )
R2 ≥ I(X; U2 |Y )
R1 + R2 ≥ I(X; U1 , U2 |Y ) + I(U1 ; U2 |Y )
where the random variables (U1 , U2 , X, Y ) satisfy the Markov
condition (U1 , U2 ) → X → Y , and there exist functions

3

We select ρ as follows:

g1 (·), g2 (·) and g0 (·, ·) such that
ˆ
X1 = g1 (U1 ),

ˆ
X2 = g2 (U2 ),

ˆ
X0 = g0 (U1 , U2 ),

CR CR
D1 D2 − exp(−2(R1 + R2 ))

ρ=−

CR CR
D1 D2

ˆ
satisfying E[d(X, Xj )] ≤ Dj for j = 0, 1, 2.

U2 = X + N2 ,

CR
D1 ≥ exp(−2R1 )

(30)

CR
D2

where (N1 , N2 ) are jointly Gaussian, independent of X, with
a covariance matrix as follows:
KN1 ,N2 =

2
σ1

ρσ1 σ2
.
2
σ2

ρσ1 σ2

(31)

D1 =

D2 =

≥ exp(−2R2 )

Rj ≥ RCR (Dj ),

Dj σ 2
= 2 X ,
σX − D j

j = 1, 2.

j = 1, 2.

= H(J1 ) + H(J2 )

(49)

(50)

1
CR
Dj

(51)

n

n

(35)

≥ H(J1 |Y ) + H(J2 |Y )

(36)

= H(J1 , J2 |Y n ) + H(J1 |Y n ) + H(J2 |Y n ) − H(J1 , J2 |Y n )
(53)

.

R1 + R2 ≥ I(X; U1 , U2 |Y ) + I(U1 ; U2 |Y )
= h(U1 |Y ) + h(U2 |Y ) − h(U1 , U2 |X)
2
2
(VX|Y + σ1 )(VX|Y + σ2 )
1
= log
2 σ 2 (1 − ρ2 )
2
σ1 2
1
1
= log
.
CR D CR (1 − ρ2 )
2
D1
2

n

n

(54)
n

= I(X ; J1 , J2 |Y ) + I(J1 ; J2 |Y )
n

≥ nRCR (D0 ) + I(J1 ; J2 |Y ).

(55)
(56)

As in [7], we deﬁne

(37)

T

exp

2
I(J1 ; J2 |Y n ) .
n

(57)

Using (56), we have the following inequalities:

(38)
(39)

CR
D1 ≥ exp(−2R1 )

(58)

(40)

CR
D2
CR
D0

≥ exp(−2R2 )

(59)

≥ exp(−2(R1 + R2 ))T.

(60)

We now obtain a lower bound on T by using Ozarow’s
technique of inducing conditional independence [7]. For this
purpose, we deﬁne an artiﬁcial Gaussian random variable as
follows:

(41)

Hence, ρ can be chosen arbitrarily as long as it satisﬁes
CR CR
D1 D2 − exp(−2(R1 + R2 ))
.
CR CR
D1 D2

(52)

= H(J1 , J2 |Y n ) + I(J1 ; J2 |Y n )

The sum-rate constraint is given as

ρ2 ≤

,

(48)

n(R1 + R2 )

where we have used (32).
The individual rate constraints for j = 1, 2 are given as
1
log
2

1
DCR

2
D + σZ
2
DσZ

We now obtain a lower bound on the sum rate R1 + R2 as
follows:

(34)

2
2
2
σX + σZ
Dj σZ
2 σ2
2
σX Z
Dj + σZ
2
2 2
σj (σX + σZ )
= 2 2
2 σ2 + σ2 σ2
σX σj + σX Z
Z j

Rj ≥ I(X; Uj |Y ) =

2 2
σX σZ
2 + σ2
σX
Z

1
log
2
1
log
2

CR
Dj ≥ exp(−2Rj ),

Also, note that we have for j = 1, 2
CR
Dj

(47)

where DCR is as deﬁned in (21). Hence, from (47) and (49),
we have the following bounds:

(33)

2
2
We select σ1 and σ2 such that the distortion constraints in
(32) are met with equality, i.e., we select
2
σj

j = 1, 2.

From [2, Eq. (25)], RCR (D) is given as

(32)

2 2 2
σX σ1 σ2 (1 − ρ2 )
.
2 2
2
2
2
σ1 σ2 (1 − ρ2 ) + σX (σ1 + σ2 − 2σ1 σ2 ρ)

√
√
, (46)
1 − ( ΠCR − ∆CR )2

3) Converse Proof under the CR Constraint: We note that
R1 and R2 must always satisfy the following bounds:

and
D0 =

(45)
1

with ΠCR and ∆CR as deﬁned in Theorem 1.

RCR (D) =

2 2
σX σ2
2 + σ2
σX
2

(44)

CR
D0 ≥ exp(−2(R1 + R2 ))

ˆ
The estimates Xi are selected as the minimum MSE (MMSE)
estimators of X given U1 , U2 and (U1 , U2 ) for decoders
1, 2 and decoder 0, respectively. Note that such a selection
automatically satisﬁes the common reconstruction constraints
for all the three decoders. The achievable distortions are hence
given as
2 2
σX σ1
2 + σ2 ,
σX
1

(43)

Substituting this value of ρ in the expression for D0 in (33)
and using (32), we arrive at

2) Gaussian MD with CR constraints: For the case in
which (X, Y ) are jointly Gaussian, we select
U1 = X + N1 ,

.

(42)

Si = Xi + Wi ,

4

i = 1, . . . , n,

(61)

where the sequence of random variables {Wi }n is indei=1
pendent and identically distributed with each element being
a zero-mean, Gaussian random variable with variance η, and
W n is assumed to be independent of (X n , Y n ). To lower
bound T , we have the following sequence of inequalities:

We now note the following sequence of inequalities:
h(X n |J1 , J2 , Y n )
= h(X n |Y n ) − I(X n ; J1 , J2 |Y n )
n
n

n

n

≥ I(J1 ; J2 |Y ) − I(J1 ; J2 |Y , S )
n

n

n

= I(J1 ; S |Y ) − I(J1 ; S |Y , J2 )

n

n

(81)
(82)

which implies that

(64)

n

2
h(X n |J1 , J2 , Y n )
n
2
h(X n |Y n ) exp(−2(R1 + R2 ))T
≥ exp
n
= (2πe)VX|Y · exp(−2(R1 + R2 ))T

= h(S |Y ) + h(S |J1 , J2 , Y )
n

n

n

n

n

n

− h(X + W |J1 , Y ) − h(X + W |J2 , Y )
n

n

n

VX|Y

2 2
σX σZ
∗
2 + σ 2 , D1
σX
Z

exp

(65)

n

≥ h(S |Y ) + h(S |J1 , J2 , Y )
n
n
∗
∗
− log(2πe(D1 + η)) − log(2πe(D2 + η))
2
2
n
= log VX|Y + η + h(S n |J1 , J2 , Y n )
2
n
n
∗
∗
− log(2πe(D1 + η)) − log(2πe(D2 + η)),
2
2
where we have deﬁned
2
D1 σZ
∗
2 and D2
D1 + σZ

(66)

(67)

T ≥

2
D2 σZ
2 .
D2 + σZ
(68)

(69)

(70)
n
ˆ 1 , Y n ) + I(ψ1 (X n ); S n |X1 , Y n ) (71)
ˆn
= h(S |ψ1 (X ), X
ˆn
≤ h(S n |ψ1 (X n ), X1 , Y n ) + n
(72)
n

[1] A. D. Wyner and J. Ziv. The rate distortion function for source coding
with side information at the decoder. IEEE Transactions on Information
Theory, 22(1):1–10, January 1976.
[2] Y. Steinberg. Coding and common reconstruction. IEEE Transactions on
Information Theory, 55(11):4995–5010, November 2009.
[3] A. Lapidoth, A. Malar, and M. Wigger. Constrained Wyner-Ziv coding.
In Proc. IEEE International Symposium on Information Theory (ISIT),
St. Petersburg, Russia, 2011.
[4] B. Ahmadi, R. Tandon, O. Simeone, and H. V. Poor. Heegard-Berger and
cascade source coding problems with common reconstruction constraints
[arXiv 1112.1762v3]. Submitted to IEEE Transactions on Information
Theory, December 2011.
[5] C. Tian, A. Steiner, S. Shamai, and S. N. Diggavi. Successive reﬁnement
via broadcast: Optimizing expected distortion of a Gaussian source over
a Gaussian fading channel. IEEE Transactions on Information Theory,
54(7):2903–2918, July 2008.
[6] R. Timo, A. Grant, and G. Kramer. Rate-distortion functions for source
coding with complimentary side information. In Proc. IEEE International
Symposium on Information Theory (ISIT), St. Petersburg, Russia, 2011.
[7] L. Ozarow. On a source coding problem with two channels and three
receivers. Bell System Technical Journal, 59:1909–1921, December 1980.
[8] S. N. Diggavi and V. A. Vaishampayan. On multiple description source
coding with decoder side information. In Proc. IEEE International
Information Theory Workshop (ITW), San Antonio, TX, 2004.
[9] R. Tandon, B. Ahmadi, O. Simeone, and H. V. Poor. Gaussian multiple
descriptions with common and constrained reconstruction constraints.
Preprint www.princeton.edu/∼rtandon/GaussianMDpreprint.pdf, 2012.

2
h(W n |J1 , J2 , Y n )
n
(76)

2
h(X n |J1 , J2 , Y n ) + (2πe)(η),
n

η ∗ (η ∗ + 1)
,
+ η ∗ ) − (η ∗ + 1) exp(−2(R1 + R2 ))

R EFERENCES

(73)

exp

= exp

+

CR
η ∗ )(D2

Hence, (58)-(60) and (86) complete the proof of Theorem 1.

= h(X n + W n |ψ1 (X n ), Y n ) + n
(74)
n
∗
≤ log (D1 + η) + n ,
(75)
2
where (72) follows from the CR constraint and application of
Fano’s inequality, and (75) follows from arguments as in [2,
Sec. V-A eqn. 69 − 72] and [2, Sec. V-C]. Next, consider the
second term in (67):
2
h(X n + W n |J1 , J2 , Y n )
n
2
≥ exp
h(X n |J1 , J2 , Y n ) + exp
n

CR
(D1

D∗

ˆn
= h(S |X1 , Y n )

≤ h(S n |ψ1 (X n ), Y n ) + n

(84)

η
j
CR
= VX|Y for j = 1, 2. The
where η∗
VX|Y and Dj
∗
following choice of η maximizes the right hand side above:
√
∆CR
∗
√
,
(85)
η =√
ΠCR − ∆CR
which upon substitution, yields the following lower bound on
T:
1
√
√
T ≥
.
(86)
CR −
1−( Π
∆CR )2

n
n

(83)

Hence, from (77) and (84), and using (67), we obtain the
following lower bound on T :

The bounding step (66) follows from the following sequence
of inequalities:
h(X n + W n |J1 , Y n )
ˆn
≤ h(X n + W n |X1 , Y n )

n

≥ h(X n |Y n ) − nR1 − nR2 + I(J1 ; J2 |Y n ),

= h(S n |Y n ) + h(S n |J1 , J2 , Y n )
n

n

≥ h(X n |Y n ) − H(J1 ) − H(J2 ) + I(J1 ; J2 |Y n )

(63)

− h(S n |J1 , Y n ) − h(S n |J2 , Y n )

(79)

n

= h(X |Y ) − H(J1 |Y ) − H(J2 |Y ) + I(J1 ; J2 |Y )
(80)

(62)

n

n

(78)

n

= h(X |Y ) − H(J1 , J2 |Y )

I(J1 ; J2 |Y n )
n

n

(77)

where in (76), we have used the fact that W n is independent
of (X n , Y n , J1 , J2 ) and the conditional version of the entropy
power inequality (EPI).

5

