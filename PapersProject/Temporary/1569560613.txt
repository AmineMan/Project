Creator:         TeX output 2012.05.03:2124
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May  3 21:24:18 2012
ModDate:        Tue Jun 19 12:54:12 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      276861 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569560613

On the equivalence between Stein identity and de
Bruijn identity
Sangwoo Park, Erchin Serpedin, and Khalid Qaraqe
Electrical and Computer Engineering Department, Texas A&M University, College Station, TX 77843, USA
Email: swpark78@neo.tamu.edu, serpedin@ece.tamu.edu, kqaraqe@tamu.edu

RV W is Gaussian, the equivalence between the generalized
Stein and de Bruijn identities is proved. In particular, when
RV Y is Gaussian, not only Stein and de Bruijn identities are
equivalent, but also they are equivalent to the heat equation
identity [5].
The second major goal of this paper is to generalize de
Bruijn identity in two distinctive ways. Considering the ﬁxed
noise distribution channel model (1), the ﬁrst-order derivative
of differential entropy of output signal Y will be expressed as
a function of the posterior mean EX|Y [·|·], while the secondorder derivative of differential entropy of output signal Y
will be represented explicitly in terms of Fisher information.
Even though some of these relationships do not include
Fisher information, they still show fundamental relationships
among basic concepts in information theory and statistical
estimation theory, and these relationships hold for arbitrary
noise channels.
Several applications are brieﬂy mentioned to illustrate the
usefulness of the proposed new results. Costa’s entropy power
inequality (EPI) is derived in a simpler and alternative way.
Cram´ r-Rao lower bound (CRLB) [6], Bayesian Cram´ r-Rao
e
e
lower bound (BCRLB) [7], and a new lower bound- tighter
than the BCRLB- for the mean square error (MSE) in Bayesian
estimation can be further derived by exploiting the proposed
results. Even though some of the proposed applications have
already been proved before, herein paper a series of novel relationships and perspectives are presented on these applications.
The rest of this paper is organized as follows. Preliminary
results and relationships between Stein and de Bruijn identities
are provided in Section II. Two distinctive extensions of de
Bruijn identity are presented in Section III. In Section IV,
several potential applications of the aforementioned results are
brieﬂy mentioned due to lack of space. Finally, conclusions are
stated in Section V.

Abstract—This paper illustrates the equivalence between two
fundamental results: Stein identity, originally proposed in the
statistical estimation realm, and de Bruijn identity, considered
for the ﬁrst time in the information theory ﬁeld. Two distinctive
extensions of de Bruijn identity are presented as well. For
arbitrary but ﬁxed input and noise distributions, the ﬁrst-order
derivative of differential entropy is expressed by means of a
function of the posterior mean, while the second-order derivative
of differential entropy is manifested in terms of a function of
Fisher information. Several applications exemplify the utility of
the proposed results.

I. I NTRODUCTION
De Bruijn identity, which shows a link between differential
entropy and Fisher information, was ﬁrst mentioned and
exploited by Stam in the context of building the ﬁrst rigorous
proof of the entropy power inequality (EPI), a fundamental
result exploited by Shannon to prove a lower bound on the
capacity of additive Gaussian noise channels. Recently, a
renewed interest was manifested in the applications of de
Bruijn identity in estimation and turbo (iterative) decoding
schemes, and in relating the input-output mutual information
with the minimum mean-square error (MMSE) of additive
Gaussian and non-Gaussian noise channels [1], [2].
A large number of approaches have been applied to prove de
Bruijn identity. Amongst them, herein paper, we consider the
most direct approach because it shares one common technique,
viz. integration by parts, with another well-known resultreferred to as Stein identity [3], [4]. Stein identity points
out a relationship between the expectations of a function
(whose variable is a Gaussian random variable (RV)) and
its ﬁrst-order derivative, and has been also widely researched
with tremendous success in many applications from diverse
ﬁelds such as statistics, economics, probability theory, decision
theory, and graph theory (see e.g., [5]).
The main theme of this paper is to investigate how de
Bruijn identity is connected to Stein identity. To compare these
identities, the following additive noise channel is considered
throughout the paper:
√
(1)
Y = X + aW,

II. R ELATIONSHIPS BETWEEN S TEIN I DENTITY AND DE
B RUIJN I DENTITY
To interconnect Stein identity and de Bruijn identity, ﬁrst
we will introduce three fundamental results.
Theorem 1 (De Bruijn’s Identity [8], [9]): Given the channel model (1), let X be an arbitrary RV with a ﬁnite secondorder moment, and W be a Gaussian RV with zero mean and
unit variance. Independence between RVs X and W is also

where a denotes a nonnegative deterministic parameter, and
RVs X and W are assumed arbitrary and independent. When
This paper was made possible by NPRP grants 09-341-2128 and 4-12932-513 from QNRF.

1

assumed. Then,

Proof: Like the proof of Theorem 3 in [5], the equivalence
is proved by showing that de Bruijn identity is derived from
Stein identity, and vice versa. Before illustrating the details of
the proof, Lemma 1 will be exploited.
Lemma 1: For RVs X and Y deﬁned in (1), the following
relations hold:

d
1
h(Y ) =
J(Y ),
da
2
where h(·) stands for differential entropy, J(Y ) =
EY [SY (Y )2 ], EY[·] is the expectation with respect to Y , SY (Y )
is deﬁned as d log fY (y; a)/dy, and log denotes the natural
logarithm.
Proof: See [9].
Theorem 2 (Generalized Stein Identity [10]): Let Y be an
absolutely continuous RV. If the probability density function
(pdf) fY (y) satisﬁes limy→±∞ k(y)fY (y) = 0, and
d
dy fY

(y)

fY (y)

[

log fY (y;a)
da
d
da

[

d
dy k(y)

(ν − t(y))
= −
+
k(y)
k(y)

for some function k(y), then
EY [r(Y ) (t(Y ) − ν)]

=

EY

[

d
dy r(Y

]
)k(Y ) ,

y=u+

=
√

EX [(y−X)2 fY |X (y|X;a)]−a
2a2 fY (y;a)

aw

,
√
y=u+ aw

EX [(u−X)(y−X)fY |X (y|X;a)]−a
√
log fY (u+ aw;a)=
2a2 fY (y;a)

d

]
=−

log fY (y;a)
dy

y=u+

,
√
y=u+ aw

EX [(y−X)fY |X (y|X;a)]

√
aw

. (5)

afY (y;a)
y=u+

√

aw

Exploiting the generalized Stein identity for
(2)

d
log fY (y; a), k(y) = 1,
dy
d
dy fY (y; a)
t(y; a) = −
, and ν = 0,
fY (y; a)
r(y; a) = −

for any function r(y) which satisﬁes EY [|r(Y )t(Y )|] < ∞,
]
[
[
]
d
EY r(Y )2 < ∞, and EY k(Y ) dy r(Y ) < ∞. In particu2
lar, when Y is a Gaussian RV with mean µy and variance σy ,
(2) is simpliﬁed to
[
]
d
2
EY [r(Y ) (Y − µy )] = σy EY dy r(Y ) .
(3)

de Bruijn identity is proved via the following steps.
[
]
1
1
d
J(Y ) = EY
r(Y ; a)
2
2
dY
[
]
d
1
dY fY (Y ; a)
= − EY r(Y ; a)
2
fY (Y ; a)
[∫ ∞
]
(y−X)
d
= −EX
fY |X (y|X; a) log fY (y; a)dy
2a
dy
∫ ∞ −∞ ∞
∫
d
(y−u)
= − fX(u)
fY|X(y|u;a) logfY (y;a)dydu. (6)

Equation (3) represents the well-known classic Stein identity.
Proof: See [10].
Theorem 3 (Heat Equation Identity [5]): Let Y be a Gaussian RV with mean µ and variance 1 + a. Assume g(y) is a
twice continuously differentiable function, and that both g(y)
d
and | dy g(y)| are1 O(ec|y| ) for some 0 ≤ c < ∞. Then,
[ 2
]
d
1
d
EY [g(Y )] =
EY
g(Y ) .
(4)
da
2
dY 2

−∞

−∞

2a

dy

(A)

Proof: See [5].
Since Stein identity (3) is valid only for Gaussian RVs, we
have to consider either a special case of de Bruijn identity in
Theorem 1 or use a generalized version of Stein identity (2) to
compare it with de Bruijn identity. Therefore, given (1) with
W a Gaussian RV, the equivalence between Stein identity and
de Bruijn identity is next established.
Theorem 4: Given the channel (1), let X be an arbitrary
RV with a ﬁnite second-order moment, and W be a Gaussian
RV with zero mean and unit variance. Independence between
RVs X and W is also assumed. If r(y; a) is deﬁned as
r(y; a) = −

]

d

Using Stein identity, dominated convergence theorem (DCT),
and Fubini’s theorem, the equalities in (6) are established.
√
Adopting the change of variables y = u + aw, the term
(A) is expressed as
(
)[
]
∫ ∞
1
w2
d
√ exp −
(A) =
log fY (y; a)
dw
√
2
da
2π
−∞
y=u+ aw
(
)
∫ ∞
√
d
1
w2
√ exp −
−
log fY (u+ aw; a)dw.(7)
da −∞ 2π
2
Due to (5) and normality of conditional pdf fY |X (y|u; a), (7)
√
is veriﬁed. Re-deﬁning w as (y −u)/ a, (6) is expressed as
∫ ∞
(∫ ∞
d
fX (u)
fY |X (y|u; a) log fY (y; a)dy
da
−∞
∫ ∞−∞
)
d
−
fY |X (y|u; a) log fY (y; a)dy du
da −∞
∫ ∞
d
= −
fY (y; a) log fY (y; a)dy
da −∞
d
=
h(Y ).
da

d
log fY (y; a),
dy

in (2), de Bruijn identity is equivalent to the generalized
Stein identity. In particular, if X is also a Gaussian RV, and
g(y; a) = − log fY (y; a) in (4), then de Bruijn, Stein, and heat
equation identities are all equivalent to one another.
1 O(·) denotes the limiting behavior of a function, i.e., g(y) = O(q(y))
if and only if there exist positive real numbers K and y ∗ such that g(y) ≤
K|q(y)| for any y which is greater than y ∗ .

2

Therefore, de Bruijn identity:
1
J(Y ) =
2

from de Bruijn identity, we derive the generalized Stein
identity as
[
]
[
]
d
fY (Y ; a)
1
1
d
dY
EY −
r(Y ; a)
=
EY
r(Y ; a)
2
fY (Y ; a)
2
dY
[
]
d
⇐⇒ EY [t(Y ; a)r(Y ; a)] = EY
r(Y ; a) ,
dY

d
h(Y ),
da

is established from the generalized Stein identity.
Second, the generalized Stein identity is derived from de
Bruijn identity as follows. First, deﬁne the function
∫ y
g(y; a) =
r(u; a)du + q(a),

d
where t(y; a) = − dy fY (y; a)/fY (y; a), and ⇐⇒ denotes
equivalence between before and after the notation.

0

and express its expectation as follows

III. E XTENSION OF DE B RUIJN ’ S I DENTITY

E [g(Y ; a)]
∫ Y∫ y
∫ 0∫ 0
∞
=
fY (y;a)r(u;a)dudy −
fY (y;a)r(u;a)dudy+q(a)
0
0
−
∞ y
))
(
∫ ∞
∫ ∞(
u−x
√
=
fX (x)
r(u; a)dudx
1 − ΦY |X
a
−∞
0
(
)
∫ ∞
∫ 0
u−x
√
− fX (x)
ΦY |X
r(u; a)dudx + q(a),
(8)
a
−∞
−∞

The main ingredient for establishing de Bruijn identity is
that Gaussian density functions satisfy the heat equation. However, general pdfs do not satisfy the heat equation. Therefore,
to extend de Bruijn identity to additive non-Gaussian noise
channels, a general relationship between differentials of a pdf
with respect to y and a of the form:
)
d
1 d (
fY |X (y|x; a) = −
(y − x)fY |X (y|x; a) , (10)
da
2a dy

where ΦY |X (·) denotes the standard conditional normal cumulative density function. Then, differentiate both sides of (8)
with respect to parameter a:
[∫ ∞
(
)
]
d
u−X
d
EY [g(Y ;a)] = −EX
ΦY|X √
r(u; a)du
da
da
a
[∫−∞(
(
))
]
∞
u−X
d
+ EX
1−ΦY|X √
r(u; a)du
a
da
0

will be exploited. Simple calculations show that (10) can be
obtained directly from equation (1). The relationship (10)
represents the key ingredient in establishing a connection
between the derivative of differential entropy and posterior
mean, as illustrated by the following theorem.
Theorem 5: Consider the channel (1), where X and W are
arbitrary RVs, independent of each other. Given the following
assumptions:
[
]
[
]
d
d
i) dy EX fY |X (y|X; a) = EX dy fY |X (y|X; a) ,
[
]
[d
]
d
da EX fY |X (y|X; a) = EX da fY |X (y|X; a) ,
∫
∫ d
d
fY (y;a) log fY (y;a)dy =
da
da (fY (y;a) log fY (y;a))dy,
[
]
[
]
ii) lim EX XfY |X (y|X; a) =EX lim XfY |X (y|X; a) ,
y→±∞
[ y→±∞
]
[
]
lim EX fY |X (y|X; a) =EX lim fY |X (y|X; a) ,

[∫
− EX

0

(

ΦY|X
−∞

)

u−X
√
a

(B)

]
d
d
r(u;a)du + q(a).(9)
da
da

(C)

∫y
Since (d/da)g(y; a) = (d/da) 0 r(u; a)du + (d/da)q(a),
equations (B) and (C) are simpliﬁed as follows:
∫ ∞
d
d
d
(B)−(C)=
fY (y; a) g(y; a)dy− q(a) = − q(a).
da
da
da
−∞

y→±∞

y→±∞

lim y 2 fY (y; a) = 0,

y→±∞

iii)

EX [XfY |X (y|X;a)]

√

fY (y;a)

< ∞,

the ﬁrst-order derivative of differential entropy can be expressed as
{
[
]}
d
1
d
h(Y ) =
1 − EY
EX|Y [X|Y ] , (11)
da
2a
dY

Therefore, (9) is expressed as
[∫ ∞
(
)
]
d
d
u−X
√
EY [g(Y ; a)] = −EX
ΦY |X
r(u; a)du
da
a
−∞ da
[∫ ∞
(
)
]
(u−X) d
u−X
ΦY |X √
r(u; a)du
= EX
2a du
a
−∞
∫ ∞ d
1
du fY (u; a)
=−
r(u; a)fY (u; a)du.
2 −∞ fY (u; a)

where EX|Y [·] denotes the posterior mean.
Proof: This theorem is proved using (10), integration
by parts, and DCT. Since the proof is similar to the one in
Theorem 6, the details of the proof are delegated to [11].
For additive non-Gaussian noise channels, differential entropy appears not to be expressible in terms of Fisher information. Instead, differential entropy is expressed by a function
of the posterior mean as shown in Theorem 5. Fortunately,
several noise distributions of interest in communication problems satisfy the required assumptions in Theorem 5 (e.g.,
Gaussian, gamma, exponential, chi-square with restrictions
on parameters, Rayleigh, etc). Therefore, Theorem 5 is quite

Since

[
]
d
d
1
d
dY fY (Y ; a)
h(Y ) =
EY [g(Y ; a)] = EY −
r(Y ; a)
da
da
2
fY (Y ; a)

and
[ 2
]
[
]
1
d
1
d
1
J(Y ) = EY
g(Y ; a) = EY
r(Y ; a) ,
2
2
dY 2
2
dY

3

1
=− 2
4a

powerful. In particular, if the posterior mean EX|Y [X|Y ] is
expressed by a polynomial function of Y , e.g., X and W
are independent Gaussian RVs in (1) or RVs belonging to the
natural exponential family of distribution [12], then (11) can
be expressed in simpler forms. We would like also to mention
that a result similar in nature to Theorem 5 was reported before
in [2] using a different approach. Similarly to [2], a series of
specialized applications of Theorem 5 could be envisaged.
Now, we consider the second-order derivative of differential
entropy. One interesting property of the second-order derivative of differential entropy is that it can always be expressed
as a function of Fisher information.
Theorem 6: Consider the channel (1), where X and W
are two arbitrary RVs, independent of each other. Given the
following assumptions:
[ 2
]
[
]
d
d2
i) dy2 EX fY |X (y|X; a) =EX dy2 fY |X (y|X; a) ,
[ 2
]
[
]
d2
d
EX fY |X (y|X; a) =EX da2 fY |X (y|X; a) ,
da2
∫
∫ d2
d2
f
da2 [ Y (y;a) log fY (y;a)dy = [da2 (fY (y;a) log fY (y;a))dy,
]
]
d
d
fY |X (y|X;a)
fY |X (y|X;a)
ii) lim EX X 2 dy √
=EX limX 2 dy √
,
fY (y;a)
fY (y;a) ]
y→±∞
[ y→±∞
[
]
lim EX XfY |X (y|X; a) =EX lim XfY |X (y|X; a) ,
y→±∞
[ y→±∞
]
[
]
lim EX fY |X (y|X; a) =EX lim fY |X (y|X; a) ,
y→±∞

−

log fY (y; a)
−∞

∫

[
]
d2
EX (y − X)2 fY |X (y|x; a) dy
dy 2
(D)

∞

log fY (y; a)
−∞

[
]
d
EX (y − X)fY |X (y|x; a) dy .
dy

The terms (D) and (E) are further simpliﬁed as follows:
{∫ ∞ 2
[
]
d
1
(D) = − 2
fY (y; a)EX|Y (Y −X)2 |Y = y dy
4a
dy 2
−∞
}
[
[
]]
−EY SY (Y )2 EX|Y (Y −X)2 |Y
[
]
[
]
1
d
= − 2 EY
SY (Y )EX|Y (Y − X)2 |Y ,
(14)
4a
dY
]
[
1
d
EX|Y [Y − X|Y ]
(E) = − 2 EY
4a
dY
1 d
=−
h(Y ).
(15)
2a da
Using (14) and (15), equation (12) is expressed as
d2
h(Y ) =
da2

y→±∞

y→±∞

EX [X 2 fY |X (y|X;a)]
(fY (y;a))3/4

∞

(E)

lim y 8 fY (y; a) = 0,

iii)

1
4a2

∫

1 d
−Ja (Y ) −
h(Y )
[ 2a da
]
1
d
2
− 2 EY
SY (Y )EX|Y [(Y − X) |Y ] ,
4a
dY

and the proof is completed. Additional details of the proof are
delegated to [11].
Although we have not enumerated all possible pdfs for Theorems 5 and 6, most of the pdfs that present an exponentiallydecaying factor in their expression satisfy the assumptions required in Theorems 5 and 6, since such a condition is sufﬁcient
for the required interchange between limit and integral.

< ∞,

the following equality holds:
d2
1 d
h(Y ) = −Ja (Y ) −
h(Y )
da2
2a da
[
]
[
]
d
1
SY (Y )EX|Y (Y − X)2 |Y ,
− 2 EY
4a
dY

IV. A PPLICATIONS
A. Applications in Information Theory
In information theory, entropy power inequality (EPI) is
one of the most important inequalities since it helps to assess
channel capacity results under different circumstances, e.g.,
the capacity of Gaussian MIMO broadcast channel and the
secrecy capacity of Gaussian wire-tap channel. Therefore,
various versions of EPI such as Costa’s EPI [13] and the
extremal entropy inequality [14] were proposed by different
authors. Herein section, we will prove Costa’s EPI, a stronger
version of a classical EPI, using Theorems 1, 4, and 6.
Lemma 2 (Costa’s EPI [13]): For an arbitrary but ﬁxed RV
X with a ﬁnite second-order moment, and a Gaussian RV W
with zero mean and unit variance,

where Ja (Y ) = EY [SYa (Y )2 ], and SYa = d log fY (y; a)/da.
Proof: Since the ﬁrst-order derivative of differential entropy with respect to a is expressed as
∫ ∞
d
d
h(Y ) = −
log fY (y; a) fY (y; a)dy,
da
da
−∞
we obtain the second-order derivative of the differential entropy with respect to a as follows:
∫ ∞
d2
d2
h(Y ) = −Ja (Y ) −
log fY (y; a) 2 fY (y; a)dy. (12)
da2
da
−∞
From (10), we derive an additional relationship between the
second-order differentials with respect to y and a:
{ 2
[
]
1
d
d2
fY (y; a) =
E (y − X)2 fY |X (y|x; a)
2
2 dy 2 X
da
4a
}
[
]
d
+ EX (y − X)fY |X (y|x; a) . (13)
dy

d2
N (Y ) ≤ 0,
(16)
da2
√
where Y = X + aW , a is a positive real number, X and W
are independent of each other, and the entropy power N (Y )
is deﬁned as N (Y ) = (1/2πe) exp(2h(Y )).
Proof: Since
(
)
d2
d2
2
N (Y ) = N (Y ) J(Y ) + 2 2 h(Y ) ,
da2
da

Upon substituting (d2 /da2 )fY (y; a) from (13) into (12), the
second term of equation (12) takes the expression:
∫ ∞
d2
−
log fY (y; a) 2 fY (y; a)dy
da
−∞

4

by Theorem 1 (de Bruijn identity), and N (Y ) ≥ 0, proving
(16) is equivalent to proving the following inequality:

details see [11]). A new lower bound, tighter than BCRLB,
for the mean square error for a Bayesian estimator can be
also derived based on Theorems 1 and 6 [11]. Second, since
Theorem 5 is equivalent to Theorem 1 in [2], Theorem 5
can be used for applications such as generalized EXIT charts
and power allocation in systems with parallel non-Gaussian
noise channels as mentioned in [2]. Finally, by Theorem 4,
we showed the equivalence among Stein, de Bruijn, and heat
equation identities. Therefore, a broad range of problems (in
probability, decision theory, Bayesian statistics, economics and
graph theory [5] that were traditionally established/analyzed
via Stein identity) could be analyzed via de Bruijn identity (in
the light of Theorems 4, 5 and 6).

d2
h(Y ) ≤ 0.
(17)
da2
Since W is a Gaussian RV, Fisher information inequality (FII)
[9], which is proved using Theorems 4 and 6, is equivalently
expressed as
J(Y )2 + 2

J(Y ) ≤

J(X)
.
1 + aJ(X)

(18)

Deﬁne the function l(a) as follows
l(a) =

−

J(X)
+ J(Y ).
1 + aJ(X)

V. C ONCLUSIONS

Since J(Y ) converges to J(X) as a approaches zero, l(0) = 0,
and the following inequality holds for an arbitrary but ﬁxed
random variable X and arbitrary small non-negative realvalued ϵ:
√
J(X)
+ J(X + ϵW ) ≤ 0.
(19)
l(ϵ) − l(0) = −
1 + ϵJ(X)

This paper mainly revealed three important informationestimation theoretic results. First, we proved that Stein identity
is equivalent to de Bruijn identity. Second, the ﬁrst-order and
second-order derivatives of differential entropy with respect
to the parameter a were expressed in terms of the posterior
mean and Fisher information, respectively. Finally, several
applications of the aforementioned results were provided.
The proposed applications illustrate that the newly developed
results are useful not only in information theory but also in
the estimation theory ﬁeld and other ﬁelds.

Therefore,
d
l(ϵ)
≤ 0,
(20)
dϵ
ϵ=0
for an arbitrary but ﬁxed random variable X.
Since the inequality in (20) holds for an arbitrary random
˜ √ ˜
˜
variable X, we deﬁne X as X + aW , where X is an arbitrary
˜ is a Gaussian random variable
but ﬁxed random variable, W
˜ ˜
whose variance is identical to the variance of W , and X, W ,
and W are independent of one another. Then, the inequality
in (20) is equivalent to the following inequalities:
(
)
2
˜ √ ˜
√
√
J(X+ aW )
d
˜
˜
+ J(X+ aW+ ϵW ) ≤0
˜ √aW )
˜
dϵ
1+ϵJ(X+
ϵ=0
ϵ=0
)
(
2
√ ˜
˜
√
J(X+ aW )
d
˜
˜
⇔
+ J(X+ a+ϵW ) ≤0
˜ √aW )
˜
dϵ
1+ϵJ(X+
ϵ=0
ϵ=0
(
)
2
˜ √aW )
˜
√
J(X+
d
˜
˜
⇔
J(X+ a+ϵW ) ≤0
+
˜ √aW )
˜
da
1+ϵJ(X+
ϵ=0
ϵ=0
√
√
˜
˜
˜
˜ + aW )2 + d J(X + aW ) ≤ 0,
(21)
⇔ J(X
da
where ⇔ denotes the equivalence between before and after
the notation.
˜
Since random variable X is arbitrary and a is an arbitrary
non-negative real-valued number in equation (21), the proof is
completed.

R EFERENCES
[1] D. Guo, S. Shamai (Shitz), and S. Verd´, “Mutual information and
u
minimum mean-square error in Gaussian channels”, IEEE Trans. Inform.
Theory, vol. 51, pp. 1261-1282, Apr. 2005.
[2] D. Guo, S. Shamai (Shitz), and S. Verd´, “Additive non-Gaussian noise
u
channels: mutual information and conditional mean estimation”, in Proc.
IEEE Int. Symp. Inform. Theory, pp. 719-723, Sep. 2005.
[3] C. Stein, “Inadmissibility of the Usual Estimator for the Mean of a
Multivariate Normal Distribution,” in Proc. Third Berkeley Symp. on
Math. Statist. and Prob., (Univ. of Calif. Press), vol. 1, pp. 197-206,
1956.
[4] H. M. Hudson, “A natural identity for exponential families with applications in multiparameter estimation”, The Annals. of Stat., vol. 6, pp.
473-484, May 1978.
[5] L. Brown, A. DasGupta, L. R. Haff, and W. E. Strawderman, “The heat
equation and Stein’s identity: Connections, applications”, Journ. of Stat.
Planning and Infer., vol. 136, pp. 2254-2278, July 2006.
[6] S. M. Kay, Fundamentals of Statistical Signal Processing: Estimation
Theory (Vol 1). Prentice-Hall, 1993.
[7] H. L. Van Trees, Detection, Estimation, and Modulation Theory: Part I.
New York: Wiley, 2001.
[8] A. J. Stam, “Some inequalities satisﬁed by the quantities of information
of Fisher and Shannon, Inf. & Contr., vol. 2, no. 2, Jun. 1959.
[9] T. M. Cover and J. A. Thomas, Elements of Information Theory. New
York: Wiley, 2nd edition, 2006.
[10] S. K. Kattumannil, “On Stein’s identity and its applications”, Stat. and
Prob. Letters, vol. 79, pp. 1444-1449, June 2009.
[11] S. Park, E. Serpedin, and K. Qaraqe, “On the equivalence between Stein
and de Bruijn identities,” IEEE Trans. Inf. Theory, Oct. 2011 (submitted).
[12] C. N. Morris, “Natural exponential families with quadratic variance
functions: statistical theory”, The Annals of Stat., vol. 11, pp. 515-529,
June 1983.
[13] M. H. Costa, “A new entropy power inequality”, IEEE Trans. Inform.
Theory, vol. 31, pp. 751-760, Nov. 1985.
[14] T. Liu and P. Viswanath, “An Extremal Inequality Motivated by Multiterminal Information-Theoretic Problems,” IEEE Trans. Inf. Theory, vol.
53, no. 5, pp. 1839 - 1851, May 2007.

B. Applications in Other Areas
Due to space limitations, we only detailed Costa’s EPI
as an application of the proposed results. However, the proposed results can be further exploited in a variety of other
applications information theory, statistical signal processing,
and wireless communications. First, both CRLB and BCRLB
can be derived based on Theorems 1, 2, 4, 5, and 6 (for

5

