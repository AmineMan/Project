Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Tue Jun 12 17:28:38 2012
ModDate:        Tue Jun 19 12:55:09 2012
Tagged:         no
Pages:          3
Encrypted:      no
Page size:      595.276 x 841.89 pts (A4)
File size:      352322 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566845

An Afﬁne Invariant k -Nearest Neighbor Regression
Estimate
G´ rard Biau
e

Luc Devroye

Vida Dujmovi´
c

Universit´ Pierre et Marie Curie
e
& Ecole Normale Superieure, France
Email: gerard.biau@upmc.fr

School of Computer Science
McGill University
Montreal, Canada
Email: lucdevroye@gmail.com

Carleton University
Ottawa, Canada
Email: vida@scs.carleton.ca

Adam Krzy˙ ak
z
Department of Computer Science
and Software Engineering
Montreal, Canada
Email: krzyzak@cs.concordia.ca
for all possible distributions of (X, Y ) with E|Y |p < ∞, whenever
kn → ∞ and kn /n → 0 as n → ∞. Thus, the k-NN estimate is
asymptotically optimal, for all distributions, or as we say, it is Lp
universally consistent.

Abstract—We propose a new k-NN regression estimate based on a
data-dependent metric in Rd which is used to deﬁne the k-nearest
neighbors of a given point. The metric is invariant under all
afﬁne transformations. With this metric, the standard k-nearest
neighbor regression estimate is asymptotically consistent under
the usual conditions on k, and minimal requirements on the input
data.

The drawback of k-NN estimate is that any afﬁne transformation
of the coordinate axes affects the k-NN estimate through the norm
. . Thus, we want to construct a regression estimate rn with the
following property

I. I NTRODUCTION
The prediction error of standard nonparametric regression methods
may be adversely affected by a linear transformation of the coordinate
axes. It typically happens with the popular k-nearest neighbor (k-NN)
regression estimate deﬁned by Fix and Hodges [8], [9], Cover and
Hart [4], Cover [2], [3], where a mere rescaling of the coordinate
axes changes the output of the estimate. This is clearly undesirable,
especially in applications where data measurements represent different physical items, for instance, weight, blood pressure, sugar level,
and the age of the patient. In this example, a simple change in, say,
the unit of weight will completely alter the results, and will thus
force the statistician to preprocess the input data prior to computing
the k-NN estimate.

rn (x; Dn ) = rn (T (x); Dn ),

where Dn = (T (X1 ), Y1 ), . . . , (T (Xn ), Yn ). We call rn afﬁne
invariant. In Rd , this invariance of k-NN estimates, it obtained
by deﬁning data-dependent afﬁne invariant distance measure. In
the next section we deﬁne an afﬁne invariant estimation procedure
featuring (I.1) which coincides with the k-NN estimate, and discuss
its consistency in Section 3.
The literature on afﬁne invariance in nonparametric estimation has
ˆ
been quite rich. The easiest approach is to compute Mn , the standard
estimate of the d × d covariance matrix of X, and to replace the data
ˆ −1
ˆ −1
{(X1 , Y1 ), . . . , (Xn , Yn )} by {(Mn X1 , Y1 ), . . . , (Mn Xn , Yn )}.
Any nonparametric procedure that uses the new data is afﬁne invariant. That method has been discussed in [6] and [14] for pattern
recognition and regression, respectively, but it has also been used in
kernel density estimation (see, e.g., Samanta [28]). Computational
issues aside, this procedure requires at the very least that the second
moment of X exists, which is something we are not willing to
assume. Our approach takes ideas from the classical nonparametric
literature using concepts such as data depth or multivariate ranks.

In this paper, we propose a version of the k-NN regression estimate
that is not affected by afﬁne transformations of the coordinate axes.
Such a modiﬁcation could save the user a subjective preprocessing
step and would simplify computation of the estimate.
We assume that the data set is a collection of independent and
identically distributed Rd × R-valued random variables Dn =
{(X1 , Y1 ), . . . , (Xn , Yn )}, independent of and with the same distribution as a pair (X, Y ) satisfying E|Y | < ∞. The space Rd is
equipped with the standard Euclidean norm . . For ﬁxed x ∈ Rd ,
our goal is to estimate the regression function r(x) = E[Y |X = x]
from the data Dn . The usual k-NN regression estimate is given by
rn (x; Dn ) =

1
kn

(I.1)

There have been attempts to obtain invariance to other transformations. They include invariance under monotone transformations of
the coordinate axes, e. g., based on the coordinatewise ranks of the
Xi ’s. One can show using an Lp norm on the d-vectors of differences
between ranks that the classical k-NN regression function estimate is
universally consistent in the sense of Stone [29], see Olshen [23] and
Devroye [5], Gordon and Olshen [12], [13], Devroye and Krzy˙ ak
z
[7], and Biau and Devroye [1]. Other important examples include the
rules based upon statistically equivalent blocks, see Quesenberry and
Gessaman [26], Gessaman [10], Gessaman and Gessaman [11], and
Devroye, Gy¨ rﬁ, and Lugosi [6].
o

kn

Y(i) (x),
i=1

where (X(1) (x), Y(1) (x)), . . . , (X(n) (x), Y(n) (x)) is a reordering of
the data according to increasing distances Xi −x of the Xi ’s to x.
(If distance ties occur, a tie-breaking strategy is used. For example,
if Xi − x = Xj − x , Xi may be declared “closer” if i <
j, i.e., the tie-breaking is done by indices.) For simplicity, we will
suppress Dn in the notation and write rn (x) instead of rn (x; Dn ).
Stone [29] showed that, for all p ≥ 1, E[rn (X) − r(X)]p → 0

1

II. A N AFFINE INVARIANT k-NN ESTIMATE

hold anymore when out transformation is applied, so a single data
point may have considerable inﬂuence on the estimate. Whether our
regression estimate remains universally consistent is an open problem.
The consistency can be shown with additional constraints, namely
that X has a density (not necessarily continuous) and that r is µalmost surely continuous. The outline of the proof is given in the
next section. The complete proof will be presented elsewhere.

The k-NN estimate discussed in this paper is based upon the notion of
empirical distance. We assume that the distribution of X is absolutely
continuous with respect to the Lebesgue measure on Rd and that
n ≥ d. Thus any collection Xi1 , . . . , Xid (1 ≤ i1 < i2 < . . . <
id ≤ n) of d points among X1 , . . . , Xn are in general position with
probability one. Consequently, there exists with probability one a
unique hyperplane in Rd containing these d random points, and we
denote it by H(Xi1 , . . . , Xid ).

III. O UTLINE OF PROOF OF THE THEOREM

We can now deﬁne the empirical distance between d-vectors x and
x by
ρn (x, x )
=

1≤i1 <...<id ≤n

1{segment

(x,x )

intersects the hyperplaneH(Xi1 ,...,Xid )} .

In other words, ρn (x, x ) counts the number of hyperplanes in Rd
passing through d out of the points X1 , . . . , Xn , that are separating
x and x . Intuitively, “near” points have fewer intersections.

Let X the random variable with distribution µ and let f be its density
with respect to the Lebesgue measure. For every ε > 0, let Bx,ε =
{y ∈ Rd : y − x ≤ ε} be the closed Euclidean ball with center at
x and radius ε. Let Ac be the complement of a subset A of Rd .
We use Jensen’s inequality to bound the Lp error of the regression
estimate
E |rn (x) − r(x)|p ≤ 2p−1 E

This concept of distance based on hyperplanes is known in the
multivariate rank tests literature as the empirical lift-interdirection
function (Randles [27], Oja [21], Hallin and Paindaveine [15],
Oja and Paindaveine [22]). It was introduced in Hettmansperger,
M¨ tt¨ nen, and Oja [16] (but not investigated), and independently
o o
suggested as an afﬁne invariant alternative to ordinary metrics in the
monograph of Devroye, G¨ orﬁ, and Lugosi [6, Section 11.6]. We
y
use the notion of distance even though, for a ﬁxed sample of size n,
ρn is only deﬁned with probability one and strictly speaking is not
a distance measure. Nevertheless, this empirical distance is invariant
under afﬁne transformations x → Ax+b, where A is some arbitrary
nonsingular linear map and b any offset vector (see, for instance, Oja
and Paindaveine [22, Section 2.4]).

1
kn

Y(i) (x) − r X(i) (x)
i=1

1
+ 2p−1 E
kn
:= 2p−1 In +

p

kn

kn

r X(i) (x) − r(x)

p

i=1
2p−1 Jn .

Next we bound Jn by
1
kn

2p ζ p E

kn

1{

X(i) (x)−x >ε}

i=1
p

+

sup

|r(y) − r(x)|

y∈Rd : y−x ≤ε

(since |Y | ≤ ζ).

Let us now deﬁne our k-NN regression estimate. Pick x ∈ Rd and let
ρn (x, Xi ) be the empirical distance between x and some observation
Xi in the sample X1 , . . . , Xn , i. e., the number of hyperplanes in
Rd passing through d out of the observations X1 , . . . , Xn , that are
cutting the segment (x, Xi )). Thus our k-NN estimate still takes the
familiar form
kn
1
rn (x) =
Y(i) (x),
kn i=1

The ﬁrst term converges to 0 by kn /n → 0 while the second therm
can be made arbitrarily small by letting ε → 0 since r is continuous
at x. Thus we have shown that Jn → 0 as n → ∞.
We next use Marcinkiewicz and Zygmund [18] inequality to bound
In .
In ≤ Cp E

with the important difference that now the data set
(X1 , Y1 ), . . . , (Xn , Yn ) is reordered according to increasing
values of the empirical distances ρn (x, Xi ), not the original
Euclidean metric. By construction, the estimate rn has the desired
afﬁne invariance property and, moreover, it coincides with the
standard (Euclidean) estimate in dimension d = 1. We have
the following universal consistency result, where µ denotes the
distribution of the random variable X.
Theorem 2.1 (Pointwise Lp consistency): Assume that X has a
probability density, that Y is bounded, and that the regression
function r is µ-almost surely continuous. Then, for µ-almost all
x ∈ Rd and all p ≥ 1, if kn → ∞ and kn /n → 0,

≤

1
2
kn

(2ζ)p Cp
p/2

p/2

kn

Y(i) (x) − r X(i) (x)

2

i=1

(since |Y | ≤ ζ),

kn

where Cp is some positive constant depending only on p. Consequently, In → 0 as kn → ∞, and this concludes the proof of the
theorem.

R EFERENCES
[1] G. Biau and L. Devroye. On the layered nearest neighbour estimate,
the bagged nearest neighbour estimate and the random forest method
in regression and classiﬁcation. Journal of Multivariate Analysis,
101:2499–2518, 2010.
[2] T.M. Cover. Estimation by the nearest neighbor rule. IEEE Transactions
on Information Theory, 14:50–55, 1968.
[3] T.M. Cover. Rates of convergence for nearest neighbor procedures.
In Proceedings of the Hawaii International Conference on Systems
Sciences, pages 413–415, Honolulu, 1968.
[4] T.M. Cover and P.E. Hart. Nearest neighbor pattern classiﬁcation. IEEE
Transactions on Information Theory, 13:21–27, 1967.
[5] L. Devroye. A universal k-nearest neighbor procedure in discrimination. In B.V. Dasarathy, editor, Nearest Neighbor Pattern Classiﬁcation
Techniques, pages 101–106, Los Alamos, 1991. IEEE Computer Society
Press.
[6] L. Devroye, L. Gy¨ rﬁ, and G. Lugosi. A Probabilistic Theory of Pattern
o
Recognition. Springer-Verlag, New York, 1996.
[7] L. Devroye and A. Krzy˙ ak. New multivariate product density estimaz
tors. Journal of Multivariate Analysis, 82:88–110, 2002.

E |rn (x) − r(x)|p → 0 as n → ∞.
Theorem 2.1 and the Lebesgue dominated convergence theorem lead
to the follwing conclusion.
Corollary 2.1 (Global Lp consistency): Assume that X has a probability density, that Y is bounded, and that the regression function r
is µ-almost surely continuous. Then, for all p ≥ 1, if kn → ∞ and
kn /n → 0,
E |rn (X) − r(X)|p → 0 as n → ∞.
Consistency proof of the standard nearest neighbor estimate given in
[29] uses a key result that a given point cannot be the nearest neighbor
of more than a constant number other points. This condition does not

2

[8] E. Fix and J.L. Hodges. Discriminatory analysis. Nonparametric
discrimination: Consistency properties. Technical Report 4, Project
Number 21-49-004, USAF School of Aviation Medicine, Randolph
Field, Texas, 1951.
[9] E. Fix and J.L. Hodges. Discriminatory analysis: Small sample performance. Technical Report 11, Project Number 21-49-004, USAF School
of Aviation Medicine, Randolph Field, Texas, 1952.
[10] M. Gessaman. A consistent nonparametric multivariate density estimator
based on statistically equivalent blocks. The Annals of Mathematical
Statistics, 41:1344-1346, 1970.
[11] M. Gessaman and P. Gessaman. A comparison of some multivariate discrimination procedures. Journal of the American Statistical Association,
67:468-472, 1972.
[12] L. Gordon and R.A. Olshen. Asymptotically efﬁcient solutions to the
classiﬁcation problem. The Annals of Statistics, 6:515–533, 1978.
[13] L. Gordon and R.A. Olshen. Consistent nonparametric regression
from recursive partitioning schemes. Journal of Multivariate Analysis,
10:611–627, 1980.
[14] L. Gy¨ rﬁ, M. Kohler, A. Krzy˙ ak, and H. Walk. A Distribution-Free
o
z
Theory of Nonparametric Regression. Springer-Verlag, New York, 2002.
[15] M. Hallin and D. Paindaveine. Optimal tests for multivariate location
based on interdirections and pseudo-Mahalanobis ranks. The Annals of
Statistics, 30:1103–1133, 2002.
[16] T.P. Hettmansperger, J. M¨ tt¨ nen, and H. Oja. The geometry of the afﬁne
o o
invariant multivariate sign and rank methods. Journal of Nonparametric
Statistics, 11:271–285, 1998.
[17] W. Hoeffding. Probability inequalities for sums of bounded random
variables. Journal of the American Statistical Association, 58:13–30,
1963.
[18] J. Marcinkiewicz and A. Zygmund. Sur les fonctions ind´ pendantes.
e
Fundamenta Mathematicae, 29:60–90, 1937.
[19] C. McDiarmid. On the method of bounded differences. In J. Siemons,
editor, Surveys in Combinatorics, 1989, London Mathematical Society
Lecture Note Series 141, pages 148–188. Cambridge University Press,
1989.
[20] K.S. Miller. Multidimensional Gaussian Distributions. Wiley, New York,
1964.
[21] H. Oja. Afﬁne invariant multivariate sign and rank tests and corresponding estimates: A review. Scandinavian Journal of Statistics, 26:319–343,
1999.
[22] H. Oja and D. Paindaveine. Optimal signed-rank tests based on
hyperplanes. Journal of Statistical Planning and Inference, 135:300–
323, 2005.
[23] R. Olshen. Comments on a paper by C.J. Stone. The Annals of Statistics,
5:632–633, 1977.
[24] K.R. Parthasarathy. Probability Measures on Metric Spaces. AMS
Chelsea Publishing, Providence, 2005.
[25] V.V. Petrov. Sums of Independent Random Variables. Springer-Verlag,
Berlin, 1975.
[26] C. Quesenberry and M. Gessaman. Nonparametric discrimination using
tolerance regions. The Annals of Mathematical Statistics, 39:664–673,
1968.
[27] R.H. Randles. A distribution-free multivariate sign test based on interdirections. Journal of the American Statistical Association, 84:1045–1050,
1989.
[28] M. Samanta. A note on uniform strong convergence of bivariate density
estimates. Zeitschrift f¨ r Wahrscheinlichkeitstheorie und verwandte
u
Gebiete, 28:85–88, 1974.
[29] C.J. Stone. Consistent nonparametric regression. The Annals of
Statistics, 5:595–645, 1977.
[30] R.L. Wheeden and A. Zygmund. Measure and Integral. An Introduction
to Real Analysis. Marcel Dekker, New York, 1977.

3

