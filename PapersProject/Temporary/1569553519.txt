Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Sun May 13 16:26:05 2012
ModDate:        Tue Jun 19 12:54:14 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      380664 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569553519

Information Masking and Ampliﬁcation:
The Source Coding Setting
Thomas A. Courtade
Department of Electrical Engineering
University of California, Los Angeles
Email: tacourta@ee.ucla.edu
between amplifying (or revealing) information about X n while
simultaneously masking the side information Y n .
Similar problems have been previously studied in the
literature on secrecy and privacy. For example, Sankar et
al. determine the utility-privacy tradeoff for the case of a single
encoder in [3]. In their setting, the random variable X is a
vector with a given set of coordinates that should be masked
and another set that should be revealed (up to a prescribed
distortion). In this context, our study of the ampliﬁcationmasking tradeoff is a distributed version of [3], in which utility
is measured by the information revealed about the database
X n . The problem we consider is distinct from those typically
studied in the information-theoretic secrecy literature, in that
the masking (i.e., equivocation) constraint corresponds to the
intended decoder, rather than an eavesdropper.
We remark that the present paper is inspired in part by the
recent, complementary works [4] and [5] which respectively
study ampliﬁcation and masking of channel state information.
We borrow our terminology from those works. We also note
that ampliﬁcation of channel state information subject to
masking constraints was investigated in [6].
This paper is organized as follows. Section II formally
deﬁnes the problems considered and delivers our main results.
The corresponding proofs are given in Section III. Final
remarks and directions for future work are left to Section IV.

Abstract—The complementary problems of masking and amplifying channel state information in the Gel’fand-Pinsker channel have recently been solved by Merhav and Shamai, and Kim
et al., respectively. In this paper, we study a related source coding
problem. Speciﬁcally, we consider the two-encoder source coding
setting where one source is to be ampliﬁed, while the other source
is to be masked. In general, there is a tension between these two
objectives which is characterized by the ampliﬁcation-masking
tradeoff. In this paper, we give a single-letter description of this
tradeoff.
We apply this result, together with a recent theorem by
Courtade and Weissman on multiterminal source coding, to solve
a fundamental entropy characterization problem.

I. I NTRODUCTION
The well known source coding with side information problem has an achievable rate region given by
Rx ≥ H(X|U ), Ry ≥ I(Y ; U )
as originally shown by Ahlswede and K¨ rner [1], and indeo
pendently by Wyner [2]. In this setting, the side information
encoder merely serves as a helper with the sole purpose of
aiding in the recovery of X n at the decoder. However, for
given rates (Rx , Ry ), there may be many different coding
schemes which permit recovery of X n at the decoder. In
some cases, it may be desirable to select a coding scheme
that reveals very little information about the side information
Y n to the decoder. We refer to this objective as masking the
side information.
To motivate this setting, consider the following example.
Suppose X is an attribute of an online customer that an
advertiser would like to speciﬁcally target (e.g., gender), and
Y is other detailed information about the same customer (e.g.,
credit history). Companies A and B separately have databases
X n and Y n corresponding to n different customers (the
databases could be indexed by IP address, for example). The
advertiser pays Companies A and B to learn as much about
the database X n as possible. Now, suppose governing laws
prohibit the database Y n from being revealed too extensively.
In this case, the material given to the advertiser must be chosen
so that at most a prescribed amount of information is revealed
about Y n .
In general, a masking constraint on Y n may render nearlossless reconstruction of X n impossible. This motivates the
study the ampliﬁcation-masking tradeoff. That is, the tradeoff

II. P ROBLEM S TATEMENT AND R ESULTS
Throughout this paper we adopt notational conventions that
are standard in the literature. Speciﬁcally, random variables are
denoted by capital letters (e.g., X) and their corresponding
alphabets are denoted by corresponding calligraphic letters
(e.g., X ). We abbreviate a sequence (X1 , . . . , Xn ) of n
random variables by X n , and we let δ( ) represent a quantity
satisfying lim →0 δ( ) = 0.
For a joint distribution p(x, y) on ﬁnite alphabets X × Y,
consider the source coding setting where separate Encoders
1 and 2 have access to the sequences X n and Y n , respectively. We make the standard assumption that the sequences
(X n , Y n ) are drawn i.i.d. according to p(x, y) (i.e., X n , Y n ∼
n
i=1 p(xi , yi )), and n can be taken arbitrarily large.
The ﬁrst of the following three subsections characterizes
the ampliﬁcation-masking tradeoff. This result is applied to
solve a fundamental entropy characterization in the second
subsection. The ﬁnal subsection comments on the connection

1

between information ampliﬁcation and list decoding. Proofs
of the main results are postponed until Section III.

for some joint distribution p(x, y, u) = p(x, y)p(u|y), where
|U| ≤ |Y| + 1.

A. The Ampliﬁcation-Masking Tradeoff
Formally, a (2nRx , 2nRy , n) code is deﬁned by its encoding
functions

B. An Entropy Characterization Result
As we previously noted, the ampliﬁcation-masking tradeoff solves a multi-letter entropy characterization problem by
reducing it to single-letter form. The reader is directed to [7]
for an introduction to entropy characterization problems. Here,
we apply our results to yield a fundamental characterization
of the information revealed about X n and Y n , respectively,
by arbitrary encoding functions fx and fy (of rates Rx , Ry ).
Deﬁnition 3: Deﬁne the region R (Rx , Ry ) as follows. The
pair (∆X , ∆Y ) ∈ R (Rx , Ry ) if and only if, for any > 0,
there exists a (2nRx , 2nRy , n) code satisfying

fx : X n → {1, . . . , 2nRx } and fy : Y n → {1, . . . , 2nRy }.
A rate-ampliﬁcation-masking tuple (Rx , Ry , ∆A , ∆M ) is
achievable if, for any > 0, there exists a (2nRx , 2nRy , n)
code satisfying the ampliﬁcation criterion:
1
∆A ≤ I (X n ; fx (X n ), fy (Y n )) + ,
(1)
n
and the masking criterion:
1
(2)
∆M ≥ I (Y n ; fx (X n ), fy (Y n )) − .
n
Thus, we see that the ampliﬁcation-masking problem is an
entropy characterization problem similar to that considered in
[7, Chapter 15].
Deﬁnition 1: The achievable ampliﬁcation-masking region
RAM is the closure of the set of all achievable rateampliﬁcation-masking tuples (Rx , Ry , ∆A , ∆M ).
Theorem 1: RAM consists of the rate-ampliﬁcationmasking tuples (Rx , Ry , ∆A , ∆M ) satisfying

Rx ≥ ∆A − I(X; U )



Ry ≥ I(Y ; U )
(3)
∆M ≥ max {I(Y ; U, X) + ∆A − H(X), I(Y ; U )} 


∆A ≤ H(X).

1
I(X n ; fx (X n ), fy (Y n )) ≤ , and
n
1
∆Y − I(Y n ; fx (X n ), fy (Y n )) ≤ .
n

∆X −

Let R (Rx , Ry ) be the closure of R (Rx , Ry ).
Ultimately we obtain a single-letter description of
R (Rx , Ry ). However, in order to do so, we require some
notation. To this end, let:
RAM (Rx , Ry ) = {(∆X , ∆Y ) : (Rx , Ry , ∆X , ∆Y ) ∈ RAM } .
Symmetrically, let RM A be the region where X n is subject
to masking ∆X and Y n is subject to ampliﬁcation ∆Y . Let
RM A (Rx , Ry ) = {(∆X , ∆Y ) : (Rx , Ry , ∆X , ∆Y ) ∈ RM A } .

for some joint distribution p(x, y, u) = p(x, y)p(u|y), where
|U| ≤ |Y| + 1.
Observe that RAM characterizes the entire tradeoff between
amplifying X n and masking Y n . We remark that maximum
ampliﬁcation ∆A = H(X) does not necessarily imply that
X n can be recovered near-losslessly at the encoder. However,
if an application demands near lossless reproduction of the
sequence X n , Theorem 1 can be strengthened to include this
case. To this end, deﬁne a rate-masking triple (Rx , Ry , ∆M ) to
be achievable if, for any > 0, there exists a (2nRx , 2nRy , n)
code satisfying the masking criterion (2), and a decoding
function

Finally, let RAA (Rx , Ry ) consist of all pairs (∆X , ∆Y )
satisfying

ˆ
X n : {1, 2, . . . , 2nRx } × {1, 2, . . . , 2nRy } → X n

where |Ux | ≤ |X |, |Uy | ≤ |Y|, and |Q| ≤ 5.
Theorem 2: The region R (Rx , Ry ) has a single-letter
characterization given by

Rx ≥ I(Ux ; X|Uy , Q)
Ry ≥ I(Uy ; Y |Ux , Q)

Rx + Ry ≥ I(Ux , Uy ; X, Y |Q)
∆X ≤ I(X; Ux , Uy |Q)
∆Y ≤ I(Y ; Ux , Uy |Q)

for some joint distribution of the form
p(x, y, ux , uy , q) = p(x, y)p(ux |x, q)p(uy |y, q)p(q),

which satisﬁes the decoding-error criterion
ˆ
Pr X n = X n (fx (X n ), fy (Y n )) ≤ .

R (Rx , Ry ) =

RAM (Rx , Ry ) ∩ RM A (Rx , Ry ) ∩ RAA (Rx , Ry ).

Deﬁnition 2: The achievable rate-masking region RM is
the closure of the set of all achievable rate-masking triples
(Rx , Ry , ∆M ).
Corollary 1: RM consists of the rate-masking triples
(Rx , Ry , ∆M ) satisfying

Moreover, restriction of the encoding functions to vectorquantization and/or random binning is sufﬁcient to achieve
any point in R (Rx , Ry ).
The second statement of Theorem 2 is notable since it
states that relatively simple encoding functions (i.e., vector
quantization and/or binning) can asymptotically reveal the
same amount of information about X n and Y n , respectively,
as encoding functions that are only restricted in rate. In

Rx ≥ H(X|U )
Ry ≥ I(Y ; U )

∆M ≥ I(Y ; X, U )

2

0.9

III. P ROOFS OF M AIN R ESULTS
R⋆(0.1, 0.7)

0.8

Proof of Theorem 1:
Converse Part: Suppose
(Rx , Ry , ∆A , ∆M ) is achievable. For convenience, deﬁne
Fx = fx (X n ), Fy = fy (Y n ), and Ui = (Fy , Y i−1 ).
First, note that ∆A ≤ H(X) is trivially satisﬁed. Next, the
constraint on Rx is given by:

R⋆(0.5, 0.6)

0.7

0.6

0.5

∆Y

nRx ≥ H(Fx ) ≥ H(Fx |Fy )
n

0.4

=

R⋆(0.4, 0.4)

0.3

i=1
n

0.2

≥

0.1

i=1

H(Xi |Fy , X i−1 ) − H(X n |Fx , Fy )
H(Xi |Fy , Y i−1 , X i−1 ) − H(X n |Fx , Fy )
n

= I(X n ; Fx , Fy ) −

0
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

∆X

x=1
0
1/2.

(6)

n

nRy ≥ H(Fy ) ≥ I(Fy ; Y n ) =
n

i=1
n

I(Yi ; Fy , Y i−1 ) =

=
i=1

I(Yi ; Fy |Y i−1 )

I(Yi ; Ui ).
i=1

Similarly, we obtain the ﬁrst lower bound on ∆M :
n

n(∆M + ) ≥ I(Y n ; Fx , Fy ) ≥ I(Y n ; Fy ) =

I(Yi ; Ui ).
i=1

The second lower bound on ∆M requires slightly more work,
and can be derived as follows:

(4)

n(∆M + ) ≥ I(Y n ; Fx , Fy )

= I(Y n ; X n , Fy ) + I(X n ; Fx , Fy ) − I(X n ; Fx , Y n )

By performing a brute-force search over the auxiliary random
variables deﬁning R (Rx , Ry ) for the distribution PX,Y , we
have obtained numerical approximations of R (·, ·) for several
different pairs of (Rx , Ry ). The results are given in Figure 1.

≥ I(Y n ; X n , Fy ) + n∆A − I(X n ; Fx , Y n ) − n

(7)

n

≥

C. Connection to List Decoding
We brieﬂy comment on the connection between an ampliﬁcation constraint and list decoding. As discussed in detail in
[4], the ampliﬁcation criterion (1) is essentially equivalent to
the requirement for a list decoder
Ln : {1, . . . , 2nRx } × {1, . . . , 2nRy } → 2X

I(Xi ; Ui ).
i=1

Equality (5) follows since Xi ↔ Fy , Y i−1 ↔ X i−1 form a
Markov chain, and inequality (6) follows since ampliﬁcation
∆A is achievable.
The constraint on Ry is trivial:

contrast, this is not true for the setting of three or more
sources, as the modulo-sum problem studied by K¨ rner and
o
Marton [8] provides a counterexample where the Berger-Tung
achievability scheme [9] is not optimal. Thus, obtaining a
characterization like Theorem 2 for three or more sources
represents a formidable challenge.
We remark that the points in R (Rx , Ry ) with ∆X =
H(X) and/or ∆Y = H(Y ) also capture the more stringent
constraint(s) of near-lossless reproduction of X n and/or Y n ,
respectively. This is a consequence of Corollary 1.
To give a concrete example of R (Rx , Ry ), consider the
following joint distribution:
x=0
1/3
1/6

(5)

n

≥ n(∆A − ) −

Fig. 1. The region R (Rx , Ry ) for joint distribution PX,Y given by (4)
and three different pairs of rates. Rate pairs (Rx , Ry ) equal to (0.1, 0.7),
(0.4, 0.4), and (0.5, 0.6) deﬁne the convex regions bounded by the black,
blue, and red curves, respectively.

PX,Y (x, y)
y=0
y=1

I(Xi ; Ui )
i=1

≥

i=1
n
i=1

I(Yi ; X n , Fy |Y i−1 ) + n∆A − H(X n ) − n
I(Yi ; Xi , Ui ) + ∆A − H(Xi ) − ,

where (7) follows since ampliﬁcation ∆A is achievable.
Observing that the Markov condition Ui ↔ Yi ↔ Xi is
satisﬁed for each i, a standard timesharing argument proves
the existence of a random variable U such that U ↔ Y ↔ X
forms a Markov chain and (3) is satisﬁed.
Direct Part: Fix p(u|y) and suppose (Rx , Ry , ∆A , ∆M )
satisfy (3) with strict inequality. Next, ﬁx > 0 sufﬁciently
small so that it is less than the minimum slack in said
˜
inequalities, and set R = I(Y ; U ) + . Our achievability
scheme uses a standard random coding argument which we
sketch below.

n

with list size and probability of error respectively satisfying
log |Ln | ≤ n(H(X) − ∆A + ), and

Pr [X n ∈ Ln (fx (X n ), fy (Y n ))] ≤ .
/

Thus maximizing the ampliﬁcation of X n subject to given rate
and masking constraints can be thought of as characterizing
the best list decoder in that setting.

3

We now consider three separate cases. First, assume ∆A ≤
I(U ; X). Then,

Codebook generation. Randomly and independently, bin
the typical xn ’s uniformly into 2n(∆A −I(X;U )+ ) bins. Let
b(xn ) be the index of the bin which contains xn . For l ∈
˜
{1, . . . , 2nR }, randomly and independently generate un (l),
n
each according to i=1 pU (ui ).
Encoding. Encoder 1, upon observing the sequence X n ,
sends the corresponding bin index b(X n ) to the decoder. If
X n is not typical, an error is declared. Encoder 2, upon
˜
observing the sequence Y n , ﬁnds an L ∈ {1, . . . , 2nR } such
n
n
that (Y , U (L)) are jointly -typical, and sends the unique
index L to the decoder. If more than one such L exists, ties
are broken arbitrarily. If no such L exists, then an error is
declared.
This coding scheme clearly satisﬁes the given rates. Further,
each encoder errs with arbitrarily small probability as n → ∞.
Hence, we only need to check that the ampliﬁcation and
masking constraints are satisﬁed. To this end, let C be the
random codebook. We ﬁrst check that the ampliﬁcation and
masking constraints are separately satisﬁed when averaged
over random codebooks C.
To see that the (averaged) ampliﬁcation constraint is satisﬁed, consider the following:
I(X n ; Fx , Fy |C) = H(X n |C) − H(X n |b(X n ), L, C)
≥ nH(X) − n(H(X) − ∆A + δ( ))

I(Y ; X, U ) + ∆A − H(X) ≤ I(Y ; X, U ) − H(X|U )
= I(Y ; U ) − H(X|Y ),

and (10) becomes
I(Y n ; Fx , Fy |C) ≤ nI(Y ; U ) − I(X n ; b(X n )|Y n , C) + n2
≤ nI(Y ; U ) + n2 .

Next, suppose that ∆A ≥ I(X; U ) + H(X|Y ). In this case,
there are greater than 2n(H(X|Y )+ ) bins in which the X n
sequences are distributed. Hence, knowing Y n and b(X n )
is sufﬁcient to determine X n with high probability (i.e.,
we have a Slepian-Wolf binning at Encoder 1). Therefore,
H(X n |Y n , b(X n ), C) ≤ n , and (10) becomes
I(Y n ; Fx , Fy |C) ≤ n(I(Y ; X, U ) + ∆A − H(X)) + n3 .

Finally, suppose ∆A = I(X; U ) + θH(X|Y ) for some
θ ∈ [0, 1]. In this case, we can timeshare between a code
C1 designed for ampliﬁcation ∆A = I(X; U ) with probability
θ, and a code C2 designed for ampliﬁcation ∆A = I(X; U ) +
H(X|Y ) with probability 1 − θ to obtain a code C with the
same average rates and averaged ampliﬁcation
I(X n ; Fx , Fy |C)

(8)

= θI(X n ; Fx , Fy |C1 ) + (1 − θ)I(X n ; Fx , Fy |C2 )

= n(∆A − δ( )),

≥ n(I(X; U ) + θH(X|Y ) − δ( )) = n(∆A − δ( )).

where (8) follows since X n is independent of C and, averaged over codebooks, there are at most 2n(H(X)−∆A +δ( ))
sequences xn in bin b(X n ) which are typical with U n (L),
˜
where L ∈ {1, . . . , 2nR }. The details are given in [10].
We now turn our attention to the masking criterion. First
note the following inequality:
n

n

n

Then, applying the inequalities obtained in the previous two
cases, we obtain:
I(Y n ; Fx , Fy |C)

= θI(Y n ; Fx , Fy |C1 ) + (1 − θ)I(Y n ; Fx , Fy |C2 )

≤ θnI(Y ; U ) + (1 − θ)n(I(Y ; X, U ) + ∆A − H(X)) + 3n

n

I(Y ; Fx , Fy |C) = I(Y ; L|C) + I(Y ; b(X )|L, C)

= nI(Y ; U ) + 3n .

≤ I(Y n ; L|C) + H(b(X n )|C) − H(b(X n )|Y n , C)

Combining these three cases proves that
1
I(Y n ; Fx , Fy |C)
n
≤ max{I(Y ; U, X) + ∆A − H(X), I(Y ; U )} + 3

= I(Y n ; L|C) + I(X n ; Y n ) − H(X n ) + H(b(X n )|C)
− H(b(X n )|Y n , C) + H(X n |Y n )

≤ I(Y n ; L|C) + I(X n ; Y n ) − H(X n ) + H(b(X n )|C)
− I(b(X n ); X n |Y n , C) + H(X n |Y n )
n

n

n

n

= I(Y ; L|C) + I(X ; Y ) − H(X ) + H(b(X )|C)
n

n

≤ ∆M + 3 .

n

n

+ H(X |Y , b(X ), C)

To show that there exists a code which satisﬁes the ampliﬁcation and masking constraints simultaneously, we con¯
struct a super-code C of blocklength N n by concatenating
N randomly, independently chosen codes of length n (each
constructed as described above). By the weak law of large
numbers and independence of the concatenated coded blocks,

(9)

Two of the terms in (9) can be bounded as follows: First, since
˜
L ∈ {1, . . . , 2nR }, we have
˜
I(Y n ; L|C) ≤ nR = n(I(Y ; U ) + ).

Pr

n(∆A −I(X;U )+ )

Second, there are 2
bins at Encoder 1 by
construction, and hence H(b(X n )|C) ≤ n(∆A −I(X; U )+ ).
Therefore, substituting into (9) and simplifying, we have:
I(Y n ; Fx , Fy |C) ≤ n(I(Y ; U, X) + ∆A − H(X))

+ H(X n |Y n , b(X n ), C) + n2 .

Pr

1
¯ ¯ ¯ ¯
I(X N n ; Fx , Fy |C = c) > ∆A − δ( )
Nn
1
¯ ¯ ¯ ¯
I(Y N n ; Fx , Fy |C = c) < ∆M + δ( )
c:
¯
Nn
c:
¯

≥ 3/4
≥ 3/4

for N and n sufﬁciently large. Thus, there must exist one
super-code which simultaneously satisﬁes both desired constraints. This completes the proof that (Rx , Ry , ∆A , ∆M ) is

(10)

4

achievable. Finally, we invoke the Support Lemma [7] to see
that |Y| − 1 letters are sufﬁcient to preserve p(y). Plus, we
require two more letters to preserve the values of H(X|U )
and I(Y ; U |X).
Proof of Corollary 1: See [10].
Proof of Theorem 2: First, we remark that the strengthened version of [11, Theorem 6] states that RAA (Rx , Ry )
is the closure of pairs (∆X , ∆Y ) such that there exists a
(2nRx , 2nRy , n) code satisfying

satisfying
1
I(X n ; fx (X n ), fy (Y n )) ≤ δ( ), and
n
1
∆Y − I(Y n ; fx (X n ), fy (Y n )) ≤ δ( ).
n

∆X −

IV. C ONCLUDING R EMARKS
In this paper, we considered a setting where two separate encoders have access to correlated sources. We gave a
complete characterization of the tradeoff between amplifying
information about one source while simultaneously masking
another. By combining this result with recent results by
Courtade and Weissman [11], we precisely characterized the
amount of information that can be revealed about X n and
Y n by any encoding functions satisfying given rates. There
are three notable points here: (i) this multi-letter entropy
characterization problem admits a single-letter solution, (ii)
restriction of encoding functions to vector quantization and/or
random binning is sufﬁcient to achieve any point the region,
and (iii) this simple characterization does not extend to three
or more sources/encoders.
Finally, we remark that in the state ampliﬁcation and
masking problems considered in [4] and [5], the authors obtain
explicit characterizations of the achievable regions when the
channel state and noise are independent Gaussian random
variables. Presumably, this could also be accomplished in our
setting using known results on Gaussian multiterminal source
coding, however, a compete investigation into this matter is
beyond the scope of this paper

1
I(X n ; fx (X n ), fy (Y n )) + ,
n
1
∆Y ≤ I(Y n ; fx (X n ), fy (Y n )) +
n

∆X ≤

for any > 0.
Suppose (∆X , ∆Y ) ∈ R (Rx , Ry ). By deﬁnition of
R (Rx , Ry ), Theorem 1, and the above statement, (∆X , ∆Y )
also lies in each of the sets RAM (Rx , Ry ), RM A (Rx , Ry ),
and RAA (Rx , Ry ). Since each of these sets are closed by
deﬁnition, we must have
R (Rx , Ry ) ⊆

RAM (Rx , Ry ) ∩ RM A (Rx , Ry ) ∩ RAA (Rx , Ry ).

Since each point in the sets RAM (Rx , Ry ), RM A (Rx , Ry ),
and RAA (Rx , Ry ) is achievable by vector quantization and/or
random binning, the second statement of the Theorem is
proved.
To show the reverse inclusion, ﬁx
> 0 and suppose
(∆X , ∆Y ) ∈ RAM (Rx , Ry )∩RM A (Rx , Ry )∩RAA (Rx , Ry ).
This implies the existence of (2nAM Rx , 2nAM Ry , nAM ),
(2nM A Rx , 2nM A Ry , nM A ), and (2nAA Rx , 2nAA Ry , nAA ) codes
satisfying:
∆X ≤
∆Y ≥
∆X ≥
∆Y ≤
∆X ≤
∆Y ≤

1
AM
AM
I(X nAM ; fx (X nAM ), fy (Y nAM )) +
nAM
1
AM
AM
I(Y nAM ; fx (X nAM ), fy (Y nAM )) −
nAM
1
M
M
I(X nM A ; fx A (X nM A ), fy A (Y nM A )) −
nM A
1
M
M
I(Y nM A ; fx A (X nM A ), fy A (Y nM A )) +
nM A
1
AA
AA
I(X nAA ; fx (X nAA ), fy (Y nAA )) + ,
nAA
1
AA
AA
I(Y nAA ; fx (X nAA ), fy (Y nAA )) + .
nAA

ACKNOWLEDGMENT
The author gratefully acknowledges the conversations with
Tsachy Weissman and comments by an anonymous reviewer
which contributed to this paper.

,

R EFERENCES

.

[1] R. Ahlswede and J. Korner, “Source coding with side information and
a converse for degraded broadcast channels,” Information Theory, IEEE
Transactions on, vol. 21, no. 6, pp. 629 – 637, Nov 1975.
[2] A. Wyner, “On source coding with side information at the decoder,” Inf.
Theory, IEEE Trans. on, vol. 21, no. 3, pp. 294 – 300, May 1975.
[3] L. Sankar, S. R. Rajagopalan, and H. V. Poor, “A theory of privacy and
utility in databases,” CoRR, vol. abs/1102.3751v1, 2011.
[4] Y.-H. Kim, A. Sutivong, and T. Cover, “State ampliﬁcation,” Information
Theory, IEEE Transactions on, vol. 54, no. 5, pp. 1850 –1859, may 2008.
[5] N. Merhav and S. Shamai, “Information rates subject to state masking,”
Inf. Theory, IEEE Trans. on, vol. 53, no. 6, pp. 2254 –2261, june 2007.
[6] O. O. Koyluoglu, R. Soundararajan, and S. Vishwanath, “State ampliﬁcation subject to masking constraints,” CoRR, vol. abs/1112.4090, 2011.
[7] I. Csiszar and J. Korner, Information Theory: Coding Theorems for
Discrete Memoryless Systems. New York: Academic Press, 1981.
[8] J. Korner and K. Marton, “How to encode the modulo-two sum of binary
sources (corresp.),” Information Theory, IEEE Transactions on, vol. 25,
no. 2, pp. 219 – 221, Mar 1979.
[9] T. Berger, Multiterminal Source Coding. In G. Longo (Ed.), The Information Theory Approach to Comms. New York: Springer-Verlag,
1977.
[10] T. A. Courtade, “Information masking and ampliﬁcation: The source
coding setting,” CoRR, vol. abs/1204.5710, 2012.
[11] T. A. Courtade and T. Weissman, “Multiterminal source coding under
logarithmic loss,” CoRR, vol. abs/1110.3069, 2011.

,
,

M
M
Also, by taking fx M , fy M to be constants, we trivially have
nM M R x nM M R y
a (2
,2
, nM M ) code satisfying

1

M
M
I(X nM M ; fx M (X nM M ), fy M (Y nM M )),
nM M
1
M
M
∆Y ≥
I(Y nM M ; fx M (X nM M ), fy M (Y nM M )).
nM M

∆X ≥

It is readily veriﬁed that, by an appropriate timesharing between these four codes, there exists a (2nRx , 2nRy , n) code

5

