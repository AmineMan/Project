Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Sat May 19 13:27:24 2012
ModDate:        Tue Jun 19 12:55:20 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      626457 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565853

On Network Coding Capacity
under On-Oﬀ Scheduling
Mayank Bakshi

Michelle Eﬀros

Institute of Network Coding
The Chinese University of Hong Kong
Hong Kong
Email: mayank@inc.cuhk.edu.hk

Department of Electrical Engineering
California Institute of Technology
Pasadena, CA, USA
Email: eﬀros@caltech.edu

The main result appears as Theorem 1 in Section III. We
begin by formally introducing the problem set-up.

Abstract—We investigate the tradeoﬀs between rate (bits per
channel use) and throughput (bits per time) in network coding
over networks of independent noisy or noiseless channels under
on-oﬀ scheduling. While some networks exhibit an inherent tradeoﬀ between rate and throughput at ﬁnite blocklengths, our main
result shows that rate and throughput can be simultaneously
maximized in the limit of large blocklengths.

II. Preliminaries
A. Network Model
We deﬁne a network N using a pair (G, C), where G = (V, E)
is a directed graph with vertex set V and directed edge set
E ⊆ V × V, and C = (Ce : e ∈ E) is a corresponding collection
of channels. For each e = (u, v) ∈ E, channel

I. Introduction
Consider a communication scenario in which nodes transmit
data to their respective destinations over a network of directed
links. The capacities of such networks are studied under a
variety of models. In acyclic network models, nodes are often
operated sequentially (e.g., [1], [2]), so that all incoming
information is available to each node before it begins to
transmit. In other models (e.g., [3], [4]), all nodes are operated
simultaneously in every time step.
This paper investigates the communications implications of
operating codes under this range of scenarios. Speciﬁcally,
since n uses of each channel require n|V| time steps in the ﬁrst
scenario and n time steps in the second, we choose α ∈ [1, |V|]
and investigate the “α-capacity,” which is deﬁned in Section II
to be the set of asymptotically achievable rates using codes that
employ n uses of each channel over at most αn time steps.
Large values of α make more information available to each
node prior to transmission, thereby potentially increasing the
rate (bits per channel use) that can be reliably delivered. This
beneﬁt is gained, however, at the cost of longer run time,
thereby potentially decreasing the same code’s throughput (bits
per unit time). Since the rate, R, and throughput, T , of a
code employing n uses of each channel over αn time steps
are related as T = R/α, the α-capacity captures the tradeoﬀ
between rate and throughput. Understanding this tradeoﬀ is
useful for considering scenarios where both power and time
are constrained resources. Restricting power may prohibit the
transmission of every node in every time step. Restricting
time may prohibit operating very few nodes at each time.
Investigating α-capacity sheds light on this tradeoﬀ as the
number of uses of each channel grows without bound.

Ce = (X (e) , pe (y(e) |x(e) ), Y (e) )

has transmitter u with input alphabet X (e) , receiver v with
output alphabet Y (e) , and conditional distribution pe . For each
e ∈ E, we assume that both X (e) and Y (e) include the notransmission symbol and that distribution pe satisﬁes
pe (y(e) |x(e) ) =

0
1

if (x(e)
, y(e) = ) or (x(e) = , y(e)
(e)
if x = , y(e) = .

The channels are independent, memoryless, and time-invariant
by assumption, and thus N is a multi-terminal channel characterized as







(e)
(e) (e)
(e) 


N=
X ,
pe (y |x ),
Y .




e∈E

e∈E

e∈E

Xt(e)

For each t ∈ N and e ∈ E, let
∈ X (e) and Yt(e) ∈ Y (e) ,
respectively, be the random variables denoting the transmitted
and received values for edge e at time t. For simplicity, we
shorten notation as C1:t = (C1 , . . . , Ct ); for example,
+

(e)
X1:t

(e)
(e)
X1 , . . . , Xt(e) and Y1:t

(e)
Y1 , . . . , Yt(e) .

We denote the node-v, time-t channel inputs and outputs by
Xt(v)

Xt(v,w) : (v, w) ∈ E and Yt(v)

Yt(w,v) : (w, v) ∈ E

and the full vector of channel inputs and outputs by
Xt

Xt(e) : e ∈ E and Yt

the corresponding alphabets are
X (v)

Both authors were at the California Institute of Technology when this
work was completed. This work was supported by NSF grant CCF-1018741.
Travel to present this work was supported by a grant from the University
Grants Committee (Project No. AoE/E-02/08) of the Hong Kong Special
Administrative Region, China.

w:(v,w)∈E

X

1

X (v,w)

e∈E

X

(e)

Y (v)
Y

Yt(e) : e ∈ E ;

w:(w,v)∈E

e∈E

Y (e) .

Y (w,v)

)

B. Network Solutions

however, simpler to analyze and also to implement. This work
focuses exclusively on ﬁxed-schedule codes.
We similarly denote the times at which node v ∈ V operates
by
T (v) = ∪w:(v,w)∈E T (v, w);

A network solution, also called a network code, is a mechanism for establishing connections across a network N. For
each u ∈ V, let
M(u) = {V ⊆ V \ {u} : |V| ≥ 1}

thus T (v) is the union of T (e) over all outgoing going edges
from node v.

denote the receiver sets for all possible unicast (|V| = 1) and
multicast (|V| > 1) connections from node u; then

D. α-Capacity

M = {(u, V) : u ∈ V, V ∈ M(u)}

Let n be the maximal number of uses of any single channel
under a given blocklength-m network coding solution S(N);
that is, n = maxe∈E |T (e)|. Then the performance of solution
S(N) is characterized by its error probability

is the set of transmitter and receiver-set pairs for all possible
connections. For each (u, V) ∈ M, let W (u→V) ∈ W (u→V)
denote the message to be transmitted from u to all nodes v ∈
V; further, let
W

(u→∗)

W

(∗→v)

=
=

(W

(u→V)

(W

(u→V)

ˆ
Pe (S(N)) = Pr ∪v∈V {W (∗→v)

: V ∈ M(u))

which is the probability that at least one node decodes at least
one message incorrectly, its rate vector

: (u, V) ∈ M, V ⊇ {v})

be the messages outgoing from node u and incoming to node
v, respectively; the corresponding alphabets are
W

(u→∗)

W (∗→v)

=
V∈M(u)

W

R(S(N))

(u,V)∈M:V⊇{v}

W (u→V) .

Given any m ∈ N, we deﬁne a blocklength-m network
solution S(N) to be a collection of encoders f = ( ft(e) : e ∈
E, t ∈ {1, . . . , m}) and decoders g = (g(v) : v ∈ V). For each
(u, v) and t, encoder
ft(u,v) : W (u→∗) × (Y (u) )t−1 → X (u,v)

at node u determines the transmitted random variable Xt(u,v)
as a function of outgoing messages W (u→∗) and prior channel
(u)
outputs Y1:t−1 . Decoder
g(v) : W (v→∗) × (Y (v) )m → W (∗→v)
at node v determines the node-v reconstruction

R(u→V) (S(N)) : (u, V) ∈ M

=

III. Main Result

ˆ
ˆ
W (∗→v) = (W (u→V,v) : (u, V) ∈ M, V ⊇ {v})

Our main result shows that α-capacity is invariant in α.
Theorem 1: Given a network N and any α ∈ [1, |V|],

of incoming messages W (∗→v) as a function of outgoing
(v)
messages W (v→∗) and channel outputs Y1:m .

Rα (N) = R1 (N).

C. Fixed-Schedule Codes

Proof: By the given deﬁnitions, R1 (N) ⊆ Rα (N) for each
α ∈ [1, |V|]. Thus, we need only show that Rα (N) ⊆ R1 (N).
Fix α ∈ [1, |V|], λ > 0, and R ∈ int(Rα (N)), where
int(Rα (N)) is the relative interior of set Rα (N). Since R ∈
int(Rα (N)), there exists a solution S(N) for N with rate
R(S(N) ≥ R, expansion α(S(N)) ≤ α, and error probability
Pe (S(N)) ≤ λ. Lemma 2 shows that given solution S(N)
˜
and any µ > 0, there exists a solution S(N) with rate
˜
˜
R(S(N)) ≥ (1 − 2H(λ))R, expansion α(S(N)) ≤ (1 + µ) and
error probability Pe (S(N)) ≤ λ. Let m be the blocklength of

The inclusion of the no-transmission symbol implies that
any network solution operates according to a transmission
schedule T = (T (e) : e ∈ E), where
(v)
T (v, w) = {t ∈ {1, . . . , m} : ft(v,w) (W (v→∗) , Y1:t−1 )

=

log |W (u→V) |
,
n
which is the vector describing the number of message bits per
channel use for each message, and its expansion factor
m
α(S(N)) = ,
n
which is the solution’s ratio of blocklength to channel uses.
Expansion factor α(S(N)) characterizes the ratio of rate to
throughput for a given solution. That is, any solution achieving
rate vector R and expansion factor α achieves throughput
vector T = R/α = (R(u→V) /α : (u, V) ∈ M).
Deﬁnition 1: Rate vector R = (R(u→V) : (u, V) ∈ M) is αachievable on network N if for any λ > 0 there exists a ﬁxedschedule solution S(N) with error probability Pe (S(N)) ≤ λ,
rate R(S(N)) ≥ R,1 and expansion ratio α(S(N)) ≤ α. The
α-capacity region R(N) for network N is the closure of the
set of α-achievable rates.
R(u→V) (S(N))

(u→V)

=

W (∗→v) } ,

}

describes the times at which at the channels associated with
the edge (v, w) is active. If T (e) is independent of the
message choice and channel operations, then we call S(N)
a ﬁxed-schedule code. Fixed-schedule codes are not optimal
in general: Allowing the no transmission symbol to be used
like any other symbol eﬀectively increases the alphabet size
of the channel and therefore the network capacity. They are,

1 Here R(S(N)) ≥ R is used as notational shorthand for R(u→V) (S(N)) ≥
R(u→V) for all (u, V) ∈ M

2

solution S(N); that is, solution S(N) is implemented over m
time steps and n ≥ m/(1 + µ) channel uses. Using solution
˜
˜
S(N), we build a solution S (N) implemented over m time
˜
steps and m channel uses. Solution S (N) sets each channel
˜
input precisely equal to the corresponding input in S(N)
in the active time steps and sends arbitrary symbols (other
than the no-transmission symbol) in the remaining time steps.
˜
Using this approach, R(S (N)) ≥ (1 − 2H(λ))R/(1 + µ),
˜ (N)) ≤ λ, and α(S (N)) = 1. Since λ and µ can be
˜
P e (S
made arbitrarily small, R ∈ R1 (N).
Theorem 1 demonstrates that asymptotically there is no
tradeoﬀ between rate and throughput. That is, since a rateR, expansion-α code has throughput T = R/α, and the space
of achievable rate vectors R is invariant in α, R and T can
always be simultaneously maximized by setting α to 1. It is
important to note, however, that the tradeoﬀ vanishes only as
the number of channel uses, n, grows without bound. For any
ﬁnite n and any error probability λ, increasing α may, in fact,
increase achievable rate while certainly decreasing throughput
– giving the anticipated tradeoﬀ between these two criteria.
What Theorem 1 demonstrates is that that tradeoﬀ becomes
negligible as n grows without bound.
Another interesting implication of Theorem 1 is that scalar
and vector coding are asymptotically equivalent. This result
does not contradict [5], [6], which together suggest a diﬀerence
between the two in the domain of linear coding. Instead, it
highlights a distinction between our system model and the
ones used in [5], [6], which consider only codes with a single
channel use over either a scalar or a vector alphabet.
Theorem 1 is related to [7, Theorem 2], which shows that
adding any ﬁnite collection of delays to a network has no
impact on the network’s capacity. Theorem 1 diﬀers from
this earlier result, however, in two critical ways. First, the
“delays” are here controlled by the code designer, and second,
the number of delays grows without bound as the number of
channel uses goes to inﬁnity.

by increasing the number of channel uses for the edge e by
picking n − |T (e)| time steps from the set {1, 2 . . . , m} \ T (e)
and transmitting arbitrary symbols at these time steps on the
edge e.
˜
We next design a non-overlapping solution S(N) with en˜
˜ and decoders g. Solution S(N) sequentially operates
coders f
˜
each time-1 encoder, followed by each time-2 encoder, and so
on to obtain a code that employs n uses of each channel over
|V|n time steps. For each i ∈ {1, . . . , |V|} and each t ∈ T (vi ),
let F(t, vi ) denote the time at which encoder ft(vi ) operates
under the above ordering; then
t−1

F(t, vi ) =
t =1

|{ j : t ∈ T (v j )}| + |{ j ≤ i : t ∈ T (v j )}|

and
E(t, vi ) = {(vi , w) : t ∈ T (vi , w)}.

˜
The transmission schedule T for the resulting code is
˜
T (v, w) = {F(t, v) : (v, w) ∈ E(t, v)}.

˜
For each t ∈ {1, . . . , n|V|}, let E(t) = {(v, w) ∈ E : t ∈
˜ (v, w)} be the set of edges that operate at time t under
T
˜
˜
schedule T ; all edges in set E(t) emanate from the same node
˜
by deﬁnition. Solution S(N) is deﬁned as follows.
(e)
˜
• For each t ∈ {1, . . . , n|V|} and e ∈ E \ E(t), f˜ = .
t
˜
• For each t ∈ {1, . . . , n|V|} and (v, w) ∈ E(t), ﬁnd τ such
that F(τ, v) = t. Then
(v)
f˜(v,w) (W (v→∗) , Y1:t ) =
t

˜
fτ(v,w) W (v→∗) , (Y (u,v) : j ∈ T (u), j < τ) : (u, v) ∈ E .
j

For each v ∈ V and (u, v) ∈ E, decoder g(v) (·) discards
˜
˜
channel outputs for all t
T (u, v) and then applies
decoder g(v) (·) to its outgoing messages and remaining
channel outputs.
˜
Since solution S(N) gives the same reconstructions as solution
˜
˜
S(N), Pe (S(N)) = Pe (S(N)) and R(S(N)) = R(S(N)). Since
˜
˜
solution S(N) operates over |V|n time steps, α(S(N)) = |V|.
•

IV. Supporting Results
Before proceeding to Lemma 2, which is used in the proof
of Theorem 1, we ﬁrst introduce Lemma 1, which is used in
the proof of Lemma 2. Lemma 1 shows that every solution
S(N) can be modiﬁed to obtain a solution in which at most
one node transmits at any time instant. We call such solutions
non-overlapping solutions, as deﬁned formally below.
Deﬁnition 2: A solution S(N) is a non-overlapping solution
if for each v1 , v2 ∈ V such that v1 v2 , T (v1 ) ∩ T (v2 ) = φ.
Lemma 1: Given any solution S(N), there exists a non˜
˜
overlapping solution S(N) with expansion factor α(S(N)) =
|V| that achieves the same error probability and rate
˜
˜
(Pe (S(N)) = Pe (S(N)) and R(S(N)) = R(S(N))).
Proof: Consider an arbitrary solution S(N) for network N
with vertices {v1 , . . . , v|V| }. Let n, m, f , and g be the number
of channel uses, number of time steps, encoders, and decoders,
respectively, for solution S(N), as deﬁned in Section II.
Without loss of generality, we assume that |T (e)| = n for
all e ∈ E; if T (e) < n for some e, then we modify S(N)

Lemma 2 uses Lemma 1 and the notion of a stacked network
introduced in [4] to show that any solution with expansion
factor α can be used to construct an expansion-1 solution
with similar error probability and rate. Before proceeding to
Lemma 2, we brieﬂy review the deﬁnition of stacked networks.
As deﬁned in [4], given any network N, the N-fold
stacked network N comprises N copies of N, denoted
by N(1), N(2), . . . , N(N), with inﬁnite-capacity bidirectional
edges connecting all N copies of a given vertex v ∈ V to
each other. Thus, for each v ∈ V,
∈ {1, . . . , N}, and
t ∈ {1, . . . , m}, the symbol X (v) ( ) transmitted by node v in
t
layer at time t may be a function of messages W (v→∗) and
received vectors Y (v) , where
1:t−1
W (v→∗)
Y (v)
1:t−1

3

=
=

(W (v→∗) ( ) : ∈ {1, . . . , N})
(Y (v) ( ) : ∈ {1, . . . , N}).
1:t−1

(u,v)
Xm+N|V|

1

W (u!V)
N|V|
(u,v)
(u,v)
XN|V| XN|V|+1

(u,v)
Xm+N|V|

S[N|V|] (N)

ˆ (u!V,v)
W N|V|

1

S[N|V|] (N)

Fig. 2.

Visualizing the solution S(N).

Fig. 1. Encoder mappings corresponding to an edge (u, v) for solutions
S[1] (N), S[2] (N), . . . , S(N)

random from W (u→V) . Let
˜
G(u→V) : W (u→V) → W (u→V)

The capacity region R(N) for stacked network N is normalized
by the number of layers N in the stack. In [4], it is shown that
the capacity regions for N and N are equal.
Lemma 2: Given any µ > 0 and any solution S(N) for
˜
network N, there exists a solution S(N) with the same error
˜
probability (Pe (S(N)) = Pe (S(N))) such that
˜
R(S(N))
˜
α(S(N))

=

be the corresponding maximum likelihood decoder.
Network solution S(N) ﬁrst channel codes each message
˜ (u→V) ∈ W (u→V) to obtain a coded message W (u→V) ∈
W
W (u→V) and then independently operates solution S[ ] (N) in
each layer ∈ {1, . . . , N} of the stack, as shown in Figure 2.
The formal deﬁnition follows.
˜
˜ (u→V) ∈ W (u→V) at node
• For each (u, V) ∈ M, message W
u is mapped to coded message W (u→V) ∈ W (u→V) using
channel code F (u→V) as

(1 − 2H(Pe (S(N))))R(S(N))

≤ 1 + µ.

Proof: By Lemma 1, there is no loss of generality in
assuming that solution S(N) is a non-overlapping solution
with expansion α(S(N)) = |V|. Let S(N) be implemented over
m = n|V| time steps and n channel uses. Let the encoder and
decoder mappings for S(N) be f and g. Let Pe (S(N)) = λ.
Given any N ∈ N+ and δ > 0, let N be the N|V|-fold stacked
network for N. The argument that follows ﬁrst constructs a
solution S(N) with message alphabet (W)N(1−δ)|V| ; solution
S(N) operates over m + N|V| − 1 time steps, achieving error
˜
probability Pe (S(N)). The next step builds a solution S(N)
that “unravels” S(N) across time – operating the solution for
the stacked network over N by appropriately sequencing its
transmissions. The ﬁnal step bounds the error probability, rate,
˜
and expansion ratio of solution S(N).

˜
W (u→V) = F (u→V) (W (u→V) ).
•

ˆ (u→V,v) = (W (u→V,v) ( ) : ∈ {1, . . . , N|V|})
ˆ
W

•

Construction of S(N): For each i ∈ {1, . . . , N|V|}, let
S[i] (N) be the solution for N that operates solution S(N) in
time steps i, . . . , i + m − 1, as shown in Figure 1. Then
(e)
f[i],t (·) =

(e)
ft−i+1 (·)

of messages W (u→V) = (W (u→V) ( ) : ∈ {1, . . . , N|V|})
for each (u, V) ∈ M and each receiver v ∈ V.
For each (u, V) ∈ M and v ∈ V, applying channel deˆ (u→V,v) yields decoded message
coder G(u→V) to vector W
ˆ
˜
ˆ (u→V,v) ).
W (u→V,v) = G(u→V) (W

Error analysis for S(N): For each
(u, V) ∈ M, and v ∈ V ,

if i ≤ t ≤ i + m − 1,
otherwise.

ˆ
Pr(W

Fix δ ∈ (H(Pe (S(N)), 1) and for each (u, V) ∈ M, let

(u→V,v)

( )

∈ {1, . . . , N|V|},

W (u→V) ( )) = Pe (S(N)).

Since solution S(N) operates the N|V| layers of N independently and the use of a random channel code with codewords
drawn uniformly from the given ﬁnite alphabet creates independent messages in each layer of the stack, the mapping of
ˆ (u→V,v) for each (u, V) ∈ M and each v ∈ V is
W (u,V) to W
equivalent to N|V| independent uses of a discrete memoryless
channel with crossover probability Pe (S(N)) and capacity no
less than 1−H(Pe (S(N))). Thus, the randomly chosen channel
encoder F (u→V) , which, by our choice of δ, has rate less than
(1 − H(Pe (S(N)))) for each (u, V) ∈ M, yields expected error

˜
W (u→V) = (W (u→V) )(1−δ)N|V|

and

For
∈ {1, . . . , N|V|}, solution S[ ] (N) is applied to
messages (W (u→V) ( ) : (u, V) ∈ M) in layer of stacked
network N, giving reconstructions

W (u→V) = (W (u→V) )N|V| .

For each (u, V) ∈ M, let

˜
F (u→V) : W (u→V) → W (u→V)

be a channel code that is designed by choosing the codeword
˜
˜
for each W (u→V) ∈ W (u→V) independently and uniformly at

4

⌧ (e, 1) ⌧ (e, 2)

⌧ (e, 3)

˜
Error and rate analysis for S(N): Since the encoder functions
that determine the codewords transmitted on an edge e under
˜
the solutions S(N) and S(N) are the same, the error probabil˜
ities for S(N) and S(N) are also the same. Therefore, there
exists N ∗ such that for all N > N ∗ , the error probability for
˜
S(N) is no larger than Pe (S(N)).
˜
For each N, the solution S(N) has throughput nN(1 −
δ)|V| / nm + nN|V| − n T bits per second and rate (1 − δ)R.
Since δ > H(Pe (S(N))) is arbitrary, this shows that a throughput of nN(1 − 2H(Pe (S(N))))|V| / nm + nN|V| − n T and rate
(1 − 2H(Pe (S(N))))R are simultaneously achievable with an
error probability Pe (S(N)) for all N > N ∗ .
By choosing n and m large enough, noting that m = n|V|,
and letting N grow without bound, this shows that for every
µ > 0, throughput (1 − 2H(Pe (S(N))))T/(1 + µ)|V| and rate
(1 − 2H(Pe (S(N))))R are simultaneously achievable with an
error probability Pe (S(N)). Finally, note that the expansion
ratio for such codes is 1 + µ to complete the proof.

S[N|V|] (N)
Fig. 3. Implementing the solution S(N) on the network N by mapping
˜
codewords for S(N) to codewords for S(N).

probability approaching zero as N grows without bound by
the channel coding theorem [8]. Speciﬁcally,
ˆ
˜
E Pr(W (u→V,v)

Pe (S(N))
˜
W (u→V) ) <
|V|2 2|V|−1

References

for N suﬃciently large. Therefore, for N large enough,
˜
E Pr(W

ˆ
˜
W)

≤
≤

˜
Pr(W (u→V)

[1] R. Ahlswede, N. Cai, S. Y. R. Li, and R. Yeung, “Network information
ﬂow,” IEEE Transactions on Information Theory, vol. IT-46, no. 4, pp.
1204–1216, Jul. 2000.
[2] R. Koetter and M. Medard, “An algebraic approach to network coding,”
Networking, IEEE/ACM Transactions on, vol. 11, no. 5, pp. 782 – 795,
oct. 2003.
[3] T. M. Cover and J. A. Thomas, Elements of Information Theory. Wiley,
1991.
[4] R. Koetter, M. Eﬀros, and M. M´ dard, “A theory of network equivalence
e
- part i: Point-to-point channels,” IEEE Transactions on Information
Theory, vol. 57, no. 2, Feb. 2011.
[5] A. Rasala-Lehman and E. Lehman, “Complexity classiﬁcation of network
information ﬂow problems,” in Proceedings of the 41st Annual Allerton
Conference on Communication, Control, and Computing, Monticello, IL,
Oct. 2003.
[6] M. Medard, M. Eﬀros, T. Ho, and D. Karger, “On coding for nonmulticast networks,” in Proceedings of the 41st Annual Allerton Conference on Communication, Control, and Computing, Monticello, IL, Oct.
2003.
[7] M. Eﬀros, “On dependence and delay: Capacity bounds for wireless
networks,” in Proceedings of 2012 IEEE Wireless Communications and
Networking Conference, Paris, France, Apr. 2012.
[8] C. E. Shannon, “A mathematical theory of communication,” Communication, Bell System Technical Journal, vol. 27, pp. 379–423, 1948.

ˆ
˜
W (u→V,v) )

(u,V)∈M,v∈V

Pe (S(N)),

where, the expectation is taken with respect to the random
channel code design. Given this bound on the expected error
probability of a randomly designed, there exists a solution
S(N) that does at least as well.
˜
Construction of S(N): Next, we operate the solution S(N)
˜
over the network N to obtain a solution S(N) with message
˜ (u→V) : (u, V) ∈ M). Solution S(N) is imple˜
alphabets (W
mented over mn + nN|V| − n time steps.
Let T be the schedule for solution S(N). For each v ∈
V, let T (e) = {τ(e, 1), . . . , τ(e, |T (e)|)} be the elements of
T (e), where τ(e, j) is increasing in j. Recall that S(N) is
implemented over n channel uses. Therefore, |T (e)| ≤ n
for each e ∈ E. Applying the notation used for S(N), we
˜
denote the network messages by (W (u→V) : (u, V) ∈ M) with
˜
˜ (u→V) ∈ W (u→V) . We denote the encoders and decoders
W
˜
for S(N) by f˜ and g. These mappings are related to the
˜
encoder and decoder mappings for S(N) as described below
and illustrated in Figure 3.
• For each e ∈ E, i ∈ {1, . . . , m + N|V| − 1}, and
˜
j ∈ {1, . . . , |T (e)|}, encoder f˜(e)
(i−1)n+ j in S(N) operates
(e)
encoder f[i+τ(e,1)−τ(e, j)],i+τ(e,1)−1 from S(N) for all e ∈ E.
If |T (e)| < n for some e ∈ V, encoder f˜(e)
(i−1)n+t transmits
symbol for all t ∈ {|T (e)| + 1, . . . , n}.
• At each v ∈ V, the decoder ﬁrst groups together
the received vectors corresponding to each S[i] (N) for
i = 1, 2, . . . , N|V|, decodes using the decoder functions
ˆ
˜
g(v) (·), and ﬁnally creates reconstructions (W (u→V,v) :
[i]
(u, V) ∈ M, v ∈ V) by passing through the decoders
(G(u→V) (·) : (u, V) ∈ M).

5

