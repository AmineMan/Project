Title:          ISIT12online1.dvi
Creator:        dvips(k) 5.991 Copyright 2011 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Mon May  7 18:31:26 2012
ModDate:        Tue Jun 19 12:54:34 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      469105 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569565165

Improved Upper Bounds on the Capacity of Binary
Channels with Causal Adversaries
Bikash Kumar Dey

Sidharth Jaggi

Michael Langberg

Anand D. Sarwate

IIT Bombay
bikash@ee.iitb.ac.in

CUHK
jaggi@ie.cuhk.edu.hk

The Open University of Israel
mikel@openu.ac.il

TTI-Chicago
asarwate@ttic.edu

of communication no matter what coding strategy Alice and
Bob use. The jamming strategy, corresponding analysis and
the resulting bound are all novel.

Abstract—In this work we consider the communication of
information in the presence of a causal adversarial jammer. In the
setting under study, a sender wishes to communicate a message to
a receiver by transmitting a codeword x = (x1 , . . . , xn ) bit-bybit over a communication channel. The adversarial jammer can
view the transmitted bits xi one at a time, and can change up to a
p-fraction of them. However, the decisions of the jammer must be
made in a causal manner. Namely, for each bit xi the jammer’s
decision on whether to corrupt it or not must depend only on
xj for j ≤ i. This is in contrast to the “classical” adversarial
jammer which may base its decisions on its complete knowledge
of x. Binary channels with causal adversarial jammers have seen
recent studies in which both lower bounds and upper bounds on
their capacity is derived. In this work, we present improved upper
bounds on the capacity which hold for both deterministic and
stochastic encoding schemes.

A. Previous and related work
Many of the following works deal with more general
channels; we mostly restrict our discussion to the binary-input
binary-output case.
Coding theory model: A very strong class of adversarial
channels is one where Calvin is omniscient – he knows Alice’s
entire codeword x prior to transmission and can tailor the
pattern of up to pn bit-ﬂips to each speciﬁc transmission. This
is the worst-case noise model studied in coding theory. For
binary channels, characterizing the capacity has been an open
problem for several decades. The best known upper bound
is due to McEliece et al. [1] as the solution of an LP, and
the best known achievable scheme corresponds to codes (GV
codes) suggested by Gilbert and Varshamov [2], [3], which
achieve a rate of 1 − H(2p). Improving either of these bounds
would be a signiﬁcant breakthrough.
Information theory model: A much weaker class of adversarial channels is one where Calvin generates coin ﬂips in
an i.i.d. manner. The original work of Shannon [4] effectively
characterized the capacity of such channels. It follows that the
capacity in Shannon’s setting is strictly greater than that of the
omniscient adversarial model.
Causal adversarial model: The class of channels considered
in this work, i.e., that of causal adversaries, fall in between
these two extremes. In one direction this is because a causal
adversary is certainly no stronger than an omniscient adversary, since he cannot tailor his jamming strategy to take into
account Alice’s future transmissions. Indeed, the work of [5]
indicates that for 2p < H −1 (1/2) 0.11, rates strictly better
than those achievable by GV codes against an omniscient
adversary are achievable against a causal adversary. However,
since it is still unknown whether GV codes are optimal against
omniscient adversaries, it is unknown whether causal adversaries are indeed strictly weaker than omniscient adversaries.
In the other direction, prior work [6] demonstrates deﬁnitively
that a causal adversary is strictly “stronger” than random noise
– in particular, for all p such that H(p) < 4p (i.e., p > 0.157),
the best rates achievable against a causal adversary are strictly
bounded away from the rate of 1 − H(p) achievable against
a random BSC(p). The results in [6] hold for deterministic

I. I NTRODUCTION
Alice wishes to transmit a message u to Bob over a binaryinput binary-output channel. To do so, she encodes u into a
length-n binary vector x and transmits it over the channel.
However, the channel is controlled by a malicious adversary
Calvin, who may observe the transmissions and attempt to jam
communication by ﬂipping up to a fraction p of the transmitted
bits. Since he must act in a causal manner, Calvin’s decisions
on whether or not to ﬂip bit xi must be a function solely of the
bits x1 , . . . , xi he has observed thus far. This communication
scenario models jamming by an adversary who is limited in
his jamming capability (perhaps due to limited processing
power or limited transmit energy) and is causal. This causality
assumption is reasonable for many communication channels,
both wired and wireless, where Calvin is not co-located with
Alice. Calvin can only corrupt a bit when it is transmitted
(and thus the error is based on his view so far). To decode the
transmitted message, Bob waits until all n bits have arrived.
Our main contribution in this work is an impossibility result
that helps make progress towards a better understanding of
the communication rates achievable against a causal adversary.
Speciﬁcally, we describe and analyze a particular jamming
strategy of Calvin, and show that it (upper) bounds the rate
Authors are in alphabetical order. Work supported in part by ISF grant
480/08, RGC GRF grants 412608 and 412809, RGC AoE grant on Institute of
Network Coding, established under the University Grant Committee of Hong
Kong, CUHK MoE-Microsoft Key Laboratory of Humancentric Computing
and Interface Technologies, the Bharti Centre for Communication in IIT
Bombay, India, and the California Institute for Telecommunications and
Information Technology (CALIT2) at UC San Diego.

1

coding schemes, and the possibility of stochastic codes is
left open. In stochastic codes, Alice may encode u to one
of several possible codewords x ∈ {x(u, r)}, where r is a
random source available to Alice but unknown to either Bob
or Calvin. Stochastic codes can in some scenarios achieve rates
greater than achievable via deterministic codes.
In this work, our contribution is two fold. We present
strictly tighter upper bounds than those presented in [6], and
in addition we show that our upper bounds hold for both
deterministic and stochastic encoding.
Arbitrarily Varying Channels: Our model is a variant of
the arbitrarily varying channel (AVC) model [7]. AVC models
have been extended to include channels with constraints on the
adversary (such as pn bit ﬂips) for cases where the adversary
has no access to the codeword [8], or has access to the full
codeword [9]. However, issues of causality and delay have
only been studied in the context of random coding (i.e., in the
presence of common randomness), but not for deterministic
codes or stochastic encoding.
Delayed adversaries: A reﬁnement of the causal adversary
model, called the delayed adversary, was studied in [10]
and [11]. In these models, the jammer’s decision on whether to
corrupt xi must depend only on xj for j ≤ i − dn for a delay
parameter d ∈ [0, 1]. The case of d = 0 is exactly the standard
causal setting, and that of d = 1 corresponds to the “oblivious
adversary” studied in [10]. The authors [11] showed that for
a large class of channels, the capacity for delay d > 0 equals
that of AVC model of [12].
Computational aspects: In the context of encoding/decoding
complexity, the works most relevant to ours are [13], [14]
which address the computational aspects of coding in settings
closely related to ours. In particular, [14] also use a symmetrization argument to prove a converse for p > 1/4.

1
0.9
0.8

C

0.6

Our
bound

0.3

1−H(p)

0.2

HL bound

0.1
0

0

0.05

0.1

0.15

0.2

0.25

p

Figure 1.
We plot all previous bounds related to the channel at hand
compared to our new bound. The upper bound of [6] is min(1−4p, 1−H(p))
for p ∈ [0, 1/4] and 0 for p > 1/4. The lower bound of [5] (denoted HL)
slightly improves the GV bound 1 − H(2p). Our improved bound appears in
between.

away from zero whenever Alice and Bob communicate at rate
higher than C(p). Roughly speaking, Calvin uses a two-phase
babble-and-push strategy. In the ﬁrst phase of (p) channel
uses, Calvin “babbles” by behaving like a BSC(¯) for some
p
p chosen as a function of p. In the second phase of n − (p)
¯
channel uses, Calvin randomly selects a codeword from Alice
and Bob’s codebook which is consistent with what Bob has
received so far. Calvin then “pushes” the remaining part of
Alice’s codeword towards his selected codeword.
Our approach improves on previous results [6] in two ways.
Firstly, we propose a stronger attack for Calvin based on
increasing the uncertainty of Bob during the “babble” phase,
as opposed to the “wait” strategy of [6]. Secondly, we show
that this attack is successful even when Alice and Bob use
stochastic encoding. Hence these bounds hold for general
codes, rather than prior work in [6], which were shown to
hold in a strong sense only for codes in which each message
u corresponded to a unique x(u) deterministically chosen by
Alice.

Our improved bounds are given in the following theorem,
and are depicted with comparisons to previous bounds in
Figure 1. As mentioned above, our bounds improve over
previous ones and hold for both deterministic and stochastic
codes. Let p ∈ [0, 1/4]. Consider any p ∈ [0, p]. Let
¯
α(p, p) = 1 − 4(p − p). In what follows, C(p) is the capacity
¯
¯
of the causal channel under study. For precise deﬁnitions and
model see Section II.
p
¯
α(p,p)
¯

0.5
0.4

B. Main result

Theorem 1. C(p) ≤ minp∈[0,p] α(p, p) 1 − H
¯
¯

1−4p

0.7

II. M ODEL
For any positive integer k, let [k] = {1, 2, . . . , k}. We
let U = [2nR ] denote Alice’s message set, and U denote
the message random variable uniformly distributed in U. A
deterministic code of rate R and blocklength n is a pair of
maps Cd = (Φ, Ψ) where Φ : U → X n and Ψ : Y n → U are
deterministic maps. The map Φ is called the encoder and the
map Ψ is called the decoder.
A code with stochastic encoding and decoding of rate R and
blocklength n is a pair of maps Cs = (Φ, Ψ) where Φ : U →
X n and Ψ : Y n → U are probabilistic maps. The random
map Φ gives a probability distribution ρ(·|u) on X n for every
u ∈ U. The mapping Ψ(y) is a random variable taking values
from U . The encoding Φ is equivalently represented by ﬁrst
picking a random variable R from a set R (with |R| ≤ |X |n )

.

It can be shown that the optimum p is approximately
¯
min{p, (1 − 4p)/8.4445}. Namely, for p > 0.0804, the capacity C(p) is bounded away from 1 − H(p) and for p ≤ 0.0804
we have p = p and our bound equals 1 − H(p).
¯
C. Techniques and Proof Overview
To prove Theorem 1 we must present a strategy for Calvin
that does not allow communication at rate higher than C(p)
(no matter which encoding/decoding scheme is used by Alice
and Bob). Speciﬁcally, the strategy we present will allow
Calvin to enforce a constant probability of error bounded

2

according to a conditional distribution ρR|U (.|u), and then
using a deterministic encoder map Φ : U ×R → X n . Note that
our deﬁnition does not preclude there existing pairs (u, r) and
(u , r ) such that Φ(u, r) = Φ(u , r ). As we are addressing
upper bounds on the capacity C(p) in this work, it is crucial
to prove our results in the stochastic setting above.
A causal adversarial strategy of blocklength n is a sequence
(i)
(i)
of random maps Adv = {fC : i ∈ [n]} where fC : X i → E
takes a code C and for each channel use i ∈ [n] uses the
past and current inputs (x1 , x2 , . . . , xi ) to choose an action
(i)
ei = fC (x1 , . . . , xi ) ∈ E at time i, with resulting output yi =
xi +ei . In our setting E = {0, 1}. The strategy obeys constraint
n
p if the Hamming weight e = i=1 ei of e = e1 , . . . , en is
at most pn almost surely over the randomness in the message,
encoder, and strategy. For a given adversarial strategy and an
input codeword x, the strategy produces a (possibly random)
e and the output is y = x ⊕ e. Let PAdv (y|x) denote the
probability of an output y given an input x under the strategy
Adv. When the blocklength is understood from the context, let
Adv(p) denote all adversarial strategies obeying constraint p.
The (average) probability of error for a code with stochastic
encoding and decoding is given by
ε=
¯

max

Adv∈Adv(p)

1
2Rn

consider uniform message distribution, henceforth, w.l.o.g., we
assume that the joint distribution ρU,R (·, ·) is uniform.
III. P ROOF

T HEOREM 1

Let p ∈ [0, 1/4] and let p ≤ p. Let ε > 0. In what
¯
follows we prove that the rate of communication over the
causal adversarial channel (with parameter p) is bounded by
p
¯
and
R = C + ε where C = α(p, p) 1 − H α(p,p)
¯
¯
α(p, p) = 1 − 4(p − p) as deﬁned in Theorem 1. Namely,
¯
¯
for any sufﬁciently large block length n, and any (n-block
stochastic) code Cs = (Φ, Ψ) shared by Alice and Bob, there
exists an adversarial jammer Adv that can impose a constant
decoding error. The decoding error we will obtain will depend
on ε > 0.
The proof for p = p follows from previous works as
¯
described in the Introduction. Thus, we assume that p − p > 0
¯
and that ε < 2(p − p). In what follows, we ﬁx a block length
¯
n (which will at times be assumed to be sufﬁciently large with
respect to 1/ε).
Adversarial strategy: Our converse bound is based on a
particular two-phase adversarial strategy for Calvin that we
call “babble-and-push.”
Let = (α + ε/2)n and without loss of generality assume
∈ N. For a vector z of length n, let z1 = (z1 , z2 , . . . , z )
and z2 = (z +1 , z +2 , . . . , zn ).
• (Babble) Calvin chooses a random subset Γ of pn indices
¯
uniformly from all (¯n)-subsets of {1, 2, . . . , }. For i ∈
p
Γ, Calvin ﬂips bit xi ; that is, for i ∈ {1, 2, . . . , }, ei = 1
for i ∈ Γ and ei = 0 for i ∈ Γ. Let yi = xi + ei . Here
/
x1 , . . . , xn is the transmitted codeword and y1 , . . . , yn is
the word received by Bob.
• (Push) Calvin constructs the set of (u, r) that have encodings x(u, r) = Φ(u, r) that are close to y1 = y1 , . . . , y .
Namely, Calvin constructs the set

2Rn

ρR|U (r|u)
u=1 r∈R

PAdv (y|Φ(u, r))P (Ψ(y) = u) ,

OF

(1)

y

where the probability P (Ψ(y) = u) is over any randomness in
the decoder. We can interpret the errors as the error in expectation over Alice choosing a message U = u and a codeword
x = Φ(u, r) according to the conditional distribution ρ(x|u).
A rate R is achievable against a causal adversary under
average error if for every δ > 0 there exist inﬁnitely many
block lengths {ni }, such that for each ni there is an ni
block length (stochastic) code of rate at least R and average
probability of error at most δ. The supremum of all achievable
rates is the capacity. We denote by C(p) the capacity of the
channel corresponding to adversaries parametrized by p.
Consider a code of blocklength n, rate R and error probability δ. We can, without loss of generality (w.l.o.g.), assume that
the encoding probabilities {ρ(x|u) : x ∈ {0, 1}n , u ∈ [2nR ]}
are rational. To see why this is the case, note that for any
small η > 0 we can ﬁnd rational numbers {ρ(x|u)} such that
˜
ρ(x|u) − η ≤ ρ(x|u) ≤ ρ(x|u). Now consider a code with
˜
encoding probabilities q(x|u) = ρ(x|u) for x = 0 and assign
˜
the remaining probability to 0. Under the same decoder, this
code has error probability at most δ + 2n+nR η, but since η
was arbitrary, the error is at most 2δ.
Now, for a given stochastic code, let N be the least common
multiple of the denominators of ρ(x|u) for all x, u. We can
imagine each codeword x of u as N ρ(x|u) copies of the same
codeword with conditional probability 1/N each. So we can
equivalently associate a random variable R with |R| = N
s.t. the conditional distribution ρR|U (·|u) is uniform, and the
encoding map Φ(u, ·) is not necessarily one-one. Since we

By1 = {(u, r) : dH (y1 , x1 (u, r)) = pn},
¯

(2)

and selects an element (u , r ) ∈ By1 uniformly at random. Calvin then considers the corresponding codeword
x = Φ(u , r ). Given the selected x , for i > , if
xi = xi , Calvin sets ei equiprobably to 0 or 1 until
i
j=1 ej = pn or i = n. Note that, under our assumption
(w.l.o.g.) of uniform ρU,R , the a posteriori distribution
of Alice’s choice (u, r) given y1 is also uniform in By1 .
Analysis: Let Adv denote the babble-and-push adversarial
strategy. We start by proving the following technical lemma
that we use in our proof.
Lemma 2. Let V be a random variable on a discrete ﬁnite set V
with entropy H(V ) > λ, and let V1 , V2 , . . . , Vm be i.i.d. copies
of V . Then
Pr ({Vi : i = 1, . . . , m} are all distinct) ≥

λ−m
log |V|

m−1

.

Proof: Fix i ≤ m and a set v1 , v2 , . . . , vi ∈ V. Let
A = {v1 , . . . , vi } and let W = 1(V ∈ A). We can write

3

Claim 4. Let ρU|y1 be the conditional distribution of U given
y1 under Adv. Let U1 , U2 , . . . , Um be m random variables
drawn i.i.d. according to ρU|y1 . Then

the distribution of V as a mixture:
Pr [V = v] =

Pr[W = j] · Pr[V |W = j].
j=0,1

We can upper bound the entropy of V :

P(E1 | E0 ) ≥ (ε/8)m−1 .

H(V ) ≤ H(V |W ) + H(W )

Proof: The proof follows from Claim 3 and Lemma 2
by using λ = nε/4 and the fact that there are at most 2n
messages. The bound then becomes (ε/4 − m/n)m−1 . For
ﬁxed m there exists an n sufﬁciently large so that m/n ≤ ε/8.

Pr[W = j]H(V |W = j) + H(W ).

=
j=0,1

Since conditioning reduces entropy and the support of V
conditioned on W is at most i, we have

Let U and X denote the random choice of Calvin’s
message and codeword in the push phase. The events E2 =
{U = U} and E3 = {dH (X2 , X2 ) ≤ 2(p − p)n − εn/8}
¯
must occur with probability bounded away from 0. The ﬁrst
event is that Calvin chooses a different message than Alice
and the second is that he chooses a codeword that is close
enough to Alice’s.
Claim 5. For the babble-and-push attack Adv,

λ ≤ 1 + log i + Pr[W = 0] log |V|.
Namely, Pr[W = 0] ≥ λ−1−log i . Thus a loose bound on the
log |V|
λ−m
desired event is ( log |V| )m−1 .
To prove the upper bound, we now present a series of
claims. Let X denote the random variable corresponding
to Alice’s input codeword and let Y be the output of the
channel. Thus X1 ∈ {0, 1} is Alice’s input during the
“babble” phase of length and X2 is her input during the
“push” phase; the randomness comes from the message U
and the stochastic encoding. Similarly, Y1 is the random
variable corresponding to the bits received by Bob during
the “babble” phase, and Y2 the n− bits of the “push” phase.
Let Adv denote the babble-and-push adversarial strategy. Let
A0 = {y1 : H(U|Y1 = y1 ) ≥ nε/4}, where the entropy
H(U|Y1 = y1 ) is measured over the randomness of the
encoder and the randomness of Calvin’s action in the “babble”
phase, and let E0 = {Y1 ∈ A0 }.

PAdv (E2 and E3 | E0 ) ≥ εO(1/ε) .

(3)

Proof: By the data processing inequality, and the choice
of Calvin’s strategy, we have
I(U; Y1 ) ≤ I(X1 ; Y1 ) ≤ (1 − H(¯n/ ))
p
p
¯
≤ (αn + εn/2) 1 − H
α + ε/2

.

Therefore
H(U|Y1 ) ≥ H(U) − n(α + ε/2) 1 − H
≥n ε+α 1−H

p
¯
α + ε/2

p
¯
α

− n(α + ε/2) 1 − H

p
¯
α + ε/2

> nε/2.

Thus the expected value of H(U|Y1 = y1 ) over y1 is at least
nε/2. The maximum value of H(U|Y1 = y1 ) is nR, so with
probability at least ε/2, we have H(U|Y1 = y1 ) ≥ nε/4 (via
the Markov inequality).
Now consider drawing m pairs (Ui , Ri ) from By1 i.i.d.
∼ ρU,R|y1 (which happens to be uniform). Note that the
marginal distribution of Ui is also i.i.d. ∼ ρU|y1 , which is
not necessarily uniform. Let
E1 = {{U1 , U2 , . . . , Um } are all distinct} .

(6)

Proof: Conditioned on E0 , the realization y1 satisﬁes
H(U|Y1 = y1 ) ≥ εn/4. We ﬁrst use Claim 4 to lower
bound the probability that E2 holds. First consider randomly
sampling a set of pairs S = {(ui , ri ) : i ∈ [m]} uniformly
from By1 , and let Xi be the codeword for (ui , ri ).
Claim 4 shows that with probability at least (ε/8)m−1 ,
all the messages in S are distinct and therefore the codewords are distinct as well. In particular, this shows that
PAdv (E2 | E0 ) ≥ (ε/8). Turning to E3 , Plotkin’s bound [15]
shows that there do not exist binary error-correcting codes
of blocklength n − and minimum distance d with more
2d
than 2d−(n− ) codewords. Setting m = 17/ε, this bound
implies that with probability at least (ε/8)m−1 there must
exist codewords x, x corresponding to (u, r) and (u , r ) in S,
2d
respectively, within a distance d that satisﬁes 17 = 2d−(n− ) .
ε
Solving for d and using = (1 − 4(p − p) + ε/2)n shows that
¯
def
d satisﬁes d < ∆ = 2(p − p)n − εn/8
¯
Let γ be the fraction of pairs (u, r) and (u , r ) in By1
that satisfy E2 and E3 . We would like to lower bound γ. A
union bound shows that the probability over the selection of S
i
j
gives the upper bound P
Xi ,Xj ∈S {dH (X , X ) < ∆} ≤
2
m γ. However, the earlier argument shows that by selecting
m = 17/ε pairs in S, we have a lower bound of (ε/8)m−1 .
Therefore
1 ε m−1 172 ε 17/ε−1
= 2
.
γ≥ 2
m 8
ε
8
Therefore E2 and E3 happen with probability at least εO(1/ε) .

Claim 3. For the babble-and-push attack Adv,
PAdv (E0 ) ≥ ε/2.

(5)

The next step is to show that Calvin does not “run out” of
bit ﬂips during the second “push” phase of his attack. This
follows directly from Sanov’s Theorem [16]. Let
E4 =

(4)

4

dH (X2 , Y2 ) ∈

d εn d εn
−
, +
2
16 2
16

.

(7)

Claim 6. For the babble-and-push attack Adv,
2

PAdv (E4 | E2 , E3 ) ≥ 1 − 2−Ω(ε

n)

Our analysis implies a reﬁned statement of Theorem 1.
Namely, let c be a sufﬁciently large constant. For any block
length n, any ε > 0 and any encoding/decoding scheme of
c
Alice and Bob of rate C(p) + n + ε, Calvin can cause a
O(1/ε)
decoding error of at least ε
.

.

Claim 6 follows from standard concentration results.
Theorem 7. For any code with stochastic encoding of rate
R = C + ε, under Calvin’s babble-and-push strategy the
average error probability ε is lower bounded by εO(1/ε) .
¯

IV. C ONCLUDING

In this work we present a novel upper bound on the rates
achievable against a causal adversary, which strictly improves
on prior upper bounds known against such adversaries over
binary channels. Further, we demonstrate that the upper bound
presented herein holds against arbitrary codes, rather than
simply against deterministic codes, as is common in the coding
theory literature. Since our analysis pertains to adversarial jamming rather than random noise, the proof techniques presented
may be of independent interest in the more general setting of
AVC’s.

Proof: Let (u, r) denote the message and randomness of
Alice, y1 be the received codeword in the babble phase, and
(u , r ) be the message and randomness chosen by Calvin for
the push phase. Let ρ(y1 , u, r, u , r ) be the joint distribution
of these variables under Alice’s uniform choice of (u, r) and
Calvin’s attack. For each y, let ρ(y|y1 , u, r, u , r ) be the
conditional distribution of y under Calvin’s attack.
The error probability ε can be written as
¯
ρ(y1 , u, r, u , r )
y1 ,u,r,u ,r

ρ(y|y1 , u, r, u , r )P(Ψ(y) = u).

R EFERENCES

y2

Let F be the set of tuples (y1 , u, r, u , r ) satisfying events
E0 , E2 , and E3 . Claims 3 and 5 show that ρ(F ) ≥ (ε/2) ·
εO(1/ε) . For (y1 , u, r, u , r ) ∈ F , we have that u = u , and
that x2 (u, r) and x2 (u , r ) are sufﬁciently close.
Let G be the set of y2 such that E4 (bounded in Claim 6) is
satisﬁed for x2 (u, r). Note that by the symmetry of Calvin’s
attack in the push phase, for (y1 , u, r, u , r ) ∈ F , E4 is also
satisﬁed for x2 (u , r ). Indeed, for y2 ∈ G, the conditional
distribution is symmetric:
ρ(y|y1 , u, r, u , r ) = ρ(y|y1 , u , r , u, r).

[1] R. J. McEliece, E. R. Rodemich, H. Rumsey, Jr., and L. R. Welch,
“New upper bounds on the rate of a code via the Delsarte-MacWilliams
inequalities,” IEEE Trans. Information Theory, vol. IT-23, no. 2, pp.
157–166, 1977.
[2] E. N. Gilbert, “A comparison of signalling alphabets,” Bell Systems
Technical Journal, vol. 31, pp. 504–522, 1952.
[3] R. R. Varshamov, “Estimate of the number of signals in error correcting
codes,” Dokl. Acad. Nauk, vol. 117, pp. 739–741, 1957.
[4] C. E. Shannon, “A mathematical theory of communication,” The Bell
System Technical Journal, vol. 27, pp. 379–423,623–656, July, October
1948.
[5] H. Ishay and M. Langberg, “Beating the Gilbert-Varshamov bound for
online channels,” Proceedings of the 2011 International Symposium on
Information Theory, pp. 1297–1301, 2011.
[6] M. Langberg, S. Jaggi, and B. Dey, “Binary causal-adversary channels,”
in Proceedings of the 2009 International Symposium on Information
Theory (ISIT), 2009.
[7] D. Blackwell, L. Breiman, and A. Thomasian, “The capacities of
certain channel classes under random coding,” Annals of Mathematical
Statistics, vol. 31, pp. 558–567, 1960.
[8] I. Csisz´ r and P. Narayan, “The capacity of the arbitrarily varying chana
nel revisited : Positivity, constraints,” IEEE Transactions on Information
Theory, vol. 34, no. 2, pp. 181–193, 1988.
[9] A. D. Sarwate and M. Gastpar, “Rateless codes for AVC models,” IEEE
Transactions on Information Theory, vol. 56, no. 7, pp. 3105–3114, July
2010. [Online]. Available: http://dx.doi.org/10.1109/TIT.2010.2048497
[10] M. Langberg, “Oblivious channels and their capacity,” IEEE Transactions on Information Theory, vol. 54, no. 1, pp. 424–429, 2008.
[11] M. Langberg, S. Jaggi, and B. Dey, “Coding against delayed adversaries,” in Proceedings of the 2010 International Symposium on
Information Theory (ISIT), 2010.
[12] I. Csisz´ r and P. Narayan, “Arbitrarily varying channels with constrained
a
inputs and states,” IEEE Transactions on Information Theory, vol. 34,
no. 1, pp. 27–34, 1988.
[13] A. Smith, “Scrambling adversarial errors using few random bits, optimal
information reconciliation, and better private codes,” in Proceedings of
the 2007 ACM-SIAM Symposium on Discrete Algorithms (SODA 2007),
2007.
[14] V. Guruswami and A. Smith, “Codes for computationally simple channels: Explicit constructions with optimal rate,” CoRR, vol.
abs/1004.4017, 2010, to appear in FOCS 2010.
[15] A. E. Brouwer, “Bounds on the size of linear codes,” Handbook of
Coding Theory, Elsevier Science, New York, NY, USA, vol. 1, pp. 295–
461, 1998.
[16] T. M. Cover and J. A. Thomas, Elements of information theory, 2nd
edition. New York, NY, USA: Wiley-Interscience, 2006.

(8)

Then for (y1 , u, r, u , r ) ∈ F , by Claim 6,
2

ρ(y2 |y1 , u, r, u , r ) ≥ 1 − 2−Ω(ε

n)

.

y2 ∈G

Now, returning to the overall error probability, let ρ(y1 ) be
the unconditional probability of Bob receiving y1 in the babble
phase, where the probability is taken over Alice’s uniform
choice of (u, r) and Calvin’s random babble e1 . We rewrite
the joint distribution as
1
ρ(y1 )
ρ(y1 , u, r, u , r ) =
|By1 |2
y
(u,r)∈By1 (u ,r )∈By1

1

=

1

ρ(y1 )
y1

(u ,r )∈By1 (u,r)∈By1

|By1 |2

= ρ(y1 , u , r , u, r).
Thus,
2¯ ≥
ε

ρ(y1 , u, r, u , r )
F

ρ(y2 |y1 , u, r, u , r )
y2 ∈G

(P(Ψ(y1 , y2 ) = u) + P(Ψ(y1 , y2 ) = u ))
≥

ρ(y1 , u, r, u , r )
F

REMARKS

ρ(y2 |y1 , u, r, u , r )
y2 ∈G
2

≥ ε/2 · εO(1/ε) · 1 − 2−Ω(ε

n)

.

5

