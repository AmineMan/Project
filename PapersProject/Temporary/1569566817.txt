Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 22:09:45 2012
ModDate:        Tue Jun 19 12:55:05 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      1431002 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566817

Combinatorial Message Sharing and Random
Binning for Multiple Description Coding
Emrah Akyol, Kumar Viswanatha, and Kenneth Rose
{eakyol, kumar, rose}@ece.ucsb.edu
University of California, Santa Barbara

Abstract—This paper proposes a new multiple description
(MD) coding method and an associated achievable rate-distortion
region for L ≥ 2 channels. The proposed scheme randomly
bins codebooks chosen from the codebook structure, similar to
that of the recently proposed combinatorial message sharing
(CMS) scheme designed for conditional codebook encoding. The
proposed scheme effectively performs multilayer random binning
for each subset of the description, which hence enables to utilize
the symmetry of a “subset” of the description rates wherever
it exists. The new scheme specializes in to the conventional
multilayer random binning as an extreme special case.
Index Terms—Multiple description coding, source coding, ratedistortion theory

used in distributed source coding. In [7], a coding scheme was
proposed, namely the source-channel erasure codes (SCEC),
which have similar structure to the (L, k) maximum distance
separable codes (MDS). In [8], these codes were layered from
(L, L) to (L, 1) resulting in a scheme that we will call the
multilayer PPR scheme. This coding scheme achieves several
cross sections of the outer bound for this fully symmetric
setting, as shown in series of papers by Wang and Wisvanath
[9], [10] and close to the outer bound in general [11]. Notably,
in [12], this scheme is enhanced by utilizing ideas from
quantization splitting and channel (or network) codes.

I. I NTRODUCTION

Here, we propose a new encoding scheme that leverages the
ﬂexibility of our recent CMS framework while incorporating
in it random binning techniques. Speciﬁcally, we perform
random binning within codebooks structured according to the
CMS principle at the base layer. CMS enables this scheme to
adapt to non-symmetric rates and distortions, while utilizing
the efﬁciency of random binning whenever there is symmetry,
including the partially at the subset level. More speciﬁcally,
we have a PPR-like multilayer random binning scheme for
each subset of the combinatorially organized base layer codebooks, hence, we efﬁciently utilize the symmetry with random
binning, while the scheme can adapt to the overall asymmetry
in rates and distortions.

The multiple description (MD) coding problem is a long
standing open problem in source coding [1]–[3]. For the
two description problem, the rate-distortion region has been
completely characterized for the quadratic Gaussian problem
[1], and the encoding scheme in [2] was shown to be optimal.
While several achievable regions exist [2], [3] for general
sources, a complete characterization remains an open problem.
Recent focus has been more on the general L-description
problem (L > 2). Venkataramani, Kramer and Goyal (VKG)
[4] extended the conditional codebook approach of [3] to L
descriptions using one common base layer that is broadcasted
to all the descriptions. Recently, we extended this scheme
by realizing the need for a combinatorial number of base
descriptions, namely “combinatorial message sharing” (CMS).
This extension allows ﬂexible adaptation to non-symmetric
rates and distortions [5]. In [6], we showed that the CMS
achievable region strictly subsumes the VKG region.
The above prior work deals with the MD problem within its
most general setting, using a conditional codebook approach:
base layer codebooks (or the reﬁnement layer codewords) are
generated conditioned the previous layer codeword. Such a
scheme is best suited for the general setting of non-symmetric
rates and distortions. We next consider a practically important
special case of symmetric MD, where all descriptions are
encoded at the same rate and the distortion only depends on the
number of descriptions received, and not the speciﬁc subset of
descriptions. An achievable rate-distortion region under these
assumptions is derived by Puri, Pradhan and Ramchandran
(PPR) [7], [8], by utilizing a random binning method often

As an example setting to provide intuition for the potential
beneﬁts of the proposed scheme, consider a scenario where
symmetry is limited to a subset of the description rates, say,
R1 > R2 = R3 = R4 = R. Then, a PPR-like scheme has
to bin the codebooks at the same rate of R, due to its highly
symmetric structure. However, an excess rate of R1 − R bits
will be unused. On the other extreme, a conditional codebook
based scheme will use all descriptions at their full rates, but
will not utilize the symmetry of R2 , R3 and R4 . The proposed
paradigm, applying multilayer random binning for each subset,
will efﬁciently utilize available partial symmetry. At the same
time, it adapts to the asymmetry of the overall rates, and hence
avoids unused excess rates.
This paper is organized as follows: In Section II, we present
the notation and an overview of prior L-channel MD schemes.
In Section III, we describe the new approach along with the
new achievable region for this scheme. Section IV provides
conclusions.

This work is supported in part by the NSF under grants CCF-0728986,
CCF-1016861 and CCF-1118075.

1

U12
U23

U1
U2

...

U1
U2

. . .
. . .
. . .

...
.
.
.

. .
. .
. .

.
.
.

.
.
.

UL

Base Layer

Fig. 1.

U13

V 2I
1

U13

V IL

U12
U23

V12
V13

V IL

U IL

Fig. 2.

...
.
.
.

. .
. .
. .

.
.
.

.
.
.

UL

Base Layer

Reﬁnement Layer

.
.
.

U IL

Reﬁnement Layer

Codebook structure in the CMS scheme

Codebook structure in the VKG scheme

shown in Figure 2 which also speciﬁes the conditioning of
the combinatorially shared codebooks. This scheme clearly
subsumes VKG as a special case and in fact strictly subsumes
its achievable region as was shown in [6].

II. P RELIMINARIES
A. Notation
Let {X(i)}i=1,2,... be a zero-mean memoryless and staˆ
tionary source taking values in alphabet X and let X be the
reconstruction alphabet. The vector [X(1), X(2), ..., X(n)] is
ˆ
compactly denoted by xn . The distortion measure d : X ×X →
+
R is bounded and additive. We use IL to denote the set
{1, 2, .., L}. For parsimony of notation, we employ H(X)
to denote the entropy of a discrete random variable X, or
differential entropy if X is continuous. For an arbitrary set A,
we use 2A to denote the set of all subsets of A with cardinality
m
greater or equal to m, i.e.,
2A
m

D. Random Binning-Source Channel Erasure Codes (SCEC)
Here, we focus on the symmetric MD coding problem,
where all descriptions are encoded at the same rate and
distortion is determined by how many rather than which
descriptions were received. Thus, the achievable rate distortion
region is a subset of the L + 1 dimensional space of vectors
(R, D1 , D2 , ..., DL ) where Dk is the distortion incurred if k
descriptions are received, and R is the description rate.
Suppose for now that we know that at least k descriptions
are received. Then one may, as a ﬁrst approach, employ the
best rate kR source code and encode it for the L channels
using a (L, k) maximum distance separable (MDS) code.
Thus, if any k descriptions are received we will have the
optimal distortion for this rate. However, if more than k
descriptions are received the decoder does not gain any new
information. Such an encoding scheme is equivalent to generating one codebook at rate kR and having it random binned
independently for each of the descriptions. We will refer to
such codes as MDS codes.
Alternatively, one could generate L independent codebooks
in order to gain new information for each received description,
and randomly bin the codewords so that the decoder can ﬁnd
only one sequence jointly typical with the source. However, it
is not obvious at ﬁrst that if we can still achieve the optimal
distortion for a rate kR code. The key result of [7] shows
that with the reception of k descriptions one may recover
the source coded at rate kR and monotonically decrease the
distortion with reception of additional descriptions. We refer
to such codes as ‘1“source channel erasure codes, SCEC. The
achievable region RDP P R1 is formalized below.
PPR-1 (SCEC) Region: Let Y1 , ..., YL be a collection of
random variables that are jointly and symmetrically distributed
with an arbitrary source random variable X. Then, if at least
k descriptions are received and

{S : S ⊆ A, |S| ≥ m}.

B. Venkataramani-Kramer-Goyal
VKG scheme is a generalization to L ≥ 2, of the conditional
codebook schemes originally designed for L = 2. Let us
present an overview of the encoding scheme. The order of
codebook generation of the auxiliary random variables is
shown in Figure 1. Observe that the codebook generation
follows the order: shared layer → base layer → reﬁnement
layer. First, the codebook associated with VIL is generated
with the marginal pVIL . Conditioned on each codeword from
VIL , the codebook for Ul where l ∈ IL , is generated
according to their respective conditional densities. Next, for
each UA , A ∈ 2I a single codeword is generated, conditioned
2
on the codeword tuples from the previous layers.
On observing a typical sequence xn , the encoder tries to
ﬁnd a jointly typical codeword tuple one from each codebook.
The codeword index of Ul is sent in the description l ∈ IL .
Along with the private messages, each descriptions also carries
a shared message, which is the codeword index of VIL . Note
that VIL is the only shared random variable. Ul where l ∈ IL
form the base layer random variables and all UA , A ∈ 2I form
2
the reﬁnement layers.

D|K| ≥ E{d(X, gK (YK )}
1
1
R ≥ H(Y1 , Y2 , ..., Yk ) − H(Y1 , Y2 , ..., YL |X)
k
L
the rate distortion vector (R, Dk , Dk+1 , ..., DL ) is achievable.

C. Combinatorial Message Sharing
In [5], we generalized VKG by including a common codebook for each subset of the base descriptions. The order
of codebook generation of the auxiliary random variables is

2

for all l ∈ IL−1 . Then, total rate for each description is Rj =

Random Code Generation: Construct a random codebook
with pYi independently for each Yi , i ∈ IL by selecting 2nR
codewords uniformly.
Random Binning: To each codebook associate 2nR bins,
each containing approximately 2n(R −R) codewords.
Encoding: Given a sourceword xn , ﬁnd the indices
i1 , i2 , ..., iL such that the corresponding codewords and source
xn , y1 (i1 ), y2 (i2 ), ..., yL (iL ) are jointly typical. Transmit the
bin index il over the channel l, ∀l ∈ L.
Decoding: For a set of received bin indices indexed by K
the decoder ﬁnds for ki ∈ K the yK (iK ) that are jointly typical
and each are contained in the bin corresponding to the received
index. If more than one such set exists, it declares an error.
Otherwise it decodes using gK (yK ). It follows from strong
typicality and counting arguments [13], [14] that error event
probability vanishes asymptotically for large n.

L

Rl,j . Let us call the achievable region obtained by this
l=1

scheme RDP P R2 .
III. P ROPOSED S CHEME
Here, we integrate the random binning approach within the
CMS coding structure. Basically, we use the PPR approach
of layering (L, k) MDS codes for each subset of the base
descriptions and continue to use the PPR enhancement descriptions, i.e., layered SCEC. Note that at the base layer,
we do not have the luxury of using independent codebooks
(hence, we have MDS codes instead of SCEC at the base
layer), but we can bin the base layer codewords, independently
and at different rates for each description. We begin with a four
description example, that illustrates the operation and the use
of the proposed scheme.

E. Layered Random Binning:
A. Four Descriptions Example

In [8], a multilayer random binning scheme is proposed,
which can be roughly described as the concatenation of (L, k)
SCEC’s for k = 1, 2, ..L. More speciﬁcally, there are a
total of L layers in this coding scheme, and the encoding
and decoding can be performed from lower layers to higher
layers sequentially. At the lth layer and for any description
j, a codebook of size 2nRl,j is generated using the marginal
distribution of Yl,j . The codewords in a codebook are then
randomly and independently assigned a total of 2nRl,j bin
indices, for all l and j.
The rates Rl,j s should be sufﬁciently large such that for any
typical source sequence, with high probability there exist code
words in these codebooks of a certain layer such that they are
jointly typical with the source sequence and the codewords
found in previous layers. Using the property of symmetric
distribution (see [8]), it can be shown that this can be done
with
L

L

Rl,j ≥
j=1

The base and reﬁnement codebook generation is shown in
Figure 3. Base and reﬁnement layer codebooks are denoted C
and C respectively. The rates of the codebooks (denoted r)
are determined by the ability to encode successfully by ﬁnding
a set of codewords jointly typical with the source and the
binning rates (denoted ρ) are determined by the requirement
of successfully decoding by ﬁnding a unique jointly typical set
of codewords in the received bins. For the base descriptions
broadcasted to all channels {1, 2, 3, 4}, there are 4 layers of
MDS codes, a (4, l) code that uses the codebook C1234,l for
l = 1, 2, 3, 4. Hence, there are 4 codebooks for that level.
The codeword from C1234,l can be decoded if l descriptions
are received. Note that there is identical coding rates for each
of the descriptions, but we bin each codebook, independently
4 times at different binning rates denoted as ρ1234,l,i , for
i = 1, 2, 3, 4 for the codebook C1234,l . In each bin there
are approximately 2n(r1234,1 −ρ1234,1,i codewords that can be
decoded with the help of side information, hence only the bin
rates contribute to the description rates. The codeword from
the codebook C1234,l can be decoded if any l descriptions
are received. For the base descriptions, broadcasted to the
subset of three channels A ∈ {123, 234, 124, ...} there are
three layers MDS codes: (3, l) MDS code that uses CA,l for
l = 1, 2, 3. Next, we have the layer with |A| = 2 and |A| = 1
in a similar fashion.
For the reﬁnement descriptions, there are three layers of
SCEC: (4, l) SCEC for l = 1, 2, 3. Note that a (4, 4) SCEC
code is not necessary because there is already an (4, 4) MDS
code in the base description as the codebook C1234,4 that is
transmitted to all the decoders. Any reﬁnement information
can be placed there, since it is decodable only if all of the
descriptions are received. Note that a (4, 4) MDS code is
identical to a (4, 4) SCEC.
For decoding, lets us consider the decoder {12} which
receives the descriptions 1 and 2. This decoder uses C1234,1 ,
C1234,2 , C123,1 , C123,2 , C134,1 , C234,1 , C12,1 , C12,2 , C13,1 ,
C23,1 , C14,1 and C24,1 as the base layer codebooks and C1,1 ,

j=1

H(Ylj ) − H(Yl,IL |X, YIL−1 ,IL )

(1)

At the decoder, with any l descriptions such that l ∈ IL−1 ,
the lower l layers are decoded sequentially from the lower
to the higher layers in l steps. More precisely, the decoder
receives descriptions in the set |A| = l. Taking an induction
approach, we assume the ﬁrst l − 1 layers of codewords are
correctly decoded, and only consider the decoding of the lth
layer. If there exists a unique set of codewords in the bins of
the corresponding codebooks, speciﬁed by the descriptions in
the set , that are jointly typical with each other, and at the
same time they are jointly typical with the correctly decoded
lower layer codewords, then the decoder reconstructs using
the single letter decoding function gA ; otherwise a decoding
failure occurs. For successful decoding, it can be shown that
the rates need to satisfy
l

Rl,j − Rl,j ≥
j∈A

j=1

H(Yl,j ) − H(Yl,Il |YIl−1 ,Il )

(2)

3

C1234,1

C123,1

2nρ1234,1,i123,1
2nr

2nr1234,1

2nρ123,1,j
2nr12,1

C123,2

C1234,2

2nρ1234,2,i
2nr123,2

2nr1234,2

￿
C1,1

C1
2nρ12,1,z

2nρ1

2nr1

￿
C1,2

C12,2
2nρ123,2,j
2nr12,2

2nρ12,2,z

2nρ1234,3,i
2nr123,3

2nr1,4

￿
C3,3

2nρ3,3

2nρ2,3 3,3
2nr

￿
C2,4

2nρ1,4

￿
C3,4

2nρ2,4nr3,4
2

2nρ3,4

Reﬁnement Layer

Base Layer

Fig. 3.

2nρ3,2

2nρ2,2 nr3,2
2

￿
C2,3

￿
C1,4

2nρ3,1

￿
C3,2

2nρ1,3 nr2,3
2

2nr1,3

2nρ1234,4,i

2nr1234,4

￿
C2,2

￿
C1,3

C1234,4

2nρ2,1 3,1
2nr

2nρ1,2

2nr1,2

2nρ123,3,j

￿
C3,1

￿
C2,1

2nρ1,1 nr2,1
2

2nr1,1

C123,3

C1234,3
2nr1234,3

C12,1

Codebook structure for the proposed scheme for L = 4 descriptions, where i = 1, 2, 3, 4, j = 1, 2, 3 and z = 1, 2.

C1,2 , C2,1 , C2,2 are the enhancement layer codebooks used to
reconstruct the source through the single letter function g1,2 .
Note that the proposed scheme can utilize the subset symmetry in our running example of R1 > R2 = R3 = R4 = R
albeit using MDS codes instead of SCEC, for the subset
{2, 3, 4}, by setting the encoding and binning rates accordingly., i.e., by setting ρ2,3,4,k,j = ρj1 for all k = 1, 2, 3 and
similarly,ρ2,3,k,j = ρ3,4,k,j = ρ2,4,k,j = ρj2 for k = 1, 2 for
the remaining base and reﬁnement layers, we have a scheme
that is very similar to PPR-2 for descriptions 2, 3 and 4. The
trivial combination of CMS base layer with PPR reﬁnement
layer will not be able to utilize this symmetry because the
symmetry lies within the subset. Similarly, CMS scheme will
not be able to use this symmetry in the subsets, since it is
designed for the general setting.

DA ≥ E{X, gA (UIΞ ,A , VK∈2A ,I|A∩K| )}, ∀A ∈ 2IL
1
1

(3)

Rl =

(4)

ρk,l +
k∈IL−1

I
A∈21 L ,
l∈A,k∈I|A|

ρA,k,l , ∀l ∈ IL

for some joint distribution, decoding functions and rates
satisfying the following
H(UK,L , VA∈T ,KA |X) ≥

A∈T ,k∈KA

+

H(VA,k ) − rA,k

H(Uk,l ) − rk,l

(5)

k∈K,l∈lL
IL

for any T ∈ 222 , KA ∈ 2IL and L ∈ 2IL and
1
1

B. Achievable Region

H(UIΞ ,A ,VK∈2A ,I|A∩K| ) ≤
1

For each subset, A of the channels that a base description
broadcasts to, we layer (|A|, k) MDS codes for k = 1, 2, .., |A|
over the channels in A. For the reﬁnement descriptions, we
layer (L, k) SCEC’s for k = 1, 2, ..., L − 1. Decoder A
will decode the reﬁnement codewords generated from the
codebooks with k ≤ |A| and subset K base descriptions of all
codebooks with k ≤ |A ∩ K|. It then reconstructs the source
jointly using all received descriptions.
The following theorem present the achievable region for the
proposed MDC scheme. We call this region, RDCM S−RB .

H(Uk,l ) + ρk,l − rk,l
k∈A,l∈IΞ

+
K∈2A ,K∈I|A∩K| ,l∈A
1

H(VK,k ) + ρK,k,l − rK,k (6)

for any A ∈ 2IL where Ξ = min{|A|, L − 1}.
1
Proof: Consider a joint distribution that satisﬁes the rate
and distortion bounds. Then, an encoding scheme with random
codebooks and binning can be constructed as follows:
Base layer codebook generation: For every A ∈ 2IL ,
2
we generate a set of |A| MDS codes, where the k th code
is a (|A|, k) MDS code, which is basically a codebook
generated with the distribution PVA , and the codeword from
this codebook is binned |A| times independently. For each

Theorem 1. An achievable rate-distortion region with the
proposed MDC scheme is the convex closure of all ratedistortion tuples satisfying

4

index, qA,k ∈ 2nrA,k we generate a length n codeword by independently choosing its symbols from a codebook generated
with the marginal PUA,k .
Reﬁnement layer codebook generation: We generate a set
of L − 1 codes where the k th code is an (L, k) SCEC, which
itself consists of a set of L codebooks, one for each channel.
The k th codebook for channel l denoted as Ck,l is generated
as follows. For each index qk,l ∈ 2nrk,l generate a length n
codeword choosing its symbols from a codebook generated
with the marginal PUk,l .
Binning: For each base codebook CA,k , we randomly bin
the codewords L independent times for each description, with
I
the binning rate ρA,k,l for all A ∈ 21 L , k ∈ I|A| , l ∈ IL .
For the reﬁnement codebooks, Ck,l , we randomly bin
(once), with the binning rate ρk,l , for all k ∈ IL−1 , l ∈ IL .
Encoding: Given a length n source sequence xn , the
encoder ﬁnds, if possible, two sets of indices qA∈2IL ,I and
|A|
1
qIL−1 ,IL such that the codewords associated with these indices
are jointly typical. If more than one is found, we choose
arbitrarily. If none is found, we declare an encoding error εE
by assigning each index to an arbitrary number not used in
the encoding.
Decoding: Decoder A, where A ∈ 2IL , receives the
1
descriptions associated with A. If these indices indicate an
encoding failure εE , the decoder estimates the source using
only the statistics and achieves a distortion bounded by a ﬁnite
constant dmax . Otherwise, it ﬁnds a unique set of quantization
indices such that the corresponding codewords are jointly
typical.
Successful encoding guarantees the existence of at least one
set of indices. If more than one is found, the decoder declares a
decoding failure εD and reconstructs the source using only the
statistics knowledge, achieves a distortion upper bounded by
dmax . Otherwise, the decoder recovers the quantization indices
perfectly and reconstruct the source using gA and achieves an
overall distortion of no more than DA + + P{εE ∪ εD }dmax
as in (3) for any and large enough n.
Next, we will show P{εE ∪ εD } → 0 as n → ∞. First,
we break up the encoding and decoding error probabilities as
follows by the union bound:

Hence, the overall decoding error vanishes if (6) holds.
Corollary 1. RDP P R1
RDCM S−RB .

⊆ RDCM S−RB , RDP P R2

⊆

Proof: Since RP P R1 ⊆ RP P R2 by construction, we will
only show RP P R2 ⊆ RCM S−RB . Setting VA,k,l = Φ in
RDCM S−RB for A ∈ 2IL , k ∈ I|A| , l ∈ IL , where Φ is
deterministic, and setting codebook and binning rates at the
reﬁnement layer to identical value for each description, i.e.,
rk,l = rk , ρk,l = ρk , ∀l ∈ IL yield RP P R2 by construction.
IV. C ONCLUSION
In this paper, we proposed a novel MD coding scheme
based on random binning within the CMS coding structure.
The proposed scheme leverages the CMS structure to achieve
efﬁcient adaptation to asymmetric rate and distortion while
utilizing partial symmetry wherever it exists, with layered
random binning. An example is given to provide intuition for
potential beneﬁts over prior schemes. Future work includes
studying the beneﬁts of the proposed scheme over CMS and
other MD schemes based on random binning.
R EFERENCES

[1] L. Ozarow, “On a source-coding problem with two channels and three
receivers,” Bell Syst. Tech. J, vol. 59, no. 10, pp. 1909–1921, 1980.
[2] A.E. Gamal and T. Cover, “Achievable rates for multiple descriptions,”
IEEE Transactions on Information Theory, vol. 28, no. 6, pp. 851–857,
1982.
[3] Z. Zhang and T. Berger, “New results in binary multiple descriptions,”
IEEE Transactions on Information Theory, vol. 33, no. 4, pp. 502–521,
1987.
[4] R. Venkataramani, G. Kramer, and V.K. Goyal, “Multiple description
coding with many channels,” IEEE Transactions on Information Theory,
vol. 49, no. 9, pp. 2106–2114, 2003.
[5] K. Viswanatha, E. Akyol, and K. Rose, “Combinatorial message
sharing for a reﬁned multiple descriptions achievable region,” in IEEE
International Symp. on Information Theory. IEEE, 2011, pp. 1312–1316.
[6] K. Viswanatha, E. Akyol, and K. Rose, “A strictly improved achievable
region for multiple descriptions using combinatorial message sharing,”
in IEEE Information Theory Workshop. IEEE, 2011, pp. 1312–1316.
[7] S.S. Pradhan, R. Puri, and K. Ramchandran, “n-channel symmetric
multiple descriptions-part i:(n, k) source-channel erasure codes,” IEEE
Transactions on Information Theory, vol. 50, no. 1, pp. 47–61, 2004.
[8] R. Puri, S.S. Pradhan, and K. Ramchandran, “n-channel symmetric
multiple descriptions-part ii: An achievable rate-distortion region,” IEEE
Transactions on Information Theory, vol. 51, no. 4, pp. 1377–1392,
2005.
P{εE ∪ εD } ≤ P{εE } +
P{εD (A)|εc }
(7)
E
[9] H. Wang and P. Viswanath, “Vector gaussian multiple description with
IL
A∈21
individual and central receivers,” IEEE Transactions on Information
Theory, vol. 53, no. 6, pp. 2133–2153, 2007.
where εD (A) denotes the decoding error event for the decoder [10] H. Wang and P. Viswanath, “Vector gaussian multiple description with
two levels of receivers,” IEEE Transactions on Information Theory, vol.
A. It follows from the well known property of typical sets and
55, no. 1, pp. 401–410, 2009.
the counting arguments used in [7] that if (5) holds, P{εE } [11] C. Tian, S. Mohajer, and S.N. Diggavi, “Approximating the gaussian
vanishes. We are left with upper bounding P{εD (A)|εc }.
multiple description rate region under symmetric distortion constraints,”
E
1
IEEE Transactions on Information Theory,, vol. 55, no. 8, pp. 3869–
Let the error exponent be Φ limn→∞ n log P{εD (A)|εc }.
E
3891, 2009.
Then, using the same error bounding approach in [7] (omitted [12] C. Tian and J. Chen, “New coding schemes for the symmetric kdue to space constraints), we get
description problem,” IEEE Transactions on Information Theory, vol.
56, no. 10, pp. 5344–5365, 2010.
Φ ≤ H(UIΞ ,A , VK∈2A ,I|A∩K| ) −
H(Uk,l ) + ρk,l − rk,l [13] T. Berger, “Multiterminal source coding,” The Information Theory
1
approach to communications, vol. 229, pp. 171–231, 1977.
k∈Al∈IΞ
[14] I. Csiszar and J. K¨ rner, Information theory: coding theorems for
o
discrete memoryless systems, Cambridge Univ Pr, 2011.
−
H(V ) + ρ
−r
(8)
K∈2A ,k∈I|A∩K| ,l∈A
1

K,k

K,k,l

K,k

5

