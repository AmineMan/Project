Title:          paper.dvi
Creator:        dvips(k) 5.96 Copyright 2005 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 02:14:49 2012
ModDate:        Tue Jun 19 12:56:35 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      463629 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569565489

The Peak Constrained Additive Inverse Gaussian
Noise Channel
Andrew W. Eckford

K. V. Srinivas

Raviraj S. Adve

Dept. of Comp. Sci. and Engg.
York University
Toronto, Ontario, Canada M3J 1P3
E-mail: aeckford@yorku.ca

Mobile R&D Division
Samsung Electronics India
Noida - 201301, UP, India
E-mail: k.v.srinivas@samsung.com

The E. S. Rogers Sr. Dept. of ECE
University of Toronto
Toronto, Ontario, Canada M5S 3G4
E-mail: rsadve@comm.utoronto.ca

channel with a peak constraint: i.e., there exists a time T at
which the communication session must end. As a result of this
restriction, many of the results in [5] require modiﬁcation.
The main contribution of this paper is a closed-form upper
bound on the capacity of the peak-constrained AIGN channel.
Few analytical results exist for this family of channels, and our
result represents an important step towards the mathematical
understanding of their capabilities and limitations.
Our paper adds to a rapidly expanding body of research on
the information theory of molecular communication, for which
an excellent survey is found in [1]. Information-theoretic
analysis of the molecular communication problem has been
considered in several recent papers: diffusion of concentrations has been considered in [6]; “active” propagation (e.g.,
using molecular motors) has been analyzed in [7], [8]; and
analysis of models including biological components has been
performed in [9], [10]. Our paper contributes to this growing
body of literature by providing an analytical result, which (in
future work) will provide a basis for performance measurement and optimal communication strategies (e.g., modulation
techniques).

Abstract—In molecular communication, messages are conveyed
in patterns of particles (e.g., arranged in time), which propagate
from transmitter to receiver by means of Brownian motion. If
there is drift from transmitter to receiver, the ﬁrst arrival time of
the particles has the Inverse Gaussian distribution, leading to the
additive inverse Gaussian noise channel. In this paper, we give
a closed-form upper bound on capacity for this channel when
the maximum waiting time is constrained, building on previous
work in which only the mean waiting time was constrained.

I. I NTRODUCTION
In diffusion-mediated molecular communication, messages
are carried by molecules (or other particles), which propagate
from transmitter to receiver by Brownian motion [1]. Mimicking the communication among bacteria and cells in the body,
molecular communication has excellent properties in terms of
energy efﬁciency, simplicity, and biocompatibility, making it
a potential solution to the problem of nanoscale networking
[2], especially in biomedical applications [3].
Consider a molecular communication system in which the
transmitter encodes a message in the timing of molecule
releases. For example, say the message consists of one bit:
a molecule is released at time T0 to represent 0, or at T1 to
represent 1. The message is distorted by the random propagation time n between the transmitter and receiver, known
as the ﬁrst arrival time: the receiver observes T0 + n if 0
was sent, or T1 + n if 1 was sent. Thus, the ﬁrst arrival
time acts as additive noise in the timing channel, analogous
to additive noise in conventional communication systems. In a
one-dimensional ﬂuid medium, in which there is a net drift
from the transmitter to the receiver, the ﬁrst arrival time
has the Inverse Gaussian (IG) distribution [4]. A molecular
communication timing channel in which the ﬁrst arrival time
is IG distributed is known as an Additive Inverse Gaussian
Noise (AIGN) channel. (Note that this does not include the
case of Brownian motion without drift.)
Many closed-form properties of the IG distribution are
known, which can be exploited in the analysis of the AIGN
channel. In previous work [5], we considered detection, modulation, and capacity of AIGN channels, in which there was a
constraint on mean waiting time. However, if only the mean
is constrained, the maximum waiting time is unlimited. In
this paper, we consider a more practical version of the AIGN

II. M ODELS
A. Physical model
Our system consists of a one-dimensional medium, with
a transmitter at the origin and a receiver located at position
d > 0, as depicted in Figure 1. Our results can be applied
to three-dimensional media where the Brownian motions in
all three dimensions are independent, and where the receiver
is an inﬁnite plane; in this case, only the Brownian motion in
the dimension perpendicular to the plane is relevant. Molecules
released into the medium propagate by Brownian motion, with
a drift velocity v > 0 from transmitter to receiver. Drift
velocity is the expected value of displacement, divided by the
time over which that displacement occurs; it can be caused
by the net motion of molecules in the medium, such as blood
ﬂow in capillaries.
This system is used to transmit information from transmitter
to receiver. Throughout this paper, we assume the molecular
communication system is ideal:
1) The transmitter and receiver are perfectly synchronized;

1

Let
ig(n; λ, µ)

λ(n − µ)2
λ
exp −
3
2πn
2µ2 n
λ/2
λ
Kn−3/2 exp − 2 n −
2µ
n

=
=

(3)
(4)

represent the Inverse Gaussian distribution with parameters µ
and λ, where
λ λ/µ
K=
e ,
(5)
2π
which is constant with respect to n. If v > 0, with µ given by
(1) and letting
d2
d2
λ= 2 =
,
(6)
σ
4D
we have that (from [4])
fN (n) = ig(n; λ, µ).

(7)

We will use the differential entropy of the IG distribution
h(N ), which can be given in closed form. For fN (n) given
by (7), and measured in nats,
Fig. 1.
Upper ﬁgure: Illustration of the system assumptions; a onedimensional, semi-inﬁnite medium with a reservoir located at the origin, and
an absorbing barrier located at the receiver. Lower ﬁgure: Illustration of three
instances of Brownian motion with drift for a barrier located at d = 10.

∞

h(N )

ig(n; λ, µ) log ig(n; λ, µ)dn

(8)

0

= log 2K−1/2 (λ/µ)µ
∂
3 ∂γ Kγ (λ/µ) γ=−1/2
2
K−1/2 (λ/µ)
λ K1/2 (λ/µ) + K−3/2 (λ/µ)
+
,
2µ
K−1/2 (λ/µ)

2) The transmitter perfectly controls the release time of the
molecules;
3) The receiver perfectly measures the arrival time of the
molecules; and
4) On ﬁrst arrival at the receiver, molecules are absorbed
and removed from the system. (If this arrival time occurs
after the time limit T , then the molecules remain in the
system.)
These assumptions may be challenging to achieve in practice,
but they lead to information-theoretically useful results: in
[11], it was shown that relaxing any of them leads to a
reduction in capacity.

+

(9)

where Kγ (·) is the order-γ modiﬁed Bessel function of the
second kind. This expression is found in [5], a special case of
results found in [4]. (An expression for the derivative of the
Bessel function with respect to its order is found in [12].)
C. Channel model
In a Peak-Constrained AIGN (PCAIGN) channel, there is
some maximum waiting time T , at which point the receiver
is required to make a decision on its observation. We call
this “peak-constrained” because the signal is expressed in the
magnitude of the timing signal; thus, the peak of the time
signal must be less than or equal to T . Letting x represent the
release time of a molecule, and letting n represent an Inverse
Gaussian random variable, the observation y is given by

B. Mathematical model
Mathematically, we model the system as follows. The
position of the molecule is a function of time, written B(t).
Suppose the molecule is released at time t = 0. Then where
B(t) = 0 for all t ≤ 0, and for each t > 0, B(t) is a Gaussian
random variable with mean µ, given by
µ = vt,

=

(1)

y=

2

and variance of σ t = 4Dt. (If the molecule is released at
time t = τ , the same properties hold, substituting t − τ for t.)
The ﬁrst arrival time n of a Brownian motion B(t) is deﬁned
as
n = min{t : B(t) = d}.
(2)

x + n, x + n ≤ T ;
∞,
x + n > T.

(10)

The use of y = ∞ in (10) represents the event that the receiver
never observes the molecule.
The message alphabet need not be binary, and can be any
arbitrary set; for instance, we can send M -ary with release
times T0 , T1 , . . . , TM−1 . As a result, and for mathematical
convenience, we measure information in nats throughout this
paper.

Since B(t) is a random process, n is a random variable,
and its distribution fN (n) is known as the ﬁrst arrival time
distribution.

2

III. R ESULTS

explanation hides many details that we resolve later.) We can
also choose an upper bound on H(Y ) to get a bound on
capacity.
Our ﬁrst step is to ﬁnd h(N |n ≤ T ). Since h(N ) is given
by (9), it will be most convenient to write

A. Main Result
Capacity C, in information units per channel use, is given
by
C = max I(X; Y ).
(11)
fX (x)

∞

Throughout this paper, we measure information in nats. In
the AIGN channel, a channel use consists of the release of a
single molecule at the transmitter, together with its observation
at the receiver. Thus, capacity per channel use is equivalent to
capacity per molecule. Our main result is a closed-form upper
bound on capacity.
Let Γ(α, z) represent the upper incomplete Gamma function, deﬁned as

T

(18)
and ﬁnd bounds for the second term.
We start by proving some bounds on integrals involving
ig(n; λ, µ) log ig(n; λ, µ). Note that
log ig(n; λ, µ) = log K −

z α−1 e−z dz.

(12)

0

For any x > 0, let
φα (x; λ, µ) = K

2µ2
λ

α− 1
2

1 λ
Γ α − , 2x .
2 2µ

1
e,
log log x
log x ,

x < ee ;
x ≥ ee ,

(20)
Proof: For the integral in (20), the index of integration is
z, and it is always true that z ≥ x. For z ≥ x > 0, we have
that

(14)

e−λ/2x exp −

3
L(x; λ, µ) = (log K)φ0 (x; λ, µ) − φ (x) (x; λ, µ)
2
λ
−
φ1 (x; λ, µ)
2µ2
λ
−
φ−1 (x; λ, µ).
(15)
2
Finally, let pT = Pr(n ≤ T ) represent the probability that the
ﬁrst arrival time n is not greater than the maximum waiting
time T . The deﬁnitions in (12)-(15) are required to state our
main result; the signiﬁcance of these quantities will become
apparent later.
The main result of this paper is stated as follows.
Theorem 1: Capacity C, measured in nats per molecule,
satisﬁes
T
− h(N ) − L(T ; λ, µ),
(16)
C ≤ pT log
pT

λ
λ/2
z−
2µ2
z
λ
≤ exp − 2 z .
2µ

≤ exp −

log z ≤ z

(21)
(22)

(x)

.

(23)

Proof: First, for any 0 < z ≤ 1, log z ≤ 0 while z α > 0
(for any α). Thus, we can restrict ourselves to z > 1 for the
remainder of the proof.
We now show that log z ≤ z 1/e , which must hold for all
z > 1. If log z ≤ z 1/e , then log log z − (1/e) log z ≤ 0, with
equality when z = ee . Taking the ﬁrst derivative,
d
1
(log log z − (1/e) log z) =
(e − log z),
dz
ze log z

B. Integral bounds
As a general outline of our strategy, capacity is given by
p(x)

λ
z
2µ2

The lemma follows from multiplying the terms in (21-22)
by Kz α−3/2 and integrating, with the appropriate changes of
variables.
We deal with the log n term in (19) by bounding it with a
monomial; we can then use Lemma 1 to deﬁne a bound. We
ﬁrst have:
Lemma 2: For (x) deﬁned in (14), and for all z > x > 0,

where h(N ) is deﬁned in Section II-B.
The remainder of the paper is dedicated to the proof of
Theorem 1.

p(x)

z α ig(z; λ, µ)dz ≤ φα (x; λ, µ).
x

and let

C = max I(X; Y ) = max (H(Y ) − H(Y |X)) .

(19)

∞

e−λ/2x φα (x; λ, µ) ≤

(13)

Further, let
(x) =

λ
λ/2
3
log n − 2 n −
.
2
2µ
n

The latter two terms require integrals of the form
nα ig(n; λ, µ), for α = 1 and α = −1, respectively; in Lemma
1, we give an integral relation for this case.
Lemma 1:

x

Γ(α, x) =

ig(z; λ, µ) log ig(z; λ, µ)dz,

h(N |n ≤ T ) = h(N ) −

(24)

for which the sign is determined by (e − log z); if z < ee ,
(e − log z) < 0, and if z > ee , then (e − log z) > 0. Thus,
z = ee is a maximum point, so log log z − (1/e) log z ≤ 0.
Finally, we show that log z ≤ z log log x/ log x , which must
hold for all z ≥ x ≥ ee . This statement is equivalent to

(17)

The quantity H(Y |X), the uncertainty in the noise given the
input, is intuitively equal to the entropy in the noise, h(N );
however, we know that n ≤ T so that the particle does not
get lost, so we must calculate h(N |n ≤ T ). (This intuitive

log log z −

3

log log x
log z ≤ 0,
log x

(25)

0

and equality is obviously achieved when z = x. For z > x,
taking the ﬁrst derivative,

−0.1

d
dz

−0.2

(26)

−0.3
Integral value

log log x
log log z −
log z
log x
log z
1
1−
log log x .
=
z log z
log x

For any x ≥ ee , log log x ≥ 1, and for any z > x,
log z/ log x > 1. Thus, the right hand side of (26) is negative
for all z > x ≥ ee , which is sufﬁcient to show that (25) is
correct.
Lemma 3:

−0.4

−0.5
µ = 2, UB
µ = 2, numerical
µ = 2, LB
µ = 3, UB
µ = 3, numerical
µ = 3, LB

−0.6

(log x)e−λ/2x φ0 (x; λ, µ)

−0.7

∞

≤

(log z)ig(z; λ, µ)dz

(27)

−0.8

x

≤

φ

(x) (x; λ, µ).

(log x)ig(z; λ, µ) ≤ (log z)ig(z; λ, µ)

25

30

and
ig(z; λ, µ).

h(N |n ≤ T )

(34)
1
≥ log pT +
(h(N ) + L(T ; λ, µ)) .
pT
One could also construct an upper bound on differential
entropy using Theorem 2. However, as we can see from Figure
2, the upper bound is much looser than the lower bound.

(30)

The lemma follows from integrating (29) and (30), using the
bounds from Lemma 1.
We now give upper and lower bounds on the integral. (The
upper bound is not used any further in this paper, but is given
for completeness.)
Theorem 2:

D. Bounds on capacity
There are two challenges to be overcome in order to
calculate capacity. First, the conditional entropy H(Y |X) can
no longer be reduced to the noise differential entropy h(N )
(from (9)), since n can’t be obtained from y − x, as in the
unconstrained case. Second, the distribution of the channel
output y has both continuous (i.e., y = x + n) and discrete
(i.e., y = ∞) components.
To address the ﬁrst challenge, consider an alternative channel model: instead of the communication session ending at
time T , as in the PCAIGN, we suppose that the molecule
“expires” exactly T seconds after release. In this channel,
letting n represent the IG channel noise, and letting

∞

ig(z; λ, µ) log ig(z; λ, µ)dz ≤

(31)

x

3
e−λ/2x log K − (log x) φ0 (x; λ, µ)
2
−λ/2x λ
− e
φ1 (x; λ, µ)
2µ2
λ
− e−λ/2x φ−1 (x; λ, µ).
2
Furthermore, using L(x; λ, µ) from (15),
∞

ig(z; λ, µ) log ig(z; λ, µ)dz ≥ L(x; λ, µ).

(32)

x

Proof: The proof follows directly from Lemmas 1-3.
Figure 2 gives an illustration of the bounds.

n∗ =

y ∗ = x + n∗ .

The integral bounds in Theorem 2 are similar to the form
required to calculate the differential entropy h(N |N ≤ T ). In
the following corollary, we complete the calculation to obtain
a lower bound on this quantity.
Corollary 1: Suppose n is an IG random variable with
parameters λ and µ, and entropy H(N ). Then
ig(n; λ, µ)
,
pT

n, n < T
,
∞, n > T

(35)

the channel outputs of this alternative channel are given by

C. Bounds on differential entropy

fN (n|n ≤ T ) =

15
20
x (starting point of integral)

(29)

for z ≥ x. Further, from Lemma 2,
(x)

10

Fig. 2.
Example illustrating the accuracy of the bounds in (31)-(32),
compared with a numerical calculation of the integral. In all curves, λ = 1.

Proof: Since log z is an increasing function,

(log z)ig(z; λ, µ) ≤ z

5

(28)

(36)

That is, the noise (rather than the channel output) is truncated
at time T . We have the following result.
Lemma 4: I(X; Y ∗ ) ≥ I(X; Y ).
Proof: From (36) and (10), y can be written as a function
of y ∗ , i.e., y = ψ(y ∗ ), where
ψ(y ∗ ) =

(33)

4

y∗, y∗ ≤ T
∞, y ∗ > T

(37)

1.6

By the data processing inequality, I(X; Y ∗ ) ≥ I(X; Y ).
To address the second challenge, we obtain an auxiliary
random variable u∗ , given by
(38)

Mutual information upper bound

0, y ∗ < ∞
1, otherwise

u∗ =

Thus, u∗ is a discrete random variable, and given that u∗ = 0,
y ∗ is a continuous random variable. Since u∗ is a function of
y∗,
I(X; Y ∗ )

= I(X; U ∗ , Y ∗ )

(39)

= I(X; Y ∗ |U ∗ ),

(40)

= H(Y ∗ |U ∗ = 0)Pr(U ∗ = 0)
+H(Y ∗ |U ∗ = 1)Pr(U ∗ = 1)

Fig. 3.

4

6

8

10
12
Maximum waiting time T

14

16

18

20

Upper bound on mutual information for various parameter settings.

values of T , the “dip” indicates that the bound is not tight in
this region; capacity must be nondecreasing with T . However,
at larger values of T , we believe the bound to be a good
approximation of the true capacity, due to the accuracy of the
integral bounds.

(43)

H(Y ∗ |U ∗ = 0)Pr(U ∗ = 0)
−H(Y ∗ |X, U ∗ = 0)Pr(U ∗ = 0).(44)

R EFERENCES
[1] S. Hiyama and Y. Moritani, “Molecular communication: Harnessing
biochemical materials to engineer biomimetic communication systems,”
Nano Communication Networks, vol. 1, no. 1, pp. 20–30, 2010.
[2] I. F. Akyildiz, F. Brunetti, and C. Blazquez, “Nanonetworks: A new
communication paradigm,” Computer Networks, vol. 52, pp. 2260–2279,
Aug. 2008.
[3] Y. Moritani, S. Hiyama, and T. Suda, “Molecular communication for
health care applications,” in 4th Ann. IEEE Intl. Conf. on Pervasive
Computing and Communications Workshops, Pisa, Italy, 2006.
[4] R. S. Chhikara and L. Folks, The Inverse Gaussian Distribution: Theory,
Methodology, and Applications. New York: Marcel Dekker, Inc., 1989.
[5] K. V. Srinivas, R. S. Adve, and A. W. Eckford, “Molecular communication in ﬂuid media: The additive inverse Gaussian noise channel,”
Submitted to IEEE Trans. Inform. Theory. arXiv:1012.0081.
[6] D. J. Spencer, S. K. Hampton, P. Park, J. P. Zurkus, and P. J. Thomas,
“The diffusion-limited biochemical signal-relay channel,” Adv. Neural
Inf. Process. Syst., vol. 16, 2004.
[7] N. Farsad, A. W. Eckford, S. Hiyama, and Y. Moritani, “Quick system
design of vesicle-based active transport molecular communication by
using a simple transport model,” Nano Comm. Networks, 2011.
[8] M. J. Moore, T. Suda, and K. Oiwa, “Molecular communication: Modeling noise effects on information rate,” IEEE Trans. Nanobioscience,
vol. 8, pp. 169–190, Jun. 2009.
[9] P. J. Thomas, D. J. Spencer, S. K. Hampton, P. Park, and J. P. Zurkus,
“The diffusion-limited biochemical signal-relay channel,” in Advances
in Neural Information Processing Systems 16 (S. Thrun, L. Saul, and
B. Sch¨ lkopf, eds.), Cambridge, MA: MIT Press, 2004.
o
[10] A. Einolghozati, M. Sardari, and F. Fekri, “Capacity of diffusionbased molecular communication with ligand receptors,” in IEEE Inform.
Theory Workshop, 2011.
[11] A. W. Eckford, “Molecular communication: Physically realistic models
and achievable information rates,” Submitted to IEEE Trans. Inform.
Theory. arXiv:cs.IT/0812.1554.
[12] Y. A. Brychkov, Handbook of Special Functions: Derivatives, Integrals,
Series and Other Formulas. Boca Raton, FL: Chapman Hall/CRC Press,
2008.

E. Proof of Theorem 1
Proof: From Lemma 4 and (38)-(44),
I(X; Y )
≤ (H(Y ∗ |U ∗ = 0) − H(Y ∗ |X, U ∗ = 0)) Pr(U ∗ = 0)
(45)
= (H(Y ∗ |U ∗ = 0) − H(Y ∗ |X, U ∗ = 0)) pT .
(46)
If u∗ = 0, then y ∗ is a continuous random variable on the
interval [0, T ]; thus,
(47)

Furthermore, given x and u∗ = 0, and for an IG random
variable n with parameters λ and µ,
H(Y ∗ |X, U ∗ = 0)
= h(N |N ≤ T )
1
(H(N ) + L(T ; λ, µ))
≥ log pT +
pT

2

(41)

Therefore,

H(Y ∗ |U ∗ = 0) ≤ log T.

0.8

0.2

and by the same argument,

I(X; Y ∗ |U ∗ ) =

1

0.4

Since Pr(Y ∗ = ∞|U ∗ = 1) = 1, then H(Y ∗ |U ∗ = 1) = 0,
and
H(Y ∗ |U ∗ ) = H(Y ∗ |U ∗ = 0)Pr(U ∗ = 0),
(42)

H(Y ∗ |X, U ∗ ) = H(Y ∗ |X, U ∗ = 0)Pr(U ∗ = 0).

1.2

0.6

where (40) follows since u∗ is only dependent on n∗ , and is
independent of x. Furthermore, note that
H(Y ∗ |U ∗ )

λ = 1, µ = 2
λ = 1, µ = 3
λ = 1, µ = 4
λ = 2, µ = 2

1.4

(48)
(49)

where (49) follows from the corollary to Theorem 2. The right
hand side of equation (16) is obtained by substituting (47)
and (49) into (46). Finally, noting that this upper bound is
independent of the input distribution f (x), it must apply to all
f (x), and is therefore an upper bound on capacity C.
Example upper bounds are given in Figure 3. As expected,
the upper bounds in Figure 3 increase as T increases. At low

5

