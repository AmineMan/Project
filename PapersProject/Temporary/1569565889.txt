Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 10:46:15 2012
ModDate:        Tue Jun 19 12:55:15 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      425285 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565889

Achieving Csisz´ r’s Source-Channel Coding
a
Exponent with Product Distributions
Adri` Tauste Campo12 , Gonzalo Vazquez-Vilar2 , Albert Guill´ n i F` bregas123 , Tobias Koch1 and Alfonso Martinez2
a
e
a
1

University of Cambridge, 2 Universitat Pompeu Fabra, 3 Instituci´ Catalana de Recerca i Estudis Avancats (ICREA)
o
¸
Email: {atauste,gvazquez,guillen,alfonso.martinez}@ieee.org, tobi.koch@eng.cam.ac.uk

Abstract—We derive a random-coding upper bound on the
average probability of error of joint source-channel coding
that recovers Csisz´ r’s error exponent when used with product
a
distributions over the channel inputs. Our proof technique for
the error probability analysis employs a code construction for
which source messages are assigned to subsets and codewords
are generated with a distribution that depends on the subset.

In [1, Prob. 5.16], Gallager provides an upper bound on
¯ using maximum a posteriori (MAP) decoding when the
codewords corresponding to different source messages are
drawn independently according to some distribution PX :

I. I NTRODUCTION

where E0 (ρ, PY |X , PX ) denotes Gallager’s channel function

¯ ≤ e−E0 (ρ,PY |X ,PX )+Es (ρ,PV ) ,

We study the problem of transmitting a length-k discrete
memoryless source over a discrete memoryless channel using
length-n block codes. The source is distributed according to
k
PV (v) = i=1 PV (vi ), v = (v1 , . . . , vk ) ∈ V k , where V is a
discrete alphabet with cardinality |V|. The channel law is given
n
by PY |X (y|x) = i=1 PY |X (yi |xi ), x = (x1 , . . . , xn ) ∈
n
X , y = (y1 , . . . , yn ) ∈ Y n , where X and Y are discrete
alphabets with cardinalities |X | and |Y|, respectively.
An encoder maps the length-k source message v to a lengthn codeword x(v), which is then transmitted over the channel.
We refer to the ratio t k/n as the transmission rate. Based
on the length-n channel output y the decoder guesses which
source message was transmitted.
We say that an error exponent E > 0 is achievable if there
exists a sequence of codes of length n such that the average
error probability (averaged over all source messages v) is
upper-bounded as
≤ e−nE+o(n) ,
(1)

ρ ∈ [0, 1],

(2)

E0 (ρ, PY |X , PX )
1+ρ
1

− log

PX (x)PY |X (y|x) 1+ρ
y

, (3)

x

and where Es (ρ, PV ) denotes Gallager’s source function
1+ρ

Es (ρ, PV )

log

PV (v)

1
1+ρ

.

(4)

v

The upper bound (2) is derived using similar techniques as
Gallager’s channel coding bound [1, p. 135]. In the context
of discrete memoryless systems and for PX being a product
n
distribution PX (x) = i=1 PX (xi ), x ∈ X n , it specializes
to
(5)
¯ ≤ e−n(E0 (ρ,PY |X ,PX )−tEs (ρ,PV )) .
Thus, for a ﬁxed t, the probability of error ¯ vanishes exponentially in n with the error exponent given by E0 (ρ, PY |X , PX )−
tEs (ρ, PV ). By minimizing (5) over PX and ρ, we obtain the
following lower bound on the error exponent EJ :

where o(n) satisﬁes limn→∞ o(n)/n = 0. The error exponent
EJ of joint source-channel coding is deﬁned as the supremum
of all achievable error exponents E.
Lower bounds on the error exponent are often derived by
drawing an ensemble of codebooks at random, and by then
analyzing the average error probability ¯, averaged over all
codebooks in the ensemble. This computation ensures the
existence of at least one codebook in the ensemble whose
average error probability is at most ¯ [1, Sec. 5.5].

G
EJ ≥ EJ

max E0 (ρ, PY |X ) − tEs (ρ, PV ) ,

ρ∈[0,1]

(6)

where we deﬁne E0 (ρ, PY |X ) maxPX E0 (ρ, PY |X , PX ).
Csisz´ r reﬁned this result using the method of types [2].
a
Csisz´ r’s approach is different from Gallager’s in several
a
ways. Firstly, Csisz´ r considers ﬁxed composition codes rather
a
than codes that are generated by product distributions – such
codes are constructed by mapping messages within a source
type onto sequences within a channel-input type. Secondly, a
suboptimal maximum mutual information decoder is used at
the receiver. This decoder ﬁrst decides on the source type that
is being transmitted and then on the source message within the

This work has been supported by the European Research Council under
ERC grant agreement 259663. A. Tauste Campo acknowledges funding
from an EPSRC (Engineering and Physical Sciences Research Council, UK)
Doctoral Prize. The work of T. Koch has received funding from the European
Community’s Seventh Framework Programme (FP7/2007-2013) under grant
agreement No. 252663. A. Martinez received funding from the Ministry of
Science and Innovation (Spain) under grant RYC-2011-08150.

1

(i)

2) Assign a channel input distribution PX to each class
Ai . Then, for each source message v ∈ Ai randomly
and independently generate codewords x(v) ∈ X n
(i)
according to PX .
3) Upper-bound the probability of error using MAP decoding and Gallager’s bounding techniques [1].
For i = 1, . . . , Nk we deﬁne

type. Csisz´ r’s code construction yields the following lower
a
bound on the error exponent:
Cs
EJ ≥ EJ

min

R
, PV
t

te

tH(V )≤R≤t log|V|

+ Er (R, PY |X ) , (7)

where e(R, PV ), referred to as the source reliability function
[3]–[5], is given by
e(R, PV )

min
Q:H(Q)≥R

D(PQ PV )

1+ρ
(i)
Es (ρ, PV )

(8)
(9)

with D(· ·) denoting the divergence, and with Er (R, PY |X )
denoting the random-coding channel exponent [1]
max E0 (ρ, PY |X ) − ρR .

ρ∈[0,1]

(10)

Nk

min

te

R
, PV
t

(i)
+Es (ρi , PV ) ,

(14)

where
h(k)

Cs
It can be checked that EJ = EJ when the minimum on the
right-hand side (RHS) of (11) is attained for a value of R such
that Esp (R, PY |X ) = Er (R, PY |X ). This holds for values of
R above the critical rate of the channel [1].
In order to obtain a clearer comparison between (6) and (7),
Zhong et al. [6] provide a compact formulation of Csisz´ r’s
a
result. Speciﬁcally, the authors invoke the Fenchel duality
theorem [7, Thm. 31.1] to rewrite (7) as

ρ∈[0,1]

exp −E0 ρi , PY |X , PX
i=1

+ Esp (R, PY |X ) . (11)

Cs
¯
EJ = max E0 (ρ, PY |X ) − tEs (ρ, PV ) ,

(i)

¯ ≤ h(k)

A lower bound on the error probability of the best code
induces an upper bound on EJ . One such upper bound is given
by the sphere-packing expoment [2, Lemma 2]
tH(V )≤R≤t log|V|

(13)

Theorem 1: For every partition Pk , for every set of channel(1)
(N )
input distributions PX , . . . , PX k , and for every set of
parameters ρ1 , . . . , ρNk ∈ [0, 1], the random-coding error
probability is upper-bounded by

ρ≥0

EJ ≤

.

v∈Ai

= sup ρR − Es (ρ, PV ) ,

Er (R, PY |X )

1

PV (v) 1+ρ

log

2Nk (k + 1)|V| (k/t + 1)|X ||Y| .

(15)

Proof: See Section III-A.
The bound (14) can be optimized over product distributions
(i)
(i)
n
PX (x) = j=1 PX (xj ), x ∈ X n , and parameters ρi ∈ [0, 1]
for i = 1, . . . , Nk to obtain
¯ ≤ ¯B (Pk )

(16)

Nk

exp − max

h(k)

(12)

ρi ∈[0,1]

i=1

¯
where E0 (ρ, PY |X ) denotes the concave-hull of E0 (ρ, PY |X ),
deﬁned as the pointwise inﬁmum over the family of afﬁne
functions that upper-bound E0 (ρ, PY |X ) as a function of ρ ∈
Cs
G
[0, 1] [7, Cor. 12.1.1]. It follows from (12) that EJ ≥ EJ ,
with the inequality possibly being strict.
In cases where the above inequality is strict, the difference
G
Cs
between EJ and EJ is typically small [6]. The methods used
to derive each exponent are conceptually different, raising a
number of questions. In this paper, we address the question
of whether ﬁxed composition codes are necessary in order to
achieve Csisz´ r’s exponent. We show that random codes gena
erated by product distributions together with MAP decoding
and bounding techniques based on Markov’s inequality can be
used to recover Csisz´ r’s exponent, answering the question in
a
the negative.

nE0 ρi , PY |X
(i)
−Es (ρi , PV )

.

(17)

If the partition Pk only has one class, i.e., A1 = V k for
k = 1, 2, . . ., the upper bound (16) recovers Gallager’s bound
on the error exponent (6):
lim −

n→∞

1
G
log ¯B (Pk ) ≥ EJ .
n

(18)

As we shall see next, with a more judicious choice of Pk
the upper bound (16) also recovers Csisz´ r’s lower bound on
a
the error exponent (7).
Speciﬁcally, (7) can be achieved by identifying the classes
A1 , . . . , ANk with the source-type classes T1 , . . . , TNk . A
source-type class Ti is deﬁned as the set of all source messages
v ∈ V k with type Pi [3, Def. 2.1]. Thus, for a given
distribution Pi on V, the source-type class Ti is the set of
all source messages v ∈ V k satisfying

II. M AIN R ESULTS
The derivation of the main results involves the following
steps:
1) Deﬁne a partition Pk of the message set V k into Nk
Nk
disjoint subsets A1 , . . . , ANk satisfying i=1 Ai = V k .
We shall refer to these subsets as classes.

Pi (a) =

1
N (a|v),
k

a ∈ V,

(19)

where N (a|v) denotes the number of occurrences of a ∈ V
in v.

2

Corollary 1: Let the classes A1 , . . . , ANk of the partition
Pk be the source-type classes T1 , . . . , TNk . Then
lim inf −
n→∞

1
Cs
log ¯B (Pk ) ≥ EJ .
n

It is easy to show that PV (·) is constant within every sourcetype class T and that PY |X (y|·) is constant within every Vshell Tm (y) of y. This allows us to deﬁne the metric

(20)

d(y, , m)

Proof: See Section III-B.
By the type counting lemma [3, Lemma 2.2], there are at
most (k+1)|V| different source-type classes. Thus, Corollary 1
demonstrates that partitions Pk with not more than (k + 1)|V|
classes are sufﬁcient to achieve Csisz´ r’s error exponent. In
a
fact, we have recently showed that Csisz´ r’s error exponent
a
can already be achieved with partitions Pk consisting of two
classes [8].

PV (v)PY |X (y|x), (v, x) ∈ T ×Tm (y) (26)

for every y ∈ Y n and
further deﬁne

= 1, . . . , Mk , m = 1, . . . , Mn . We
(j)

Gj (y, a)

PX (x). (27)
v∈Aj x:PV (v)PY |X (y|x)≥a

Using (26) and (27) in (25) yields
Nk

Mk Mn

d(y, , m) min 1,

¯(i) =
III. P ROOFS

y

A. Proof of Theorem 1

=1 m=1
(i)

×

(28)

We now focus on the last double sum term in (28), which can
be upper-bounded as

Nk

¯(i),

PX (x).
v∈T ∩Ai x∈Tm (y)

The random-coding error probability ¯ is upper-bounded by
the random-coding union (RCU) bound [9], [10]
¯≤

Gj y, d(y, , m)
j=1

(21)

(i)

PX (x)

i
v∈T ∩Ai x∈Tm (y)

where

(i)

(29)

PX (x)

(30)

≤

¯(i)

(i)

PV (v)

PX (x)PY |X (y|x)

v∈T ∩Ai x:PV (v)PY |X (y|x)
=d(y, ,m)

x,y

v∈Ai

× min







Nk
(j)

1,






PX (¯ )
x







v∈Ai x:PV (v)PY |X (y|x)
≥d(y, ,m)

. (22)

= Gi y, d(y, , m)

(31)

for every y ∈ Y n and = 1, . . . Mk , m = 1, . . . , Mn . Here
(29) follows from the fact that not every x satisfying

Let {T1 , . . . , TMk } be the set of all source-type classes in
V k . Furthermore, let {T1 (y), . . . , TMn (y)} be the set of all
V-shells of y in X n , where the V-shell Tm (y) is deﬁned as
the set of all channel inputs x ∈ X n with conditional type
Vm given y ∈ Y n [3, Def. 2.4]. Thus, for a given conditional
type Vm given y ∈ Y n , the V-shell Tm (y) is the set of all
channel inputs x ∈ X n satisfying
a ∈ X , b ∈ Y,

(i)

≤






j=1 v ∈Aj x:PV (¯ )PY |X (y|¯ )
¯
¯
v
x
≥PV (v)PY |X (y|x)

N (a, b|x, y) = N (b|y)Vm (a|b),

PX (x)

PV (v)PY |X (y|x) = d(y, , m),

v∈T

(32)

must be in Tm (y), and (30) follows from summing over the
entire message set Ai and a larger codeword set. Combining
(31) with (28) and (21) yields
Nk

(23)

Mk Mn

¯≤

d(y, , m)
i=1

where N (a, b|x, y) denotes the number of occurrences of
(a, b) ∈ X × Y in (x, y). Note that, by the type counting
lemma,

y

=1 m=1
Nk

× min 1,

Gj y, d(y, , m)

Gi y, d(y, , m) (33)

j=1

Mk ≤ (k + 1)|V|

and Mn ≤ (n + 1)|X ||Y| .

(24)

Nk Nk

Using the above deﬁnitions, we can rewrite (22) as

(i)
PX (x)PV

=

(v)PY |X (y|x)

Nk Nk

=1 m=1 v∈T ∩Ai x∈Tm (y)

× min






=1 m=1

× min 1, Gj y, d(y, , m)

Mk Mn







d(y, , m)
i=1 j=1 y

¯(i)

y

Mk Mn

≤

Nk
(j)

1,

PX (¯ )
x
j=1 v ∈Aj x:PV (¯ )PY |X (y|¯ )
¯
¯
v
x
≥PV (v)PY |X (y|x)







d(y, , m)
i=1 j=1 y

=1 m=1

× min 1, Gj y, d(y, , m)

. (25)

+ min 1, Gi y, d(y, , m)

3

(34)

Mk Mn

≤






Gi y, d(y, , m)

Gj y, d(y, , m)
Gi y, d(y, , m)

, (35)

where in (34) we have used the inequality min{1, x + y} ≤
min{1, x}+min{1, y}, x, y ≥ 0, and in (35) we have used the
inequality min{1, x}y ≤ min{1, x}x + min{1, y}y, x, y ≥ 0.
By rearranging the terms in (35), we obtain
Nk

H(Vi ) denotes the entropy of Vi ), we have the following
inequalities [3, Lemmas 2.3 & 2.6]:
log |Ti |
≤ Ri ,
n

(45)

Mk Mn

¯ ≤ 2Nk

≤ −kD (Pi PV )

PV (v)

log

d(y, , m)
y

i=1

× min 1, Gi y, d(y, , m)

Gi y, d(y, , m) .

≤ −k

(36)

≤ −ke
(i)

Gi (y, a) =

PX (¯ )
x
¯
x: PV (¯ )PY |X (y|¯ )
v
x

PX (¯ )
x
¯
¯
v ∈Ai x

(37)
si

, (38)

for a, si > 0. Using that min{1, x} ≤ xρi for x ≥ 0 and
1
ρi ∈ [0, 1], and choosing si = 1+ρi in (38), we obtain

Nk

min 1, Gi (y, a) Gi (y, a)
1+ρi

(39)
PV (¯ )PY |X (y|¯ )
v
x
a

(i)

≤

PX (¯ )
x
¯
¯
v ∈Ai x

i=1

. (40)

≤ h(k)Nk e

x
1+ρi

PV (v) 1+ρi

(41)

v∈Ai
Nk

e−E0

= 2Nk Mk Mn

Er (Ri ,PY |X )+te(

Ri
t

,PV )

(49)

(50)

−n

min

{Er (R,PY |X )+te( R ,PV )}
t

0<R≤t log|V|

,

h(k)Nk ≤ (k + 1)3|V| (k/t + 1)|X ||Y| ,

1

×

{E0 (ρi ,PY |X )−ρi Ri }+te( Ri ,PV )
t

(51)

where in (50) we have used the deﬁnition of the randomcoding channel exponent (10), and where (51) follows from
minimizing the exponent over all possible values of Ri .
Using the deﬁnition of h(k) in (14) and the bound (43), we
have that

1+ρi

y

max

i=1

1
(i)
PX (x)PY |X (y|x) 1+ρi

¯ ≤ 2Nk Mk Mn

(48)

ρi ∈[0,1]

e−n

= h(k)

By applying (40) with a = d(y, , m) to (36), we ﬁnally
obtain
Nk

−n

e
i=1
Nk

1+ρi

1
1+ρi

,

¯B (Pk )
≤ h(k)

≤ Gi (y, a)

Ri
, PV
t

D (Pj PV ) (47)

where in (48) we have used the deﬁnitions of Ri and of the
source reliability function (8).
Using (44)–(48), and using that t = k/n, we can upperbound (17) as

≥asi

PV (¯ )PY |X (y|¯ )
v
x
a

(i)

≤

si

min

j=1,...,Nk :
H(Vj )≥H(Vi )

We next use Markov’s inequality to upper-bound

¯
v ∈Ai

(46)

v∈Ti

=1 m=1

(52)

which is polynomial in k. Hence, (51) yields

(i)
ρi ,PY |X ,PX

(i)
+Es (ρi ,PV

)

.

(42)

lim inf −
n→∞

i=1

≥

Theorem 1 follows then by upper-bounding Mk and Mn using
(24).

Er R, PY |X + te

R
, PV
t

. (53)

Since Er R, PY |X + te (R/t, PV ) is a decreasing function
of R for 0 < R ≤ tH(V ), it follows that the RHS of (53) is
equal to the RHS of (7), thus proving Corollary 1.

Let the classes A1 , . . . , ANk of the partition Pk be the
source-type classes T1 , . . . , TNk . We ﬁrst note that, by the type
counting lemma,
Nk ≤ (k + 1)|V| .
(43)

IV. E XTENSION TO G ENERAL A LPHABETS
One of the strengths of Gallager’s error bound (2) is that
it can be easily generalized to nondiscrete channels without
resorting to limiting arguments applied to ever-ﬁner quantizations of X and Y.
While in the derivation of our new bound we mostly used
the same techniques as Gallager, there are some steps that
rely on the method of types. In particular, to analyze (22), we
partitioned V k into source-type classes and X n into V-shells of
y ∈ Y n . Nevertheless, these partitions were merely introduced
to simplify the analysis and are not essential. Indeed, Csisz´ r’s
a

(i)

Furthermore, Es (ρ, PV ), i = 1, . . . , Nk , can be rewritten as
PV (v) ,

min
0<R≤t log|V|

B. Proof of Corollary 1

(i)
Es (ρ, PV ) = ρi log |Ti | + log

1
log ¯B (Pk )
n

(44)

v∈Ti

since PV (·) is constant within each source-type class Ai = Ti .
Let Vi be a random variable whose distribution is the type
Pi associated with Ti . Then, if we deﬁne Ri tH(Vi ) (where

4

error exponent can also be obtained by partitioning V k × X n
into κ + 1 sets of the form

The method to obtain the new bound uses a speciﬁc randomcoding construction with MAP decoding. Speciﬁcally, we
partition the message set into disjoints classes and assign
to each class an input distribution according to which the
codewords are randomly generated.
By partitioning the message set into source-type classes, and
by choosing for each class the input distribution to be a product
distribution, the new bound on the error probability recovers
Csisz´ r’s lower bound on the error exponent, answering the
a
question of whether ﬁxed composition codes are required to
achieve Csisz´ r’s exponent in the negative.
a

S (y) = (v, x) ∈ V k × X n : PV (v)PY |X (y|x) ∈ [α , β )
(54)
for every y ∈ Y n , where κ is a linear function of n, and where
α0 = 0, α = e−nγ e −1 , = 1, . . . , κ, and β = e−nγ e ,
= 0, . . . , κ − 1, βκ = ∞ for some γ > 0.
Following the arguments in Sections III-A while treating S0
and Sκ separately we obtain the upper bound
Nk

¯ ≤ 2Nk κe

− max

e

ρi ∈[0,1]

(i)

E0 ρi ,PY |X ,PX

(i)
−Es (ρi ,PV )

R EFERENCES

i=1
Nk

[1] R. G. Gallager, Information Theory and Reliable Communication. New
York: John Wiley & Sons, Inc., 1968.
[2] I. Csisz´ r, “Joint source-channel error exponent,” Probl. Contr. Inf.
a
Theory, vol. 9, pp. 315–328, 1980.
[3] I. Csisz´ r and J. K¨ rner, Information Theory: Coding Theorems for
a
o
Discrete Memoryless Systems, 2nd ed. Cambridge University Press,
2011.
[4] R. E. Blahut, “Hypothesis testing and information theory,” IEEE Trans.
Inf. Theory, vol. IT-20, no. 4, pp. 405–417, 1974.
[5] F. Jelinek, Probabilistic Information Theory. New York: McGraw-Hill,
1968.
[6] Y. Zhong, F. Alajaji, and L. L. Campbell, “On the joint source-channel
coding error exponent for discrete memoryless systems,” IEEE Trans.
Inf. Theory, vol. 52, no. 4, pp. 1450–1468, April 2006.
[7] R. T. Rockafellar, Conjugate duality and optimization. SIAM, 1974.
[8] A. Tauste Campo, G. Vazquez-Vilar, A. Guill´ n i F` bregas, T. Koch,
e
a
and A. Martinez, “Random coding bounds that attain the joint sourcechannel exponent,” in Proc. 46th Ann. Conf. Inf. Sciences and Systems
(CISS), Princeton University, NJ, March 2012.
[9] Y. Polyanskiy, H. V. Poor, and S. Verd´ , “Channel coding rate in the
u
ﬁnite blocklength regime,” IEEE Trans. Inf. Theory, vol. 56, no. 5, pp.
2307–2359, 2010.
[10] A. Tauste Campo, G. Vazquez-Vilar, A. Guill´ n i F` bregas, and A. Mare
a
tinez, “Random-coding joint source-channel coding bounds,” in Proc.
IEEE Int. Symp. on Inf. Theory, Saint Petersburg, Russia, July-Aug.
2011.

Pr PV (V )PY |X Y |X i < e−nγ

+Nk
i=1
Nk

Pr PV (V )PY |X Y |X i ≥ e−nγ+κ−1 , (55)

+Nk
i=1

(i)

where X i is a random variable with distribution PX , for
i = 1, . . . , Nk . By judiciously choosing γ and κ, and by maximizing (55) over product distributions, we recover Csisz´ r’s
a
error exponent.
A lower bound on EJ for nondiscrete channels follows
along the same lines by replacing the channel law PY |X in
(54) with the corresponding Radon-Nikodym derivative fY |X .
V. C ONCLUDING R EMARKS
We have presented an upper bound on the random-coding
error probability for joint source-channel coding that recovers
Gallager’s and Csisz´ r’s lower bounds on the error exponent
a
for discrete memoryless systems. Thus, the new expression
gives the actual error exponent at least in the cases where
Csisz´ r’s exponent is tight.
a

5

