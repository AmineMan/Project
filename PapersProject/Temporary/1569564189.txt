Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 23:15:38 2012
ModDate:        Tue Jun 19 12:54:31 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      528397 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569564189

The capacity region of the two-receiver vector
Gaussian broadcast channel with private and
common messages
Yanlin Geng, Chandra Nair
Dept. of Information Engineering
The Chinese University of Hong Kong
Sha Tin, N.T., Hong Kong
Y2 to recover messages (M0 , M2 ); both events being required
to occur with high probability. For previous work one may
refer to Chapters 5, 8, and 9 in [4].
A broadcast channel is characterized by a probability transition matrix q(y1 , y2 |x). The following broadcast channel is
referred to as the vector additive Gaussian broadcast channel

Abstract—We develop a new method for showing the optimality
of the Gaussian distribution in multiterminal information theory
problems. As an application of this method we show that Marton’s inner bound achieves the capacity of the vector Gaussian
broadcast channels with common message.

I. I NTRODUCTION

Y1 = G1 X + Z1
Y2 = G2 X + Z2 .

Channels with additive Gaussian noise are a commonly used
model for wireless communications. Hence computing the
capacity regions or bounds on the capacity regions for these
classes of channels are of wide interest. Usually these bounds
or capacity regions are represented using auxiliary random
variables and distributions on these auxiliary random variables.
Evaluations of these bounds then become an optimization
problem of computing the optimal auxiliary random variables.
In several instances involving Gaussian noise channels, it turns
out that the optimal auxiliaries and the inputs are Gaussian.
However proving the optimality of Gaussian distributions is
usually very cumbersome and involves certain non-trivial applications of the entropy-power-inequality(EPI), a non-trivial
result in itself.
For the two-receiver vector Gaussian broadcast channel
with private messages, the capacity region was established[1]
by showing that certain inner and outer bounds coincide.
This argument was indirect and hence the approach has been
hard to generalize to other situations, including the common
message case. In the following sections we develop a novel
way of proving the optimality of Gaussian input distribution
for additive Gaussian noise channels. There are many potential
straightforward applications of this new approach which will
yield new results as well as recover the earlier results in a
simple manner. For the purpose of this article, we will restrict
ourselves to two-receiver vector Gaussian channels.

In the above X ∈ Rt , G1 , G2 are t × t matrices, and Z1 , Z2
are Gaussian vectors independent of X.
Remark 2. We assume, w.l.o.g. that Z1 , Z2 ∼ N (0, I).
A product broadcast channel is a broadcast channel
whose transition probability has the form q1 (y11 , y21 |x1 ) ×
q2 (y12 , y22 |x2 ). A vector additve Gaussian product broadcast
channel can be represented as
Y11
Y12

=

G11
0

0
G12

X1
X2

+

Z11
Z12

Y21
Y22

=

G21
0

0
G22

X1
X2

+

Z21
Z22

.

In the above Z11 , Z12 , Z21 , Z22 are independent Gaussian
vectors, also independent of X1 , X2 .
Remark 3. In this paper we assume that all our channel
gain matrices are invertible. Since the set of all matrices is
dense (with respect to say, Frobenius norm) by continuity, our
capacity results extend to non-invertible cases.
We present some simple claims regarding additive Gaussian
channels which will be useful later.
Claim 1. Consider the following vector additive Gaussian
product channel with identical components
Y1 = GX1 + Z1

Remark 1. Due to page limitation, we are unable to do a proper
justice to the proofs; the full version is in arXiv [2].

Y2 = GX2 + Z2
Further let Z1 , Z2 be independent and distributed as N (0, I).
Deﬁne

A. Preliminaries
Broadcast channel[3] refers to a communication scenario
where a single sender, usually denoted by X, wishes to
communicate independent messages (M0 , M1 , M2 ) to two
receivers Y1 , Y2 . The goal of the communication scheme is to
enable receiver Y1 to recover messages (M0 , M1 ) and receiver

1
˜
X = √ (X1 + X2 ),
2
1
˜ = √ (Y1 + Y2 ),
Y
2

1

1
X = √ (X1 − X2 ),
2
1
Y = √ (Y1 − Y2 ).
2

˜
˜
Then I(X1 , X2 ; Y1 , Y2 ) = I(X, X ; Y, Y ).

We also deﬁne Sλ (X|V ) := v p(v)Sλ (X|V = v) for ﬁnite
V and its natural extension for arbitrary V .
For a product broadcast channel q1 (y11 , y21 |x1 ) ×
q2 (y12 , y22 |x2 ) let Sλ (X1 , X2 ) denote the corresponding
upper concave envelope. The following claim is referred to
as the “factorization of Sλ (X1 , X2 )”.

Proof: The proof is a trivial consequence of the fact that
˜
˜
˜
˜
h(Y, Y ) = h(Y1 , Y2 ) and h(Y, Y |X, X ) = h(Z, Z ) =
1
˜
h(Z1 , Z2 ) = h(Y1 , Y2 |X1 , X2 ) where Z = √2 (Z1 +
1
Z2 ), Z = √2 (Z1 − Z2 ).
Remark 4. An interesting consequence of Gaussian noise is
˜
that Z and Z are again independent and distributed according
˜
to N (0, I). Hence Y, Y can be regarded as the outputs of the
Gaussian channel when the inputs are distributed according
˜
to X, X . This observation is peculiar to additive Gaussian
channels.

Claim 3. The following inequality holds for product broadcast
channels
Sλ (X1 , X2 ) ≤ Sλ (X1 |Y22 ) + Sλ (X2 |Y11 )
≤ Sλ (X1 ) + Sλ (X2 ).
For additive Gaussian noise channels if p(v|x1 , x2 ) realizes
Sλ (X1 , X2 ), i.e. Sλ (X1 , X2 ) = sλ (X1 , X2 |V ), and equality
is achieved above i.e. Sλ (X1 , X2 ) = Sλ (X1 ) + Sλ (X2 ), then
all of the following must be true
1) X1 and X2 are conditionally independent of V
2) V, X1 achieves Sλ (X1 )
3) V, X2 achieves Sλ (X2 ).

Claim 2. In vector additive Gaussian product broadcast
channels, the random variables Y11 and Y22 are independent
if and only if X1 and X2 are independent.
The proof can be found in full version[2].
II. O PTIMALITY OF G AUSSIAN VIA FACTORIZATION OF
CONCAVE ENVELOPES

Proof: For any p(v|x1 , x2 ) observe the following

We devise a new technique to show that Gaussian distribution achieves the maximum value of an optimization problem,
subject to a covariance constraint. Though some of the results
have been known earlier[5], the technique presented here
allows us to obtain much broader results.
The main idea behind the approach is to show that if a
certain X (say zero mean) achieves the maximum value of
1
an optimization problem, then so does √2 (X1 + X2 ) and
1
√ (X1 − X2 ); where X1 , X2 are two i.i.d. copies of X.
2
1
1
Further we will show that √2 (X1 +X2 ) and √2 (X1 −X2 ) have
to be independent as well, which forces the initial distribution
to be Gaussian, see Theorem 1 in [6] or the full version
[2]. To show the ﬁrst step we go to the two-letter version1
of the channel, use a factorization property of the function
involved and then Claim 1 to move from the pair X1 , X2 to
1
√ (X1 + X2 ).
2

I(X1 , X2 ; Y11 , Y12 |V ) − λI(X1 , X2 ; Y21 , Y22 |V )
= I(X1 ; Y11 |V ) + I(X2 ; Y12 |V, Y11 )
− λI(X2 ; Y22 |V ) − λI(X1 ; Y21 |V, Y22 )
= I(X1 ; Y11 |V, Y22 ) + I(X2 ; Y12 |V, Y11 )
− λI(X2 ; Y22 |V, Y11 ) − λI(X1 ; Y21 |V, Y22 )
− (λ − 1)I(Y11 ; Y22 |V )
≤ Sλ (X1 |Y22 ) + Sλ (X2 |Y11 ) − (λ − 1)I(Y11 ; Y22 |V )
≤ Sλ (X1 ) + Sλ (X2 ) − (λ − 1)I(Y11 ; Y22 |V )
≤ Sλ (X1 ) + Sλ (X2 ).

Since equality holds all inequalities are tight. Hence Y1 and
Y2 are conditionally independent of V implying that X1 and
X2 are conditionally independent of V (Claim 1). Hence
I(X1 ; Y11 |V, Y22 ) − λI(X1 ; Y21 |V, Y22 )
= I(X1 ; Y11 |V ) − λI(X1 ; Y21 |V ) = Sλ (X1 ),
I(X2 ; Y12 |V, Y11 ) − λI(X2 ; Y22 |V, Y11 )
= I(X2 ; Y12 |V ) − λI(X2 ; Y22 |V ) = Sλ (X2 ).

A. Example 1: Difference of mutual informations
Consider a vector additive Gaussian broadcast channel. For
λ > 1 let the following function of p(x) be deﬁned by

This completes the proof.
1) Maximizing the concave envelope subject to a covariance constraint: Consider an Additive Gaussian Noise broadcast channel q(y1 , y2 |x). For K 0, deﬁne

sλ (X) := I(X; Y1 ) − λI(X; Y2 ).
Let sλ (X|V ) := I(X; Y1 |V ) − λI(X; Y2 |V ).
Further deﬁne

Vλ (K) =

Sλ (X) := C(sλ (X))

sup

Sλ (X).

Claim 4. There is a pair of random variables (V∗ , X∗ ) with
|V∗ | ≤ t(t+1) + 1 such that
2

denote the upper concave envelope2 of sλ (X). It is a straightforward exercise to see that
C(sλ (X)) =

sup
X:E(XXT ) K

Vλ (K) = sλ (X∗ |V∗ ).

I(X; Y1 |V ) − λI(X; Y2 |V )

p(v|x):
V →X→(Y1 ,Y2 )

Proof: Please see the full version, [2].
The goal of this section is to show that a single Gaussian
distribution achieves Vλ (K), i.e. we can take V to be trivial
and X ∼ N (0, K ), K
K. (This result is known and was
ﬁrst shown by Liu and Vishwanath[5] using perturbation based
techniques. We use this here as a non-trivial illustration of our
technique and then our ﬁnal result in the next section is new.)

= sup sλ (X|V ).
p(v|x)
1 A two letter version of a channel q(y|x) is a product channel consisting
of identical components q(y1 |x1 ) × q(y2 |x2 ).
2 The upper concave envelope of a function f (x) is the smallest concave
function g(x) such that g(x) ≥ f (x), ∀x.

2

Consider a product channel consisting of two identical
components q(y11 , y21 |x1 ) × q(y12 , y22 |x2 ).

Remark: Notice that we never used the precise form of
Sλ (X) but just used that the implications of Claim 3. In the
next section we will deﬁne a new concave envelope that will
also satisfy a condition similar to Claim 3, and then establish
the optimality of Gaussian.

Notation: In the remainder of the section we assume that
p∗ (v, x) achieves Vλ (K) , |V | = m ≤ t(t+1) + 1 and Xv be
2
a centered random variable (zero-mean) distributed according
to p(X|V = v). Further let Kv = E(Xv XT ). Then we have
v
m
K and in particular that Kv ’s are bounded.
v=1 p∗ (v)Kv

Corollary 1. If X ∼ N (0, K) then ∃X∗ ∼ N (0, K ), K
K such that Sλ (X) = sλ (X∗ ) = Vλ (K).

Claim 5. Let (V1 , V2 , X1 , X2 ) ∼ p∗ (v1 , x1 )p∗ (v2 , x2 ) be two
i.i.d. copies of p∗ (v, x). We assume that |V | ≤ t(t+1) + 1. Let
2

Proof: Clearly from Theorem 1 and deﬁnition of Vλ (K)
we have
Sλ (X) ≤ Vλ (K) = sλ (X∗ ).

1
˜ ˜
˜
V = (V1 , V2 ), X| V = (v1 , v2 ) ∼ √ (Xv1 + Xv2 ) ,
2
1
˜
X | V = (v1 , v2 ) ∼ √ (Xv1 − Xv2 ) .
2

On the other hand let X ∼ N (0, K − K ) be independent of
X∗ . Note that X ∼ X + X∗ and
Sλ (X) = sup sλ (X|V ) ≥ sλ (X|X ) = sλ (X∗ ).

In the above we take Xv1 and Xv2 to be independent random
variables. Then the following hold:
˜
˜
1) X, X are conditionally independent given V .
˜
˜ , X achieves Vλ (K).
2) V
˜
3) V , X achieves Vλ (K).

V

Hence proof completed.
B. A more complicated example
The function we considered in the previous section can
be used to determine the capacity region of vector Gaussian
broadcast channel with only private messages[5]. The function
we consider in this section will enable us to determine the
capacity region of vector Gaussian broadcast channel with
common message as well (see Section III).
For λ0 , λ1 , λ2 > 0 and for α ∈ [0, 1] (and α := 1 − α)
¯
consider the following function of p(x) deﬁned by

Proof:
2Vλ (K) = sλ (X1 |V1 ) + sλ (X2 |V2 )
(b)

(a)
˜
˜
˜
= sλ (X1 , X2 |V1 , V2 ) = sλ (X, X |V ) ≤ Sλ (X, X )
(c)

˜
≤ Sλ (X) + Sλ (X ) ≤ Vλ (K) + Vλ (K) = 2Vλ (K).

Here the ﬁrst equality comes because p∗ (v, x) achieves
Vλ (K), the second one because (V1 , X1 ) and (V2 , X2 ) are
independent. Equality (a) is a consequence of Claim 1,
inequality (c) is a consequence of Claim 3, and the last
inequality follows from the following:
˜˜
E(XXT ) = E(X X T ) =

p∗ (v1 )p∗ (v2 )
v1 ,v2
m

=

p∗ (v)Kv

tλ (X) := − λ0 αI(X; Y1 ) − λ0 αI(X; Y2 )
¯
+ (λ1 + λ2 )I(X; Y2 ) + λ1 S λ1 +λ2 (X).
λ1

Further let

(Kv1 + Kv2 )
2

Tλ (X) := C(tλ (X))
denote the upper concave envelope of tλ (X). Note that

K,

¯
C(tλ (X)) = sup −λ0 αI(X; Y1 |W ) − λ0 αI(X; Y2 |W )
p(w|x)

v=1

+ (λ1 + λ2 )I(X; Y2 |W ) + λ1 S λ1 +λ2 (X|W ).

and the deﬁnition of Vλ (K). Since the extremes match,
all inequalities must be equalities. Hence (b) must be an
˜
equality, p(˜, x, x ) achieves Sλ (X, X ); and since (c) is
v ˜
˜
also equality from Claim 3 we conclude that X, X are
˜ . Furthermore, we also obtain
conditionally independent of V
that p(˜|˜ ) achieves Sλ (˜ ), which from the last inequality
vx
x
matches Vλ (K). Similarly for p(˜|X ).
v
As a consequence, Xv1 , Xv2 are independent random variables and (Xv1 + Xv2 ) , (Xv1 − Xv2 ) are also independent
random variables. Thus (Theorem 1 in [6]) Xv1 , Xv2 are Gaussians, say having the same distribution as Xv ∼ N (0, K ).
Since v1 , v2 are arbitrary, all Xvi are Gaussians, having the
same distribution as Xv . Then

λ1

For a product broadcast channel q1 (y11 , y21 |x1 ) ×
q2 (y12 , y22 |x2 ) let Tλ (X1 , X2 ) denote the corresponding upper concave envelope. The following claim is referred to as
the “factorization of Tλ (X1 , X2 )”.
Claim 6. When λ0 > λ1 + λ2 the following inequality holds
for product broadcast channels
Tλ (X1 , X2 ) ≤ Tλ (X1 |Y22 ) + Tλ (X2 |Y11 )
≤ Tλ (X1 ) + Tλ (X2 ).
For additive Gaussian noise broadcast channels if p(w, x1 , x2 )
realizes Tλ (X1 , X2 ) and equality is achieved above then all
of the following must be true
1) X1 and X2 are conditionally independent of W
2) W, X1 achieves Tλ (X1 )
3) W, X2 achieves Tλ (X2 ).

m

Vλ (K) =

p∗ (vi )sλ (Xvi ) = sλ (Xv ).
i=1

Hence we obtain the following theorem.
Theorem 1. There exists X∗ ∼ N (0, K ), K
Vλ (K) = sλ (X∗ ).

K such that

Proof: The proof is similar to proof of Claim 3 and is
omitted. Details can be found in [2].

3

For K

0, deﬁne
ˆ
Vλ (K) =

III. T HE CAPACITY REGION
sup
X:E(XXT ) K

Consider a vector Gaussian broadcast channel with common
and private message requirements. Let C be the capacity
region. Assume λ0 > λ1 + λ2 . We will seek to maximize
the following expression

Tλ (X).

Claim 7. There exists a pair (W∗ , X∗ ) with |W∗ | ≤
ˆ
such that Vλ (K) = tλ (X∗ |W∗ ).

t(t+1)
+1
2

max

Notation: In remainder of the section we assume that p∗ (w, x)
ˆ
achieves Vλ (K), |W | = m ≤ t(t+1) + 1 and Xw be a
2
centered random variable (zero-mean) distributed according
to p(X|W = w). Further let Kw = E(Xw XT ). Then we
w
m
have w=1 p∗ (w)Kw
K and in particular that Kw ’s are
bounded.

(R0 ,R1 ,R2 )∈C

Remark 5. The case of maximizing λ0 R0 +(λ1 +λ2 )R1 +λ2 R2
can be dealt with similarly. On the other hand if λ0 ≤ (λ1 +
λ2 ) then it sufﬁces to consider the private messages capacity
region. Actually the setting λ0 ≥ 2λ1 + λ2 can be deduced
from the degraded message sets capacity region and this is
also known; however this will be subsumed in our treatment.
Hence the setting we are considering is the only interesting
unestablished case.

2

Claim 8. Let (W1 , W2 , X1 , X2 ) ∼ i=1 p∗ (wi , xi ) be two
i.i.d. copies of p∗ (w, x). We assume that |W | ≤ t(t+1) + 1.
2
Let
1
˜ ˜
˜
W = (W1 , W2 ), X| W = (w1 , w2 ) ∼ √ (Xw1 + Xw2 ),
2
1
˜ = (w1 , w2 ) ∼ √ (Xw1 − Xw2 ).
X| W
2

In this section we consider the UVW outer bound[7] and
Marton’s inner bound[8] to the capacity region of the broadcast channel with private and common messages.
Bound 1 (UVW outer bound). The union of rate triples
(R0 , R1 , R2 ) satisfying

In the above we take Xw1 and Xw2 to be independent random
variables. Then the following hold:
˜
˜
1) X, X are conditionally independent given W .
˜ ˜
ˆ
2) W , X achieves Vλ (K).
˜
ˆ
3) W , X achieves Vλ (K).

≤ min{I(W ; Y1 ), I(W ; Y2 )}
≤ min{I(W ; Y1 ), I(W ; Y2 )} + I(U ; Y1 |W )
≤ min{I(W ; Y1 ), I(W ; Y2 )} + I(V ; Y2 |W )
≤ min{I(W ; Y1 ), I(W ; Y2 )} + I(V ; Y2 |W )
+ I(X; Y1 |V, W )
R0 + R1 + R2 ≤ min{I(W ; Y1 ), I(W ; Y2 )} + I(U ; Y1 |W )
+ I(X; Y2 |U, W )
R0
R0 + R1
R0 + R2
R0 + R1 + R2

Proof: The proof is similar to proof of Claim 5 and is
omitted. Details can be found in [2].
As a consequence, Xw1 , Xw2 are independent random variables and (Xw1 + Xw2 ) , (Xw1 − Xw2 ) are also independent
random variables. Thus (Theorem 1 in [6]) Xw1 , Xw2 are
Gaussians, say Xw ∼ N (0, K ). Since w1 , w2 are arbitrary,
all Xwi are Gaussians, having the same distribution as Xw .
Then
m
ˆ
V (K) =
p∗ (wi )t (Xw ) = t (Xw ).
λ

λ

i

λ0 R0 + λ1 R1 + (λ1 + λ2 )R2 .

over all (U, V, W ) → X → (Y1 , Y2 ) forms an outer bound to
the broadcast channel.
Denote this region as O.
Denote the region by Marton’s inner bound as I. (see [2]
for the description).

λ

i=1

Impose a covariance constraint K on X and denote
IK , CK , OK to be the corresponding inner bound, capacity
region, and the outer bound respectively. Trivially we have

Hence we obtain the following theorem.
Theorem 2. There exists X∗ ∼ N (0, K ), K
ˆ
Vλ (K) = tλ (X∗ ).

K such that

max
(R0 ,R1 ,R2 )∈IK

Corollary 2. If X ∼ N (0, K) then there exists X1∗ ∼
N (0, K1 ) and an independent random variable X2∗ ∼
N (0, K2 ), K1 + K2 = K
K such that Tλ (X) = tλ (X1∗ +
ˆ
X2∗ ) = Vλ (K) and S λ1 +λ2 (X1∗ + X2∗ ) = s λ1 +λ2 (X1∗ ) =
V λ1 +λ2 (K1 + K2 ).

λ1

≤

λ0 R0 + λ1 R1 + (λ1 + λ2 )R2

max

λ0 R0 + λ1 R1 + (λ1 + λ2 )R2

(R0 ,R1 ,R2 )∈CK

≤

max
(R0 ,R1 ,R2 )∈OK

λ1

λ0 R0 + λ1 R1 + (λ1 + λ2 )R2 .

For any α ∈ [0, 1] observe that (from ﬁrst, third, and fourth
constraints of UVW outer bound)

λ1

ˆ
Proof: Clearly from Theorem 2 and deﬁnition of Vλ (K)
we have
ˆ
Tλ (X) ≤ Vλ (K) = tλ (X∗ ).

max
(R0 ,R1 ,R2 )∈OK

≤

λ0 R0 + λ1 R1 + (λ1 + λ2 )R2

sup

αλ0 I(W ; Y1 ) + αλ0 I(W ; Y2 )
¯

(V,W )→X→(Y1 ,Y2 )
E(XXT ) K

On the other hand let X ∼ N (0, K − K ) be independent of
X∗ . Note that X ∼ X + X∗ and

+ (λ1 + λ2 )I(V ; Y2 |W ) + λ1 I(X; Y1 |V, W )
≤

Tλ (X) = sup tλ (X|W ) ≥ tλ (X|X ) = tλ (X∗ ).
W

max

E(XXT ) K

ˆ
(αλ0 I(X; Y1 ) + αλ0 I(X; Y2 )) + Vλ (K).
¯

For details on the manipulations, see [2].
We know that the ﬁrst term is maximized when X ∼
ˆ
N (0, K) and Vλ (K) is achieved by tλ (X1∗ + X2∗ ) where

Now splitting of X∗ into X1∗ , X2∗ is possible by Corollary
1.

4

X1∗ , X2∗ are independent and X1∗ ∼ N (0, K1 ), X2∗ ∼
N (0, K2 ), K1 + K2
K, and S λ1 +λ2 (X1∗ + X2∗ ) =

I(U ; Y1 |W ) − I(U ; V |W ), R2 = I(V ; Y2 |W ). Hence
max

λ1

s λ1 +λ2 (X1∗ ). See Theorem 2 and Corollary 2. Now let

(R0 ,R1 ,R2 )∈OK

λ1

≤

W∗ ∼ N (0, K − (K1 + K2 )) be independent of X1∗ , X2∗ and
let X = W∗ + X1∗ + X2∗ . Observe that this choice attains
both maxima simultaneously. For conforming to more standard
notation, let us call V∗ = X2∗ , thus X = W∗ + X1∗ + V∗ .
Thus
max
(R0 ,R1 ,R2 )∈OK

λ0 R0 + λ1 R1 + (λ1 + λ2 )R2

sup

λ0 min{I(W ; Y1 ), I(W ; Y2 )}

(U,V,W )→X→(Y1 ,Y2 )
E(XXT ) K

≤

+ (λ1 + λ2 )I(V ; Y2 |W ) + λ1 I(U ; Y1 |W ) − λ1 I(U ; V |W )
max
λ0 R0 + λ1 R1 + (λ1 + λ2 )R2 .
(R0 ,R1 ,R2 )∈IK

λ0 R0 + λ1 R1 + (λ1 + λ2 )R2

Hence Marton’s inner bound and UVW outer bound match
and further the boundary is achieved via Gaussian signalling.

≤ αλ0 I(X; Y1 ) + αλ0 I(X; Y2 ) − αλ0 I(X; Y1 |W∗ )
¯
− αλ0 I(X; Y2 |W∗ ) + (λ1 + λ2 )I(X; Y2 |W∗ )
¯
+ λ1 I(X; Y1 |V∗ , W∗ ) − (λ1 + λ2 )I(X; Y2 |V∗ , W∗ )
= αλ0 I(W∗ ; Y1 ) + αλ0 I(W∗ ; Y2 )
¯
+ (λ1 + λ2 )I(V∗ ; Y2 |W∗ ) + λ1 I(X; Y1 |V∗ , W∗ )
= αλ0 I(W∗ ; Y1 ) + αλ0 I(W∗ ; Y2 )
¯
+ (λ1 + λ2 )I(V∗ ; Y2 |W∗ ) + λ1 I(X1∗ ; Y1 |V∗ , W∗ )

IV. C ONCLUSION
We developed a new method to show the optimality of
Gaussian distributions. We illustrated this technique for two
examples and computed the capacity region of the two-receiver
vector Gaussian broadcast channel with common and private
messages.
ACKNOWLEDGEMENT

Now using dirty-paper-coding idea (see [2]) choose U∗ =
X1∗ + AV∗ with A = K1 GT (GK1 GT + I)−1 to have

A lot of this work was motivated by the work on the discrete
memoryless broadcast channel; a lot of which was jointly
developed with Amin Gohari. The authors are also grateful
to Venkat Anantharam, Amin Gohari, Young-Han Kim, and
Abbas El Gamal for their comments on early drafts and
suggestions on improving the presentation.

I(X1∗ ; Y1 |V∗ , W∗ ) = I(U∗ ; Y1 |W∗ ) − I(U∗ ; V∗ |W∗ ).

Hence
max
(R0 ,R1 ,R2 )∈OK

R EFERENCES

λ0 R0 + λ1 R1 + (λ1 + λ2 )R2

[1] H. Weingarten, Y. Steinberg, and S. Shamai, “The capacity region of the
gaussian multiple-input multiple-output broadcast channel,” Information
Theory, IEEE Transactions on, vol. 52, pp. 3936 –3964, sept. 2006.
[2] Y. Geng and C. Nair, “The capacity region of the two-receiver vector
gaussian broadcast channel with private and common messages,” arXiv,
2012.
[3] T. Cover, “Broadcast channels,” IEEE Trans. Info. Theory, vol. IT-18,
pp. 2–14, January, 1972.
[4] A. El Gamal and Y.-H. Kim, Network Information Theory. Cambridge
University Press, 2012.
[5] T. Liu and P. Viswanath, “An extremal inequality motivated by multiterminal information-theoretic problems,” Information Theory, IEEE
Transactions on, vol. 53, pp. 1839 –1851, may 2007.
[6] S. G. Ghurye and I. Olkin, “A characterization of the multivariate normal
distribution,” The Annals of Mathematical Statistics, vol. 33, no. 2, pp. pp.
533–541, 1962.
[7] C. Nair, “A note on outer bounds for broadcast channel,” Presented at
International Zurich Seminar, 2010, http://arXiv.org/abs/1101.0640.
[8] K. Marton, “A coding theorem for the discrete memoryless broadcast
channel,” IEEE Trans. Info. Theory, vol. IT-25, pp. 306–311, May, 1979.
[9] Y. Geng, A. Gohari, C. Nair, and Y. Yu, “The capacity region of
classes of product broadcast channels,” Proceedings of IEEE International
Symposium on Information Theory, pp. 1549–1553, 2011.

≤ αλ0 I(W∗ ; Y1 ) + αλ0 I(W∗ ; Y2 ) + (λ1 + λ2 )I(V∗ ; Y2 |W∗ )
¯
+ λ1 (I(U∗ ; Y1 |W∗ ) − I(U∗ ; V∗ |W∗ ))
≤
sup
αλ0 I(W ; Y1 ) + αλ0 I(W ; Y2 )
¯
(U,V,W )→X→(Y1 ,Y2 )
E(XXT ) K

+ (λ1 + λ2 )I(V ; Y2 |W ) + λ1 (I(U ; Y1 |W ) − I(U ; V |W ))

Since the above holds for all α ∈ [0, 1], we have
max
(R0 ,R1 ,R2 )∈OK

≤ min

λ0 R0 + λ1 R1 + (λ1 + λ2 )R2
sup

α∈[0,1] (U,V,W )→X→(Y1 ,Y2 )

αλ0 I(W ; Y1 ) + αλ0 I(W ; Y2 )
¯

E(XXT ) K

+ (λ1 + λ2 )I(V ; Y2 |W ) + λ1 I(U ; Y1 |W ) − λ1 I(U ; V |W ).

Claim 9. We claim that
min

sup

α∈[0,1] (U,V,W )→X→(Y1 ,Y2 )

αλ0 I(W ; Y1 ) + αλ0 I(W ; Y2 )
¯

T

E(XX ) K

+ (λ1 + λ2 )I(V ; Y2 |W ) + λ1 I(U ; Y1 |W ) − λ1 I(U ; V |W )
=
sup
λ0 min{I(W ; Y1 ), I(W ; Y2 )}
(U,V,W )→X→(Y1 ,Y2 )
E(XXT ) K

+ (λ1 + λ2 )I(V ; Y2 |W ) + λ1 I(U ; Y1 |W ) − λ1 I(U ; V |W ).

Proof can be found in full version [2] (and follows from a
result in [9]).
Now using Marton’s inner bound we can always achieve
the triples: R0 = min{I(W ; Y1 ), I(W ; Y2 )}, R1 =

5

