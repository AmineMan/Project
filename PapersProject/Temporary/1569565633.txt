Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 17:26:33 2012
ModDate:        Tue Jun 19 12:54:56 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      466104 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565633

Secure Transmission of a Gaussian Source over
Gaussian Channels with Side Information
Joffrey Villard

Pablo Piantanida

Shlomo Shamai (Shitz)

Dept. of Telecommunications
SUPELEC
91192 Gif-sur-Yvette, France
Email: joffrey.villard@supelec.fr

Dept. of Telecommunications
SUPELEC
91192 Gif-sur-Yvette, France
Email: pablo.piantanida@supelec.fr

Dept. of Electrical Engineering
Technion - Israel Institute of Technology
Technion city, Haifa 32000, Israel
Email: sshlomo@ee.technion.ac.il

works [8], [9] show that under some less-noisy conditions separation holds, but a simple counterexample in [9] introduces a
pure analog scheme being optimal performs strictly better than
existing digital ones. This observation motivates the authors in
[10] to investigate hybrid digital/analog schemes for discrete
memoryless sources and channels, where they have shown that
a joint approach based on analog/digital schemes yields better
results for some speciﬁc scenarios.

Abstract—This paper investigates the problem of sourcechannel coding for secure transmission of a Gaussian source over
a Gaussian wiretap channel in presence of arbitrarily correlated
side informations at both receivers. By means of an appropriate
coding scheme (either hybrid digital/analog or pure digital), the
optimal rate-distortion-equivocation region is characterized for
most of the scenarios. These results provide the best achievable
tradeoff between the requirement on the distortion level at the
legitimate receiver and the equivocation rate at the eavesdropper.

In this paper, we consider the secure transmission of a Gaussian source over a Gaussian wiretap channel with matched
bandwidth, and in the presence of side information at the
receiving terminals, as depicted in Fig. 2. This setting can be
seen as the uniﬁcation of the problems of secure source coding
with side information at the decoders [7] and the wiretap
channel [3]. Our main goal is to understand how Alice can take
simultaneous advantage of the statistical differences among
the side informations and the channel noises to reveal the
minimum amount of information to Eve, while still satisfying
the required distortion level at Bob. The central difﬁculty of
this problem lies in the characterization of the equivocation at
Eve. Indeed, the side information available to Eve which can
be exploited in together with its channel observation prevents
from directly applying secrecy capacity results [3].

I. I NTRODUCTION
Consider a multiterminal system composed of three nodes
where each one is measuring an analog source –random ﬁeld–
as a function of time. The encoder –referred to as Alice–
wishes to send its observation to a second terminal node –
referred to as Bob– through a noisy channel with additive
white Gaussian noise (AWGN). In this setting, Bob can use
his own analogue source as side information to improve on the
received information and reﬁne his estimate of Alice’s source.
The third terminal –referred to as Eve– is assumed to be an
eavesdropper node that can listen to the messages sent by Alice
through another AWGN channel. Obviously, Alice does not
trust Eve and hence she wishes to leak the smallest possible
amount of information during her communication with Bob.
The above scenario involves the information-theoretic notion of secrecy (and its application to source/channel coding
problems), source coding with side information, as well as
joint source-channel coding for transmission of Gaussian
sources over AWGN channels. The information-theoretic notion of secrecy was ﬁrst introduced by Shannon [1] and then
used by Wyner [2] to propose the wiretap channel, which
was further extended by Csiszár-Körner [3]. Source coding
with side information has been presented by Slepian-Wolf [4],
and Wyner-Ziv [5]. Recent results [6], [7] investigate such
settings with an additional eavesdropper which must be kept as
ignorant as possible of the transmitted source. Although most
of the existent work on physical layer security focused on the
separation principe (i.e. separate source and channel coding),
under security constraints there is no general result of separation, unlike simple point-to-point problems. Actually recent

We identify four different cases: 1 Bob has a less-noisy
channel and side information than Eve; 3 - 4 Eve has a
less-noisy channel; and 2 Bob has a less-noisy channel
while Eve has less-noisy side information than Bob. For
cases 1 , 3 - 4 , a careful maximization of our inner and outer
bounds in [9] yields the optimal rate-distortion-equivocation
region. In these cases, separation holds and thus a pure digital
scheme, combining secure-source coding with channel coding
for broadcast channels with conﬁdential messages, turns to
be optimal. However, since these bounds do not match for
case 2 , we restrict our focus on the hybrid digital/analog
scheme introduced in [10]. From this strategy we obtain a
new achievable region that is shown to be optimal in case 2
when the side information is only present at the eavesdropper.
Notation: For any sequence (xi )i∈N∗ , the vector xn stands
for the collection (x1 , . . . , xn ). Let X, Y and Z be three
random variables (RVs) with PD p, if p(x|yz) = p(x|y) for
each x, y, z, then they form a Markov chain X −− Y −− Z.

The work of J. Villard is supported by DGA (French Armament Procurement Agency). This research is partially supported by the FP7 Network of
Excellence in Wireless COMmunications NEWCOM#.

1

i.i.d.

n
NB ∼ N (0, PB )

Bn

+

A

n i.i.d.

Xn

Alice

∼ N (0, 1)

+

i.i.d.

n
NZ ∼ N (0, PZ )
i.i.d.

n
NE ∼ N (0, PE )

Yn

+

i.i.d.

n
NY ∼ N (0, PY )

+




1

n

n i=1

Bob

D


2
E Xi

Zn

≤ P

Eve

∆

En

Figure 1: Transmission of a Gaussian source over a Gaussian wiretap channel with side information at the receivers.

II. P ROBLEM D EFINITION

side information (resp. one channel) is always a stochastically
degraded version of the other. Hence there are four different
cases which are summarized in Table I.
rf

Consider the transmission model depicted in Fig. 1 where
the source A at Alice is standard Gaussian, and the corresponding analogue observations (side information) at Bob
and Eve are modeled as the outputs of independent additive
white Gaussian noise (AWGN) channels with input A and
respective noise powers PB and PE . Communication channels
from Alice-to-Bob and Alice-to-Charlie are AWGN channels
with respective noise powers PY and PZ . The average input
power of this channel is limited to P . We assume a matched
bandwidth, i.e., one channel use is allowed per source symbol.
Euclidean distance on R is used to measure distortion at
Bob d(a, b) = (a − b)2 , for all a, b ∈ R. Differential entropy
h(·) measures uncertainty yielding equivocation rates ∆ ∈ R.
We also introduce quantity DE = 22∆ /(2πe), which provides
a lower bound on the minimum mean-square error of any
estimator of A at Eve [11, Theorem 8.6.6].
Deﬁnition 1 (Code): A source-channel n-code for the
present setup depicted in Fig. 1 is deﬁned by
n
n
• A (stochastic) encoding function at Alice F : R → R ,
n n
deﬁned by some transition probability PF (x |a ),
n
n
n
• A decoding function at Bob g : R × R → R .

rc
n

A

III. C ASES 1 , 3 , AND 4 : D IGITAL C ODING
From [9], we already know that separation holds for the
settings 1 , 3 , and 4 . In these cases, the optimal scheme
consists of a source encoder that outputs two layers which
are further encoded by using the channel code of a broadcast
channel with conﬁdential messages [3] (see Fig. 2). The ﬁrst
layer rc can be seen as a common message, which is considered to be known at Eve, while the second layer rp forms a
private message which is –partially– protected by introducing
an independent random noise rf . In the setup considered in
this paper, Gaussian variables are optimal, yielding the closedfrom expressions given by Theorems 1 and 2 below.
Theorem 1 (optimal region for cases 1 and 3 ): Assume
that PB ≤ PE , a tuple (D, DE ) is achievable if and only if
1
1
D≥
,
1 ·
P
1 + PB 1 + PY

2

with channel input X n as the output of the encoder F (An ).
PB ≤ PE

PB > P E

1

2

PY ≥ PZ

3

Xn

Recent works [9], [10] deal with a similar setup for the case
of discrete sources and channels. A general outer bound as
well as two inner bounds (based on digital and hybrid coding
schemes) have been derived. Along the same lines, including
the input constraint Var [X] ≤ P , achievable rate regions can
be readily extended (see [12] for further details) to the present
setting of a quadratic Gaussian source over AWGN channels.
In what follows, we consider separately each of the cases in
Table I, for which we will need to rely (although omitted for
space limitations) on the achievable regions in [9], [10].

An − g(B n , Y n )
≤D+ε ,
1
1
h(An |E n Z n ) ≥ log (2πe DE ) − ε ,
n
2
n
1
2
E Xi ≤ P + ε ,
n i=1

PY < PZ

rp

Channel
encoder

Figure 2: Digital scheme.

Deﬁnition 2 (Achievability): A tuple (D, DE ) ∈ R∗ 2 is
+
said achievable if, for any ε > 0, there exists a source-channel
n-code (F, g) satisfying the accuracy and security constraints:
E

Source
encoder

4

DE ≤

Table I: The four different cases that can be identiﬁed.
Proposition 1: Due to the additive Gaussian noises, and depending on the relative values of PB , PE (resp. PY , PZ ), one

1+
1
1 ,
1 + PE 1 +

1
PB
1
PE

· D · max 1 ;

1+
1+

P
PY
P
PZ

.

Proof: The proof of the converse part will be omitted. In
fact, it relies on the general outer bound in [9, Prop. 2] together

2

rf ∼ U {1,...,2nRf }

with the entropy power inequality (EPI) [13]. For further
details we refer the reader to [12, Appendix F]). Whereas
the direct part easily follows from [9, Prop. 1] by choosing
appropriate Gaussian auxiliary random variables.
Theorem 2 (optimal region for case 4 ): Let PB > PE ,
PY ≥ PZ , a tuple (D, DE ) is achievable if and only if
D≥

vn

An

+

1
1
·
,
P
1 + P1 1 + PY
B

DE ≤ min

1
,
1 + P1
E

×

+

1
PE

−

1
PB

.

Figure 3: Hybrid digital/analog scheme.
Proposition 3 (Hybrid scheme for all cases): A tuple (D,
DE ) ∈ R∗ 2 is achievable if
+

Proof: A sketch of proof of the converse part is given
in Appendix A. Details of the proof of direct part are omitted
since it easily follows by evaluating the corresponding achievable region with an appropriate choice of Gaussian auxiliary
RVs (see [9, Prop. 2] for further details).

D≥

V = αA + γN
X = βA − γN

α2
γ2

+

1
+

P
PZ

P
PY

(α + β)2

,

1


 1+
1 +

1+
1
PB

1
PB

+

γ2
PE

+

P
PY

α2
γ2

+

P
PY

γ2
PB


P 
,
; 1 + γ2
PZ 
(α + β)2

1+

1 − β 2 and such that

α2
P
P
γ2
+
(α + β)2 ≤
1+
.
γ2
PY
PY
PB
Proof: Proposition 3 follows from [10, Theorem 3] after
some algebraic manipulations on the involved covariance
matrices (see [12, Appendix I]).
As a matter of fact, the hybrid scheme of Proposition 3
does not seem to be optimal in the general case as we will
see in Section V. However, it achieves the outer bound of
Proposition 2 in the special case when PY < PZ and PB → ∞
that we refer to as case 2 , as stated in the following theorem.
Theorem 3 (optimal region for case 2 ): Assume
that
∗2
PB → ∞ with PY < PZ , then a tuple (D, DE ) ∈ R+ is
achievable if and only if
1
D≥
,
P
1 + PY
1
DE ≤
.
P
1 + PZ
1
1
max 1 ; D ·
+ PE
P
1 + PY

.

1
1
+
−
PE
PB

(common description)
√

+

for some α ∈ R, β ∈ [0, 1) with γ =

Proof: The proof of this outer bound is similar to the one
of the converse part of Theorem 2 (given in Appendix A). For
further details we refer the reader to see [12, Appendix H].
We now use the hybrid digital/analog scheme proposed
in [10] which is depicted in Fig. 3. Roughly speaking, it
consists in sending independent digital random noise rf using
a Gelfand-Pinsker code [14] for the state-dependent channel
with input X, state A and output (B, Y ). As in [10, Section IV], we choose U , V and X of [10, Theorem 3] as:
U =∅

1
PE

· min

D≥

1

1
PB

1+

As a matter of fact, the case 2 where Bob has “better”
channel (PY < PZ ) and “worse” side information (PB >
PE ) than Eve, is still open. A careful maximization for the
quadratic Gaussian case of the variables involved in the outer
bound in [9, Theorem 2] yields the proposition below.
Proposition 2 (outer bound for case 2 ): Let PB > PE
and PY < PZ , any achievable tuple (D, DE ) satisﬁes the
following inequalities:

P
PZ
P
PY

1+

DE ≤

IV. C ASE 2 : H YBRID D IGITAL /A NALOG C ODING

1
1
·
,
P
1 + P1 1 + PY
B
1
DE ≤ min
,
1 + P1 1 1 +
E
D ·
1+

Xn

×
√
P

(α + β)
1

1
D

+

−

Proof: The converse part directly follows from Proposition 2 by letting PB tend to inﬁnity. Whereas the direct part
follows after Proposition 3 by letting PB tend to inﬁnity and

(private description)
P (channel input)

where α ∈ R, β ∈ [0, 1), γ = 1 − β 2 and N ∼ N (0, 1) is
a standard Gaussian random variable independent of A.
In [12], it is shown that care must be exercised when using
this coding scheme. Notice that unlike dirty-paper coding
for communication without secrecy constraints but with noncausal channel state information to the encoder [15], the source
A –viewed as the channel state of the corresponding equivalent
channel– and the bin index are not independent.

choosing, for every distortion level D ∈

1
P
1+ P

Y

β + γ2
α=

β=

3

1
D

P
PY

P
1 + γ 2 PY

PZ
P

1+

−

1+

PY

P
PZ

−β ,

P
P
−D 1+
PZ
PY

P

, 1+ PZ
P

.

,

Full details are provided in [12, Appendix J].

in the evaluation of the uncertainty at Eve. In spite of the
aforementioned limitations, several interesting extensions of
this work can be identiﬁed, e.g., Gaussian vector settings, a
generalization of the well-known CEO problem to incorporate
security constraints, etc.

0.7

DE

0.6

0.5

0.5

0.45
0.4

0.3
0.2

0.3

0.4

0.5

0.6

DE

Digital (Th. 1)
Analog
Hybrid (Prop. 3)

0.4
0.35
Outer bound (Prop. 2)
Digital
Analog
Hybrid (Prop. 3)

0.7

D

0.3

Figure 4: Case 1 (P = 1, PY = 0.5, PZ = 1, PB = 1, PE = 2)
0.25
0.2

0.3

0.4

0.5

0.6

0.7

D

V. N UMERICAL R ESULTS AND D ISCUSSIONS
In this section, we plot largest achievable DE as a function
of the distortion level at Bob D, for three cases of particular
interest and compare (with numerical optimization whether it
is needed) the different coding schemes:
• the digital scheme,
• a pure analog scheme, consisting in directly sending a
scaled version of the source over the channel,
• the hybrid digital/analog scheme of Proposition 3.
First of all, we observe that any of the proposed schemes
is sufﬁcient to achieve the optimal performance in all cases.
According to Theorems 1 and 2, the digital scheme of [9] is
optimal in case 1 (see Fig. 4). However, it is strictly suboptimal in case 2 , as shown by Fig. 5. In this case, the
proposed hybrid digital/analog scheme outperforms both pure
analog and digital schemes. According to Theorem 3, it is
moreover optimal in case 2 (see Fig. 6). Interestingly enough,
a time-sharing combination of digital and analog schemes falls
short to achieve the entire region in this case.
Of course, at a ﬁrst look the above conclusion may not be
surprising because it is well-known that joint source-channel
coding/decoding performs better for general broadcast settings [16]. Indeed, this holds true under no secrecy constraints
and when perfect reconstruct of the source is intended at all
the decoders. Nevertheless, the present settings under secure
constraints is rather different since Alice only wants to help
Bob while keeping the uncertainty at Eve as higher as possible.
Therefore the intuition would indicate that the optimal strategy
should not be a joint source-channel coding scheme, since
separate source and channel coding can allow the encoder
to protect by introducing uniform noise the information (e.g.
Theorems 1 and 2). Finally, it should be worth to mention
here that different coding is needed to achieve the optimal
tradeoffs in cases 1 , 3 - 4 and 2 . Whereas in order to fully
solve case 2 it is clear that a novel scheme would be needed.
The main difﬁculty in exploring other coding schemes relies

Figure 5: Case 2 (P = 1, PY = 0.5, PZ = 1, PB = 2, PE = 1)

0.5

DE

0.45
0.4
0.35
Digital
Analog
Hybrid (Th. 3)

0.3
0.25
0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

D

Figure 6: Case 2 (P = 1, PY = 0.5, PZ = 1, PB → ∞, PE = 1)
A PPENDIX A
P ROOF OF THE C ONVERSE PART OF T HEOREM 2
Assume that PB > PE and PY ≥ PZ , and let (D, DE ) be
an achievable tuple. From [9, Proposition 2], there exist RVs
U , V , X with joint PD1
p(uvabexyz) = p(u|v)p(v|a)p(ae)p(b|e) p(x)p(yz|x),
ˆ
and a function A, verifying
I(V ; A|B) ≤ I(X; Y ) ,
ˆ
D ≥ E d(A, A(V, B)) ,
∆ ≤ h(A|V B) + I(A; B|U ) − I(A; E|U ) ,
Var [X] ≤ P ,
1 Since P > P , the Markov chain A−
−E −
−B can be assumed without
B
E
loss of generality.

4

where ∆ = 1 log(2πe DE ).
2
From the long Markov chain U −− V −− A −− E −− B,

where the last equality follows from Var [E|AB] =
PE
PE 1 − PB after some manipulations.
We now introduce the parameter ν = 22h(A|V B) /(2πe).
From the fact that conditioning reduces the entropy and classical properties of the differential entropy, the above parameter
is bounded as follows:
1
.
ν≤
1 + P1
B

I(A; B|U ) − I(A; E|U ) = −I(A; E|U B)
≤ −I(A; E|V B),
and tuple (D, ∆) veriﬁes the following inequalities:
I(V ; A|B) ≤ I(X; Y ) ,
ˆ
D ≥ E d(A, A(V, B)) ,

Hence from standard properties of differential entropy, and the
fact that distortion measure d is the Euclidean distance on R,
the following sequence of inequalities holds true:
ˆ
E d(A, A(V, B)) ≥ Var [A|V B] ≥ ν .

∆ ≤ h(A|V B) − h(E|V B) + h(E|AB) ,
Var [X] ≤ P .
Moreover, the side informations write:
E = A + NE ,

Gathering the above equations, tuple (D, DE ) veriﬁes the
following inequalities:

¯
¯
B = A + NE + NB = E + NB ,
¯
where NB ∼ N (0, PB − PE ) is independent of A and NE ∼
N (0, PE ). In order to ﬁnd an upper bound on the r.h.s. of (1),
we need the following expansion of E, for any γ ∈ R:

1
1 + P1
B

E = γB + (1 − γ)A + C ,
where
¯
C = (1 − γ)NE − γ NB .

R EFERENCES
[1] C. Shannon, “Communication theory of secrecy systems,” Bell System
Technical Journal, vol. 28, pp. 656–715, 1949.
[2] A. Wyner, “The wire-tap channel,” Bell System Technical Journal,
vol. 54, no. 8, pp. 1355–1387, 1975.
[3] I. Csiszár and J. Körner, “Broadcast channels with conﬁdential messages,” IEEE Trans. Inf. Theory, vol. 24, no. 3, pp. 339–348, 1978.
[4] D. Slepian and J. Wolf, “Noiseless coding of correlated information
sources,” IEEE Trans. Inf. Theory, vol. 19, no. 4, pp. 471–480, 1973.
[5] A. Wyner and J. Ziv, “The rate-distortion function for source coding
with side information at the decoder,” IEEE Trans. Inf. Theory, vol. 22,
no. 1, pp. 1–10, 1976.
[6] V. Prabhakaran and K. Ramchandran, “On secure distributed source
coding,” in Proc. ITW, 2007, pp. 442–447.
[7] J. Villard and P. Piantanida, “Secure lossy source coding with side
information at the decoders,” in Proc. Allerton, Allerton, IL, 2010, pp.
733–739.
[8] N. Merhav, “Shannon’s secrecy system with informed receivers and its
application to systematic coding for wiretapped channels,” IEEE Trans.
Inf. Theory, vol. 54, no. 6, pp. 2723–2734, 2008.
[9] J. Villard, P. Piantanida, and S. Shamai, “Secure lossy source-channel
wiretapping with side information at the receiving terminals,” in Information Theory Proceedings (ISIT), 2011 IEEE International Symposium
on, 2011, pp. 1141–1145.
[10] ——, “Hybrid digital/analog schemes for secure transmission with side
information,” in Information Theory Workshop (ITW), 2011 IEEE, 2011.
[11] T. Cover and J. Thomas, Elements of information theory (2nd Ed).
Wiley-Interscience, 2006.
[12] J. Villard, P. Piantanida, and S. Shamai, “Secure transmission of sources
over noisy channels with side information at the receivers,” submitted
to the IEEE Trans. Inf. Theory (arXiv 1201.2315), pp. 1–65, 2012.
[13] C. Shannon, “A mathematical theory of communication,” Bell System
Technical Journal, vol. 27, pp. 379–423, 623–656, 1948.
[14] S. Gel’fand and M. Pinsker, “Coding for channel with random parameters,” Probl. Control Inf. Theory, vol. 9, pp. 19–31, 1980.
[15] M. Costa, “Writing on dirty paper,” IEEE Trans. Inf. Theory, vol. 29,
no. 3, pp. 439–441, 1983.
[16] E. Tuncel, “Slepian-Wolf coding over broadcast channels,” IEEE Trans.
Inf. Theory, vol. 52, no. 4, pp. 1469–1482, 2006.

¯
E[BC] = (1 − γ) E [BNE ] − γ E B NB
= (1 − γ)PE − γ (PB − PE )
= PE − γPB = 0 .
Finally, since V only depends on A, C, it is independent of
(V, A, B). By using expansion (A), we now write
h(E|V B) = h(γB + (1 − γ)A + C|V B)
= h((1 − γ)A + C|V B) .
And from the above paragraph, the conditional EPI holds
between A and C (given (V, B)):
22h((1−γ)A+C|V B) ≥ 22h((1−γ)A|V B) + 22h(C|V B) .
Since C is independent of (V, A, B), the last entropy writes
h(C|V B) = h(C|AB)
= h(E|AB) ,
where the last equality follows from expansion (A).
Gathering the above equations, (1) yields
1
∆ ≤ h(A|V B) − log (1 − γ)2 22h(A|V B) + 22h(E|AB)
2
+ h(E|AB)

=

1
log
2
1
log
2

1
1
22h(A|V B)

+

(1−γ)2
22h(E|AB)

2πe
+ P1 −
22h(A|V B)
E
2πe

1
PB

,

Eliminating parameter ν in the above system proves the
converse part of Theorem 2.

Note that (A, B, C) is a Gaussian vector, and that A and C are
independent for any γ. The usefulness of the above expansion
PE
comes from the fact that C is also independent of B if γ = PB
for which we have that

=

1
,D
1 + P1
B
1
P
· ≤1+
,
ν
PY
1
.
DE ≤ 1
+ P1 − P1
ν
E
B
ν ≤ min

,

5

