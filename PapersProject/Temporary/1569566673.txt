Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 00:34:35 2012
ModDate:        Tue Jun 19 12:54:29 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      498468 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569566673

Lossy Common Information of Two Dependent
Random Variables
Kumar Viswanatha, Emrah Akyol and Kenneth Rose
ECE Department, University of California - Santa Barbara
{kumar,eakyol,rose}@ece.ucsb.edu
Abstract—The two most prevalent notions of common information are due to Wyner and Gács-Körner and both the notions
can be stated as two different characteristic points in the lossless
Gray-Wyner region. Although these quantities can be easily
evaluated for random variables with inﬁnite entropy (eg. continuous random variables), the operational signiﬁcance underlying
their deﬁnition is applicable only to the lossless framework. The
primary objective of this paper is to generalize these two notions
of common information to the lossy Gray-Wyner network, which
extends the theoretical intuition underlying their deﬁnitions for
general sources and distortion measures. We begin with the lossy
generalization of Wyner’s common information, deﬁned as the
minimum rate on the shared branch of the Gray-Wyner network
at minimum sum rate when the two decoders reconstruct the
sources subject to individual distortion constraints. We derive a
complete single letter information theoretic characterization for
this quantity and use it to compute the common information
of symmetric bivariate Gaussian random variables. We then
derive similar results to generalize Gács-Körner’s deﬁnition to
the lossy framework. These two characterizations allow us to
carry the practical insight underlying the two notions of common
information to general sources and distortion measures.
Index Terms—Wyner’s common information, Gács and
Körner’s common information, Lossy Gray-Wyner network

Figure 1.

The Gray-Wyner network

transmission rate to the minimum, H(X, Y ). He obtained the
single letter characterization for CW (X, Y ) as,
CW (X, Y ) = inf I(X, Y ; U )

(1)

where the inﬁmum is over all random variables, U , such
that X ↔ U ↔ Y form a Markov chain in that order.
We note that although CGK (X, Y ) and CW (X, Y ) were
deﬁned from theoretical standpoints, they play critical roles
in understanding the performance limits in several practical
networking and database applications, see for eg. [4]. We
further note in passing that several other deﬁnitions of CI, with
applications in different ﬁelds, have appeared in the literature
[6], [7], but are less relevant to us here.
Although the quantity in (1) can be evaluated for random variables with inﬁnite entropy (eg. continuous random
variables), for such random variables it lacks the underlying
theoretical interpretation of Wyner’s notion of CI as one of
the distinctive operating points in the Gray-Wyner region and
thereby lacks the fundamental intuition behind the deﬁnition.
This largely compromises its practical signiﬁcance and calls
for a useful generalization which can be easily extended to
inﬁnite entropy distributions. Our primary step is to characterize a lossy coding extension of Wyner’s CI (denoted
by CW (X, Y ; D1 , D2 ), deﬁned as the minimum rate on the
shared branch of the Gray-Wyner network at minimum sum
rate when the sources are decoded at respective distortions of
D1 and D2 . Note that the minimum sum rate at distortions
D1 and D2 is given by the Shannon’s rate distortion function,
hereafter denoted by RX,Y (D1 , D2 ). In this paper, our main
objective is to derive an information theoretic characterization
for CW (X, Y ; D1 , D2 ) for general sources and distortion
measures. We note that although there is no prior work on
characterizing CW (X, Y ; D1 , D2 ), in a recent work [8], Xu et

I. I NTRODUCTION
The quest for a meaningful and useful notion of common
information (CI) of two random variables (denoted by X and
Y ) has been actively pursued by researchers in information
theory for over three decades. An early seminal approach to
quantify CI is due to Gács and Körner [1] (denoted here
by CGK (X, Y )), who deﬁned it as the maximum amount of
information relevant to both random variables, one can extract
from the knowledge of either one of them. Their result was
of considerable theoretical interest, but also fundamentally
negative in nature. They showed that CGK (X, Y ) is typically
much smaller than the mutual information and only depends
on the zeros of the joint distribution. Unsatisﬁed with the
negative implication, Wyner proposed an alternative idea of
CI [2] (denoted here by CW (X, Y )) inspired by earlier work
in multi-terminal source coding [3]. He deﬁned the CI as the
minimum rate on the shared branch of the lossless Gray-Wyner
network (described in section I-A and Fig. 1), when the sum
rate is constrained to be the joint entropy. His conception was
to look at the minimum amount of shared information that
must be sent to both decoders, while restricting the overall
The work was supported by the NSF under grants CCF-0728986, CCF 1016861 and CCF-1118075

1

al. deﬁned the asymptotic quantity CW (X, Y ; D1 , D2 )1 and
showed that there exists a region of distortions around the
origin where CW (X, Y ; D1 , D2 ) is equal to the Wyner’s single
letter characterization in (1).
We note that much of the focus in the beginning of this
paper in section II will be towards characterizing Wyner’s
deﬁnition of CI in the lossy Gray-Wyner setting. However,
similar results and proof techniques are used in section III to
extend Gács and Körner’s deﬁnition to the lossy framework.
We further note that there have been other physical interpretations of both the notions of common information besides
the Gray-Wyner network, see for example [1], [2]. We are not
yet sure of the relations between such interpretations and the
lossy generalizations we consider in this paper. We will be
investigating these connections are part of our future work.

II. W YNER ’ S L OSSY C OMMON I NFORMATION
A. Deﬁnition
We next deﬁne Wyner’s CI generalized to the lossy framework denoted by CW (X, Y ; D1 , D2 ). It is deﬁned as the
inﬁmum over all shared rates R0 , such that (R0 , R1 , R2 ) ∈
{RGW (D1 , D2 ) ∩ Pangloss plane}, where for any distortion
pair (D1 , D2 ) the plane R0 + R1 + R2 = RX,Y (D1 , D2 ) is
deﬁned as the Pangloss plane. We note that the above operational deﬁnition of CW (X, Y ; D1 , D2 ), has already appeared
recently in [8]. However, a complete single letter information
theoretic characterization of CW (X, Y ; D1 , D2 ) has never
been considered in any earlier work. The primary objective of
section II-B is to characterize CW (X, Y ; D1 , D2 ) for general
sources and distortion measures. It is important to note that
Wyner gave the complete single letter characterization of
CW (X, Y ; 0, 0) when X and Y have ﬁnite joint entropy. His
result is stated formally as:

A. Gray-Wyner Network [3]
Let (X, Y ) be any two dependent random variables taking
ˆ
ˆ
values in the alphabets X and Y respectively. Let X and Y
be two given reconstruction alphabets for random variables X
and Y respectively. We denote the set {1, 2 . . . M } by IM for
any positive integer M , n iid samples of a random variable by
X n and the corresponding alphabet by X n . In what follows,
for any pair of random variables X and Y , RX (·), RY (·) and
RX,Y (·, ·) denote the respective rate distortion functions.
A rate-distortion tuple (R0 , R1 , R2 , D1 , D2 ) is said to be
achievable for the Gray-Wyner network if for all ￿ > 0, there
exists encoder mappings fE : X n × Y n → IM0 × IM1 × IM2
(X)
(Y )
ˆ
and decoder mappings fD : IM0 ×IM1 → X n , fD : IM0 ×
n
ˆ
IM2 → Y such that the following hold:
Mi
∆X ≤ D 1 + ￿

≤

2nRi + ￿ i ∈ {0, 1, 2}
∆Y ≤ D 2 + ￿

CW (X, Y ) = CW (X, Y ; 0, 0) = inf I(X, Y ; U )
where the inﬁmum is over all U satisfying X ↔ U ↔ Y .
B. Single Letter Characterization of CW (X, Y ; D1 , D2 )

In
the
following
theorem,
we
characterize
CW (X, Y ; D1 , D2 ). We denote the set of all channels
X,Y
which achieve RX,Y (D1 , D2 ) by PD1 ,D2 , i.e.,
ˆ ˆ
inf I(X, Y ; X, Y )

I(X, Y ; X ∗ , Y ∗ )

(5)

∀P (X ∗ , Y ∗ |X, Y ) ∈
where the inﬁmum is over
ˆ
ˆ
channels such that E(dX (X, X)) ≤ D1 , E(dX (X, X)) ≤ D2 .
Hereafter we assume that, for every distortion pair (D1 , D2 ),
there exists at least one channel P (X ∗ , Y ∗ |X, Y ) such that
I(X, Y ; X ∗ , Y ∗ ) = RX,Y (D1 , D2 ). We note that, our results
can be easily extended to all other ‘well behaved’ continuous
joint densities using standard techniques from probability
measures.

(2)

Theorem
1. A single letter
CW (X, Y ; D1 , D2 ) is given by:

characterization

CW (X, Y ; D1 , D2 ) = inf I(X, Y ; U )

of
(6)

where the inﬁmum is over all joint densities (X, Y, X ∗ , Y ∗ , U )
such that the following Markov conditions hold:
X∗ ↔

(X, Y ) ↔

U
∗

∗

(X , Y )

↔Y∗
↔U

(7)
(8)

X,Y
and where P (X ∗ , Y ∗ |X, Y ) ∈ PD1 ,D2 is any joint distribution
which achieves the rate distortion function at (D1 , D2 ).

(3)

ˆ
ˆ
Remark 1. If we set X = X , Y = Y and consider the
Hamming distortion measure, at (D1 , D2 ) = (0, 0), it is easy
to show that Wyner’s common information is obtained as a
special case, i.e., CW (X, Y ; 0, 0) = CW (X, Y ).

ˆ
ˆ
if E(dX (X, X)) ≤ D1 and E(dY (Y, Y )) ≤ D2 . The closure
of the achievable rate distortion tuples over all such joint
densities is the complete rate distortion region for the GrayWyner network.
1 They

=

X,Y
PD1 ,D2 ,

(X)
(Y )
ˆ
ˆ
where X n = fD (S0 , S1 ), Y n = fD (S0 , S2 ) and ∆X =
￿n
1
ˆ i ), ∆Y = 1 ￿n dY (Yi , Yi ) for some
ˆ
i=1 dX (Xi , X
i=1
n
n
well deﬁned single letter distortion measures dX (·, ·) and
dY (·, ·). The convex closure over all such achievable ratedistortion tuples is called the achievable region for the GrayWyner network. The set of all achievable rate tuples for any
given distortion D1 and D2 is denoted here by RGW (D1 , D2 ).
Gray and Wyner [3] gave the following complete characˆ ˆ
terization for RGW (D1 , D2 ). Let (U, X, Y ) be any random
variables jointly distributed with (X, Y ) and taking values over
ˆ
ˆ
alphabets U , X and Y respectively, for some arbitrary U . Let
ˆ ˆ
the joint density be P (X, Y, U, X, Y ). All rate-distortion tuples (R0 , R1 , R2 , D1 , D2 ) satisfying the following conditions
are achievable:

R0 ≥ I(X, Y ; U )
ˆ
ˆ
R1 ≥ I(X; X|U ), R2 ≥ I(Y ; Y |U )

(4)

Proof: We note that, although there are arguably simpler
methods to prove this theorem, we choose the following

denote CW (X, Y ; D1 , D2 ) by C3 (D1 , D2 )

2

approach as it uses only the Gray-Wyner theorem without recourse to any supplementary results. We also assume that there
exists a unique channel P ∗ (X ∗ , Y ∗ |X, Y ) which achieves
RX,Y (D1 , D2 ). The proof of the theorem when there are
X,Y
multiple channels in PD1 ,D2 follows directly.
Our objective is to show that every point in the intersection of RGW (D1 , D2 ) and the Pangloss plane has R0 =
I(X, Y ; U ) for some U jointly distributed with (X, Y, X ∗ , Y ∗ )
and satisfying conditions (7) and (8). We ﬁrst prove that
every point in the intersection of the Pangloss plane and
RGW (D1 , D2 ) is achieved by a joint density satisfying (7)
and (8). Towards showing this, we begin with an alternate
characterization of RGW (D1 , D2 ) (which is also complete)
ˆ ˆ
due to Venkataramani et.al in [10]2 . Let (U, X, Y ) be any
random variables jointly distributed with (X, Y ) such that
ˆ
ˆ
E(dX (X, X)) ≤ D1 and E(dY (Y, Y )) ≤ D2 . Then any rate
tuple (R0 , R1 , R2 ) satisfying the following conditions belongs
to RGW (D1 , D2 ):
R0
R1 + R0
R2 + R0
R0 + R 1 + R 2

≥

and hence we have:
ˆ
ˆ
H(X|U ) + H(Y |U )
ˆ ˆ
I(X, Y ; U, X, Y )

ˆ ˆ
ˆ ˆ
I(X, Y ; U, X, Y ) + I(X; Y |U ) (9)

≥
≥(a)
≥

RX,Y (D1 , D2 )

=

I(X, Y ; U )

R0 + R2

=
≥(b)
=

R0 + R 1 + R2

=(c)
=

(12)

I(X, Y ; U ) + I(X, Y ; Y ∗ |U, X ∗ )
I(X, Y ; U ) + I(X, Y ; Y ∗ |U )
I(X, Y ; Y ∗ , U )

I(X, Y ; X ∗ , Y ∗ , U )
∗

(13)
(14)

∗

I(X, Y ; X , Y ) = RX,Y (D1 , D2 )

where (b) and (c) follow from the fact that the joint density
satisﬁes (7) and (8). Hence, we have shown the existence
of one point in R(U ) satisfying (12) for every joint density
(X, Y, X ∗ , Y ∗ , U ) satisfying (7) and (8) proving the theorem.
We note that, in general, it is hard to establish convexity/monotonicity of CW (X, Y ; D1 , D2 ) with respect to
(D1 , D2 ). This makes it hard to establish conclusive inequality
relations between CW (X, Y ; D1 , D2 ) and CW (X, Y ) for all
distortions. However, in the following lemma, we establish
sufﬁcient conditions on (D1 , D2 ) for CW (X, Y ; D1 , D2 ) ≶
CW (X, Y ).

ˆ
ˆ
+H(X|U ) + H(Y |U )
ˆ ˆ
ˆ ˆ
I(X, Y ; X, Y ) − H(X, Y |U )

ˆ
ˆ
+H(X|U ) + H(Y |U )
ˆ ˆ
RX,Y (D1 , D2 ) − H(X, Y |U )
RX,Y (D1 , D2 )

(11)

Consider
the
point,
(R0 , R1 , R2 )
=
(I(X, Y ; U ), I(X, Y ; X ∗ |U ), I(X, Y ; Y ∗ |U, X ∗ )) for any
joint density (X, Y, X ∗ , Y ∗ , U ) satisfying (7) and (8). Clearly
the point satisﬁes the ﬁrst two conditions in (9). Next, we
note that:

R0 + R 1 + R2
ˆ ˆ
ˆ ˆ
I(X, Y ; U, X, Y ) − H(X, Y |U )

ˆ
ˆ
+H(X|U ) + H(Y |U )

=

R0

≥

≥

RX,Y (D1 , D2 )

R 0 + R1 + R 2

ˆ
I(X, Y ; U, X)
ˆ
I(X, Y ; U, Y )

=

=

From our assumption, there is a unique channel,
ˆ ˆ
P ∗ (X ∗ , Y ∗ |X, Y ) which achieves I(X, Y ; X, Y )
=
RX,Y (D1 , D2 ). It therefore follows that every point in
RGW (D1 , D2 ) that lies on the Pangloss plane satisﬁes (9)
for some joint density satisfying (7) and (8).
What remains is to show that any joint density
(X, Y, X ∗ , Y ∗ , U ) satisfying (7) and (8) leads to a subregion of RGW (D1 , D2 ) which has at least one point on
the Pangloss plane with R0 = I(X, Y ; U ). Formally, denote by R(U ), the region (9) achieved by a joint density
(X, Y, X ∗ , Y ∗ , U ) satisfying (7) and (8). Then we have to
show that ∃(R0 , R1 , R2 ) ∈ R(U ) such that:

It is very easy to show that the above characterization is equivalent to (3). As the above characterization is complete, this
implies that, if a rate-distortion tuple (R0 , R1 , R2 , D1 , D2 ) is
achievable for the Gray-Wyner network, then we can always
ˆ ˆ
ˆ
ﬁnd random variables (U, X, Y ) such that E(dX (X, X)) ≤
ˆ
D1 , E(dY (Y, Y )) ≤ D2 and satisfying (9). We are further
interested in characterizing the points in RGW (D1 , D2 ) which
also lie on the Pangloss plane, i.e., R0 + R1 + R2 =
RX,Y (D1 , D2 ). Therefore, for any rate tuple (R0 , R1 , R2 ) on
the Pangloss plane in RGW (D1 , D2 ), we have the following
series of inequalities:
RX,Y (D1 , D2 )

=

ˆ ˆ
H(X, Y |U )
ˆ ˆ
I(X, Y ; X, Y )

ˆ ˆ
I(X, Y ; X, Y )

I(X, Y ; U )

≥
≥

=

(10)

Lemma 1. For any pair of random variables (X, Y ),

ˆ ˆ
where (a) follows because (X, Y ) satisfy the distortion
constraints. As the LHS and RHS of the above series of
inequalities are the same, all the inequalities must be equalities

(i) CW (X, Y ; D1 , D2 ) ≤ CW (X, Y ) at (D1 , D2 ) if
˜ ˜
˜
˜
∃(D1 , D2 ) such that D1 ≤ D1 , D2 ≤ D2 and
˜ 1 , D2 ) = CW (X, Y )
˜
RX,Y (D
• (ii) CW (X, Y ; D1 , D2 ) ≥ CW (X, Y ) if Shannon lower
bound for RXY (D1 , D2 ) is tight at (D1 , D2 )
Proof: The proof of (i) is rather straightforward and hence
we choose to omit it. Towards proving (ii), it is easy to
show using standard techniques [11], [9] that the conditional
•

2 We note that the theorem can be proved even using the original GrayWyner’s characterization. However, if we begin with that characterization, we
would require the random variables to satisfy two more Markov conditions
beyond (7) and (8). These Markov chains can in fact be shown to be redundant
using Kuhn-Tucker conditions. We choose this alternate approach to avoid
recourse to such supplementary results.

3

distribution P (X ∗ , Y ∗ |X, Y ) which achieves RX,Y (D1 , D2 )
when Shannon lower bound is tight has independent backward
channels, i.e.:

X
Y

PX,Y |X ∗ ,Y ∗ (x, y|x∗ , y ∗ ) = QX|X ∗ (x|x∗ )QY |Y ∗ (y|y ∗ ) (15)
Let us consider any U which satisﬁes (X ↔ U ↔ Y ) and
(X, Y ) ↔ (X ∗ , Y ∗ ) ↔ U . It is easy to verify that any such
joint density also satisﬁes X ↔ U ↔ Y . As the inﬁmum
for CW (X, Y ) is taken over a larger set of joint densities, we
have CW (X, Y ; D1 , D2 ) ≥ CW (X, Y ).
The above lemma highlights the anomalous behavior of
CW (X, Y ; D1 , D2 ) with respect to the distortions. Determining the conditions for equality in Lemma 1.(ii) is an interesting
problem in its own right. For the symmetric setting, i.e.,
D1 = D2 = D, it was shown in [8] using a completely
different approach leveraging prior results from conditional
rate distortion theory (see for eg. [9]) that CW (X, Y ; D, D) =
−1
−1
CW (X, Y ) iff D ≤ RX,Y (CW (X, Y )), where RX,Y (·) denotes the distortion-rate function. We will further explore the
underlying connections between these results as part of our
future work.
∗

∗

∗
∗

=
=

￿
￿

ρ
U∗ +
1−D
ρ
U∗ +
1−D

￿
￿

(1 − D)2 − ρ ˜
N1
1−D
(1 − D)2 − ρ ˜
N2
1−D

(19)

˜
˜
where N1 and N2 are independent standard Gaussian random variables independent of both N1 and N2 . Therefore
there exists a joint density over (X, Y, X ∗ , Y ∗ , U ∗ ) satisfying
X ∗ ↔ U ∗ ↔ Y ∗ and (X, Y ) ↔ (X ∗ , Y ∗ ) ↔ U ∗ . This shows
that CW (X, Y ; D, D) ≤ CW (X, Y ). Therefore in the range
0 < D ≤ 1 − ρ, we have CW (X, Y ; D, D) = CW (X, Y ). We
note that this speciﬁc result was already deduced in [8].
However CW (X, Y ; D, D) in the range 1−ρ ≤ D ≤ 1, has
never been considered to date. Note that the Shannon lower
bound for RX,Y (D, D) is not tight in this range. However,
the RD-optimal conditional distribution P (X ∗ , Y ∗ |X, Y ) in
this distortion range is such that X ∗ = Y ∗ . Therefore
the only U which satisﬁes (X ∗ ↔ U ↔ Y ∗ ) is U =
X ∗ = Y ∗ . Therefore from Theorem 1, we conclude that
CW (X, Y ; D, D) = RX,Y (D, D) for 1 − ρ ≤ D ≤ 1. Of
course, for D > 1, CW (X, Y ; D, D) = 0. Hence we have
completely characterized CW (X, Y ; D, D) for (X, Y ) jointly
Gaussian for all symmetric distortions D1 = D2 = D > 0.

C. Bivariate Gaussian Example
Let X and Y be jointly Gaussian random variables with
zero mean, unit variance and a correlation coefﬁcient of ρ. We
focus on the symmetric distortion scenario, i.e. D1 = D2 =
D, under mean squared error distortion metric. The joint rate
distortion function is given by [11]:

￿
￿
 1 log 1−ρ2
if 0 < D ≤ 1 − ρ
2
D2
￿
￿
RX,Y (D, D) =
1+ρ
 1 log
if 1 − ρ ≤ D ≤ 1
2
2D−1+ρ
(16)
We ﬁrst consider the range 0 < D ≤ 1 − ρ. The RD-optimal
random encoder is such that P (X|X ∗ ) and P (Y |Y ∗ ) are two
independent zero mean Gaussian channels with variance D. It
is easy to verify that the optimal reproduction distribution (for
(X ∗ , Y ∗ )) is jointly Gaussian with zero mean. The covariance
matrix for (X ∗ , Y ∗ ) is:
￿
￿
ρ
1 − D 1−D
(17)
ΣX ∗ Y ∗ =
ρ
1−D
1−D

III. G ÁCS -KÖRNER ’ S CI
Gács and Körner [1] deﬁned CI of X and Y as the maximum rate of the codeword that can be generated individually
at two encoders observing X n and Y n separately. Formally:
1
H(f1 (X n ))
(20)
n
where sup is taken over all f1 and f2 such that P (f1 (X n ) ￿=
f2 (Y n )) → 0. They showed that CGK (X, Y ) is equal to
the entropy of the random variable which deﬁnes the ergodic
decomposition of the stochastic matrix of conditional probabilities P (X = x|Y = y), i.e., if J is a random variable such
￿
that J = j iff x ∈ Xj ⇔ y ∈ Yj , where X × Y = j Xj × Yj ,
then CGK (X, Y ) = H(J).
Gács-Körner’s original deﬁnition of CI was naturally unrelated to the Gray-Wyner network, which it predates. However,
an equivalent and insightful characterization of CGK (X, Y )
was given by Ahlswede and Körner [5] in terms of RGW (0, 0)
as follows:
CGK (X, Y ) = sup

At these distortions, the Shannon lower bound is tight and
hence from Lemma 1, CW (X, Y ; D, D) ≥ CW (X, Y ). Further, it was shown in [8] that CW (X, Y ) = CW (X, Y ; 0, 0) =
1+ρ
1
∗
2 log 1−ρ and the inﬁmum achieving U is a standard Gaussian random variable jointly distributed with (X, Y ) as:
√ ∗ ￿
X =
ρU + 1 − ρN1
√ ∗ ￿
Y =
ρU + 1 − ρN2
(18)

CGK (X, Y ) = max R0 : (R0 , R1 , R2 ) ∈ RGW (0, 0)

(21)

subject to,
R0 + R1 = H(X)

R0 + R2 = H(Y )

(22)

Speciﬁcally, Ahlswede and Körner showed that:
H(J) = CGK (X, Y ) = sup I(X, Y ; U )

where N1 and N2 are independent standard Gaussian random
variables. We can generate (X ∗ , Y ∗ ) by passing U ∗ through
independent Gaussian channels as follows:

(23)

subject to,
Y ↔ X ↔ U and X ↔ Y ↔ U

4

(24)

Corollary 1. CGK (X, Y ; D1 , D2 ) ≤ CGK (X, Y )

Though the original deﬁnition of Gács-Körner’s CI does not
have a direct lossy interpretation, the equivalent deﬁnition
given by Ahlswede and Körner in terms of the lossless GrayWyner region can be easily extended to the lossy setting similar to Wyner’s CI. These generalizations provide theoretical
insight into the performance limits of practical databases for
fusion storage of correlated sources as described in [4].
We deﬁne the lossy generalization of Gács-Körner’s CI at
(D1 , D2 ), denoted by CGK (X, Y ; D1 , D2 ) as.
sup R0 : (R0 , R1 , R2 ) ∈ RGW (D1 , D2 )

Proof: This corollary follows directly from Theorem 2 as
conditions in (24) are a subset of the conditions in (28).
It is easy to show that if the random variables (X, Y ) are
jointly Gaussian with a correlation coefﬁcient ρ < 1, then
CGK (X, Y ) = 0. Hence from Corollary 1, it follows that, for
jointly Gaussian random variables with correlation coefﬁcient
strictly less than 1, CGK (X, Y ; D1 , D2 ) = 0 ∀D1 , D2 under
any distortion metric. It is well known that CGK (X, Y )
is typically very small (usually zero) and depends only on
the zeros of the joint distribution. In the general setting,
as CGK (X, Y ; D1 , D2 ) ≤ CGK (X, Y ), it would seem that
Theorem 2 has very limited practical signiﬁcance. However
in a separate paper, currently under preparation, we show that
CGK (X, Y ; D1 , D2 ) plays a central role in scalable coding of
sources that are not successively reﬁnable.

(25)

subject to,
R0 + R1 = RX (D1 )

R0 + R2 = RY (D2 )

(26)

We provide an information theoretic characterization for
CGK (X, Y ; D1 , D2 ) in the following theorem. We again assume that there exists channels which achieve RX (D1 ) and
RY (D2 ) respectively, noting that the results can be easily
extended to more general source densities. We denote the set
X
of all channels which achieve RX (D1 ) and RY (D2 ) by PD1
X
and PD1 respectively.
Theorem
2. A single letter
CGK (X, Y ; D1 , D2 ) is given by:

characterization

CGK (X, Y ; D1 , D2 ) = sup I(X, Y ; U )

IV. C ONCLUSION
In this paper we derived single letter information theoretic
characterizations for the lossy generalizations of the two most
prevalent notions of common information due to Wyner and
Gács-Körner. These generalizations allow us to extend the
theoretical interpretation underlying their original deﬁnitions
to sources with inﬁnite entropy (eg. continuous random variables). We use these information theoretic characterizations
to derive the common information of symmetric bivariate
Gaussian random variables.

of

(27)

˜ ˜
where the supremum is over all joint densities (X, Y, X, Y , U )
such that the following Markov conditions hold:
Y ↔X↔U
˜
X↔X↔U

X↔Y ↔U
˜
Y ↔Y ↔U

R EFERENCES
[1]

P. Gács and J. Körner, “Common information is far less than mutual
information”, Problems of Control and Information Theory, vol. 2, pp.
149-162, 1973.
[2] A. Wyner, “The common information of two dependent random variables”, IEEE Trans. on Information Theory, vol. 21, pp. 163 – 179, Mar
1975.
[3] R. Gray and A. Wyner, “Source coding for a simple network”, Bell
systems technical report, Dec 1974.
[4] K. Viswanatha, E. Akyol, K. Rose, “An optimal transmit-receive rate
tradeoff in Gray-Wyner network and its relation to common information," in Proc. IEEE Information Theory Workshop (ITW), pp.105-109,
Oct 2011.
[5] R. Ahlswede and J. Körner, “On common information and related
characteristics of correlated information sources”, preprint presented at
the 7th Prague Conference on Information Theory, 1974.
[6] H. Yamamoto, "Coding theorems for Shannon’s cipher system with
correlated source outputs and common information," IEEE Trans. on
Information Theory, vol. 40, no. 1, pp. 85-95, Jan 1994.
[7] S. Kamath and V. Anantharam, “A new dual to the Gács - Körner
common information deﬁned via the Gray-Wyner system,” in Proc.
48th Allerton Conf. on Communication, Control, and Computing, pp.
1340–1346, 2010.
[8] G. Xu, W. Liu, B. Chen; , “Wyners common information for continuous
random variables - A lossy source coding interpretation," in Proc. 45th
Annual Conference on Information Sciences and Systems (CISS), pp.16, Mar 2011.
[9] R. Gray, “A new class of lower bounds to information rates of stationary sources via conditional rate-distortion functions," IEEE Trans. on
Information Theory, vol. 19, no. 4, pp. 480- 489, Jul 1973.
[10] R. Venkataramani, G. Kramer, V.K. Goyal, “Multiple description coding
with many channels," IEEE Trans. on Information Theory, vol. 49, no.
9, pp. 2106- 2114, Sept 2003.
[11] T. Berger, “Rate distortion theory”, Englewood, New Jersey, PrenticeHall, 1971.

(28)

X
Y
˜
˜
where P (X|X) ∈ PD1 and P (Y |Y ) ∈ PD2 .

Proof: The proof follows in very similar lines to the proof
of Theorem 1. The original Gray-Wyner’s characterization is
in fact sufﬁcient in this case. Again we ﬁrst assume that there
˜
˜
are unique channels P (X|X) and P (Y |Y ) which achieve
RX (D1 ) and RY (D2 ) respectively. The proof extends directly
to the case of multiple RD optimal channels.
We are interested in characterizing the points in
RGW (D1 , D2 ) which lie on both the planes R0 + R1 =
RX (D1 ) and R0 + R2 = RY (D2 ). Therefore we have the
following series of inequalities:
RX (D1 )

=

R 0 + R1

≥

ˆ
I(X, Y ; U ) + I(X; X|U )
ˆ
I(X; X, U ) + I(Y ; U |X)

=
≥

ˆ
I(X; X) ≥ RX (D1 )

Writing similar inequality relations for Y and following the
same arguments as in Theorem 1, it follows that for all
X
˜
joint densities satisfying (28) and for which P (X|X) ∈ PD1
Y
˜
and P (Y |Y ) ∈ PD2 , there exists at least one point in
RGW (D1 , D2 ) which satisﬁes both R0 + R1 = RX (D1 ) and
R0 + R2 = RY (D2 ) and for which R0 = I(X, Y ; U ). This
proves the theorem.

5

