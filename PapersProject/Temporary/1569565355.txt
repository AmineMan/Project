Title:          shaping_isit2012.dvi
Creator:        dvips(k) 5.97 Copyright 2008 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May 16 18:24:42 2012
ModDate:        Tue Jun 19 12:54:12 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      968631 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565355

The Capacity Loss of Dense Constellations
Tobias Koch

Alfonso Martinez

Albert Guill´ n i F` bregas
e
a

University of Cambridge
tobi.koch@eng.cam.ac.uk

Universitat Pompeu Fabra
alfonso.martinez@ieee.org

ICREA & Universitat Pompeu Fabra
University of Cambridge
guillen@ieee.org

Recently, Wu and Verd´ have studied the information rates
u
that are achievable over the Gaussian channel when the input
takes value in a ﬁnite constellation with N signal points [9].
For every ﬁxed SNR, they show that the difference between
the capacity and the achievable rate tends to zero exponentially
in N. For the optimal constellation, the peak-to-average-power
ratio grows linearly with N, inducing no capacity loss. This
is in contrast to the constellations considered by Ungerboeck
[2] and Ozarow and Wyner [3], which have a ﬁnite peak-toaverage-power ratio.
In this work, we adopt an information-theoretic perspective
to study the capacity loss incurred by signal constellations with
a bounded support over the Gaussian channel for sufﬁciently
small noise variance. In particular, we use the duality-based
upper bound to the mutual information in [10] to provide a
lower bound on the capacity loss. The results are valid for both
peak- and average-power constraints and generalize directly
to other additive-noise channel models. For sufﬁciently high
SNR, our results recover the power loss of 1.53dB for square
signal constellations without invoking geometrical arguments.

Abstract—We determine the loss in capacity incurred by
using signal constellations with a bounded support over general
complex-valued additive-noise channels for suitably high signalto-noise ratio. Our expression for the capacity loss recovers the
power loss of 1.53dB for square signal constellations.

I. I NTRODUCTION
As it is well known, the channel capacity of the complexvalued Gaussian channel with input power at most P and noise
variance σ 2 is given by [1]
CG (P, σ) = log 1 +

P
σ2

.

(1)

Although inputs distributed according to the Gaussian distribution attain the capacity, they suffer from several drawbacks
which prevent them from being used in practical systems.
Among them, especially relevant are the unbounded support
and the inﬁnite number of bits needed to represent signal
points.
In practice, discrete distributions with a bounded support
are typically preferred—in this case, the number of points
is allowed to grow with the signal-to-noise ratio (SNR).
Ungerboeck computed the rates that are achievable over the
Gaussian channel when the channel input takes value in a
ﬁnite constellation [2]. He observed that, when transmitting
at a rate of R bits per channel use, there is not much to
be gained from using constellations with size N larger than
2R+1 . Ozarow and Wyner provided an analytic conﬁrmation
of Ungerboeck’s observation by deriving a lower bound on the
rates achievable with ﬁnite constellations [3]. In both works,
the channel inputs are assumed to be uniformly distributed on
a lattice within some enclosing boundary, where the size of
the boundary is scaled in order to ensure unit input-power.
A related line of work considered signal constellations
with favorable geometric properties, e.g., minimum Euclidean
distance or minimum average error probability. For signal
constellations with a large number of points, i.e., dense constellations, Forney et al. [4] estimated the loss in SNR with
respect to the Gaussian input to be 10 log10 πe ≈ 1.53dB by
6
comparing the volume of an n-dimensional hypercube with
that of an n-dimensional hypersphere of identical average
power. Later, Ungerboeck’s work led to the study of multidimensional constellations based on lattices [5]–[8].

II. C HANNEL M ODEL AND C APACITY
We consider a discrete-time, complex-valued additive noise
channel, where the channel output Yk at time k ∈ Z (where
Z denotes the set of integers) corresponding to the time-k
channel input xk is given by
Yk = xk + σWk ,

k ∈ Z.

(2)

We assume that {Wk , k ∈ Z} is a sequence of independent
and identically distributed, centered, unit-variance, complex
random variables of ﬁnite differential entropy. We further
assume that the distribution of Wk does neither depend on
σ > 0 nor on the sequence of channel inputs {xk , k ∈ Z}.
The channel inputs take value in the set S, which is assumed
to be a bounded Borel subset of the complex numbers C. We
further assume that S has positive Lebesgue measure and that
0 ∈ S.
The set S can be viewed as the region that limits the signal
points. For example, for a square signal constellation, it is a
square:
S

{x ∈ C : − A ≤ Re (x) ≤ A, −A ≤ Im (x) ≤ A} (3)

for some A > 0. Here Re (x) and Im (x) denote the real
and imaginary part of x, respectively. Similarly, for a circular
signal constellation,

The research leading to these results has received funding from the
European Community’s Seventh Framework Programme (FP7/2007-2013)
under grant agreement No. 252663 and from the European Research Council
under ERC grant agreement 259663.

S•

1

{x ∈ C : |x| ≤ R},

for some R > 0.

(4)

We study the capacity of the above channel under an
average-power constraint P on the inputs. Since the channel
is memoryless, it follows that the capacity CS (P, σ) (in nats
per channel use) is given by
CS (P, σ) =

sup
X∈S,E[|X|2 ]≤P

I(X; Y )

which together with (9) yields (10).
Let PU denote the average power of a random variable that
is uniformly distributed over S, i.e.,

(5)

lim CC (P, σ) − CS (P, σ) .

2

sup
E[|X|2 ]≤P

I(X; Y ).

sup
X∈S,E[|X|2 ]≤P

1
.
σ2

I(X; Y ) − log

L ≤ log P + log(πe) − log

sup
X∈S,E[|X|2 ]≤P

h(X).

L ≤ log(πe) + log

(10)

1
(11)
σ2
which follows from the behavior of differential entropy under
deterministic translation and under scaling by a complex
number. Extending [10, Lemma 6.9] (see also [11]) to complex
random variables yields then that, for every E |X|2 < ∞ and
E |W |2 < ∞, the ﬁrst differential entropy on the right-hand
side (RHS) of (11) satisﬁes
I(X; Y ) = h(X + σW ) − h(W ) + log

(12)

σ↓0

Consequently, we obtain
lim
σ↓0

≥
=

sup
X∈S,E[|X|2 ]≤P

sup
X∈S,E[|X|2 ]≤P

sup
X∈S,E[|X|2 ]≤P

I(X; Y ) − log

1
σ2

lim I(X; Y ) − log
σ↓0

h(X) − h(W )

2

S

e−λ|x | dx

− λ P.

(17)

S

|x|2 dx − 2 log

S

dx .

(18)

Specializing (18) to a square signal constellation (3) yields
(irrespective of A)
πe
(19)
L ≤ log
6
which corresponds to a power loss of roughly 1.53dB. Hence,
we recover the rule of thumb that “square signal constellations
have a 1.53dB power loss at high signal-to-noise ratio.”
For a circular signal constellation (4), the upper bound (18)
becomes (irrespective of R)
e
(20)
L• ≤ log
2
recovering the power loss of 1.33dB [4].
The inequality in (17) holds with equality if the capacityachieving input-distribution does not depend on σ, cf. (13).
However, this is in general not the case. For example, for
circularly-symmetric Gaussian noise and a circular signal
constellation (4), it was shown by Shamai and Bar-David [13]
that, for every σ > 0, the capacity-achieving input-distribution
is discrete in magnitude, with the number of mass points
growing with vanishing σ. Nevertheless, the following theorem
demonstrates that the RHS of (17) is indeed the capacity loss.
Theorem 1 (Main Result): For the above channel model,
we have

(9)

Indeed, we have

lim h(X + σW ) = h(X).

(16)

For P = PU (and hence λ = 0), this becomes

By choosing an input distribution that does not depend on
σ, we can achieve1
L ≤ log P + log(πe) −

e−λ|x| |x|2 dx
=P
e−λ|x |2 dx
S

for P < PU . Here I {statement} denotes the indicator function:
it is equal to one if the statement in the brackets is true and
it is otherwise equal to zero.
Applying (15) to (10) yields

(7)

L = log P + log(πe) − h(W )
σ↓0

(15)

2

S

For small σ, we have [1]
P
(8)
CC (P, σ) = log 2 + log(πe) − h(W ) + o(1)
σ
where the o(1)-term vanishes as σ tends to zero. (Here log(·)
denotes the natural logarithm and h(·) denotes differential
entropy.) The capacity loss (6) can thus be written as

− lim

x∈C

where λ = 0 for P ≥ PU , and where λ satisﬁes

(Theorem 1 ahead asserts the existence of the limit.) Here
CC (P, σ) denotes the capacity of the above channel when the
support-constraint S is relaxed, i.e.,
CC (P, σ) =

(14)

e−λ|x|
f (x) =
I {x ∈ S} ,
−λ|x |2 dx
Se

(6)

σ↓0

|x|2 dx
.
dx
S

A small modiﬁcation of the proof in [12, Th. 12.1.1] shows
that the density that maximizes h(X) for X ∈ S with
probability one and E |X|2 ≤ P has the form

where the supremum is over all input distributions with
essential support in S that satisfy E |X|2 ≤ P.
We focus on CS (P, σ) in the limit as the noise variance σ
tends to zero. In particular, we study the capacity loss, which
we deﬁne as
L

S

PU

1
σ2

L = log P + log(πe) − log

(13)

2

S

e−λ|x | dx

− λP

(21)

where λ = 0 for P ≥ PU , and where λ satisﬁes (16) for
P < PU .
Proof: See Section III.

deﬁne h(X) = −∞ if the distribution of X is not absolutely
continuous with respect to the Lebesgue measure.
1 We

2

where S c denotes the complement of S ; and where λ is zero
for P ≥ PU and satisﬁes (16) for P < PU . Some useful
properties of K ,σ are summarized in the following lemma.
Lemma 2: The normalizing constant K ,σ satisﬁes

Note 1: It is not difﬁcult to adapt the proof of Theorem 1
to other regions S and moment constraints. For example, the
same proof technique can be used to derive the capacity loss
when S is a Borel subset of the real numbers and the channel
input’s ﬁrst-moment is limited, i.e., E[|X|] ≤ A.
Equations (11)–(13) demonstrate that the capacity loss (21)
can be achieved with a continuous-valued channel input having density f (·). Using the lower-semicontinuity of relative
entropy [14], it can be further shown that (21) can also be
achieved by any sequence of discrete channel inputs {XN }
for which the number of mass points N grows with vanishing
σ, provided that
L

XN → X

as N → ∞

inf K

S

(22)

+ λ P − h(W ).

D W (·|x)

D W (·|x)

R(·) dQ(x)

K

,σ

S

e

dy +
Sc

is a normalizing constant; where S
neighborhood of S
S

y ∈ C : |y − x | ≤ , for some x ∈ S ;

p(y|x) log r(y) dy dQ(x) (29)

1
σ2

(30)

and some algebra applied to the second summand in (29)
allows us to write it as
−

(23)

p(y|x) log r(y) dy dQ(x)
= log K

,σ

+ λ E |Y |2 I {Y ∈ S }

+ log π 2 σ 2 Pr Y ∈ S c
|Y |
+ E log
I {Y ∈ S c }
σ
|Y |2
+ E log 1 + 2 I {Y ∈ S c } .
σ

(24)

(31)

Combining (30) and (31) with (29) and (24) yields
I(X; Y )
≤ −h(W ) + log

1
+ log K
σ2

,σ

+ λ E |Y |2 I {Y ∈ S }

+ log π 2 σ 2 Pr Y ∈ S c + E log
+ E log 1 +

|Y |2
σ2

We next show that, for
lim

sup

sup

σ↓0 X∈S,E[|X|2 ]≤P

(26)

lim

-

sup

σ↓0 X∈S,E[|X|2 ]≤P

lim

sup

σ↓0 X∈S,E[|X|2 ]≤P

(27)

3

|Y |
σ

I {Y ∈ S c } .

I {Y ∈ S c }
(32)

> 0,
E |Y |2 I {Y ∈ S }

≤ P (33a)

log π 2 σ 2 Pr Y ∈ S c

= 0 (33b)

σ↓0 X∈S,E[|X|2 ]≤P

lim

denotes the

(28b)

R(·) dQ(x)

h(Y |X) = h(W ) − log

where
1
1
dy
π 2 σ|y| 1 + |y|2 /σ 2

e−λ|y| dy.

where p(y|x) denotes the conditional probability density function of Y , conditioned on X = x.
Evaluation of the conditional differential entropy gives

where Q(·) denotes the input distribution; W (·|x) denotes the
conditional distribution of the channel output, conditioned on
X = x; and R(·) denotes some arbitrary distribution on the
output alphabet. Every choice of R(·) yields an upper bound
on I(X; Y ), and the inequality in (24) holds with equality
if R(·) is the actual distribution of Y induced by Q(·) and
W (·|·).
To derive an upper bound on I(X; Y ), we apply (24) with
R(·) having density
⎧
2
⎪ e−λ|y|
⎪
⎪
,
y∈S
⎨
K ,σ
r(y) =
(25)
⎪ 1
1
1
⎪
⎪
,
y∈S
/
⎩
K ,σ π 2 σ|y| 1 + |y|/σ 2
−λ|y|2

2

S

= −h(Y |X) −

The claim follows then by combining (23) with (17). To this
end, we use the upper bound on the mutual information [10,
Th. 5.1]
I(X; Y ) ≤

=

(28a)

Proof: Omitted.
We return to the analysis of I(X; Y ) and apply (24) together
with the density (25) to express the upper bound as

In view of (9), in order to prove Theorem 1 it sufﬁces to
show that
1
lim
I(X; Y ) − log 2
sup
σ↓0 X∈S,E[|X|2 ]≤P
σ
2

,σ

↓0 σ↓0

III. P ROOF OF T HEOREM 1

e−λ|x | dx

>0

lim lim K

where X is a continuous random variable having density
L
f (·). (Here → denotes convergence in distribution.) Such
a sequence can, for example, be obtained by approximating the distribution function corresponding to f (·) by twodimensional step functions.

≤ log

,σ

>0,
σ>0

E log

|Y |
σ

E log 1 +

I {Y ∈ S c }

|Y |2
σ2

= 0 (33c)

I {Y ∈ S c } = 0. (33d)

The ﬁrst claim (33a) follows by upper-bounding
sup

X∈S,E[|X|2 ]≤P

≤

E |Y | I {Y ∈ S }

sup
X∈S,E[|X|2 ]≤P

=

from which (39)—and hence (33c)—follows by noting that
the RHS of (42) vanishes as σ tends to zero.
To prove (33d), we use Jensen’s inequality and (34) to
obtain
|Y |2
E log 1 + 2 I {Y ∈ S c }
σ

2

sup
X∈S,E[|X|2 ]≤P
2

E |Y |2
E |X|2 + σ 2 E |W |2

≤P+σ

≤ Pr Y ∈ S c log 1 +

(34)

where the second step follows because X and W are independent, and the third step follows because E |X|2 ≤ P and
E |W |2 = 1.
To prove (33b), we ﬁrst note that
Pr Y ∈ S c ≤ Pr σ|W | >

.

It then follows that, for σ ≤

σ2
2

− Pr Y ∈ S c

|Y |2
I {Y ∈ S c }
σ2
σ2
P
σ2
σ2
σ2
≤ 2 log 1 + 2 + 2 − 2 log 2
(44)
σ
from which (33d) follows by noting that the RHS of (44)
vanishes as σ tends to zero.
Combining (33a)–(33d) with (32) yields
0 ≤ E log 1 +

(36)

1
π,

σ2

0 ≤ − log π 2 σ 2 Pr Y ∈ S c ≤ − log π 2 σ 2

2

(37)

lim

where the right-most term vanishes as σ tends to zero. This
proves (33b).
We next turn to (33c). We ﬁrst note that every y ∈ S c must
satisfy |y| > , since otherwise |y − x | ≤ for x = 0, which
by assumption is in S. Therefore,
E log

|Y |
σ

I {Y ∈ S c } ≥ log
≥ 0,

Pr Y ∈ S c
σ
for σ ≤ .

σ↓0

|Y |
σ

σ↓0

I {Y ∈ S c } ≤ 0.

(38)

(39)

lim
E[|Y | I {Y ∈ S }]
σPr Y ∈ S c
(40)

L(σ)

σ2
σ2
log 2
2 2

,σ

+ λP

(45)

2

S

e−λ|y| dy − λ P.

(46)

h(X + σW ) =

sup
X∈S,E[|X|2 ]≤P

h(X). (47)

CC (P, σ) − CS (P, σ),

σ > 0.

(48)

Indeed, combining (43), (40), and (34) with (32) yields
1
I(X; Y ) ≤ −h(W ) + log 2 + log K ,σ + λ P + σ 2
σ
+ log+ π 2 σ 2 Pr Y ∈ S c
1
P
+ Pr Y ∈ S c log 1 + 2
2
σ
P
+ Pr Y ∈ S c log 1 + 2 + Pr Y ∈ S c
σ
3
(49)
− Pr Y ∈ S c log Pr Y ∈ S c
2

(41)

E log

−

+ λP

While this approach may seem simpler, our approach has
the advantage that it also allows for a lower bound on the
nonasymptotic capacity loss

Using (36) together with the fact that ξ → −ξ log ξ is monotonically increasing for ξ ≤ e−1 , we obtain for σ ≤ e−1/2
|Y |
I {Y ∈ S c }
σ
1 σ2
P
≤
log 1 + 2
2
2
σ

sup

σ↓0 X∈S,E[|X|2 ]≤P

c

E[|Y |2 ] Pr Y ∈ S c .

,σ

IV. N ONASYMPTOTIC C APACITY L OSS
A natural approach to prove Theorem 1 would be to
generalize (12) to

where the last step follows from the Cauchy-Schwarz inequality
E[|Y | I {Y ∈ S c }] ≤

1
σ2

where the last equation follows from the continuity of x →
log(x) for x > 0. Letting tend to zero, and using (28b) in
Lemma 2, we prove (23) and therefore the desired
L = log P + log(πe) − log

1
P + σ2
Pr Y ∈ S c log
2 Pr Y ∈ S c
2
σ

≤

I(X; Y ) − log

= − h(W ) + log lim K

I {Y ∈ S c }

≤ Pr Y ∈ S c log

X∈S,E[|X|2 ]≤P

σ↓0

By Jensen’s inequality, we have
E log

sup

≤ − h(W ) + lim log K

To prove (33c), it thus remains to show that
|Y |
sup
E log
lim
σ↓0 X∈S,E[|X|2 ]
σ

(43)

Using (36) together with the fact that ξ → −ξ log ξ is monotonically increasing for ξ ≤ e−1 , we obtain for σ ≤ e−1/2

(35)

.

P
+ Pr Y ∈ S c
σ2
log Pr Y ∈ S c .

≤ Pr Y ∈ S c log 1 +

Indeed, if |σw| ≤ , then we have |y − x | = |x+ σw − x | ≤
for x = x ∈ S, so y ∈ S . By Chebyshev’s inequality [15,
Sec. 5.4], this can be further upper-bounded by
Pr Y ∈ S c ≤

E |Y |2 I {Y ∈ S c }
σ 2 Pr Y ∈ S c

(42)

4

Gauss-Hermite quadratures [16], as described for example
in [17, Sec. III]. Since for a ﬁxed m the information rate
corresponding to 2m -ary QAM is bounded by m bits, the
rate loss of 2m -ary QAM tends to inﬁnity as σ tends to
zero. We observe that the lower bound on L(σ) converges to
L = log(πe/6) ≈ 0.353 as σ tends to zero, but is rather loose
for ﬁnite σ. However, in the proof of Theorem 1 we chose the
density (25) to decay sufﬁciently slowly, so as to ensure that
the lower bound on L holds for every unit-variance noise of
ﬁnite differential entropy. For Gaussian noise, a density can be
chosen that decays much faster, giving rise to a tighter bound.

0.5

210 -ary QAM

216 -ary QAM

222 -ary QAM

0.45

L(σ)

0.4

asymptotic capacity loss L

0.35

0.3

lower bound on L(σ)

0.25

0.2

ACKNOWLEDGMENT

0.15
−10

0

10

20

30

1/σ 2 [dB]

40

50

60

The authors would like to thank Alex Alvarado for helpful
discussions and for providing the QAM curves in Figure 1.

70

Fig. 1. The capacity loss L(σ) for circularly-symmetric Gaussian noise and
square constellations with P = PU .

where log+ (ξ)
K

,σ

R EFERENCES
[1] C. E. Shannon, “A mathematical theory of communication,” Bell System
Techn. J., vol. 27, pp. 379–423 and 623–656, July and Oct. 1948.
[2] G. Ungerboeck, “Channel coding with multilevel/phase signals,” IEEE
Trans. Inform. Theory, vol. 28, pp. 55–67, Jan. 1982.
[3] L. H. Ozarow and A. D. Wyner, “On the capacity of the Gaussian
channel with a ﬁnite number of input levels,” IEEE Trans. Inform.
Theory, vol. 36, pp. 1426–1428, Nov. 1990.
[4] G. D. Forney, Jr., R. G. Gallager, G. R. Lang, F. M. Longstaff, and
G. R. Qureshi, “Efﬁcient modulation for band-limited channels,” IEEE
J. Select. Areas Commun., vol. SAC-2, pp. 632–647, Sept. 1984.
[5] G. D. Forney, Jr. and L.-F. Wei, “Multidimensional constellations—Part
I: Introduction, ﬁgures of merit, and generalized cross constellations,”
IEEE J. Select. Areas Commun., vol. 7, no. 6, pp. 877–892, Aug. 1989.
[6] G. D. Forney, Jr., “Multidimensional constellations—Part II: Voronoi
constellations,” IEEE J. Select. Areas Commun., vol. 7, no. 6, pp. 941–
958, Aug. 1989.
[7] A. R. Calderbank and L. H. Ozarow, “Nonequiprobable signaling on
the gaussian channel,” IEEE Trans. Inform. Theory, vol. 36, no. 4, pp.
726–740, July 1990.
[8] F. R. Kschischang and S. Pasupathy, “Optimal nonuniform signaling
for gaussian channels,” IEEE Trans. Inform. Theory, vol. 39, no. 3, pp.
913–929, May 1993.
[9] Y. Wu and S. Verd´ , “The impact of constellation cardinality on
u
Gaussian channel capacity,” in Proc. 48th Allerton Conf. Comm., Contr.
and Comp., Allerton H., Monticello, Il, Sept. 29– Oct. 1, 2010.
[10] A. Lapidoth and S. M. Moser, “Capacity bounds via duality with
applications to multiple-antenna systems on ﬂat fading channels,” IEEE
Trans. Inform. Theory, vol. 49, no. 10, pp. 2426–2467, Oct. 2003.
[11] T. Linder and R. Zamir, “On the asymptotic tightness of the Shannon
lower bound,” IEEE Trans. Inform. Theory, vol. 40, no. 6, pp. 2026–
2031, Nov. 1994.
[12] T. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd ed.
John Wiley & Sons, 2006.
[13] S. Shamai (Shitz) and I. Bar-David, “The capacity of average and
peak-power-limited quadrature Gaussian channels,” IEEE Trans. Inform.
Theory, vol. 41, no. 4, pp. 1060–1071, July 1995.
[14] E. C. Posner, “Random coding strategies for minimum entropy,” IEEE
Trans. Inform. Theory, vol. 21, pp. 388–391, July 1975.
[15] R. G. Gallager, Information Theory and Reliable Communication. John
Wiley & Sons, 1968.
[16] M. Abramowitz and I. A. Stegun, Handbook of Mathematical Functions
with Formulas, Graphs, and Mathematical Tables. Dover Publications,
1972.
[17] A. Alvarado, F. Br¨ nnstr¨ m, and E. Agrell, “High SNR bounds for
a
o
the BICM capacity,” in Proc. Inform. Theory Workshop (ITW), Paraty,
Brazil, Oct. 16–20, 2011.

max{0, log ξ}, ξ > 0. By upper-bounding

≤

2

S

e−λ|y| dy + 1 −

2
tan−1
π
σ

(50)

(where tan−1 (·) denotes the arctangent function), and by using
(35) together with the fact that ξ → −ξ log ξ is monotonically
increasing for ξ ≤ e−1 and that −ξ log ξ ≤ 1/e for 0 < ξ < 1,
we obtain, upon minimizing over > 0,
CS (P, σ)
≤ inf −h(W ) + log
>0

2

+ log
S

1
+ λ P + σ2
σ2

e−λ|y| dy + 1 −

2
tan−1
π
σ

+ log+ π 2 σ 2 Pr σ|W | >
P
1
+ Pr σ|W | > log 1 + 2
2
σ
P
+ Pr σ|W | > log 1 + 2 + Pr σ|W | >
σ
3
− Pr σ|W | > log Pr σ|W | >
2
× I Pr σ|W | > ≤ 1/e
+

3
I Pr σ|W | >
2e

> 1/e

. (51)

This together with (48) yields a lower bound on L(σ).
Figure 1 shows the lower bound on L(σ) for circularlysymmetric Gaussian noise and a square signal constellation
(3) with P = PU . It further shows the information-rate
losses of 2m -ary quadrature amplitude modulation (QAM) for
m = 10, 16, and 22, which were numerically obtained using

5

