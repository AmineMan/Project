Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 16:29:43 2012
ModDate:        Tue Jun 19 12:55:15 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      583645 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566619

On Linear Transforms in Zero-delay Gaussian
Source Channel Coding
Emrah Akyol and Kenneth Rose
{eakyol, rose}@ece.ucsb.edu
University of California, Santa Barbara, CA-93106

A motivating factor for considering the low SNR regime in
communications, is that the absolute value of the slope of
the capacity-cost function is large, which indicates that one
achieves the most channel capacity per unit cost at low SNR,
as was shown by Verdu [8]. In [9], it was shown that at low
SNR, the optimal estimator converges to linear, for a Gaussian
channel, regardless of the density of the source. Recently,
Ostergaard and Zamir combined the results in [9], and the
incremental channel method introduced in [10], to analyze the
additive rate distortion function [11] at the low SNR regime.
Our main result here establishes the ﬁrst order optimality
of linear coding at low SNR. We show that asymptotically,
at low channel SNR, the power-distortion function of the
zero delay linear coding approaches the unbeatable powerdistortion bound. In other words, zero-delay linear coding
is asymptotically (at low SNR) optimal. To the best of our
knowledge, this perhaps surprising fact has not been previously
known.
Next, we study zero-delay linear source-channel coding
within the decoder side information setting. We show that the
optimal encoding transform is the product of the “conditional”
KLT of the source, a diagonal power allocation matrix and the
KLT of the channel. It is a “conditional” relative of the seminal
result in the point-to-point setting.
Finally, we analyze the optimal linear transforms for a structured nonlinear zero-delay mapping: a linear code followed
by nonlinear “scalar” mappings for each transform coefﬁcient
(channel). We show, using majorization tools, that the optimal
linear transform remains the same, and hence surprisingly does
not depend on the subsequent nonlinearities, as long as these
nonlinear mappings are scale invariant.
This paper is organized as follows: In Section II, we provide
a brief review of the known results in matrix analysis and
majorization theory. In Section III, we present an alternative
proof for the well known point-to-point result. In Section IV,
we study the optimality of the linear schemes. Section V
presents the results in the decoder side information setting.
Discussions are presented in Section VI.

Abstract—This paper is concerned with the optimal linear
transforms in zero-delay source channel coding for Gaussian
sources and channels. In preparation for the main results, we
ﬁrst consider the classical problem in the point-to-point setting,
which had already been solved by Lee and Petersen, where we
provide an alternative proof using majorization. We then analyze
the performance of linear source-channel coding in the low signalto-noise ratio (SNR) regime. We show that at asymptotically low
SNR, or equivalently as the power tends to zero, the powerdistortion function of zero delay linear coding achieves optimum
theoretically achievable performance. Finally, we consider zerodelay source-channel coding with decoder side information. Here,
subject to structural constraints on the encoder, we ﬁnd the
optimal encoder-decoder pair in closed form. We analyze two
structures: i) the encoder is constrained to be linear where we
show that the optimal transform is product of matrices including
as factor the conditional Karhunen Lo´ ve transform (KLT) of the
e
source given the side information. ii) the encoder consists of linear
transformation followed by individual optimal nonlinear mapping
of each transform coefﬁcient. Using majorization principles,
we show that the optimal transform does not depend on the
nonlinearities introduced, as long as they are scale invariant,
hence the optimal transform is also a product involving the
conditional KLT of the source.
Index Terms—Linear transforms, zero delay source-channel
coding

I. I NTRODUCTION
Zero delay source channel coding has attracted much recent
interest [1]–[6] due to its low complexity and zero delay
advantages, and also due to the fact that it provides graceful
degradation with channel signal to noise ratio (SNR) mismatch. Most prior efforts were focused on mapping optimization at low dimensions, eg., mapping two source samples to
one channel symbol, due to the complexity of such optimizations. In this work, we consider tractable generalization to
multi-dimensional settings via structured mappings.
The problem of zero delay linear source-channel codes was
previously studied by Lee and Petersen [7], where the optimal
encoding transform was found to be the product of three
matrices, the Karhunen Lo´ ve transform (KLT) of the source,
e
a diagonal scaling matrix, and the KLT of the channel. We ﬁrst
revisit this problem from a different perspective and provide an
alternative proof, based on the majorization theory, that gives
new insights on this problem and its extensions.
One of our main results pertains to the asymptotic optimality of zero-delay linear coding at the low SNR regime.

II. P RELIMINARIES
A. Notation
In general, lowercase letters (e.g., c) denote scalars, boldface
lowercase (e.g., x) vectors, uppercase (e.g., C, X) matrices
and random variables. I denotes the identity matrix. E[·], RX ,
and RXZ denote the expectation, auto-covariance of X and

This work is supported in part by the NSF under grants CCF-0728986,
CCF-1016861 and CCF-1118075.

1

N1
N1

.
.
.

ˆ
X1

+

X1

.
.
.

C

.
.
.

B

+

ˆ
Xm

(a) Point-to-point linear mapping

.
.
.

C

.
.
.

.
.
.

C

(b) Linear mapping with side information

RX = QX ΛX QT , and RN = QN ΛN QT
X
N

.
.
.

.
.
.

+

gk

hk

ˆ
Xm

(c) Structured nonlinear mapping with side information

Majorization: y ∈ Rm majorizes x if and only if
k

k

x(i), 1 ≤ k ≤ m

i=1
m

(5)

x(i),

y(i) ≥

(6)

i=1
m

y(i) =
i=1

i=1

and denoted as y x.
Schur concave functions: A real valued function f is said
to be Schur-concave if and only if

(1)

x

y ⇒ f (x) ≤ f (y)

(7)

We reproduce the following well known result, without
proof, see eg. [13].
Lemma 1. Let be A a Hermitian matrix with ordered diagonal
elements denoted by the vector a and ordered eigenvalues
denoted by the vector λ. Then λ a.

(2)

C. A Matrix Analysis Result
In this section, we introduce a matrix analysis result.

Since RX|Z is a covariance matrix, it allows the eigendecomposition:
RX|Z = QX|Z ΛX|Z QT
(3)
X|Z

Lemma 2. The solution to the minimization problem:
K ∗ = min T r ΛX − ΛX K T KK T + I

where
=
= I and ΛX|Z is a diagonal
matrix with ordered eigenvalues as entries. QT
X|Z is also
referred as conditional KLT of X ∈ Rm with respect to
Z ∈ Rk [12].
We repeatedly use the simple trace identity

K

QT QX|Z
X|Z

T r(ABC) = T r(CAB) = T r(BCA)

B

Problem settings

where QX QT = QN QT = I and ΛX and ΛN are
X
N
diagonal matrices, having ordered eigenvalues as entries,
i.e., ΛX = diag{λX (1), λX (2), ..., λX (m)} and ΛN =
diag{λN (1), λN (2), ..., λN (k)} where λX (1) ≥ λX (2) ≥
, ..., ≥ λX (n) and λN (1) ≥ λN (2) ≥, ..., ≥ λN (k). QT
X
and QT are the KLT matrices of the source and the channel,
N
respectively.
Similarly, let us deﬁne the covariance of the prediction error
of the optimal linear predictor of X given observation Z
−1 T
RX − RXZ RZ RXZ

ˆ
X1

h1

Z

Z

cross covariance of X and Z respectively. AT denotes the
transpose of matrix A. (x)+ denotes the function max(0, x).
Throughout this paper, we assume that the source is an mdimensional Gaussian with zero mean and covariance RX .
The channel noise is additive k-dimensional Gaussian, of zero
mean and covariance RN . Covariance matrices RX and RN
allow the diagonalization

QX|Z QT
X|Z

Xm

+

g1

Nk

+

Fig. 1.

RX|Z

.
.
.

ˆ
Xm

B

X1

Nk
Xm

Nk
Xm

.
.
.

N1

ˆ
X1

+

X1

−1

K

(8)

where K is the space of k × m matrices K that satisfy the
constraint
T r K T ΛN KΦ ≤ P
(9)
where Φ is a symmetric, positive deﬁnite m × m matrix, is a
diagonal k × m matrix.

(4)

Proof: Since variants of this result appeared before [14],
we only provide a sketch of the proof for brevity. We ﬁrst
note, using the matrix inversion lemma [15], that

where A, B, C are matrices of appropriately matching dimensions.
B. Majorization Theory

I − K T KK T + I

This paper builds on the results from majorization theory.
Let us introduce the basic notion of majorization and state
some important auxiliary results that will be of use later (see
[13] for complete reference on majorization). Roughly speaking majorization measures how spread out the components of a
vector is. Let x ∈ Rm be such that x(1) ≥ x(2) ≥ ... ≥ x(m).

−1

K = (I + K T K)−1

(10)

Given the fact that T r ΛX (I + K T K)−1 is a Schurconcave function of the vector consisting the diagonal elements of K and that the constraint is convex, it follows that
the objective is minimized when K is diagonal, due to Lemma
1 [14].

2

Using Lemma 2, we obtain that K is diagonal, hence C =
QN ΣQT where Σ is diagonal. The entries of Σ and the disX
tortion, (12), can be obtained applying Kuhn-Tucker necessary
conditions for each scalar channel.

III. P OINT TO P OINT S ETTING
The problem setting, depicted in Figure 1-a, can be stated
as the following optimization problem: ﬁnd the encoder and
decoder matrices, C and B that minimize distortion, D =
ˆ
E{||(X − X||2 }, subject to the power constraint E ||Y ||2 ≤
P where Y = CX. Recall that this is the classical problem
solved in the seminal paper of Lee and Petersen [7]. Our
approach gives an alternative proof as well as some new
results. Also note that a general framework for solving such
convex problems was presented in [14].

IV. O N F IRST O RDER O PTIMALITY OF L INEAR C ODING
In this section, we derive the conditions under which
zero delay linear coding achieves the asymptotic performance
bound which overall would require asymptotic delay.
A. Asymptotic Bounds
To obtain asymptotically (in delay) achievable bounds, we
have to consider the rate distortion function and channel
capacity. In this section, we assume RX = I for simplicity.
1) Channel capacity: The capacity of k-dimensional Gaussian channel with covariance matrix RN with total power P
is given parametrically as

Theorem 1. The encoding transform that minimizes the MSE
distortion subject to the power constraint P is
C = QN ΣQT
X

(11)

where Σ is diagonal power allocation matrix. Moreover the
total distortion is
w

( λX (i)λN (i)

µ
))+ , P =
C=
(log(
λN (i)
i=1

m

i=1

D=

k

2

P+

w

+
λN (i)

λX (i)

(12)

i=1

where w is the number of active channels determined by the
power P .
= 0 we

have
ˆ
D = E T r{(X − X)X T }

R=

(13)

−1 ˆ
ˆ
Noting that the optimal decoder is X = RX Y RY Y . Plugging
ˆ
ˆ
ˆ
Y = CX + N , we have

RX Y = RX C T and RY = CRX C T + RN
ˆ
ˆ
−1

(14)

(15)

and the constraint becomes
T r CRX C T

≤P

and, we have
−1

CRX

(17)

Plugging (1) in (17) we have

Let us deﬁne
−1/2

1/2

QT CQX ΛX
N

(19)

then, our objective becomes minimize
D = T r ΛX − ΛX K T KK T + I

−1

K

(20)

which must be minimized subject to
T r{K T ΛN K} ≤ P

(log(
i=1

λX (i) +
)) , D =
θ

m

min(θ, λX (i))

(23)

i=1

Proof: The key observation here is that as P → 0, the
active channels must have the same eigenvalue. Let us call this
eigenvalue λN . Also, the number of active channels will be
identical in asymptotic and zero-delay coding. Let w be the
number of active channels, then we have
P
)
(26)
C = w log(1 +
wλN
using (22). Plugging R = C in (24), we obtain

(18)

K = ΛN

m

for any m, k.

D =T r ΛX − ΛX QT C T (CQX ΛX QT C T + QN ΛN QT )−1
X
X
N
CQX ΛX }

1
2

Theorem 2. For a m-dimensional Gaussian source with
RX = I and k-dimensional Gaussian channel, as P → 0,
linear coding achieves the (asymptotic) OPTA in ﬁrst order,
i.e.,
dDlin
dDopta
=
(25)
dP P =0
dP P =0

(16)

D = T r RX − RX C T CRX C T + RN

(22)

i=1

for some θ ≥ 0. Plugging RX = I in the above expressions,
we get
−2R
}
(24)
D = m exp{
m
B. First Order Optimality
Here, we present our main result on ﬁrst order optimality of
zero-delay linear source-channel coding for Gaussian sources
and channels.

Then, the optimal decoder matrix B is
B = RX C T CRX C T + RN

(µ − λN (i))+

for some µ ≥ 0, where we use natural logarithm for convenience.
2) Rate-distortion: The rate distortion function for the
m-dimensional Gaussian source with covariance matrix RX
under mean square error distortion measure is given by the
inverse water-ﬁlling formula

w+1

ˆ ˆT
Proof: Using orthogonality, E (X − X)X

k

Dopta = m exp −

(21)

3

w
P
log 1 +
m
wλN

(27)

Plugging RX = I in (12), we have
Dlin = m − k +

Plugging (3) in (32) we have

w
P
1 + wλN

D = T r ΛX|Z − ΛX|Z QT C T
X|Z

(28)

(CQX|Z ΛX|Z QT C T +QN ΛN QT )−1 CQX|Z ΛX|Z
N
X|Z

Evaluating the slope at P = 0, we have
dDlin
dP

=
P =0

dDopta
dP

=
P =0

Deﬁne

−1
λN

−1/2

K = ΛN

(29)

1/2

QT CQX|Z ΛX|Z
N

(33)

then, our problem becomes: minimize
D = T r ΛX|Z − ΛX|Z K T KK T + I

V. S IDE I NFORMATION S ETTING

−1

K

(34)

subject to

In this section, we focus on the settings where side information is available to the decoder. The ﬁrst result derives the
optimal encoding and decoding matrices in the side information setting. Next, we allow scalar nonlinearities per transform
coefﬁcient, and obtain the optimal encoding and decoding
matrices.

T r K T ΛN KΦ ≤ P
−1/2

(35)

−1/2

where Φ = ΛX|Z QT RX QX|Z ΛX|Z is symmetric and
X|Z
positive deﬁnite. Applying Lemma 2, we obtain the result.
B. Nonlinear Mappings for Wyner-Ziv Setting

A. Linear Mapping

In this section, we allow scalar nonlinearities in the problem
deﬁnition. Unlike the point-to-point case, the optimal zerodelay scalar mappings for the point-to-point setting are highly
nonlinear in the side information setting. In [4], [5] an algorithm was proposed to ﬁnd such mappings for this setting.
Figure 2 presents examples of such encoding mappings. Interestingly, the analog mapping captures the central characteristic
observed in digital Wyner-Ziv mappings, in the sense of manyto-one mappings, or multiple source intervals are mapped to
the same channel interval, which will potentially be resolved
by the decoder given the side information. Within each bin,
there is a mapping function which is approximately linear in
this case (scalar Gaussian sources and channel).
Here we consider generalizing this highly non-linear scalar
mapping to vector spaces. This generalization, albeit being
algorithmically infeasible, can be done through a structured
combination of linear m to k mapping followed by scalar
nonlinear functions, as illustrated in Figure 1-c. The ith
transform coefﬁcient is nonlinearly transformed through the
mapping gi and it is estimated through another nonlinear
mapping hi . The problem of interest here is ﬁnding the optimal
linear transforms, which, in general, might depend on the
nonlinearities introduced. For simplicity, we assume RN = I.
The following theorem states that if the functions are scale
invariant, the optimal transform is identical to the case where
no nonlinearities were present, hence it does not depend on the
nonlinearities. Let us deﬁne scale invariance in our context.
Let {g1 , g2 , ...} : R → R be a set of functions. If for all
i, j, gi (x) = βgj (αx), for some α, β ∈ R then we call these
functions as scale invariant. Note that if we use these functions
as encoding mappings in conjunction with the associated
optimal decoding mappings, the distortion depend on the
source variance only through scaling, in other words in the
2
power distortion function can be expressed as D = σX f (P ),
2
where function f is independent of σX .
In passing, we note that Ziv showed the optimal encoding
mappings cannot be scale invariant for the point-to-point

Consider the setting shown in Figure 1-b. The following
theorem states that the optimal encoding transform C is the
product of the conditional KLT of the source given the side
information, a diagonal power allocation matrix and the KLT
of the noise.
Theorem 3. The encoding transform that minimizes the MSE
distortion, subject to a power constraint is
C = QN ΣQT
X|Z

(30)

where Σ is a diagonal power allocation matrix.
Proof: It follows from standard linear estimation principles [16] that
RX C T
ˆ
X=
RXZ

CRX C T +RN
T
RXZ C T

CRXZ
RZ

−1

CX +N
Z

and
D =T r RX −

RX C T
RXZ

CRX C T + RN
T
RXZ C T

CRXZ
RZ

−1

RX C T
RXZ

T

(31)

We make use of an auxiliary lemma from matrix analysis [15].
Lemma 3 (Schur’s complement). Let U, V, A, B, C be matrices of appropriate dimensions. Then, the following holds:
U
V

T

A
BT

B
C

U
V

= ΨT (A − BC −1 B T )−1 Ψ

where Ψ = U − BC −1 V .
Using Lemma 3 and (2) in (31),
T
D = T r RX|Z − RX|Z C T (CRX|Z C T + RN )−1 CRX|Z
(32)

4

Encoder mapping

Encoder mapping

6

setting, other than the trivial scalar Gaussian source-channel
case [17]. We conjecture, based on the observations from our
experimental results in [5], that for Gaussian source, channel
and side information, the optimal mappings are scale invariant.

6

0

0

Theorem 4. In the setting where gi ’s are scale invariant, the
encoding transform that minimizes MSE distortion is

−2

−2

−4

−4

C = ΣQT
X|Z

4

−1

0

1

2

3

−6
−3

−2

−1

0

1

2

3

(b) ρ = 0.95

Fig. 2. Example mappings for correlation coefﬁcient ρ = 0.9 and ρ = 0.9,
Gaussian scalar source, channel and side information [5].

showed that, similar to the point-to-point setting, the optimal
linear transform is conditional KLT of the source, followed
by a diagonal power allocation matrix and the channel noise
KLT. Finally, we considered a practical structured nonlinear
mapping, as the combination of a linear transform with scalar
nonlinearities. We showed that the optimal linear transform is
independent of the the scalar nonlinear mappings as long as
these mappings are scale invariant.

k

(37)

i=1
2
by manipulating σYi |Z through the choice of S. Consider
2
2
2
σ = [σY1 |Z , σY2 |Z , ..., σYk |Z ] = diag{SRX|Z S T }

−2

(a) ρ = 0.9

Proof: From matrix differentiation, it follows that the
optimal transform must be in the form C = ΣS where S is
a unitary matrix, and Σ is diagonal. Now, we will show that
S = QT . Let P = [P1 , P2 , .., Pk ] be the power allocation
X|Z
2
vector. It follows from scale invariance that Di = σYi |Z f (Pi )
for each Yi . Then, the problem is to minimize the function
2
σYi |Z f (Pi )

2

−6
−3

(36)

where Σ is a diagonal power allocation matrix .

D=

4

2

R EFERENCES

(38)

[1] X. Chen and E. Tuncel, “Zero-delay joint source-channel coding for
the Gaussian Wyner-Ziv problem,” in IEEE International Symposium
on Inf. Theory, 2011, pp. 2929–2933.
[2] F. Hekland, P.A. Floor, and T.A. Ramstad, “Shannon-Kotelnikov
mappings in joint source-channel coding,” IEEE Trans. on Comm., vol.
57, no. 1, pp. 94–105, 2009.
[3] M. Kleiner and B. Rimoldi, “A tight bound on the performance
of a minimal-delay joint source-channel coding scheme,” in IEEE
International Symp. on Inf. Theory, 2010, pp. 136–140.
[4] J. Karlsson and M. Skoglund, “Optimized low-delay source-channelrelay mappings,” IEEE Trans. on Communications,, vol. 58, no. 5, pp.
1397–1404, 2010.
[5] E. Akyol, K. Rose, and T. Ramstad, “Optimized analog mappings
for distributed source-channel coding,” in IEEE Data Compression
Conference, 2010, pp. 159–168.
[6] M.N. Khormuji and M. Skoglund, “On instantaneous relaying,” IEEE
Trans. on Inf. Theory,, vol. 56, no. 7, pp. 3378–3394, 2010.
[7] K.H. Lee and D. Petersen, “Optimal linear coding for vector channels,”
IEEE Trans. on Communications, vol. 24, no. 12, pp. 1283–1290, 1976.
[8] S. Verdu, “On channel capacity per unit cost,” IEEE Trans. on Inf.
Theory,, vol. 36, no. 5, pp. 1019–1030, 1990.
[9] E. Akyol, K. Viswanatha, and K. Rose, “On conditions for linearity of
optimal estimation,” in IEEE Inf. Theory Workshop, 2010, pp. 1–5.
[10] D. Guo, S. Shamai, and S. Verd´ , “Mutual information and minimum
u
mean-square error in Gaussian channels,” IEEE Trans. on Inf. Theory,,
vol. 51, no. 4, pp. 1261–1282, 2005.
[11] J. Ostergaard and R. Zamir, “Incremental reﬁnement using a Gaussian
test channel,” in IEEE International Symposium on Inf. Theory, 2011,
pp. 2233–2237.
[12] M. Gastpar, P.L. Dragotti, and M. Vetterli, “The distributed KarhunenLo` ve transform,” IEEE Trans. on Inf. theory, vol. 52, no. 12, pp.
e
5177–5196, 2006.
[13] AW Marshall and I. Olkin, Inequalities: Theory of Majorization and Its
Applications , Academic Press, New York, 1979.
[14] D.P. Palomar, J.M. Ciofﬁ, and M.A. Lagunas, “Joint TX-RX beamforming design for multicarrier MIMO channels: A uniﬁed framework for
convex optimization,” IEEE Trans. on Signal Processing, vol. 51, no.
9, pp. 2381–2401, 2003.
[15] R.A. Horn and C.R. Johnson, Matrix Analysis, Cambridge University
Press, 1985.
[16] T. Kailath, A.H. Sayed, and B. Hassibi, “Linear Estimation,” Upper
Saddle River, NJ, 2000.
[17] J. Ziv, “The behavior of analog communication systems,” IEEE Trans.
on Inf. Theory,, vol. 16, no. 5, pp. 587–594, 1970.

Note that SRX|Z S T has eigenvalues λX|Z . From Lemma 1,
we know that λX|Z majorizes σ. This implies that σ is in the
convex hull of k! permutations of λX|Z . Hence, our objective
is to minimize D over the convex polytope deﬁned by the
permutations of λX|Z . Note that D is linear in σi . It is well
known that the corner point minimizes a linear function over
a convex polytope, i.e., σ = λX|Z will minimize (37). This
establishes that the optimal transform is a conditional KLT,
i.e., S = QT .
X|Z
Remark 1. This results is perhaps surprising in that although the transform effects the particular nonlinear mapping
employed, via changing the correlation between Yi and Z,
optimal transform remainsS = QT as in Theorem 3.
X|Z
Remark 2. The linear coding scheme in the previous section is
a special case of this setting with the trivial identity mappings,
gi (x) = hi (x) = x, ∀i. Hence, the proof of Theorem 4 also
implies Theorem 3 when RN = I.
Remark 3. The nonlinear coding scheme can also be extended to the case with RN = I, with the help of a linear
transformation of the outputs of the scalar nonlinear functions.
Such optimal transform would be QN . Detailed extensions are
omitted due to space constraints.
VI. C ONCLUSION
In this paper, we presented several results regarding linear
zero-delay source-channel coding. First, using the majorization
tools, we presented an alternative solution to the classical
problem in the point-to-point setting. Next, we showed that
zero-delay linear source channel coding achieves OPTA in the
ﬁrst order at asymptotically low SNR. We then studied the
optimal linear transform for the side information setting and

5

