Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 21:52:23 2012
ModDate:        Tue Jun 19 12:56:24 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      431170 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565847

Optimal Linear Codes with a
Local-Error-Correction Property
N. Prakash, Govinda M. Kamath, V. Lalitha and P. Vijay Kumar
Dept. of ECE, Indian Institute of Science, Bangalore - 560012, India
email: {prakashn, govinda, lalitha, vijay}@ece.iisc.ernet.in.
Abstract—Motivated by applications to distributed storage,
Gopalan et al recently introduced the interesting notion of
information-symbol locality in a linear code. By this it is meant
that each message symbol appears in a parity-check equation
associated with small Hamming weight, thereby enabling recovery of the message symbol by examining a small number of
other code symbols. This notion is expanded to the case when all
code symbols, not just the message symbols, are covered by such
“local” parity. In this paper, we extend the results of Gopalan
et. al. so as to permit recovery of an erased code symbol even
in the presence of errors in local parity symbols. We present
tight bounds on the minimum distance of such codes and exhibit
codes that are optimal with respect to the local error-correction
property. As a corollary, we obtain an upper bound on the
minimum distance of a concatenated code.

such multiple node failures exist in practice [2][3][4]. This
motivates the deﬁnition of the class of (r, d, δ) local-errorcorrection codes given below.

I. I NTRODUCTION

Since the dual of a punctured code is a shortened code,
this also implies that we may regard the parity-check matrix
H of the code as containing for some νi , 1 ≤ νi ≤ n − k,
a (νi × n) submatrix Hi having rank νi , support Si , and the
property that any δ − 1 columns of Hi with indices drawn
from Si , are linearly independent.
A systematic [n, k, d] linear code C will be said to be an
(r, δ)i code, if all k message (or information) symbols have
locality (r, δ). We will also refer to such a code as having
information locality (r, δ). It is clear that if we employ an
(r, δ)i code for the distributed storage of data, a systematic
node can be locally repaired by connecting to r other nodes,
even if δ − 2 other nodes fail. An additional advantage of
an (r, δ)i code is that even when the other nodes are intact,
the code provides multiple options for locally repairing a
failed systematic node, which in a network setting, can be
used to balance trafﬁc across the network 1 . The (r, d) codes
introduced by Gopalan et al correspond to (r, 2)i codes in
the present notation.
By using properties of the generalized Hamming
weights [5] of a code (also known as minimum support
weights [6]), we will show that the minimum distance of
an (r, δ)i code is upper bounded (Theorem 2) by

Deﬁnition 1: The ith code symbol ci , 1 ≤ i ≤ n, in an
[n, k, d] linear code C, will be said to have locality (r, δ) if
there exists a punctured subcode of C with support containing
i, whose length is at most r + δ − 1, and whose minimum
distance is at least δ. Equivalently, there exists a subset Si ⊆
[n] = {1, . . . , n} such that
• i ∈ Si and |Si | ≤ r + δ − 1,
• the minimum distance of the code C|Si obtained by
deleting code symbols ci , i ∈ [n]\Si , is least δ.

In [1], Gopalan et al introduced the interesting and practically relevant notion of locality of information. The ith codesymbol ci , 1 ≤ i ≤ n, of an [n, k, d] linear code C over
the ﬁeld Fq is said to have locality r if this symbol can be
recovered by accessing at most r other code symbols of code
C. Equivalently, for any coordinate i, there exists a row in the
parity-check matrix of the code of Hamming weight at most
r + 1, whose support includes i. An (r, d) code was deﬁned
as a systematic linear code C having minimum distance d,
where all k message symbols have locality r. It was shown
that the minimum distance of an (r, d) code is upper bounded
by
k
d ≤ n−k−
+ 2.
(1)
r
A class of codes constructed earlier and known as pyramid
codes [2] are shown to be (r, d) codes that are optimal with
respect to this bound.
The concept of an (r, d) code was motivated by the problem of designing efﬁcient codes for the distributed storage
of data across nodes in a network. Since nodes are prone
to failure, there is need to protect the data using an errorcorrecting code. A second important requirement in this
setting, is the ability to efﬁciently bring up a failed node.
Here, (r, d) codes offer the advantage that in the event of
a single node failure, the node can be locally recovered by
connecting to at most r other nodes.
A natural extension to the concept of an (r, d) code, is a
code that would allow local recovery of a failed node, even
in the presence of failures in other nodes of the network.
Multiple node failures are not uncommon in distributed data
storage, and a number of coding schemes for tolerating

d ≤ n−k+1−

k
− 1 (δ − 1).
r

(2)

As was the case with the (r, d) codes introduced in [1], a class
of pyramid codes turns out to provide examples of optimal
(r, δ)i codes, i.e., (r, δ)i codes in which the bound in (2) is
achieved with equality. For the special case when r|k, we
1 By connecting to any r out of the r + δ − 2 nodes which locally protect
the failed node, one can recover the failed node.

1

The following lemma [5] relates the GHWs of C to those of
C⊥.

will identify conditions that the parity check matrix of an
optimal (r, δ)i code must necessarily satisfy.
We will term a code in which all the n symbols of an
[n, k, d] code have locality (r, δ) as codes as having allsymbol locality (r, δ) and denote such codes as (r, δ)a codes.
Thus, whenever we speak of either an (r, δ)i or else an
(r, δ)a code, it will be assumed that the length, dimension
and minimum distance of the linear code are understood from
the context and are typically denoted by n, k, d respectively.
Clearly, codes with all-symbol locality are a subset of the
set of codes with just information locality. Nevertheless,
it turns out that when (r + δ − 1)|n, one can show the
existence of codes with all-symbol locality (r, δ), which
satisfy the upper bound on minimum distance given in (2).
We will also present an explicit code having all-symbol
locality, for the case when the code length n is of the form
n = k (r + δ − 1).
r
Through out this write up, we will assume without loss of
generality, that the [n, k, d] code C under study, is systematic,
with information symbols present in the ﬁrst k coordinates.
For a codeword c ∈ C, we will use Supp(c) to denote the
support {i ∈ [n] | ci = 0 } of the codeword. The support of a
subcode D of C, is deﬁned by Supp(D) ∪c∈D Supp(c). For
a set S ⊂ [n], we will use C|S and C S to denote respectively,
the punctured and shortened codes of C associated with the
coordinate set S. By this we mean that under either the
puncturing or shortening operation, the coordinates of the
code lying in [n]\S are suppressed. Also, for any set S, the
cardinality of the set will be denoted by |S|.
Section II presents background on generalized Hamming
weights, while codes with information and all-symbol locality
are treated in Sections III and IV respectively. In the ﬁnal
section V, we present as a corollary, an upper bound on the
minimum distance of a concatenated code.

Lemma 1:
{di , 1 ≤ i ≤ k} = [n]\{n+1−d⊥ , 1 ≤ j ≤ n−k}. (5)
j
In terms of the gaps of the dual code C ⊥ , (5) can be
rewritten as
⊥
di = (n + 1) − gk−i+1 , 1 ≤ i ≤ k.

In particular, the minimum distance d of C and the largest
⊥
gap gk of C ⊥ are related by
⊥
d = d1 = (n + 1) − gk .

III. C ODES WITH INFORMATION LOCALITY
In this section, Theorem 2 will establish the upper bound
appearing in (2), on the minimum distance of (r, δ)i codes. It
will then be shown that pyramid codes, under an appropriate
choice of parameters, are optimal with respect to this bound.
Necessary conditions for optimality of an (r, δ)i code for the
case when r|k, are identiﬁed in Theorem 6.
Theorem 2: The minimum distance d of an (r, δ)i code C
is upper bounded by

min |Supp(D)| ,

(8)

Proof: From (7), the minimum distance of C, in terms
of the largest gap of C ⊥ is given by
⊥
d = (n + 1) − gk .

(9)

The desired upper bound on d will be obtained by showing
⊥
the corresponding lower bound on gk . This lower bound
⊥
on gk will in turn, be deduced from an appropriate upper
bound on the k − 1 (δ − 1)th GHW, d⊥ k −1 (δ−1) , of
r
)
( r
C ⊥ . It will be established in the next subsection, that under
the conditions of Theorem 2,
k
− 1 (δ − 1)
r
d⊥
(

(3)

k
r

−1)(δ−1)

<
≤

n − k,
k
− 1 (r + δ − 1).(10)
r

Let d⊥ k −1 (δ−1) = s. Then the number of gaps in the
( r
)
dual that do not exceed s is given by

where D < C, is used to denote a subcode D of C.
It is well known that
d = d1 < d2 < . . . < dk = n.

k
− 1 (δ − 1).
r

d ≤ n−k+1−

In this section, we review the deﬁnition of the generalized
Hamming weight (GHW) of a code [5], [7] and see how the
GHWs of a code are related to those of its dual. We introduce
the notion of a gap which will play an important role in our
subsequent proofs.
Deﬁnition 2: The ith , 1 ≤ i ≤ k, generalized Hamming
weight of a code C is deﬁned by
D<C
dim(D)=i

(7)

This relation will be used to derive an upper bound on the
minimum distance of (r, δ)i codes.

II. G ENERALIZED H AMMING WEIGHTS

di (C) = di =

(6)

k
− 1 (δ − 1)
r
k
k
− 1 (r + δ − 1) −
− 1 (δ − 1)
r
r
k
r
− r < k.
(11)
r

⊥
⊥
gj | gj ≤ s

(4)
≤

We will call the complement of the set {di , 1 ≤ i ≤ k}, in
[n], as the set of gap numbers (more simply, gaps) of the code
C and denote them by the set {gi , 1 ≤ i ≤ n − k}, where
{gi , 1 ≤ i ≤ n − k} = [n] \ {di , 1 ≤ i ≤ k}. Similarly,
⊥
let the sets {d⊥ , 1 ≤ j ≤ n − k} and {gi , 1 ≤ i ≤ k}
j
respectively denote the GHWs and gaps of the dual code C ⊥ .

=

=

s−

Since there are a total of k gaps in the dual code C ⊥ , there
must be at least an additional k − s − k − 1 (δ − 1)
r

2

⊥
gaps that exceed s and hence the last gap in the dual, gk ,
satisﬁes the lower bound:
⊥
gk

≥

k +

For the remaining subspaces Vi , i = a + 1, a + 2, · · · , k, we
once again deﬁne

k
− 1 (δ − 1)
r

s + k− s−

=

∆νi = dim(Wi ) − dim(Wi−1 ), ∆si =| Ψi \ Ψi−1 | .

k
− 1 (δ − 1).
r

Wi = Wi−1 + Vi , Ψi = Ψi−1 ∪ Si

(12)
and also set

Combining (12) and (9), we get (8).

∆νi = dim(Wi ) − dim(Wi−1 ), ∆si =| Ψi \ Ψi−1 | . (17)

A. Proof of (10)

For i in the range a+1 ≤ i ≤ k, we must have ∆νi ≤ (δ −2)
and also

We begin with a useful lemma.
Lemma 3: Let C be a systematic [n, k, d] linear code
whose ﬁrst k coordinates correspond to message symbols.
Let S be a subset of [n] of size s, such that [k] ⊆ S. Let P
denote a sub code, supported on S, of the dual code C ⊥ , i.e.,
every code symbol in every codeword in P is zero outside
of S. Also, let Q = [Am×k |Bm×(n−k) ] be a rank p, (m × n)
matrix whose row space is the code P. Then we must have
rank(B) = p and hence s − k ≥ p.
Proof: Suppose rank(B) < p. Then the row space
of Q would contain nonzero vectors which are supported
(i.e., nonzero in) only in the ﬁrst k message symbol coordinates. This is not possible as this would imply a relationship amongst the message symbols of the code C. Hence
rank(B) = p. We also know that the number of nonzero
columns in B is less than or equal to s − k. It follows that
s − k ≥ p.
We are now ready to prove that
and
d⊥
(

k
r

−1)(δ−1)

k
r

k

a

| Supp(H) |

≥

Vj

a(r + (δ − 1)) +

∆si .
i=a+1

However, from Lemma 3, it must be that
k

|Supp(H)| ≥ k + rank(H) = k +

∆νi .

(19)

i=1

From the two expressions above for the size of the support
of H, we obtain that
k

a(r + (δ − 1)) +
=⇒

(13)

a≥

k

∆si ≥ k +
i=a+1

∆νi
i=1

k
,
r

(20)

where (20) follows from (18) and the fact that for 1 ≤ i ≤ a,
∆νi ≥ (δ − 1).
It follows that the rank(H) ≥ a(δ − 1) > (δ − 1)( k − 1).
r
Also, since rank(H) ≤ (n − k), we get that (n − k) >
(δ − 1)( k − 1) and it is hence meaningful to speak of
r
d⊥ k −1 (δ−1) . Since the support of each submatrix Hi is
( r
)
≤ (r + δ − 1), we have that

δ − 1. (14)

d⊥ k
( r

i

Wi =

∆si
i=a+1
k

In other words, each subspace Vij contributes at least (δ − 1)
to the total dimension. Clearly, such an a exists, for a ≥ 1
is trivially true. Without loss of generality, we reorder the
indices so that Vij = Vj , 1 ≤ j ≤ a.
We next deﬁne W0 = {0}, Ψo = φ and for 1 ≤ i ≤ a,
Ψi =

∆si ,

k

∆si +
i=1

1≤j≤a,j=j0

∪i Sj ,
j=1

=

≤

Vij 

k
i=1

Then, rank(H) =
i=1 ∆νi and |Supp(H)| =
which can be upper bounded as

then for every j0 , 1 ≤ j0 ≤ a, we have


dim(Wa ) − dim 

(18)

Hk

− 1 (δ − 1) < n − k

= Vi1 + Vi2 + · · · Via ,

∆νi .

Equation (18) follows since any subset of (δ − 1) or less
columns of each matrix Hi , with indices drawn from Si ,
forms a linearly independent set.
Next, let


H1
 . 
H =  . .
.

For i ∈ [k], let the ith code (message) symbol be locally
protected by a code associated to the parity check matrix
Hi , whose support is Si of size |Si | = si ≤ r + δ − 1. Let
Vi denote the row space of Hi and let νi be its dimension.
Since the nullspace of Hi must deﬁne a code whose minimum
distance is ≥ δ, we must have that νi ≥ δ − 1, ∀i ∈ [k]. Let
us set Ψ = ∪k Si and s := |Ψ|.
i=1
Let a be the largest integer such that there exists a subset
{Vij }a with the property that if
j=1
Wa

≤

∆si

k
− 1 (r + δ − 1).
r

≤

(16)

−1)(δ−1)

≤

k
− 1 (r + δ − 1),
r

and with this, we have recovered the two inequalities appearing in (10).

(15)

j=1

3

substituting i = δ − 1, we get that d⊥(δ−1) = k + k (δ − 1).
k
r
r
Hence, it must be true that

Corollary 4: For an (r, δ)i code C that achieves the bound
in (8) with equality, we have
d⊥
(

k
r

−1)(δ−1)+i

=k+

for 1 ≤ i ≤ n − k − (δ − 1)

k
− 1 (δ − 1) + i,
r
k
r

−1

dim(W k )

(21)

r

|Ψ k | =

.

r

We will now show that for the case δ ≤ d, under a suitable
choice of parameters, Pyramid codes [2] achieve the bound
in Theorem 2 with equality.
Consider an [k + d − 1, k, d] systematic MDS code over
Fq having generator matrix of the form
Ik×k

Qk×(d−1)

.

(22)

Theorem 6: If an [n, k, d] linear code C having information locality (r, δ) achieves the bound in (8) with equality
and d < r + 2δ − 1, then δ ≤ d and up to a reordering of
columns, the parity check matrix, H of C can be assumed to
be of the form:


Q1
Iδ−1


..
..

.
.
0 


H=
,
Q( k )
Iδ−1


r


A

0

where A = A1 | A2 | . . . | A k
r

Qi
Ai

Iδ−1
0

and ∀i ∈
0
Id−δ

Id−δ
k
r

(29)
, the matrix
(30)

generates an [r + d − 1, d − 1, r + 1] MDS code. The matrices
Qi and Ai appearing above are of sizes (δ − 1) × r and
(d − δ) × r respectively.
Proof: Only a sketch of the proof will be provided, for
lack of space. The ﬁrst step is to show that a = k . Suppose
r
not, and if a ≥ k + 1, then Theorem 5 can be used to show
r
that n > k + d − δ + k (δ − 1). But this would contradict
r
the assumption that the code is optimal (see (8)) and hence
a = k . To show that δ ≤ d, once again note from Theorem
r
5 that the length of an optimal (r, δ)i code must be at least
k
r (r+δ−1). Now if one assumes δ > d, then from (8), we get
under the assumption of optimality that n < k (r + δ − 1),
r
which results in a contradiction. Hence we conclude that,
under optimality, δ ≤ d. Combining the facts a = k and
r
δ ≤ d, and also using Theorem 5, we get that the parity check
matrix, H, for the code C has the form (up to permutation of
columns) given in (29). For the rest of the proof, we consider
those shortened codes of C that are generated by [Ir | Qt |
i
At ], i = 1, . . . , k . Arguments based on minimum distance
i
r
reveal that these shortened codes are MDS and hence so are
their duals.

(25)

Clearly, by comparing the matrices G and G , it follows
that the code, C, generated by G has minimum distance no
smaller than d. Furthermore, C is an (r, δ)i code. Hence, it
follows from (25) that C is an optimal (r, δ)i code.
C. The structure of an optimal (r, δ)i code, when r|k
In this section, we will assume that r|k. We borrow
notation and intermediate steps used in the proof of Theorem
2.
Theorem 5: If an [n, k, d] linear code C having information locality (r, δ) achieves the bound in (8) with equality,
then |Si | = r + δ − 1, Si ∩ Sj = φ, 1 ≤ i < j ≤ a and
Si
C⊥
is MDS, 1 ≤ i ≤ a, where a is as deﬁned together
by (13) and (14).
Proof: Since, from (20), we have a ≥ k , we get that
r
dim(W k ) ≥ k (δ − 1) and |Ψ k | ≤ k (r + δ − 1), where
r
r
r
r
W k and Ψ k are as deﬁned in (15). But from Corollary 4,
r

(28)

Combining (26) and (28), we also get that dim((C ⊥ )Si ) =
δ − 1, ∀i ∈ [a]. This implies that the dual of (C ⊥ )Si , which
is the code, C |Si , has dimension |Si | − (δ − 1) = r. Now,
noting C |Si has parameters [r + δ − 1, r, δ], it follows that
C |Si and hence (C ⊥ )Si are MDS, ∀i ∈ [a].

where Qi , 1 ≤ i ≤ α are matrices of size r × (δ − 1), Qα+1
is of size β × (δ − 1) and Q is a k × (d − δ) matrix. Consider
a second generator matrix G obtained by splitting the ﬁrst
(δ − 1) columns of Q as shown below:


Q1
Ir


..
..

.
.
Q ,
G =



Ir
Qα
Iβ
Qα+1
(24)
Note that G is a k × n full rank matrix, where
k
− 1)(δ − 1).
r

(27)

Si ∩ Sj = φ, 1 ≤ i < j ≤ a.

We will now proceed to modify G to obtain the generator
matrix for an optimal(r, δ)i code. Let k = αr + β, 0 ≤ β ≤
(r − 1) and δ ≤ d. We now partition Q into submatrices as
shown below:


Q1


.
.

Q ,
.
(23)
Q=


 Qα
Qα+1

n=k+d−1+(

(26)

Now, since ∀i ∈ [a], |Si | ≤ r + δ − 1, from (27), it follows
that |Si | = r + δ − 1 and

B. Optimality of Pyramid Codes for Information Locality

G =

k
(δ − 1),
r
k
(r + δ − 1).
r

=

Corollary 7: If r|k, d < r+2δ−1 and equality is achieved
in (8), then d⊥ = r + i,
1 ≤ i ≤ δ − 1.
i

r

4

IV. C ODES WITH A LL -S YMBOL L OCALITY
In this section, we study (r, δ)a codes for the case when
(r + δ − 1)|n and δ ≤ d. Firstly, for the case when n =
k
r (r + δ − 1), we will give an explicit construction of a
code with all-symbol locality by splitting this time, rows of
the parity check matrix of an appropriate MDS code. We will
refer to this as the parity-splitting construction. The code so
obtained is optimal with respect to (8). We will also state the
existence of optimal codes with all-symbol locality without
the restriction n = k (r+δ −1). The proof of this is similar
r
to that of Theorem 17 in [1], and will be omitted for lack of
space.

V. A N UPPER BOUND ON THE MINIMUM DISTANCE OF
CONCATENATED CODES

Consider a (serially) concatenated code (see [8], [9])
having an [n1 , k1 , d1 ] code A as the inner code and an
[n2 , k2 , d2 ] code B as the outer code. Clearly, a concatenated
code falls into the category of an (r, δ)a code with δ = d1 ,
r = n1 − d1 + 1. Hence, the bound in (2) applies to
concatenated codes as well. Using the fact that a concatenated
code has length n = n1 n2 , dimension k = k1 k2 , we obtain
from the results of the present paper, the following upper
bound on minimum distance d:
k1 k2
− 1 (d1 − 1).
d ≤ n1 n2 − k1 k2 + 1 −
n1 − d1 + 1
(35)
Well known bounds on the minimum distance of a concatenated codes are

A. Explicit and Optimal (r, δ)a Codes via Parity-Splitting
Theorem 8: Let n = k (r + δ − 1) and δ ≤ d. Then, for
r
q > n, there exists an explicit and optimal (r, δ)a code over
Fq .
Proof: Let H be the parity check matrix of an [n, k , d]
Reed-Solomon code over Fq , where k = k+( k −1)(δ−1)
r
and d = n−k +1 = n−k +1−( k −1)(δ −1). Such codes
r
exist if q > n. We choose H(n−k )×n to be a Vandermonde
matrix. Let
Q(δ−1)×n
H =
.
(31)
A(n−k +1−δ)×n

d1 d2 ≤ d ≤ n1 d2 .

In practice, concatenated codes often employ an interleaver
between the inner and outer codes in order to increase the
minimum distance [10]. In this case, while the upper bound
in (36) no longer holds, the bound in (35) continues to hold.
VI. ACKNOWLEDGEMENTS

We partition the matrix Q in terms of submatrices as shown
below
Q = Q1 | Q2 | . . . | Q k ,
(32)

This research is supported in part by the DRDO-IISc
Program on Advanced Research in Mathematical Engineering
and in part by the NetApp Faculty Fellowship program. The
third author is supported by TCS Research Scholarship.

r

k
r

where Qi , 1 ≤ i ≤
are matrices of size δ−1×(r+δ−1).
Next consider the code C whose parity check matrix, H, is
obtained by splitting the ﬁrst δ − 1 rows of H as follows:


Q1


..


.
(33)
H=
.

Q k 

R EFERENCES
[1] P. Gopalan, C. Huang, H. Simitci, and S. Yekhanin, “On the locality
of codeword symbols,” Arxiv preprint arXiv:1106.3625, 2011.
[2] C. Huang, M. Chen, and J. Li, “Pyramid codes: Flexible schemes to
trade space for access efﬁciency in reliable data storage systems,” in
Network Computing and Applications, 2007. NCA 2007. Sixth IEEE
International Symposium on. IEEE, 2007, pp. 79–86.
[3] M. Blaum, J. Brady, J. Bruck, and J. Menon, “EVENODD: an efﬁcient
scheme for tolerating double disk failures in RAID architectures,”
Computers, IEEE Transactions on, vol. 44, no. 2, pp. 192–202, 1995.
[4] P. Corbett, B. English, A. Goel, T. Grcanac, S. Kleiman, J. Leong, and
S. Sankar, “Row-diagonal parity for double disk failure correction,”
in Proceedings of the 3rd USENIX Conference on File and Storage
Technologies, 2004, pp. 1–14.
[5] V.K. Wei, “Generalized hamming weights for linear codes,” Information Theory, IEEE Transactions on, vol. 37, no. 5, pp. 1412–1418,
1991.
[6] T. Helleseth, T. Klove, V.I. Levenshtein, and O. Ytrehus, “Bounds on
the minimum support weights,” Information Theory, IEEE Transactions on, vol. 41, no. 2, pp. 432–440, 1995.
[7] W.C. Huffman and V. Pless, Fundamentals of error-correcting codes.
Cambridge Univ Press, 2003.
[8] G.D. Forney Jr, “Concatenated codes,” 1966, MIT Press, Cambridge.
[9] I. Dumer, “Concatenated codes and their multilevel generalizations,”
Handbook of Coding Theory, vol. 2, pp. 1911–1988, 1998.
[10] S. Benedetto, D. Divsalar, G. Montorsi, and F. Pollara, “Serial concatenation of interleaved codes: Performance analysis, design, and iterative
decoding,” Information Theory, IEEE Transactions on, vol. 44, no. 3,
pp. 909–926, 1998.

r

A
Due to the Vandermonde structure of H , all rows of H are
linearly independent. Thus Rank(H) = n−k +( k −1)(δ−
r
1). Thus dim(C) = k − ( k − 1)(δ − 1) = k. It is also clear
r
from the construction that this code is an (r, δ)a code.
Let dmin be the minimum distance of C. Since any set of
columns2 of H which are linearly dependent are also linearly
dependent in H , dmin ≥ d = n − k + 1 − ( k − 1)(δ − 1).
r
But, by (8), we must have dmin ≤ d. Hence dmin = d.
Remark 1: In the above construction, let k = ar + b. Let
δk = r − b. Then, it is not hard to see that d = δ + δk . In
particular if r|k, δk = 0 and hence d = δ.
B. Existence of Optimal (r, δ) codes with All-Symbol Locality
Theorem 9: Let q > knk and (r + δ − 1)|n. Then there
exists an [n, k, d] linear code over Fq with (r, δ)a all-symbol
locality, where δ − 1 ≤ d, satisfying
d=n−k+1−
2 set

k
− 1 (δ − 1).
r

(36)

(34)

here indicates indices of the columns

5

