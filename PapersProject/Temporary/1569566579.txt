Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Tue May 15 15:04:11 2012
ModDate:        Tue Jun 19 12:54:43 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      444369 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566579

Piecewise Constant Prediction
Erik Ordentlich

Marcelo J. Weinberger

Yihong Wu∗

Information Theory Research
Hewlett-Packard Laboratories
Palo Alto, CA 94304
Email: erik.ordentlich@hp.com

Information Theory Research
Hewlett-Packard Laboratories
Palo Alto, CA 94304
Email: marcelo.weinberger@hp.com

Statistics Department, the Wharton School
University of Pennsylvania
Philadelphia, PA 19104, USA
Email: yihongwu@wharton.upenn.edu

of a small minimax regret are severely offset by the cost
of computing the minimax strategy for each round of the
game. For example, as argued in [1], in an energy-constrained
environment in which the role of data compression is to
save storage or transmission power, the assessment of the
beneﬁt of data compression should take into account the
implementation cost of the data compression algorithm. Savings in this cost obtained by “freezing” the adaptation of the
algorithm within a block may thus be beneﬁcial despite the
corresponding compression loss. Piecewise constant predictors
are also used in prediction games where the losses depend on
previous moves [3] and in limited-delay lossy universal data
compression [4]. In both cases, switching predictions entails
a cost, which is amortized by freezing the predictor over a
block. Piecewise constant prediction algorithms which are not
minimax are proposed in [5].
The binary prediction problem was ﬁrst studied in the
framework of the sequential decision problem [6]. The minimax strategy for Hamming loss was devised by Cover [7],
whereas for data compression it is given by the Normalized Maximum-Likelihood (NML) code, due to Shtarkov [8].
Cover’s minimax scheme yields the same regret over all
sequences, its leading asymptotic term being n/(2π). For
data compression, the leading asymptotic term of the redundancy of the NML code is (1/2) log n in the binary case. A
horizon-independent, simpler to implement approximation of
this minimax strategy, which achieves the leading asymptotic
term of the minimax redundancy, is given by the KrichevskiiTroﬁmov (KT) probability assignment [9].
A variant of this problem which, as we shall see, is quite
related to the piecewise constant setting, was proposed in [2].
In this variant, the predictor has access to a delayed version
of the sequence, or is forced to make inferences on the
observations a number of instants in advance. The delay d,
which is assumed known, affects the prediction strategy in that
pt+1 is now based on x1 x2 · · · xt−d only. It is shown in [2]
that, in the delayed prediction setting, the minimax strategy
consists of sub-sampling the data at a 1/(d + 1) rate, and
applying the (non-delayed) minimax strategy to each of the
d + 1 sub-sampled sub-sequences. This strategy results in a
minimax regret of d + 1 times the non-delayed minimax regret
for sequences of length n/(d + 1).
One can consider more general prediction games, in which
the sequence of observations belongs to some ﬁnite alphabet

Abstract—Minimax prediction of binary sequences is investigated for cases in which the predictor is forced to issue a
piecewise constant prediction. The minimax strategy is characterized for Hamming loss whereas, for logarithmic loss, an
asymptotically minimax strategy which achieves the leading term
of the asymptotic minimax redundancy, is proposed. The average
redundancy case is also analyzed for i.i.d. distributions. The
piecewise constant prediction paradigm may be of relevance to
resource constrained settings.

I. I NTRODUCTION
Consider a game in which, as a predictor observes a binary
sequence xn = x1 x2 · · · xn , it makes causal predictions on
each bit xt+1 , t = 0, 1, · · · , n−1, based on the observed preﬁx
xt . These predictions take the form of probability assignments
pt+1 (a|xt ), a ∈ {0, 1}. Once xt+1 is revealed, the predictor
incurs a loss, e.g., − log pt+1 (xt+1 |xt ) in the data compression
problem, or 1 − pt+1 (xt+1 |xt ) for (expected) Hamming loss,
which accumulates over time. The goal of the predictor is to
approach the cumulative loss of the best constant predictor
for xn (determined in hindsight, with full knowledge of xn ,
termed Bayes envelope), which is the empirical entropy of
xn in the data compression problem, or min(n0 (xn ), n1 (xn ))
for Hamming loss, where na (xn ) denotes the number of
occurrences of a ∈ {0, 1} in xn . In one version of the
game, the goodness of the predictor is assessed by its excess
loss over the Bayes envelope (termed regret, or redundancy
in the data compression case) for the worst case sequence
(maximum regret), the best strategy is thus termed minimax,
and the corresponding maximum regret is the minimax regret.
Notice that the minimax strategy (and the minimax regret) may
depend on the horizon n of the game.
Now, imagine a situation in which the predictor is forced
to “freeze” its prediction for a number of prediction rounds.
In the simplest such scenario, for a given block length T ,
the probability assignments piT +1 , piT +2 , · · · , p(i+1)T , i =
0, 1, · · · , m − 1, must all coincide, where we assume that
n = mT for some positive integer m. Thus, piT +j , j =
1, 2, · · · , T , can only depend on xiT and, in particular, must
be independent of xiT +1 , xiT +2 , · · · , xiT +T −1 . The question
arises: How badly can the minimax regret be affected by the
constraint of piecewise constant prediction? This question is
of practical importance in scenarios in which the beneﬁts
∗

The work of Yihong Wu was essentially done while visiting Hewlett-Packard
Laboratories, Palo Alto, CA.

1

A, and the player causally assigns probability distributions
to a corresponding sequence of actions b1 b2 · · · bn , taken
from an action space B. The observation and action incur an
expected loss. Notice that piecewise constant binary prediction
with block length T and Hamming loss can be cast as an
unconstrained game over a sequence of length n/T , in which
the observation alphabet is A = {0, 1}T , the action space is
B = {0T , 1T }, and the loss function is Hamming. Thus, a
general minimax solution for the unconstrained game would
solve our problem. Unfortunately, minimax strategies for the
general game cannot be characterized as easily as Cover’s
scheme for the binary case with Hamming loss [10].
In this paper, we start by characterizing, in Section II, the
piecewise constant minimax strategy for Hamming loss, under
the assumption that n = mT for a given block length T . The
corresponding regret turns out to be T times the minimax
regret for horizon m. The fact that this value is a lower bound
follows easily from considering piecewise constant sequences
xn . To show that it is an upper bound requires recursively
deriving the minimax strategy. This is in contrast to the
delayed prediction setting for which the optimal strategy is
easily analyzed while the converse proof is more involved.
Notice that, since the minimax regret grows as the square
root of the horizon, the asymptotic penalization due √ the
to
piecewise constant constraint is a multiplicative factor T .
In Section III we study the piecewise constant binary data
compression problem. While the argument leading to a lower
bound extends to this case, yielding a multiplicative factor T
as asymptotic penalization, we are unable to give a complete
characterization of the minimax strategy. However, we prove
that a simple variant of the KT probability assignment [9],
in which the estimate is obtained by adding T /2 (instead
of 1/2 as in the usual KT estimate) to the counts of 0’s
and 1’s, is asymptotically piecewise constant minimax. Thus,
the main asymptotic term of the piecewise constant minimax
redundancy takes the form (T /2) log n. Again, the analysis
of the upper bound is signiﬁcantly more involved than in the
delayed data compression setting.
In Section IV we study the piecewise constant data compression problem in a probabilistic setting, in which the
observations are assumed to be drawn from an (unknown)
i.i.d. distribution, where again, for simplicity, we assume
binary sequences. The goal here is to minimize the average
(rather than the maximum) redundancy, for which lower and
upper bounds with main term (1/2) log n are well known
for the unconstrained case (i.e., respectively, Rissanen’s lower
bound [11], which holds for all distributions except for a set
of measure zero, and the KT probability assignment, which is
asymptotically optimal for every distribution). The question
then is: Does the piecewise constant constraint affect the
achievable average redundancy? Notice that the answer to the
counterpart question in the delayed data compression setting is
straightforwardly negative. Indeed, for i.i.d. distributions, the
expected loss incurred at time t for a delayed strategy (with
delay d) is the same as the expected loss that the predictor
would incur, without delay, at time t − d. Therefore, ignoring

the delay and assigning at time t the same probability that
a non-delayed compression strategy would have assigned at
time t − d, we incur for the sequence xn the same loss as,
d+1
without delay, for the sequence xn−d .
The conclusion in the piecewise constant scenario turns
out to be similar, but the analysis is far less straightforward.
By a nonstandard application of Rissanen’s lower bound, we
show that applying any asymptotically optimal (unconstrained)
strategy at times (i−1)T +1 and “freezing” the assigned
probability for the entire i-th block, the average redundancy
achieves Rissanen’s lower bound for all distributions, except
possibly for a set of vanishing volume. In fact, for such a
general result, an exception set is unavoidable: indeed, we
construct an asymptotically optimal (unconstrained) strategy
whose piecewise constant version fails to yield optimal average
redundancy for some distributions. However, we further show
that if we specialize this approach to the (asymptotically
optimal) KT probability assignment, then the frozen scheme
is asymptotically optimal for all distributions, with no exceptions.
Throughout the sequel, for ﬁxed positive integers T (block
size) and m (number of blocks), we shall formally deﬁne a
piecewise constant probability assignment p with block size
ˆ
T and horizon n = mT as a probability distribution on
binary sequences xn whose conditional probabilities satisfy
pt+1 (·|xt ) = pT t/T +1 (·|xT t/T ) for all t. Thus, the conˆ
ˆ
ditional probabilities are constant over blocks of size T , and,
therefore, such an assignment is completely determined by the
conditional probabilities piT +1 (·|xiT ), for i = 0, . . . , m − 1.
ˆ
The set of all piecewise constant probabilities with block size
T and horizon n(= mT ) shall be denoted by PT,n .
II. P IECEWISE CONSTANT MINIMAX PREDICTION
In this section, we study piecewise constant minimax prediction of binary sequences under Hamming loss. The (expected)
Hamming loss of a piecewise constant predictor corresponding
to a p ∈ PT,m T on a sequence xm T ∈ {0, 1}m T is given
ˆ
mT
by L(ˆ, xm T ) = t=1 (1 − pt (xt |xt−1 )). Fixing throughout
p
ˆ
the horizon n = mT , let B( )
min( , n − ) denote
the Bayes envelope for a sequence of length n containing
ones. For a given sequence xkT , 0 ≤ k ≤ m and subset
Y ⊆ {0, 1}(m−k)T , deﬁne
Rk,T (xkT , Y) =

min

max L(ˆ, y) − B(n1 (xkT y))
p

p∈PT ,(m−k)T y∈Y
ˆ

where xkT y denotes the concatenation of the
sequences xkT and y. We are interested in the cases
Rk,T (xkT )
Rk,T (xkT , {0, 1}(m−k)T ) and Rk,T (xkT )
Rk,T (xkT , {0T , 1T }m−k ). Notice that the difference between
Rk,T and Rk,T is in the inner maximization: in the latter, y
is constrained to be piecewise constant, while in the former
it is unconstrained. The domains of Rk,T and Rk,T are
both unconstrained binary sequences of length kT . Notice
also that Rk,T and Rk,T depend on xkT only through its
appearance in the Bayes envelope of the concatenation of

2

xkT and y (thereby affecting the maximizing sequence y
and the minimizing strategy p). In particular, for k > 0, the
ˆ
cumulative loss and the Bayes envelope in the deﬁnition of
Rk,T correspond to sequences of different lengths.

min

= min

= min

max

p∈[0,1] z∈{0,1}T

max

p∈[0,1] z∈{0,1}T

p(T −n1 (z))+pn1 (z)

+ Rk,T (n1 (x(k−1)T )+n1 (z))

(2)

= min max pT +Rk,T (n1 (x(k−1)T )),
p∈[0,1]

pT +Rk,T (n1 (x(k−1)T )+T )

(3)

where (3) follows by the convexity (in n1 (z)) of the expression
in the maximization in (2) (implying that the maximum occurs
at one of the extremes), which, in turn, follows from the
auxiliary induction hypothesis that Rk,T (N ) is convex in
N . Now, starting from Rk−1,T (x(k−1)T ), the same chain of
equalities leading to (1) can be followed, with the maximization constrained to piecewise constant sequences. Clearly, the
result would be the expression in the right-hand side of (3).
Therefore, by (3), Rk−1,T (x(k−1)T )=Rk−1,T (x(k−1)T ), establishing the induction step for the main claim, as well as the
fact that Rk−1,T depends on x(k−1)T through n1 (x(k−1)T ).
To prove the remaining auxiliary induction steps, we note
that (3) can be solved (by equating the two terms in the
maximum) to yield that the minimizing p takes the form

max

p∗ =

1 Rk,T (n1 (x(k−1)T )+T ) − Rk,T (n1 (x(k−1)T ))
+
(4)
2
2T

which satisﬁes p∗ ∈[0, 1] by the Lipschitz condition induction
hypothesis that |Rk,T (N +1)−Rk,T (N )| ≤ 1. Substituting (4)
into (3) yields

zy)) .

Notice that a causal strategy p∈PT,(m−k+1)T can be viewed
ˆ
as a ﬁxed predictor p1 = p for the ﬁrst block z, which incurs
ˆ
a loss p(T −n1 (z))+pn1 (z) (where p=1−p), followed by a
causal strategy p∈PT,(m−k)T , the choice of which can depend
ˆ
on z, for the remaining blocks y, incurring a loss L(ˆ, y).
p
Clearly, this dependency can be expressed by switching the
maximum on z with the minimum on p∈PT,(m−k)T , yielding
ˆ
Rk−1,T (x(k−1)T ) = min

p(T −n1 (z))+pn1 (z)+Rk,T (x(k−1)T z)

Rk−1,T (x(k−1)T ) = min

z∈{0,1}T
y∈{0,1}(m−k)T

L(ˆ, zy) − B(n1 (x
p

p(T −n1 (z))+pn1 (z)+Rk,T (x(k−1)T z)

where the last equality follows by the induction hypothesis that
Rk,T = Rk,T . By the induction hypothesis that Rk,T depends
on x(k−1)T z through n1 (x(k−1)T )+n1 (z), we further have

The fact that RT (mT ) ≥ T R1 (m) is straightforward for
more general prediction settings, including data compression.
Indeed, the loss of a piecewise constant strategy over a
piecewise constant sequence equals T times the loss of a corresponding (unconstrained) strategy on sequences of length m,
obtained by sub-sampling the original sequences. By deﬁnition
of minimax regret, there exists a sequence xm for which the
latter loss is at least R1 (m) plus the Bayes envelope of xm .
The desired inequality then follows by noticing that a T -fold
replication of xm increases its Bayes envelope by a factor of
T . As claimed in Corollary 1, in the case of binary prediction
with Hamming loss, the above lower bound is actually tight.
Proof of Theorem 1: The proof will proceed by backward
induction, with base case k = m. Each induction step will also
include a proof of the following auxiliary claims: Rk,T (xkT )
depends on xkT only through N = n1 (xkT ), is a convex
function of (integer) N , and for all nonnegative integers N ,
with a slight abuse of notation, |Rk,T (N +1)−Rk,T (N )| ≤ 1.
For the base case, from the above deﬁnitions, it is immediate
that Rm,T (xmT ) = Rm,T (xmT ) = −B(n1 (xmT )), and from
the deﬁnition of B, it is immediate that the claimed (auxiliary)
properties of Rm,T hold. For the induction step, we have

(k−1)T

max

p∈[0,1] z∈{0,1}T

Corollary 1. The minimax regret RT (mT ) for the piecewise
constant prediction problem with Hamming loss, block length
T , and horizon mT , satisﬁes RT (mT ) = T R1 (m).

min

L(ˆ, y)−B(n1 (x(k−1)T zy))
p

(1)

By deﬁnition, R0,T is the minimax regret for piecewise
constant strategies (the object of the study in this section),
denoted RT (mT ), whereas R0,T is T times Cover’s minimax
regret R1 (m) for unconstrained strategies with horizon m. The
main result of this section, Corollary 1, follows.

p∈PT ,(m−k+1)T
ˆ

max

p∈[0,1] z∈{0,1}T

Theorem 1. For all k, 0 ≤ k ≤ m, and xkT ∈ {0, 1}kT ,
Rk,T (xkT ) = Rk,T (xkT ).

Rk−1,T (x(k−1)T ) =

max

p∈PT ,(m−k)T y∈{0,1}(m−k)T
ˆ

Rk−1,T (n1 (x(k−1)T )) =
Rk,T (n1 (x(k−1)T )+T ) + Rk,T (n1 (x(k−1)T ))
T
+
2
2

(5)

from which the convexity and Lipschitz condition induction
steps both readily follow from the respective induction hypotheses.
By deﬁnition, the minimax piecewise constant probability
assignment pkT +1 (·|xkT ) for Hamming loss is the ﬁrst move
ˆ
in the prediction strategy that achieves Rk,T (xkT ), given by

p(T −n1 (z))+pn1 (z)+

3

p∗ in (4). The recursion (5) can be explicitly solved and,
substituting into (4), we obtain
pkT +1 (1|xkT ) =
ˆ

1
+
2

y∈Y

assume that (11) holds for m. Let n1 (x(m+1)T ) = a + c and
n0 (x(m+1)T ) = b + d, with c + d = T, 0 ≤ c, d ≤ T . By the
probability assignment in (9),

B(xkT 0T y) − B(xkT 1T y)
2n−k T

(m+1)T

q (x
ˆ

where Y={0T , 1T }n−k−1 . For T = 1, this expression is the
probability that xkT y contains a majority of 1’s when y is
drawn uniformly from Y, with ties broken randomly (see [7]).
For general T , however, the same interpretation holds only
when the number of 1’s in xkT is divisible by T .

q (x(m+1)T ) ≥
ˆ

a+ T
2
(m+1)T

REDUNDANCY

p∈PT ,n
ˆ

max

log

1
ˆ
− nH(xn )
p(xn )
ˆ

=

×
=

a

b
mT

a+ T
2
a+c

c

a
a+c

b+d

b+d
(m+1)T
a

b+ T
2
b+d

d

b

b
b+d
(12)

T
1
((m+1)T )− 2
2
a+c
b+d
a+c
(m+1)T
(m+1)T

T
1
((m+1)T )− 2
2

a+c
(m+1)T

b+d

eT e−c e−d
a+c

b+d
(m+1)T

b+d

In this section, we consider the piecewise constant universal compression problem from an average redundancy point
of view. Recall that, given a compressor expressed as a
probability assignment p(xn ) on sequences of length n, the
ˆ
average case redundancy with respect to an i.i.d. source with
˜
marginal p is deﬁned as Ravg (ˆ, p) = D(p⊗n ||ˆ), where
p
p
p⊗n denotes the i.i.d. distribution on {0, 1}n with marginal
p. As discussed in Section I, it was observed in [2] that the
difference between the average code length of any compression scheme and its delayed counterpart is O(1). Thus, the
average redundancy of the delayed version of any compression
scheme with average redundancy meeting Rissanen’s lower
bound of (1/2) log n+o(log n) (for binary sources), also meets
Rissanen’s lower bound. Theorem 3 below is a counterpart of
this result for the piecewise constant setting.
˜
Given any probability assignment p with Ravg (ˆ, p) =
ˆ
p
(1/2) log n + o(log n) for all p, and block length T , let p(T )
ˆ

b

(11)

We proceed by induction on m, in the spirit of [12, Appendix
T
II]. For m = 1, q (xT ) = 2−T ≥ 1 T − 2 for all T ≥ 1. Next,
ˆ
2
1 We

a+c

a+c
(m+1)T

d

CASE REDUNDANCY

(9)

.

b+ T
2
(m+1)T

b

IV. P IECEWISE CONSTANT DATA COMPRESSION : AVERAGE

Proof: In view of the lower bound (7) and the deﬁnition
of minimax redundancy (6), it sufﬁces to show that, for every
sequence xmT , we have
T
1
ˆ mT
q (xmT ) ≥ (mT )− 2 2−mT H(x ) .
ˆ
(10)
2
To this end, we show that, for any xmT , with n1 (xmT ) = a
and n0 (xmT ) = b = mT − a,
a
mT

(m+ 1 )T
2

b
mT

which completes the induction step.

0 ≤ i < m (add- T estimator).
2

T
1
(mT )− 2
2

1
m

c

a

q (x(m+1)T ) ≥
ˆ

Theorem 2. The minimax piecewise constant redundancy
˜
satisﬁes RT (n) = T log n + O(1),1 the right-hand side
2
of which is attained by the piecewise constant probability
assignment speciﬁed by

q (xmT ) ≥
ˆ

.

where the equality follows by a simple reordering of the terms.
1
1 m+ 2
. It can be shown [12] that G
Now, let G(m)
1+ m
is strictly decreasing on (0, ∞), so that G(m) ≥ G(∞) = e.
c
a
a+ T
a
2
Also, let F (a, c)
, which can be shown to
a+c
a+c
be a decreasing function of a on (0, ∞) (we omit the proof
due to space limitations). Therefore, F (a, c) ≥ F (∞, c) = e−c
and, similarly, F (b, d) ≥ e−d . It then follows from (12) that

˜
As discussed in Section I, R1 (n), attained by the NML
code [8] for each n, satisﬁes the asymptotics
1
˜
R1 (n) = log n + O(1), n → ∞,
(8)
2
where the leading term in (8) is achieved by the KT probability
assignment [9]. The main result of this section is given by the
following theorem.

n1 (xiT )+T /2
qiT +1 (1|x ) =
ˆ
iT +T

d

b+ T
2
(m + 1)T

(6)

(7)

iT

T
1
((m+1)T )− 2
2

× 1+

ˆ
where H(xn ) denotes the (normalized) empirical entropy of
xn and all logarithms are in base 2, satisﬁes
˜
˜
RT (n) ≥ T R1 (m) .

a
mT

T
1
(mT )− 2
2

×

In this section, we consider the piecewise constant minimax
binary data compression problem. For simplicity, we maintain
the assumption that n=mT , although the results hold in more
generality. As observed in Section II, the minimax piecewise
constant redundancy, deﬁned as
xn ∈{0,1}n

)

c

a+ T
2
(m + 1)T

Thus, by the induction hypothesis,

III. P IECEWISE CONSTANT DATA COMPRESSION : M INIMAX

˜
RT (n) = min

) = q (x
ˆ

mT

suppress a possible dependence on T in the O(1) term.

4

denote the T -piecewise constant version of p, deﬁned via the
ˆ
conditional probability assignments
(T )

pt+1 (·|xt ) = pT
ˆ
ˆ

t/T +1 (·|x

T t/T

excess redundancy over the Laplace estimator is O(1). Since
the latter is well known to attain an average redundancy of
(1/2) log n+o(log n), it follows that p does so as well (for
ˆ
p = 0, 1). For p = 0, 1, on the other hand, a simple calculation shows that the redundancy arising from odd samples
is (1/2) log n+o(log n), while the redundancy arising from
even samples is o(log n). Thus, p meets the conditions of
ˆ
Theorem 3. As for the corresponding p(2) , it corresponds to
ˆ
using the Laplace estimator conditional probabilities (13) over
each block of size 2 and again a simple calculation shows that
the average redundancy is log n+o(log n), for p = 0, 1. This
fact is inherently due to the known analogous redundancy for
the underlying Laplace estimator for these deterministic cases.
Our ﬁnal result for piecewise universal compression under
average redundancy concerns the T -piecewise constant version
of the KT probability assignment, denoted by pKT .
ˆ

).

(T )

Thus, pt+1 (·|xt ) is ﬁxed throughout each block of indices
ˆ
iT, iT +1, . . . , iT +T −1, with i being a nonnegative integer,
to be the conditional probability that p would have induced at
ˆ
the start of the block.
Theorem 3. Given any probability assignment p with
ˆ
˜
maxp Ravg (ˆ, p) = (1/2) log n + o(log n), the T -piecewise
p
˜
constant version p(T ) satisﬁes Ravg (ˆ(T ) , p) = (1/2) log n+
ˆ
p
o(log n) for Lebesgue almost all p.
Proof: In analogy to p(T ) , for δ = 0, 1, . . . , T − 1, we can
ˆ
deﬁne the (T, δ)-piecewise constant versions of p as
ˆ
(T,δ)

pt+1 (·|xt ) = pT
ˆ
ˆ

(t−δ)/T +δ+1 (·|x

T (t−δ)/T +δ

)

(T )
˜
Proposition 1. For all p ∈ [0, 1] and all T , Ravg (ˆKT , p) =
p
1
2 log n + O(1).

(T,δ)

with pt (·|·) = 1/2 for t ≤ δ. Thus, pt+1 (·|xt ) is ﬁxed over
ˆ
ˆ
blocks as above, but the blocks are offset by δ with respect to
those deﬁning p(T ) . Note that p(T,0) coincides with p(T ) .
ˆ
ˆ
ˆ
The i.i.d. nature of the source and regrouping conditional
T −1 ˜
1
˜
p
p
probabilities yields T δ=0 Ravg (ˆ(T,δ) , p) = Ravg (ˆ, p) +
˜ avg (ˆ, p) =
O(1), which, in turn, implies that, if R
p
(1/2) log n+o(log n) and, for some >0 and inﬁnitely
˜
many values of n, Ravg (ˆ(T ) , p)>(1/2) log n+ log n, then
p
˜ avg (ˆ(T,δ) , p) < (1/2) log n − ( /(2T )) log n, for some δ
R
p
and inﬁnitely many values of n. However, by Rissanen’s lower
bound [11], this can only happen for p in a set of measure zero.
The theorem then follows since δ is ﬁnite-valued.
The following example (for T = 2) shows that the measure
zero exception set in Theorem 3 can arise, with the set being
{0, 1} in this case. Deﬁne p as follows. For odd t, let
ˆ
n1 (xt−1 )+1
t+1

R EFERENCES
[1] Y. Wu, E. Ordentlich, and M. J. Weinberger, “Energy-Optimized Lossless Compression: Rate-Variability Tradeoff,” Proceedings of the 2011
IEEE International Symposium on Information Theory (ISIT’11), St.
Petersburg, Russia, Aug. 2011.
[2] M. J. Weinberger and E. Ordentlich, “On Delayed Prediction of Individual Sequences,” IEEE Trans. Inform. Theory, Vol. IT-48, pp. 1959–1976,
July 2002.
[3] N. Merhav, E. Ordentlich, G. Seroussi, and M. J. Weinberger, “On
Sequential Strategies for Loss Functions with Memory,” IEEE Trans.
Inform. Theory, Vol. IT-48, pp. 1947–1958, July 2002.
[4] N. Merhav and T. Weissman, “On Limited-Delay Lossy Coding and
Filtering of Individual Sequences,” IEEE Trans. Inform. Theory, Vol.
IT-48, pp. 721–733, March 2002.
[5] S. Geulen, B. Voecking, and M. Winkler, “Regret Minimization for
Online Buffering Problems Using the Weighted Majority Algorithm,”
in Proc. 23rd Annual Conf. Learning Theory (COLT 2010), pp. 132–
143, 2010.
[6] J. F. Hannan, “Approximation to Bayes risk in repeated plays,” in
Contributions to the Theory of Games, Volume III, Ann. Math. Studies,
vol. 3, pp. 97–139, Princeton, NJ, 1957.
[7] T. M. Cover, “Behavior of sequential predictors of binary sequences,” in
Proc. 4th Prague Conf. Inform. Theory, Statistical Decision Functions,
Random Processes, (Prague), pp. 263–272, Publishing House of the
Czechoslovak Academy of Sciences, 1967.
[8] Y. M. Shtarkov, “Universal sequential coding of single messages,”
Problems of Inform. Trans., vol. 23, pp. 175–186, July 1987.
[9] R. E. Krichevskii and V. K. Troﬁmov, “The performance of universal
encoding,” IEEE Trans. Inform. Theory, vol. IT-27, pp. 199–207, March
1981.
[10] T. H. Chung, Minimax Learning in Iterated Games via Distributional
Majorization. PhD thesis, Department of Electrical Engineering, Stanford University, 1994.
[11] J. Rissanen, “Universal coding, information, prediction, and estimation,”
IEEE Trans. Inform. Theory, vol. IT-30, pp. 629–636, July 1984.
[12] F. M. J. Willems, Y. M. Shtarkov, and T. J. Tjalkens, “The contexttree weighting method: Basic properties,” IEEE Trans. Inform. Theory,
vol. IT-41, pp. 653–664, May 1995.

(13)

−1
t
+ 1(xt−1 =1)
n1 (xt−1 )+1
−1
+ 1(xt−1 = 1) + 1(xt−1 =0)

Thus, in the context of the previous theorem, the T piecewise constant version of the KT probability assignment
turns out to attain Rissanen’s lower bound for all p. Proposition 1 can be proved by bounding the difference between certain conditional expectations of the logarithms of the piecewise
constant and original KT probability assignments.

(14)

pt (1|xt−1 ) =
ˆ
whereas, for even t, let
pt (1|xt−1 ) =
ˆ

p(xt−1 1)
ˆ
p(xt−1 )
ˆ

(t+1)
=
t

t−1
n1 (xt−1 )

where 1(xt−1 = 1) (resp. 1(xt−1 = 0)) is 1 (resp. 0) if
xt−1 is all 1’s (resp. 0’s) and 0 otherwise. Notice that (13)
corresponds to Laplace’s estimator while (14) corresponds to
the conditional probability distribution induced by an equal
mixture of the Laplace probability assignment (corresponding
to the Dirichlet-1 mixture of product distributions) and the
respective deterministic distributions of all 1’s and all 0’s.
For p = 0, 1, the above p coincides with the Laplace
ˆ
estimator after the ﬁrst occurrence of a bit differing from x1 .
The probability of this event taking i symbols to occur falls
off exponentially in i, and the code length difference between
both estimators is O(log i) in the worst case. Therefore, the

5

