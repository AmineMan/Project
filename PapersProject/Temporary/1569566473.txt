Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Mon May 21 15:33:10 2012
ModDate:        Tue Jun 19 12:54:15 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      482835 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566473

Is Gaussian Noise the Worst-Case
Additive Noise in Wireless Networks?
Ilan Shomorony and A. Salman Avestimehr
Cornell University, Ithaca, NY
Once we go beyond point-to-point channels, Gaussian noise
is only known to be the worst-case additive noise in some
special wireless networks, such as the Multiple Access Channel, the Degraded Broadcast Channel and MIMO channels. In
all such cases the capacity has been fully characterized and
is known to be achievable with Gaussian inputs. Therefore,
similar arguments to the one above can be used to show
that, in these cases, Gaussian noise is indeed the worst-case
additive noise. However, for more general wireless networks
where the capacity is unknown, we lack the tools to make such
an assertion. The recent constant-gap capacity approximations
for the Interference Channel [3] and for single-source singledestination relay networks [4, 5] can only be used to state
that Gaussian noise is “approximately” the worst-case additive
noise in these cases. Nonetheless, in a leap of faith, most of
the research concerning such systems and many other wireless
networks views the AWGN channel model as the standard
wireless link model. As a result, it remains a fundamental open
question whether Gaussian noise is the worst-case additive
noise in general wireless networks.
In this work, we make signiﬁcant progress towards showing
that the Gaussian noise is in fact the worst-case noise for
arbitrary wireless networks with additive noises that are independent of the transmit signals. We consider wireless networks
with unrestricted topologies and fairly general trafﬁc demands.
We show that any coding scheme with ﬁnite reading precision
that achieves a given set of rates on a network with Gaussian
additive noises can be used to construct a coding scheme that
achieves the same set of rates on a network with same topology
and trafﬁc demands, but with non-Gaussian additive noises.
A coding scheme is said to have ﬁnite reading precision if
all its relaying and decoding functions only depend on the
received signals read up to a ﬁnite number of digits after
the decimal point. This precision can be chosen arbitrarily
large, as long as it is ﬁnite, and is allowed to tend to
inﬁnity along the sequence of coding schemes. Hence, this
is a very mild restriction, and, in practice, almost all coding
schemes satisfy it. Moreover, in [6], we relax this restriction
and consider instead a class of coding schemes with inﬁnite
reading precision called “truncatable” coding schemes.
We start our coding scheme construction by applying a
transformation at the transmit and received signals of all
nodes in the network to create an “approximately Gaussian”
effective network. The technique resembles OFDM in that it
uses the Discrete Fourier Transform to mix together multiple
uses of the same channel. This mixing causes the effective

Abstract—An important classical result in Information Theory
states that the Gaussian noise is the worst-case additive noise in
point-to-point channels. In this paper, we signiﬁcantly generalize
this result and show that, under very mild assumptions, Gaussian
noise is also the worst-case additive noise in general wireless
networks with additive noises that are independent from the
transmit signals. More speciﬁcally, we prove that, given a coding
scheme with ﬁnite reading precision for an AWGN network, one
can build a coding scheme that achieves the same rates on an
additive noise wireless network with the same topology, where
the noise terms may have any distribution with same mean and
variance as in the AWGN network.

I. I NTRODUCTION
The modeling of background noise in point-to-point wireless channels as an additive Gaussian noise is well supported
from both theoretical and practical viewpoints. In practice,
we have witnessed that current wireless systems designed
based on the assumption of additive Gaussian noise perform
quite well. This is intuitively explained by the fact that, from
the Central Limit Theorem, the composite effect of many
independent noise sources (e.g., thermal noise, shot noise, etc.)
should approach a Gaussian distribution. From a theoretical
point of view, Gaussian noise has been proven to be the worstcase noise for additive noise channels. This follows mainly
from the fact that the Gaussian distribution maximizes the
entropy subject to a variance constraint. More precisely, from
the Channel Coding Theorem [1], the capacity of a channel
f (y|x) is given by
C=

max

f (x):E[X 2 ]≤P

I(X; Y ).

Thus, if we choose X to be distributed as N (0, P ), we have
C ≥ h(X) − h(X|Y ) =

1
2

log (2πeP ) − h(X|Y ).

In the case of an additive noise (AN) channel Y = X + Z,
where E[Z] = 0 and E Z 2 = σ 2 , the fact that the Gaussian
distribution maximizes the entropy implies that h(X|Y ) ≤
P σ2
1
2 log 2πe P +σ 2 . We conclude that
CAN ≥

1
2

log 1 + P/σ 2 = CAWGN ,

where CAWGN is the capacity of the AWGN channel, which is
achieved by a Gaussian input distribution. Moreover, a more
operational justiﬁcation of the fact that Gaussian is the worstcase noise for additive noise channels was provided in [2],
where it was shown that random Gaussian codebooks and
nearest-neighbor decoding achieve the capacity of the corresponding AWGN channel on a non-Gaussian AN channel.

1

where the message transmitted by source si , Wi , is assumed
to be chosen uniformly at random from {1, ..., 2nRi }, for i =
1, ..., |L|.

additive noise terms to converge in distribution to Gaussian
additive noise terms, which are not i.i.d. over time. The
dependence between noise realizations is handled through the
combination of an interleaving technique and an outer code.
The interleaving operation creates multiple blocks of network
uses inside which the additive noises are i.i.d. and almost
Gaussian. Since our original coding scheme has ﬁnite reading
precision, its peformance on the approximately AWGN blocks
does not deviate much from its performance on a real AWGN
network, and a mutual-information argument can be used to
show that our outer codes achieve arbitrarily close to the set
of rates of our original coding scheme.

Deﬁnition 3. A coding scheme C = (n, R) has ﬁnite reading
precision ρ ∈ N if the relaying/decoding functions of a node
v in the network can only depend on
Yv [i]

u∈I(v)

Deﬁnition 4. Rate tuple R is achievable by coding schemes
with ﬁnite reading precision if we have a sequence of coding
schemes Cn = (n, R), where Cn has ﬁnite reading precision
ρn , which achieves rate tuple R.
Remark: It may be the case that ρn → ∞ as n → ∞.
The assumption of ﬁnite reading precision on a coding
scheme is certainly mild from a practical point of view.
Moreover, in [6], we show that coding schemes with ﬁnite
reading precision are in fact optimal for AWGN networks.
III. M AIN R ESULT
In this section, we state and prove our main result.
Theorem 1. Suppose a rate tuple R is achievable by coding
schemes with ﬁnite reading precision on an AWGN wireless
network (G, L). Then it is possible to build a sequence
of coding schemes that achieves arbitrarily close to R on
the same |L|-unicast additive noise wireless network (G, L)
where, for each v, the distribution of Nv is replaced with an
arbitrary absolutely continuous distribution with E[Nv ] = 0
2
2
and E Nv = σv .

hu,v Xu [t] + Nv [t],

where I(v) = {u ∈ V : (u, v) ∈ E}, and the additive noise
Nv is assumed to be i.i.d. over time and satisﬁes E[Nv ] = 0
2
2
and E Nv = σv < ∞. We also assume that the noise terms
are independent from all transmit signals and from the noise
terms at distinct nodes, and that each Nv has an absolutely
continuous distribution. If all the additive noises in the network
2
are N (0, σv ), we say the network is an AWGN network.

To prove Theorem 1, we start by describing an OFDM-like
scheme that is applied to all nodes in the network in section
III-A. By applying an Inverse Discrete Fourier Transform
(IDFT) to the block of transmit signals of each node, and
a Discrete Fourier Transform (DFT) to the block of received
signals of each node, we create effective additive noise terms
that are weighted averages of the additive noise realizations
during that block. This mixture of noises can be shown to
converge in distribution to a Gaussian additive noise term.
To overcome the fact that the resulting noise terms are not
i.i.d. over time, in section III-B, we apply the OFDM-like
scheme over multiple blocks, and then we interleave the
effective network uses from distinct blocks. This effectively
creates several blocks in which the network behaves as an
approximately AWGN network (with i.i.d. noises). Then our
original code for the AWGN network can be applied to each
approximately AWGN block. The fact that this code has
ﬁnite reading precision guarantees that, when applied to the
approximately AWGN block, its error probability is close to
its error probability on a true AWGN network. Finally, we take
care of the dependence between the noises of different blocks
created in the interleaving operation by using a random outer
code for each source-destination pair, and we can show via a
mutual-information argument that we can achieve a rate tuple

Deﬁnition 1. A coding scheme C with block length n ∈ N
and rate tuple R = (R1 , ..., R|L| ) ∈ R|L| for an |L|-unicast
additive noise wireless network consists of:
1. An encoding function fi : {1, ..., 2nRi } → Rn for each
source si , i = 1, ..., |L|, where each codeword fi (w), w ∈
{1, ..., 2nRi }, satisﬁes an average power constraint of P .
(t)
2. Relaying functions rv : Rt−1 → R, for t = 1, ..., n,
for each relay v ∈ V that is not a source, satisfying the
average power constraint
1
n

n
(t)
rv (y1 , ..., yt−1 )

2

≤ P,

t=1

for all (y1 , ...yn ) ∈ Rn .
3. A decoding function gi : Rn → {1, ..., 2nRi } for each
destination di , i = 1, ..., |L|.
Deﬁnition 2. A rate tuple R is said to be achievable for an
|L|-unicast wireless network (G, L) if there exists a sequence
of coding schemes Cn = (n, R) for which the probability that
at least one decoder makes an error tends to zero, i.e.,
Perror (Cn )

Pr

|L|
i=1 {Wi

2−ρ 2ρ Yv [i] , for i = 1, ..., n,

as opposed to the complete binary expansions of the Yv [i]’s.

II. P ROBLEM S ETUP AND D EFINITIONS
An |L|-unicast additive noise wireless network (G, L) consists of a directed graph G = (V, E), and a set L ⊂ V × V
of source-destination pairs. We assume that all sources and
destinations are distinct nodes, but more general settings are
considered in [6]. All nodes in V which are not a source will
function as relays. We associate a real-valued channel gain
hu,v with each edge (u, v) ∈ E.
Communication in (G, L) is performed over n discrete time
steps. At time t = 1, ..., n, each u ∈ V transmits a real-valued
n
2
signal Xu [t], satisfying t=1 Xu [t] ≤ nP , ∀ u ∈ V , for some
P ≥ 0. The signal received by node v at time t is
Yv [t] =

ρ

= gi (Ydn )} → 0, as n → ∞,
i

2

arbitrarily close to R on the non-Gaussian wireless network.
A diagram illustrating the proof steps is shown in Figure 1.
Non-Gaussian OFDM-like
scheme
i.i.d. Noise
Network

Approximately
Interleaving
Gaussian dependent
Noise Network

By considering the real and imaginary parts of each component
˜
˜
Yv,i of Yv , for i = 0, ..., b − 1, separately, we obtain the
following 2b − 2 effective received signals:

Approximately AWGN per
block (dependent noise
across blocks) Network

˜
Yv,0 =

˜
Yv,i =

Outer code
achieves
same rate as

From Lemma 2, codes with finite reading
precision perform similarly

AWGN
Network

˜
Yv,b/2 =

˜
Yv,i = −

E
E

(1)

≤ P/2, for i = 1, ..., b − 1,

=

Xu
1
b

2

=

1
b

≤ P,

(3)

b−1
i=0

E

˜
di

E d2 + E d2
0
b−1 + 2

2
E d2
2i−1 + d2i

u∈I(v)

hu,v Xu + Nv .

u∈I(v)

hu,v du,2(b−i)−1 +

[DFT(Nv )i ]

+ 1, ..., b − 1,

u∈I(v)
b
2

(8)

(9)

hu,v du,2(b−i) +

[DFT(Nv )i ]

+ 1, ..., b − 1,

(10)

b−1

N [i] cos
i=0

2πi
b

b

,

(11)

Now consider the additive noise term in (6). It is the real
part of (4), which, by Lemma 1, converges in distribution to
2
N (0, σv /2), as b → ∞. Moreover, Lemma 1 can be restated
with sines replacing the cosines, and the same result will hold.
Thus, the additive noise in (7) also converges in distribution
2
to N (0, σv /2). Finally, for the received signals in (5) and
(8), it is easy to see that the additive noise in (4) only has
a real component, and by the usual Central Limit Theorem, it
2
converges in distribution to N (0, σv ).

By applying a DFT to its block of b received signals, node v
will obtain
˜
Yv = DFT(Yv ) =

(7)

for some b ∈ {1, ..., b − 1} \ {b/2}. Then, Zb converges in
distribution to N (0, σ 2 /2) as b → ∞.

≤ P.

Therefore, u may transmit Xu over b time-slots, and a receiver
node v (i.e., either a destination node or a relay node) will
receive, over the same b time-slots,
Yv =

b
2

1
Zb = √
b

2
b/2−1
i=1

[DFT(Nv )i ]

Lemma 1. Let N [0], N [1], N [2], ... be i.i.d. random variables
that are zero-mean, have variance σ 2 and have an absolutely
continuous distribution, and let

(2)

and we must have, by Parseval’s relationship,
1
bE

hu,v du,2i +

(6)

However, from the conjugate symmetry of DFT(Nv ) (since
Nv is a real-valued vector), we have that all the received
signals from (9) and (10) are repetitions (up to a change
of sign) of the received signals in (6) and (7). Therefore,
we conclude that we have effectively b distinct real-valued
received signals with additive noise. It is important to notice,
though, that the additive noise terms are dependent across
these b received signals.
Through an application of Lindeberg’s Central Limit Theorem [7], the following lemma is proved in [6], showing that
the effective additive noises in the received signals (6) and (7)
converge in distribution to Gaussian noise terms.

˜
˜
˜
Next, node u takes the IDFT of du = (d0 , ..., db−1 ) to obtain
˜ u ). We assume that DFT and IDFT refer to the
Xu = IDFT(d
˜
unitary version of the DFT and IDFT. Since du is conjugate
b
symmetric, Xu is a real vector (in R ). We will require the
original real-valued signals to satisfy
d2
i
d2
b

[DFT(Nv )i ]

hu,v du,b−1 + DFT(Nv )b/2 ,

u∈I(v)

for i =

+ 1, ..., b − 1

E d2 ≤ P,
0

u∈I(v)

for i =

˜
db/2 = db−1
b
2

u∈I(v)

˜
Yv,i =

b
for i = 1, ..., 2 − 1

for i =

hu,v du,2i−1 +

(5)

b
for i = 1, ..., 2 − 1,

A. An OFDM-like scheme to mix the noises over time
Assume that a node u ∈ V has b real-valued signals
d0 , d1 , ..., db−1 which are the inputs to the effective channels
we intend to create. We assume that b is even to simplify the
expressions. Node u will “pack” these signals into b complex
˜
˜
numbers d0 , ..., db−1 as follows.
˜
d0 = d0

˜
˜
di = d∗
b−i

u∈I(v)

˜
Yv,i =

Diagram of proof steps.

˜
di = d2i−1 + jd2i

hu,v du,0 + DFT(Nv )0 ,

b
for i = 1, ..., 2 − 1,

Approximately AWGN
Network

Fig. 1.

u∈I(v)

˜
hu,v du + DFT(Nv ).

˜
Next, by looking at each component of Yv , we notice that
we have effectively b complex-valued received signals. The
additive noise on the th received signal is given by
1
b−1
−j2π i
b
DFT(Nv ) = √
i=0 Nv [i]e
b
b−1
b−1
j
1
= √b i=0 Nv [i] cos 2πi − √b i=0 Nv [i] sin 2πi . (4)
b
b

B. Interleaving and Outer Code
In the previous section, we saw that by choosing b sufﬁciently large, it is possible to make the effective additive
noise at node v arbitrarily close (in the distribution sense) to
2
2
a zero-mean Gaussian noise with variance σv /2 or σv . Notice
that, since in (2) we restricted the power used in the network

3

We let Zb ∈ Rk|V | be the random vector associated with
the effective additive noises at all nodes in V during this
length-k block, assuming that we performed the OFDM-like
scheme in blocks of size b. Since each component of Zb is
independent and they all converge in distribution to a zeromean Gaussian random variable, we have that Zb converges
in distribution to a Gaussian random vector. We let Z be this
limiting distribution, and we know that the component of Z
2
corresponding to node v and time is distributed as N (0, σv )
2
(or N (0, σv /2), depending on the length-k block chosen), for
any ∈ {1, ..., k}. Now notice that, if we ﬁx the messages
|L|
chosen at the sources to be w ∈ i=1 {1, ..., 2kRi }, then,
whether Ck makes an error is only a deterministic function of
|L|
Zb . Therefore, for each w ∈ i=1 {1, ..., 2kRi }, we can deﬁne
an error set Aw , corresponding to all realizations of Zb that
cause coding scheme Ck to make an error. It is important to
notice that Aw is independent of the actual joint distribution
of the noise terms; it only depends on the coding scheme Ck .
Then we can write

uses corresponding to (6) and (7) to P/2, all of our effective
channels have the same SNR they would have if the transmit
2
signals had power P and noise variance σv .
In this section, we address the fact that the additive noise at
node v in the b effective network uses are dependent of each
other. Moreover, we show that our scheme can be implemented
without violating the causality of the relays. We consider using
the network for a total of bk times, performing the OFDM-like
approach from section III-A within each block of b time steps.
Then, by interleaving the symbols, it is possible to view the
result as b blocks of k network uses. This idea is illustrated
in Figure 2. Notice that, within each (interleaved) block of k
bk

b

OFDM-­‐like  scheme  

k,b

...

k

= 2−k
−k

=2

|L|
i=1
|L|
i=1

Ri
w

Ri
w

Pr [Zb ∈ Aw ]

Pr [Z ∈ Aw ] .

(12)
(13)

Our next goal is to show that b,k → k as b → ∞. Recall
that a Borel set A ⊆ Rm is said to be a µ-continuity set
for a probability measure µ on Rm , if µ(∂A) = 0, where
∂A is the boundary of A. By the portmanteau Theorem
[7], since Zb converges to Z in distribution, we must have
limb→∞ µb (A) = µ(A) for all µ-continuity sets A, where µb
and µ are the probability measures on Rk|V | associated to Zb
and Z respectively.

k

Fig. 2. Interleaving the effective network uses obtained from the OFDM-like
scheme.

network uses, the additive noises are independent, but they are
dependent among distinct blocks.
Since, from the statement of Theorem 1, the rate tuple R is
achievable by coding schemes with ﬁnite reading precision, we
may assume that we have a sequence of coding schemes Ck =
(k, R) with ﬁnite reading precision ρk , whose error probability
when used on the AWGN network is k = Perror (Ck ) and
satisﬁes k → 0 as k → ∞. Now, we consider applying this
code over each of the b blocks of length k that we obtained
from the interleaving. Notice that, in order to apply code Ck
on a length-k block other than the ﬁrst or the last one, we will
√
have to divide the output transmit signal of all the nodes by 2
to satisfy (2), but since the additive noises in these blocks have
their variance divided by 2 as well, √
each node can re-scale its
received signal by multiplying it by 2, and the code performs
in the exact same way. It is also important to note that this
scheme presents no causality issues. For each i ∈ {1, ..., k},
during times (i − 1)b + 1, ..., ib, a node v transmits the IDFT
of the b signals which correspond to the ith signal that node
v would transmit in each of the b coding schemes Ck that we
use. These signals depend on the ﬁrst through (i−1)th signals
from each of the b coding schemes, which were received (after
applying the DFT) in the previous i − 1 length-b blocks.
Now, if b is chosen fairly large, over each (interleaved)
block of length k, the noises at all nodes are independent and
i.i.d. over time, and are very close to Gaussian in distribution,
and, intuitively, the error probability we obtain should be close
to k . We let k,b be the largest error probability obtained when
we apply Ck to one of the b blocks of length k.

Lemma 2. Suppose we have a coding scheme C = (k, R)
with ﬁnite reading precision ρ. Then, for any choice of w ∈
|L|
kRi
}, the error set Aw is a µ-continuity set.
i=1 {1, ..., 2
Proof: We ﬁx w and use the fact that C has ﬁnite reading
precision ρ to show that the set Aw and its complement Ac =
w
Rk|V | \Aw can be represented as a countable union of disjoint
convex sets, which will then imply the µ-continuity. Recall
from Deﬁnition 3 that, in a coding scheme with ﬁnite reading
precision ρ, a node v only has access to Yv ρ . Thus, we will
call Yv ρ the effective received signal at v. The set
Y = (y1 , ..., yk|V | ) ∈ Rk|V | : yi = yi ρ , i = 1, ..., k|V |
can be understood as the set of all possible values of the
effective received signals at all nodes in V during a length-k
block. It is clear that Y is a countable set for any ﬁnite ρ.
For our ﬁxed choice of messages w, the vector y ∈ Y
corresponding to the effective received signals at all nodes
during the length-k block is a deterministic function of the
value of all the noises in the network during the length-k block,
z ∈ Rk|V | . Therefore, for each y ∈ Y, we deﬁne Q(y) ⊆
Rk|V | to be the set of noise realizations z that will result in y
being the effective received signals. We claim that Q(y) is a
convex set. To see this, consider two noise realizations z, z ∈
Q(y) and ﬁx some α ∈ [0, 1]. We will show that if we replace

4

one of the components of z with the corresponding component
of αz+(1−α)z , the resulting noise realization is still in Q(y).
Then, by induction, it will follow that αz+(1−α)z is itself in
Q(y). So let us focus on the component corresponding to node
v at time . Let yv [ ]∗ be the noiseless version of the received
signal at v at time with its complete binary expansion. Since
z and z result in the same y, we have that
yv [ ] = yv [ ]∗ + zv [ ]

ρ

= yv [ ]∗ + zv [ ]

ρ

a probability of error (within that block) that tends to k as
b → ∞. However, since we have a total of b blocks of length
k, we make an error if we make an error in any of the b blocks
of length k. It turns out that a simple union bound does not
work here, since the error probability would be of the form
b b,k and we would not be able to guarantee that it tends to 0
as b and k go to inﬁnity. Instead we consider using an outer
code for each source-destination pair.
The idea is to apply coding scheme Ck to each of the b
length-k blocks, and then view this as creating a discrete
channel for each source-destination pair. More speciﬁcally,
for each length-bk block, source sj chooses a symbol (rather
than a message) from {1, ..., 2kRj }b and transmits the b
corresponding codewords from Ck . Then destination dj will
apply the decoder from code Ck inside each length-k block and
obtain an output symbol also from {1, ..., 2kRj }b . Notice that,
by viewing the input to bk network uses as a single input to this
discrete channel, we make sure we have a discrete memoryless
channel, and we can use the Channel Coding Theorem. We
ˆ
can view Wjb and Wjb as the discrete input and output of the
channel between sj and dj . We will then construct a code
(whose rate is to be determined) for this discrete memoryless
channel between sj and dj by picking each entry uniformly
at random from {1, ..., 2kRj }b . Then, source-destination pair
(sj , dj ) can achieve rate
1
1
ˆ
ˆ
I(Wjb ; Wjb ) =
H(Wjb ) − H(Wjb |Wjb )
bk
bk
1
b
ˆ
H(Wj [i]|Wj [i])
≥ Rj −
bk i=1
(i)
1
1
≥ Rj − (1 + b,k kRj ) = Rj (1 − b,k ) − ,
k
k
where (i) follows from Fano’s Inequality, since, within each
length-k block, we are applying code Ck and we have an
average error probability of at most b,k .
Thus, by choosing b and k sufﬁciently large, it is possible
for each source-destination pair to achieve arbitrarily close to
rate Rj , and our coding scheme can achieve arbitrarily close
to the rate tuple R. This concludes the proof of Theorem 1.

.

Now, if we assume wlog that zv [ ] ≤ zv [ ], we have
yv [ ]∗ + zv [ ]

ρ

≤ yv [ ]∗ + αzv [ ] + (1 − α)zv [ ]
≤ yv [ ]∗ + zv [ ]

ρ

ρ.

Thus, yv [ ] = yv [ ]∗ + αzv [ ] + (1 − α)zv [ ] ρ , and by replacing zv [ ] with αzv [ ] + (1 − α)zv [ ], we obtain a noise
realization that is still in Q(y), and the claim follows.
In [6], it is shown that, for any convex set S, λ(∂S) = 0,
where λ is the Lebesgue measure. Moreover, since our measure µ is absolutely continuous, it follows by deﬁnition that
λ(S) = 0 ⇒ µ(S) = 0, for any Borel set S. Thus, since
λ(∂Q(y)) = 0, we have that µ(∂Q(y)) = 0. This, in turn,
clearly implies that
µ (Q(y)◦ ) = µ Q(y) = µ (Q(y)) ,

(14)

where we use S ◦ to represent the interior of S and S to represent its closure. Next, let YAw = {y ∈ Y : Aw ∩ Q(y) = ∅}.
Notice that all noise realizations z ∈ Q(y) will cause all nodes
and, in particular, the destination nodes to effectively receive
the exact same signals. Therefore, it must be the case that, if
Aw ∩ Q(y) = ∅, then Q(y) ⊆ Aw , which implies that
y∈YAw

Q(y) = Aw .

Moreover, it is obvious that any noise realization must belong
to exactly one set Q(y), and we have
y∈Y\YAw

Q(y) = Ac .
w

Finally, we obtain

IV. ACKNOWLEDGEMENTS
The research of A. S. Avestimehr and I. Shomorony was
supported in part by NSF Career Award 0953117, NSF Grant
CCF-1144000, and AFOSR YIP award FA9550-11-1-0064.

µ (A◦ ) ≥ µ ∪y∈YAw Q(y)◦
w
(i)

=

(ii)

y∈YAw

µ (Q(y)◦ ) =

=1−
=
=

y∈YAw

y∈Y\YAw µ (Q(y)) = 1 −
1 − µ ∪y∈Y\YAw Q(y)◦ ≥ 1 −
◦ c
= µ Aw ,
µ (Ac )
w

µ (Q(y))

y∈Y\YAw
◦
µ (Ac )
w

µ (Q(y)◦)

R EFERENCES
[1] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley
Series in Telecommunications and Signal Processing, 2nd edition, 2006.
[2] A. Lapidoth. Nearest neighbor decoding for additive non-gaussian noise
channels. IEEE Trans. on Info. Theory, 42(5):1520–1529, Sept. 1996.
[3] R. Etkin, D. Tse, and H. Wang. Gaussian interference channel capacity to
within one bit. IEEE Transactions on Information Theory, 54(12):5534–
5562, December 2008.
[4] A. S. Avestimehr, S. Diggavi, and D. Tse. Wireless network information
ﬂow: A deterministic approach. IEEE Transactions on Information
Theory, 57(4), April 2011.
[5] S. H. Lim, Y. H. Kim, A. El Gamal, and S. Y. Chung. Noisy network
coding. IEEE Transactions on Info. Theory, 57(5):3132–3152, 2011.
[6] I. Shomorony and A. S. Avestimehr. Worst-case additive noise in wireless
networks. Submitted to IEEE Trans. on Information Theory, April 2012.
[7] P. Billingsley. Convergence of Probability Measures. Wiley, NY, 1968.

where (i) follows from the countability of YAw and the fact
that Q(y1 ) ∩ Q(y2 ) = ∅ for y1 = y2 , and (ii) follows from
(14). We conclude that µ(∂Aw ) = µ Aw − µ (A◦ ) = 0.
w
Now it follows from the portmanteau Theorem and Lemma
2 that, for all message choices w, we will have
lim Pr [Zb ∈ Aw ] = Pr [Z ∈ Aw ] ,

b→∞

which implies that b,k → k as b → ∞. Thus, we can apply
code Ck within each of the b blocks of length k and obtain

5

