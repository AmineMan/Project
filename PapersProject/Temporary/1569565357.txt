Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Mon May 14 15:52:29 2012
ModDate:        Tue Jun 19 12:54:31 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      425759 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565357

An Outer Bound for the Vector Gaussian
CEO Problem
Ersen Ekrem

Sennur Ulukus

Department of Electrical and Computer Engineering
University of Maryland, College Park, MD 20742
ersen@umd.edu
ulukus@umd.edu
N1

Abstract—We study the vector Gaussian CEO problem, and
provide an outer bound for its rate-distortion region. We obtain
our outer bound by evaluating an outer bound for the multiterminal source coding problem by means of a technique relying
on the de Bruijn identity and the properties of the Fisher
information. Next, we address the tightness of our outer bound,
and show that our outer bound does not provide the ratedistortion region in general. In particular, we provide a speciﬁc
example where the rate-distortion region is strictly contained in
our outer bound.

Y1

.
.

Nℓ
X

Yℓ

Source

YL

Fig. 1.

We study the vector Gaussian CEO problem, where there is
a vector Gaussian source which is observed through some linear additive vector Gaussian channels by an arbitrary number
of agents. The agents process their observations independently
and communicate them to a central unit (the so-called CEO)
through orthogonal and rate-limited links (see Figure 1). The
goal of the agents is to describe their observations to the CEO
in a way that the CEO can reconstruct the source within a
given distortion. The trade-off between the rate spent by the
agents and the distortion attained by the CEO is characterized
by the rate-distortion region, which is unknown in general.
The CEO problem is introduced in [1] for a discrete
memoryless setting, where the CEO is interested in estimating
a discrete source with the minimum Hamming distance. The
scalar Gaussian CEO problem is introduced in [2], where
there is a scalar Gaussian source which is observed through
some linear Gaussian channels by the agents. The CEO wants
to estimate the Gaussian source with the minimum mean
square error (MMSE). In [2], it is shown that the MMSE
decays inversely proportionally with the rate expenditure of
the agents, for sufﬁciently large number of agents. The scalar
Gaussian CEO problem is further studied in [3], [4], where the
entire rate-distortion region is characterized. The achievability
of this region follows from the Berger-Tung inner bound [5],
and the converse proof relies on the entropy-power inequality.
Recently, [6] provided an alternative proof for the sum-rate
of the scalar Gaussian CEO problem without invoking the
entropy-power inequality.
Although entropy-power inequality is useful to provide
converse proofs for scalar Gaussian problems, it might be
restrictive for vector Gaussian problems [7], [8]. For the

Rℓ
Agent ℓ

CEO

ˆ
X

.
.

NL

I. I NTRODUCTION

R1
Agent 1

RL
Agent L

The vector Gaussian CEO problem.

vector Gaussian CEO problem, this observation is noticed
in [9], where a lower bound for the sum-rate of the vector
Gaussian CEO problem is proposed by using the entropypower inequality. This lower bound is shown to be tight
under certain conditions, although it is not tight in general.
Recently, [10] provided an outer bound for the rate-distortion
region of the vector Gaussian CEO problem when there are
only two agents. The outer bound in [10] is obtained by using
a generalized version of the extremal inequality in [11].
Since the outer bound in [10] relies on the extremal inequality in [11], the generalization of this outer bound [10]
to more than two agents requires the generalization of the
extremal inequality in [11] as well. Here, we generalize the
outer bound in [10] to an arbitrary number of agents without
any recourse on the extremal inequality in [11]. To this end, we
ﬁrst consider the outer bound provided in [12] for the multiterminal source coding problem, and evaluate it for the vector
Gaussian CEO problem at hand. In this evaluation, we use the
de Bruijn identity [13], a connection between the differential
entropy and the Fisher information, along with the properties
of the MMSE and the Fisher information. This evaluation
technique which relies on the de Bruijn identity is useful in
the sense that it is able to alleviate some shortcomings of
the entropy-power inequality in vector Gaussian problems [8],
[14].
Next, we address the tightness of our outer bound, and show
that neither our outer bound nor the outer bound in [10], which
is, in fact, a special case of our outer bound, provides the
rate-distortion region in general. To show this, we provide a
speciﬁc example, where the rate-distortion region is strictly
contained in our outer bound. In particular, we consider the
parallel Gaussian model, for which we obtain the entire ratedistortion region explicitly by using the outer bound in [12].

This work was supported by NSF Grants CNS 09-64632, CCF 09-64645,
CCF 10-18185, and CNS 11-47811.

1

We then show that the rate-distortion region of the parallel
Gaussian model is strictly contained in outer bound, which
implies that our outer bound is not tight in general.
Finally, we note that in our set-up (see Figure 1), the agents
observe the vector Gaussian source X through only an additive
Gaussian noise, i.e., their observations are Y = X + N .
However, our outer bound can be generalized to a broader
model [15], [16], where the agents observe some linear combinations of the vector Gaussian source through an additive
Gaussian noise, i.e., their observations are Y = H X + N .
These generalizations can be found in [17].

where the lower bound on the distortion constraint D corresponds to the MMSE matrix obtained when the CEO has
direct access to the observations of the agents {Y }L . In [17,
=1
Appendix A.2], we show that imposing the lower bound on D
in (5) does not incur any loss of generality, while imposing the
upper bound on D in (5) might incur some loss of generality.
The rate-distortion region R(D) of the vector Gaussian
CEO problem is deﬁned as the closure of all rate tuples
(R1 , . . . , RL ) that can achieve the distortion D.
The main result of this paper is the following outer bound
on the rate-distortion region R(D):

II. P ROBLEM S TATEMENT AND THE M AIN R ESULT

Theorem 1 The rate-distortion region of the Gaussian CEO
problem R(D) is contained in the region Ro (D) which is
given by the union of rate tuples (R1 , . . . , RL ) satisfying

In the CEO problem, there are L sensors, each of which
getting a noisy observation of a source. The goal of the sensors
is to describe their observations to the CEO such that it can
reconstruct the source within a given distortion. In the vector
Gaussian CEO problem, there is an i.i.d. vector Gaussian
source {Xi }n with zero-mean and covariance KX . Each
i=1
sensor gets a noisy version of this Gaussian source
Y

= Xi + N ,i ,

,i

= 1, . . . , L

∈A

+
∈A

(1)

where {N
is an i.i.d. sequence of Gaussian random vectors with zero-mean and covariance Σ . Moreover, {N ,i }L
are independent ∀i. The distortion of the reconstructed vector
is measured by its mean square error matrix
ˆ
ˆ
Xi − Xi Xi − Xi

(6)

−1

Σ−1 (Σ − D ) Σ−1

D

(7)

0

D

Σ,

∀

(8)

+

and log x = max(log x, 0).
We obtain this outer bound by evaluating the outer bound
given in [12]. This evaluation relies on the de Bruijn identity
along with the properties of the Fisher information and the
MMSE. The proof of Theorem 1 can be found in Section III.
Next, we provide the following inner bound for the ratedistortion region R(D).
Theorem 2 An inner bound for the rate-distortion region
of the vector Gaussian CEO problem is given by the region Ri (D) which is described by the union of rate tuples
(R1 , . . . , RL ) satisfying

n
n
n
mmse(Xi |B1 , . . . , BL )

(3)

i=1
∈A

n
n
n
mmse(Xi |B1 , . . . , BL )

D

∈A

−1
−1

+

Σ

D

KX

K−1 +
X
K−1 +
X
1
|Σ |
log
2
|D |

∈Ac
L
=1

Σ−1 (Σ − D ) Σ−1

Σ−1 (Σ − D ) Σ−1

−1
−1

(9)

for all A ⊆ {1, . . . , L}, where the union is over all positive
semi-deﬁnite matrices {D }L ∈ D.
=1

where D is a strictly positive deﬁnite matrix. Throughout the
paper, we assume that the distortion matrix D satisﬁes
L

1
R ≥ log
2
+

(4)

i=1

K−1
X

|D|

(2)

Hence, a rate tuple (R1 , . . . , RL ) is said to achieve the
distortion D if there exists an (n, R1 , . . . , RL ) code such that
1
n→∞ n

−1

=1

ˆ
where Xn denotes the reconstructed vector.
An (n, R1 , . . . , RL ) code for the CEO problem consists of
an encoding function at each sensor f n : RM ×n → B n =
{1, . . . , 2nR }, i.e., B n = f n (Yn ) where B n ∈ B n ,
=
n
1, . . . , L, and a decoding function at the CEO g n : B1 × . . . ×
n
n
n
ˆ
BL → RM ×n , i.e., Xn = g n (B1 , . . . , BL ), where M denotes
the size of the vector Gaussian source X.
Since the MMSE estimator, which is the conditional mean,
minimizes the mean square error, the decoding function g n
ˆ
can be chosen as the MMSE estimator. Hence, we have Xi =
E Xi |{B n }L
using which in (2), we get
=1

lim

Σ−1 (Σ − D ) Σ−1

L

i=1

1
ˆ
Dn =
n

∈Ac

|Σ |
1
log
2
|D |

K−1 +
X

n

E

K−1 +
X

for all A ⊆ {1, . . . , L}, where the union is over all positive
semi-deﬁnite matrices {D }L ∈ D, and D contains all
=1
{D }L matrices satisfying the following constraints
=1

n
,i }i=1

1
ˆ
Dn =
n

1
R ≥ log+
2

This inner bound is obtained by evaluating the BergerTung inner bound [5] by jointly Gaussian auxiliary random
variables.

(5)

=1

2

Since log |·| is continuous in positive semi-deﬁnite matrices,
there exists a matrix D in the following form

We note that for both the outer bound in Theorem 1 and the
inner bound in Theorem 2, the feasible sets to which {D }L
=1
belong are identical and given by D. On the other hand,
rate bounds differ as seen through (6) and (9). Despite this
difference, there are cases where the outer and inner bounds
match, providing a complete characterization of the ratedistortion region. Here, we note a general sufﬁcient condition
under which the outer and inner bounds coincide. If the
boundary of the outer bound in Theorem 1 can be attained
by {D∗ }L matrices which achieve the distortion constraint
=1
in (7) with equality, then the outer and inner bounds match,
giving the rate-distortion region. For example, the outer and
inner bounds match for the scalar Gaussian model [17].

D = α J−1 (Y |X, W, U ) + α mmse(Y |X, W, U ) (17)
¯
with α = 1 − α ∈ [0, 1], which satisﬁes
¯
1
log |(2πe)D |
2
Hence, using (18) in (15), we have
h(Y |X, W, U ) =

I(Y ; U |X, W ) =

2

(12)

=1

where the union is over all joint distributions
2
p(x, {y , u }2=1 , w) = p(x)p(w) =1 p(y |x)p(u |y , w)
satisfying
mmse(X|U1 , U2 )

D

(21)

I(X; U1 |U2 ) = h(X|U2 ) − h(X|U1 , U2 )
(22)
1
≥ h(X|U2 ) − log |(2πe)mmse(X|U1 , U2 )|
(23)
2
1
≥ h(X|U2 ) − log |(2πe)D|
(24)
2
1
≥ h(X|U2 , W ) − log |(2πe)D|
(25)
2
1
1
≥ log |(2πe)J−1 (X|U2 , W )| − log |(2πe)D| (26)
2
2
where (23) comes from the fact that h(X|U1 , U2 ) is maximized by jointly Gaussian (X, U1 , U2 ), (24) follows from
the monotonicity of log | · | function in positive semi-deﬁnite
matrices in conjunction with the distortion constraint in (13),
(25) comes from the fact that conditioning cannot increase
entropy, and (26) is due to [17, Lemma 2].

(10)

I(Y ; U |X, W )

(20)

Next, we consider the following mutual information term

(11)

R ≥ I(X; U1 , U2 ) +
=1

mmse(Y |X, W, U )

D

which is the desired order on D stated in Theorem 1.

Theorem 3 ( [12, Theorem 1]) The rate-distortion region of
the CEO problem R(D) is contained in the union of rate
tuples (R1 , R2 ) satisfying

2

(19)

Σ

0
Here, we provide a sketch of the proof of Theorem 1 for
L = 2. The proof of Theorem 1 for an arbitrary L can be
found in [17]. We ﬁrst state the following outer bound for the
rate-distortion of the CEO problem.

R1 ≥ I(X; U1 |U2 ) + I(Y1 ; U1 |X, W )

= 1, 2

Moreover, using the Cramer-Rao inequality [17, Lemma 1]
and the fact that conditioning reduces MMSE, the following
bounds on D can be obtained

III. P ROOF OF T HEOREM 1

R2 ≥ I(X; U2 |U1 ) + I(Y2 ; U2 |X, W )

1
|Σ |
log
,
2
|D |

(18)

(13)

We evaluate this outer bound to obtain the outer bound in
Theorem 1 for L = 2. In this evaluation, we use properties of
the Fisher information, MMSE and differential entropy along
with connections among them. The corresponding background
information on these quantities can be found in [17, Section
6.1].
Now, we consider the following mutual information terms

Next, we obtain a lower bound for J−1 (X|U2 , W ), which,
in turn, will yield a lower bound for h(X|U2 , W ). To obtain
a lower bound for h(X|U2 , W ), we will use an identity
between the Fisher information matrix and the MMSE matrix,
which holds for additive Gaussian models as we have here. In
particular, using [17, Lemma 3], we can get
J−1 (X|U2 , W )

I(Y ; U |X, W ) = h(Y |X, W ) − h(Y |X, W, U )
(14)
1
= log |(2πe)Σ | − h(Y |X, W, U ) (15)
2
Using [17, Lemma 2] and the fact that jointly Gaussian
(X, W, U , Y ) maximizes h(Y |X, W, U ), we have the following bounds for the second term in (15)

= K−1 + Σ−1 − Σ−1 mmse(Y2 |X, W, U2 )Σ−1
2
2
2
X

−1

(27)

Using the order in (20) in (27), we get
J−1 (X|U2 , W )

K−1 + Σ−1 − Σ−1 D2 Σ−1
2
2
2
X

−1

(28)

Moreover, in view of the monotonicity of log | · | in positive
semi-deﬁnite matrices, using (28) in (26), we can get

1
log |(2πe)J−1 (Y |X, W, U )| ≤ h(Y |X, W, U )
2
1
≤ log |(2πe)mmse(Y |X, W, U )| (16)
2
where J(·|·) denotes the conditional Fisher information matrix.

1
I(X; U1 |U2 ) ≥ log+
2

K−1 + Σ−1 − Σ−1 D2 Σ−1
2
2
2
X

−1

|D|
(29)

3

{σ 2m }M , respectively. In the parallel Gaussian model, there
m=1
is a separate-distortion constraint on each component of the
source as follows

where the positivity operator comes from the non-negativity
of the mutual information. Using (19) and (29) in (10), we
can get

n

−1

1
n
n
lim
mmse(Xm,i |B1 , . . . , BL ) ≤ Dm , ∀m (37)
1
|Σ1 |
n→∞ n
+ log
i=1
|D|
2
|D1 |
(30) where we have the following constraints on {Dm }M
m=1
−1
which is the desired bound on R1 given in Theorem 1.
L
1
1
2
Similarly one can get the desired bound on R2 as well.
+
≤ Dm ≤ σm , ∀m
(38)
2
σm
σ 2m
Next, we consider the sum-rate R1 + R2 . To this end,
=1
we note that using the maximum entropy theorem and the
We note that the constraints on Dm in (38) are the scalar
distortion constraint in (13), one can get
versions of the constraints in (5), which were for the vector
1
|KX |
Gaussian model. For the parallel Gaussian model, we obtain
I(X; U1 , U2 ) ≥ log
(31)
the rate-distortion region Rp ({Dm }M ) as follows.
2
|D|
m=1
1
R1 ≥ log+
2

K−1 + Σ−1 − Σ−1 D2 Σ−1
2
2
2
X

using which, and the identities in (19) for the sum-rate bound
in (12), we have
R1 + R2 ≥

|KX |
1
log
+
2
|D|

2

=1

1
|Σ |
log
2
|D |

Theorem 4 The rate-distortion region Rp ({Dm }M ) of the
m=1
parallel Gaussian CEO problem is given by the union of rate
tuples (R1 , . . . , RL ) satisfying

(32)

M

R ≥

which is the desired bound on the sum-rate given in Theorem 1.
Finally, we establish a connection between D and (D1 , D2 ),
which will complete the proof of Theorem 1. To this end, we
note that similar to (28), one can obtain the following lower
bound for J−1 (X|U1 , U2 , W )

=1

m=1 ∈A

1
+
2
σm

(33)

mmse(X|U1 , U2 , W )

∈Ac

−1
m

(39)

m

(36)

L

σ 2m − D
σ 4m

−1
m

= Dm ,

∀m

(40)

≤ σ 2m ,

∀( , m)

(41)

(35)

D

1
log
2
D

(34)

mmse(X|U1 , U2 )

σ 2m − D
σ 4m

for all A ⊆ {1, . . . , L}, where the union is over all
{D m }∀ ,∀m satisfying the following constraints

=1

J−1 (X|U1 , U2 , W )

1
+
2
σm
σ 2m

+

Σ−1 D Σ−1

Σ−1 −

K−1 +
X

m=1
M

−1

2

2

∈A

1
1
log
2
Dm

=1

0≤D

m

Since the distortion constraints in (40) are met with equality,
the ﬁrst log(·) in (39) is always positive, and hence, we do not
need a positivity operator. We obtain the rate-distortion region
of the parallel Gaussian CEO problem in two steps. In the
ﬁrst step, we specialize the outer bound in [12] to the parallel
model. In the second step, we evaluate the outer bound we
obtain in the ﬁrst step, and show that it matches the inner
bound given in Theorem 2.
Next, we consider the case L = M = 2, and provide
an example where our outer bound strictly contains the ratedistortion region, i.e., our outer bound includes rate pairs
which are outside of the rate-distortion region. In the example
we provide, we assume that the following conditions hold:

where (34) is due to the Cramer-Rao inequality [17, Lemma 1],
(35) comes from the fact that conditioning reduces MMSE,
and (36) follows from the distortion constraint in (13). The
order in (36) gives us the desired order among D and D;
completing the proof.
IV. PARALLEL G AUSSIAN M ODEL AND A
C OUNTER -E XAMPLE
Here, ﬁrst, we consider the parallel Gaussian model, and
obtain its rate-distortion region. Next, we consider a speciﬁc
parallel Gaussian model and show that our outer bound in
Theorem 1 is not tight. In other words, we show that, in
general, there are rate tuples (R1 , . . . , RL ) that lie inside our
outer bound and are not contained in the rate-distortion region.
In the parallel Gaussian model, we have the Gaussian source
Xi = [X1,i . . . XM,i ] where {Xm,i }M are zero-mean indem=1
2
pendent Gaussian random variables with variances {σm }M ,
m=1
respectively. Moreover, the noise at the th sensor N ,i is given
by N ,i = [N 1,i . . . N M,i ], where {N m,i }M are zerom=1
mean independent Gaussian random variables with variances

1
1
1
µ2 1
2 < σ2 + σ2 − D
µ1 σ12
2
2
22
µ2
1
1
2 < σ2
µ1 − µ2 σ2
22
µ1
1
1
1
< 2+ 2
µ1 − µ2 D2
σ2
σ22
1
D1

4

1
1
2 + σ2
σ1
21

−1

>

µ1 − µ2
D2
µ1

1
1
2 + σ2
σ2
22

(42)
(43)
(44)
(45)

Under the constraints in (42)-(45)1 , the rate-distortion region
Rp (D1 , D2 ) can be characterized as follows.

identical, which is due to the assumption in (44). Equation
(54) implies that there are some rate pairs (R1 , R2 ) in our
outer bound which are outside of the rate-distortion region of
the parallel Gaussian model. Hence, our outer bound strictly
contains the rate-distortion region of the vector Gaussian CEO
problem, i.e., our outer bound is not tight in general.

Corollary 1 Assume that (42)-(45) hold. Then, we have
Tp =

min

µ1 R1 + µ2 R2

(R1 ,R2 )∈Rp (D1 ,D2 )

=

min
(D11 ,D21 )∈D1

+

f1 (D11 , D21 ) +

µ2
σ2
log 2
2
D2

1
1
1
2 + σ2 − D
σ2
2
22

1
µ2
log 2
2
σ22

V. C ONCLUSIONS
Here, we study the vector Gaussian CEO problem and
provide an outer bound for its rate-distortion region. To obtain
our outer bound, we consider the rather general outer bound
for the multi-terminal source coding problem in [12], and
evaluate it for the vector Gaussian CEO problem at hand. We
accomplish this evaluation by using a technique that relies on
the de Bruijn identity along with the properties of the MMSE
and Fisher information. Next, we address the tightness of our
outer bound and show that our outer bound does not provide
the exact rate-distortion region in general. We show this by
providing an example where our outer bound strictly includes
the rate-distortion region.

−1

(46)

where f1 (D11 , D21 ) is given by
2

f1 (D11 , D21 ) =
=1

+

µ
σ2
µ2
σ2
log 1 +
log 1
2
D1
2
D1
2
σ21 − D21
1
2 +
4
σ1
σ21

µ1 − µ2
1
log
2
D1

−1

(47)

and the set D1 consists of (D11 , D21 ) pairs satisfying
1
2 +
σ1

2

=1

σ 21 − D
σ 41

1

=

1
D1

R EFERENCES

(48)

[1] T. Berger, Z. Zhang, and H. Viswanathan. The CEO problem. IEEE
Trans. Inf. Theory, 42(3):887–902, May 1996.
[2] H. Viswanathan and T. Berger. The quadratic Gaussian CEO problem.
IEEE Trans. Inf. Theory, 43(5):1549–1559, Sep. 1997.
Next, we ﬁnd an upper bound for our outer bound in
[3] Y. Oohama. Rate-distortion theory for Gaussian multiterminal source
Theorem 1 as follows.
coding systems with several side informations at the decoder. IEEE
Trans. Inf. Theory, 51(7):2577–2593, Jul. 2005.
[4] V. Prabhakaran, D. Tse, and K. Ramchandran. Rate region of the
Corollary 2 Assume that (42)-(45) hold. Then, we have
quadratic Gaussian CEO problem. In IEEE ISIT, page 119, Jun. 2004.
[5] S.-Y. Tung. Multiterminal source coding. PhD thesis, Cornell University,
T+ =
min
µ1 R1 + µ2 R2
(50)
o (D ,D )
(R1 ,R2 )∈R
Ithaca, NY, 1978.
1
2
−1 [6] J. Wang, J. Chen, and X. Wu. On the sum rate of Gaussian multiterminal
µ1 1
1
1
µ2
source coding: New proofs and results. IEEE Trans. Inf. Theory,
log
≤
min
f1 (D11 , D21 ) +
2
2 + σ2
56(8):3946–3960, Aug. 2010.
2
µ2 σ22 σ2
(D11 ,D21 )∈D1
22
[7] H. Weingarten, Y. Steinberg, and S. Shamai (Shitz). The capacity region
−1
2
µ2
σ
µ1 − µ2
µ1
1
1
1
of the Gaussian multiple-input multiple-output broadcast channel. IEEE
+
log 2 +
log
Trans. Inf. Theory, 52(9):3936–3964, Sep. 2006.
2 + σ2
2
D2
2
µ1 − µ2 D2 σ2
22
[8] E. Ekrem and S. Ulukus. The secrecy capacity region of the Gaus(51)
sian MIMO multi-receiver wiretap channel. IEEE Trans. Inf. Theory,
57(4):2083–2114, Apr. 2011.
where the function f1 (D11 , D21 ) is given by (47) and the set
[9] S. Tavildar and P. Viswanath. On the sum-rate of the vector Gaussian
D1 is given by the union of (D11 , D21 ) satisfying (48)-(49).
CEO problem. In Asilomar Conf. on Signals, Systems and Computers,
pages 3–7, Oct. 2005.
Now, we are ready to compare our outer bound with the [10] J. Chen and J. Wang. On the vector Gaussian CEO problem. In IEEE
ISIT, pages 2050–2054, Aug. 2011.
rate-distortion region for the parallel Gaussian model. Using
[11] T. Liu and P. Viswanath. An extremal inequality motivated by mulCorollary 1 and Corollary 2, we have
titerminal information theoretic problems. IEEE Trans. Inf. Theory,
53(5):1839–1851, May 2007.
−1
µ1
1
1
1
µ2
[12] A. B. Wagner and V. Anantharam. An improved outer bound for
+
p
log
1−
T −T ≤
multiterminal source coding. IEEE Trans. Inf. Theory, 54(5):1919–1937,
2 + σ2
2
µ2
D2 σ2
22
May 2008.
−1
[13] D. P. Palomar and S. Verdu. Gradient of mutual information in linear
µ1 − µ2
µ1
1
1
1
vector Gaussian channels. IEEE Trans. Inf. Theory, 52(1):141–154, Jan.
+
log
(52)
2 + σ2
2
µ1 − µ2 D2 σ2
2006.
22
[14] E. Ekrem and S. Ulukus. An alternative proof for the capacity region
µ1
<
log 1
(53)
of the degraded Gaussian MIMO broadcast channel. IEEE Trans. Inf.
2
Theory, 58(4):2427–2433, Apr. 2012.
=0
(54) [15] Y. Oohama. Distributed source coding of correlated Gassign observations. In IEEE ISIT, pages 51–55, Jun. 2010.
where (53) comes from the facts that log(·) is strictly concave, [16] Y. Yang, Y. Zhang, and Z. Xiong. The generalized quadratic Gassign
CEO problem: New cases with tight rate region and applications. In
and the two terms inside the log(·) functions in (52) are not
IEEE ISIT, pages 21–25, Jun. 2010.
[17] E. Ekrem and S. Ulukus. An outer bound for the vector Gaussian CEO
1 We note that if one selects σ 2 = σ 2
2
2
m
m = σ , D1 = 2/5σ , D2 =
problem. Submitted to IEEE Trans. Inf. Theory, Jan. 2012. Also available
4/5σ 2 and µ1 /µ2 = 4, the four assumptions in (42)-(45) hold in addition
at [arXiv:1202.0536].
to the original constraints on (D1 , D2 ) given in (38).

0≤D

1

≤ σ 21 ,

= 1, 2

(49)

5

