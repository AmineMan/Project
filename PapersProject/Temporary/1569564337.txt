Creator:         TeX output 2012.05.17:1546
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 15:46:14 2012
ModDate:        Tue Jun 19 12:55:53 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      445754 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569564337

WarmL1: A Warm-start Homotopy-based
Reconstruction Algorithm for Sparse Signals
Tien-Ju Yang, Yi-Min Tsai, Chung-Te Li, and Liang-Gee Chen

DSP/IC Design Lab, Graduate Institute of Electronics Engineering,
National Taiwan University, Taiwan
{denru,ymtsai,ztdjbdy,lgchen}@video.ee.ntu.edu.tw
of these algorithms assume there is no relationship between
signals or systems, and reconstruct each signal independently.
This induces high computational efforts for signal reconstruction in real applications. In many situations, signals or systems
do have relationship, such as frames in a video, nearby blocks
in an image or reconstruction with sequential measurements.
Using the previous reconstructed signal as a starting point can
increase the performance signiﬁcantly.
Assuming we have reconstructed a signal by (2), we would
like to reconstruct the next signal through solving the new
problem
2
1 ˜
Ax − y + λ ∥x∥1 ,
˜
(3)
min
x
2
2
˜
˜
˜
where y ∈ RM ×1 and A ∈ RM ×N are the new measurement
˜
vector and the corresponding sensing matrix, respectively. The
˜
dimension of y and A may be different from that of y and
˜
A. If the solutions of these two problems ((2) and (3)) are
similar, the concept of warm-start can be applied to speed up
the reconstruction without sacriﬁcing accuracy. Several warmstart methods are presented in [5]–[8], and the concept of
warm-start has been proved to be effective and efﬁcient.
In this paper, we propose a warm-start algorithm for quickly
reconstructing a sparse signal, and call it WarmL1. We present
a novel formulation and adopt the homotopy continuation principle for warm-start sparse signal reconstruction. Moreover,
we show that WarmL1 is the generalized and more efﬁcient
algorithm against previous methods. The key contributions
of this work are as follows. We propose a single algorithm,
WarmL1, to unify many existent homotopy-based warm-start
˜
methods [5]–[7]. By adopting some special A and y , WarmL1
˜
is simpliﬁed to these algorithms (see Sec. III-A and Sec. III-C).
Moreover, WarmL1 also handles other update problems that
can not be solved by these algorithms, such as warm-start
reconstruction with sensing matrix modiﬁcation (see Sec. III-B
and Sec. III-D).
The remainder of this paper is organized as follows: Sec.
II presents the formulation to this warm-start problem and
the proposed algorithm. We discuss possible applications and
the usage of WarmL1 in Sec. III. Experimental results and
discussions are shown in Sec. IV. Sec. V concludes this paper.

Abstract—A sparse signal can be reconstructed from a small
amount of random and linear measurements by solving a
system of underdetermined equations. In this paper, we study
the reconstruction problem while the system undergoes dynamic modiﬁcations. Resolving this problem from scratch requires high computational efforts. Therefore, we propose an
efﬁcient homotopy-based reconstruction algorithm with warmstart, named WarmL1. WarmL1 quickly updates the previous
solution to the desired one. Based on the concept of homotopy,
WarmL1 breaks the reconstruction procedure into simple steps,
and solves the problem iteratively. Four possible applications are
presented and discussed to demonstrate the usage of WarmL1
for different warm-start situations. Experiments on these applications are performed. The results show that WarmL1 achieves
3.2× to 37.5× speeding up or up to 1/5100 l2-error at the same
computational cost compared to related works.

I. I NTRODUCTION
The sparsity is the nature of natural signals. In recent years,
compressed sensing (CS) [1], [2] makes progress in the signal
processing ﬁeld. CS has been extended to many applications,
such as image denoising, MRI imaging and sensor network.
It claims that a signal can be reconstructed from only few
measurements even below Nyquist-Shannon sampling rate if
the signal is sparse. The measurements are generated by the
equation
y = A˘,
x
(1)
where y ∈ RM ×1 is the measurement vector, x ∈ RN ×1 is
˘
a signal. A ∈ RM ×N is the sensing matrix, which can be
an incoherent random matrix. Because N is larger than M,
the signal reconstruction process includes solving a system
of underdetermined equations. The CS theory states that if the
signal is K-sparse and M ≥ cKlog(N/K) for a small constant
c, it can be reconstructed by using the l1-norm minimization:
min
x

1
2
∥Ax − y∥2 + λ ∥x∥1 ,
2

(2)

where x ∈ RN ×1 is a dummy variable, and λ > 0 is a
regularization parameter. (2) is also known as LASSO or
BPDN equation. In addition to CS, (2) is also the kernel of
sparse representation. Sparse representation models a signal
as a sparse linear combination of entries in a dictionary and
has been proved to achieve state-of-the-art performance in the
computer vision ﬁeld [3], [4].
In the literature on sparse signal reconstruction, there have
been many methods proposed to solve (2). However, most

II. P ROPOSED WARM L1 A LGORITHM
In this section, we propose and derive the warm-start signal
reconstruction algorithm, WarmL1, which is based on the

1

homotopy continuation principle. The homotopy continuation
principle provides a general framework for transforming one
formulation into another. Homotopy methods trace a solution
path, characterized by homotopy parameters, along with the
formulation transformation. The solution path is followed with
optimality constraints satisﬁed. In the transforming process,
these homotopy parameters evolve from initial values to given
values. After given homotopy parameters are achieved, the
formulation transformation terminates and the desired solution
is solved.
In the following, we propose a formulation for connecting
(2) and our objective (3), and ﬁnd out its optimality constraints. The optimality constraints show that the solution path
is piecewise linear. The equations of the direction and the
step size to the nearest breakpoint are then derived. Finally,
we show the scheme for updating the homotopy parameter, the
solution and the support (a set formed from indices of nonzero
elements) of the solution.

or
˜S ˜
˜
AT (Ax − (1 − µ)Ax0 − µ˜) +
y
T
(1 − µ)AS (Ax0 − y) = −λ sgn(xS ),
˜
˜
˜
∥AT C (Ax − (1 − µ)Ax0 − µ˜) +
y
S
T
(1 − µ)AS C (Ax0 − y)∥∞ ≤ λ,

where S is the support of x, S is the complement of S,
˜
AS ∈ RM ×|S| is formed with columns of A corresponding
to S, xS ∈ R|S|×1 is a vector composed of the elements
in x corresponding to S, and |·| indicates the size of a
set. Therefore, the solution path is piecewise linear when µ
changes, and this is the key to the success of the following
steps.
B. Breakpoint Determination
In this step, we calculate the direction and the step size of x
to the nearest breakpoint. Denote the temporary solution x and
the homotopy parameter µ in the ℓth iteration as xℓ and µℓ ,
respectively. Before reaching a breakpoint, xℓ moves to any
solution x′ through x′ = xℓ + γ ∗ ∂xℓ /∂µ, where ∂xℓ /∂µ is
the direction and γ ∗ is a step size.The direction can be solved
through differentiating (9) by µ, and is
˜S ˜
˜S ˜
˜
(∂xℓ /∂µ)S = (AT AS )−1 (AT (Ax0 − y) − AT (Ax0 − y )),
S
⃗
(11)
(∂xℓ /∂µ)S C = 0.

where x0 is the solution of (2). When µ equals to 0, the formulation is just the equation (2). While µ is 1, this formulation
becomes the equation (3). Therefore, with the same λ as (2),
we vary µ from 0 to 1, and the formulation transforms from
(2) to (3). It is worth to note that the objective function of the
formulation (4) is convex for any µ ∈ R.
Proof: Because the l1-norm is convex and λ is always
positive, we only need to prove the convexity of the l2-norm
part. Let
2
2

The breakpoint is caused by two conditions. The ﬁrst scenario
leading to a breakpoint is when (10) is activated, i.e. an
element needed to be added to the support. Let
˜ ˜
˜
cℓ = AT (Axℓ − (1 − µℓ )Ax0 − µℓ y ) + (1 − µℓ )AT (Ax0 − y).
˜
(12)
Assuming that (10) is activated when µℓ moves to µ′ or µ′′ ,
ℓ
ℓ
we have equations (13) and (14).
˜
˜ ℓ
˜
λ = max{AT C (Ax′ − (1 − µ′ )Ax0 − µ′ y ) +
ℓ
ℓ˜
S
′
T
(1 − µℓ )AS C (Ax0 − y)}elem ,
˜
˜ ℓ
˜
−λ = min{AT C (Ax′′ − (1 − µ′′ )Ax0 − µ′′ y ) +
ℓ
ℓ˜

−
2

∥Ax − (1 − µ)Ax0 − µy∥2 .
˜ ˜
∂ h(x)/∂x = 2AT A.
2

(6)

˜
Since A A is positive semideﬁnite, h(x) is convex. Hence, the
objective function of the formulation (4) is convex for every
µ ∈ R.
Therefore, from the idea of the classical convex optimization, ∃i, ∂f (x)/∂xi = 0 is the necessary condition for
getting the extreme value of a generic convex function f (x).
Hence, the constraints of x and µ can be derived through
differentiating the objective function of (4) by x and setting it
to zero, and is

{

where
∂ ∥x∥1 /∂x =

sgn(xi ), xi ̸= 0
[−1, 1] , xi = 0,

(1 − µ′′ )AT C (Ax0 − y)}elem ,
ℓ
S

(14)

where x′ and x′′ are the solutions when the homoℓ
ℓ
topy parameter equals to µ′ or µ′′ respectively, and
ℓ
ℓ
max{·}elem /min{·}elem indicates a maximum/minimum operator to select the maximal/minimal element. The two possible
step sizes are derived with (12), (13) and (14), and are

˜T

˜ ˜
˜
AT (Ax − (1 − µ)Ax0 − µ˜) + (1 − µ)AT (Ax0 − y)
y
= −λ∂ ∥x∥1 /∂x,

(13)

S

(5)

We take the second derivative of h(x) by x and give
2

(10)

c

A. Formulation and Optimality Constraints
To link the two equations (2) and (3), we introduce a homotopy parameter µ ∈ R and give the following formulation:
2
1
2
˜
˜
min (∥Ax − y∥2 + Ax − (1 − µ)Ax0 − µ˜ −
y
x
2
2
2
(4)
∥Ax − (1 − µ)Ax0 − µy∥2 ) + λ ∥x∥1 ,

2
˜
˜
h(x) = ∥Ax − y∥2 + Ax − (1 − µ)Ax0 − µ˜
y

(9)

λ − cℓ (j)
}+ ,
qℓ (j)
λ + cℓ (j)
= µ′′ − µℓ = min {
}+ ,
ℓ
j∈S c
−qℓ (j)

+
γℓ,1 = µ′ − µℓ = min {
ℓ
c
j∈S

+
γℓ,2

(15)

where min{·}+ indicates a minimum operator which is only
taken over the positive elements, m(j) is deﬁned as the jth
element in a generic vector m, and
˜ ˜
˜
qℓ = AT (A∂xℓ /∂µ + Ax0 − y ) − AT (Ax0 − y).
˜

(7)

(16)

Therefore, the step size of the support addition is given by
(8)

+
+
+
γℓ = min{γℓ,1 , γℓ,2 }.

2

(17)

Initial System

Algorithm 1 WarmL1.
Input:
The previous measurements y and the corresponding sensing
matrix A;
The next measurements y and the corresponding sensing matrix
˜
˜
A;
The initialization signal x0 ;
The regularization parameter λ;
Output:
The reconstructed signal xrec ;
1: Initialize. Find the support S of x0 , and set µ to 0 and ℓ to 0.
2: Compute direction. Calculate the direction ∂xℓ /∂µ with
Eq. (11).
3: Compute step size. Calculate the step size γℓ with Eq. (12, 15,
16, 17, 18, 19).
4: Update.
Condition 1: If γℓ < (1 − µℓ ), update xℓ and µℓ with Eq. (20),
update the support S, increment ℓ, and repeat (2)-(4).
Condition 2: If γℓ ≥ (1 − µℓ ), update xℓ and µℓ with the
following equation, update the support S and increment ℓ:
µℓ+1 = 1, xℓ+1 = xℓ + (1 − µℓ )∂xℓ /∂µ.
5: Output. Set xrec to xℓ , and return xrec .

ypre

ypre

Apre

Aseq
(M+α)x1 (M+α)xN

xℓ+1 = xℓ + γℓ ∂xℓ /∂µ.

ylef

Alef

yrem
(M-α)x1

Apre

xnex

Nx1

Mx1

MxN

Nx1

D. Matrix-varying Rec.

Arem
(M-α)xN

ypre
xpre or
ynex

Anex

xnex

Nx1

MxN

Nx1

Mx1

The schematic illustrations for four applications.

A. Sequential Reconstruction
This application is regarding the scenario that the measurements are input sequentially. A signal is ﬁrst reconstructed
from already available measurements, and then updated by
taking the newly received measurements into account. Let
Apre ∈ RM ×N and ypre ∈ RM ×1 be the sensing matrix and
the measurement vector of the previous reconstructed signal.
yseq ∈ Rα×1 and Aseq ∈ Rα×N are the newly received
measurement vector and its corresponding sensing matrix. We
can apply WarmL1 in this application with the following steps:
• Substitute A and y with Apre and ypre .
T
T
˜
• Substitute A and y with [AT AT ]T and [ypre yseq ]T .
˜
pre
seq

(18)

(19)

(20)

B. Removal Reconstruction

To enable the next iteration of WarmL1, the support should
be updated, too. How to update the support depends on which
−
step size is chosen. If γℓ is chosen, remove the element
+
changing to 0 from the support. If γℓ is used, add the element
causing (10) activated to the support. Moreover, WarmL1
terminates when the homotopy parameter µ reaches 1.
The algorithm ﬂow of WarmL1 is presented in Algorithm 1.
One of the most computationally intensive parts is computing
˜ ˜
(AT AS )−1 (for ∂xℓ /∂µ). In practice, it is more efﬁcient to
S
compute ∂x/∂µ by solving the following linear system with
Cholesky decomposition:
˜S ˜
˜S ˜
AT AS (∂xℓ /∂µ)S = AT (Ax0 − y) − AT (Ax0 − y ).
˜
S

xpre ynex

B. Removal Rec.

C. Solution and Parameters Updating
Finally, after calculating both the direction ∂xℓ /∂µ and the
maximum step size γℓ , we can update the homotopy parameter
µℓ and the solution xℓ by
µℓ+1 = µℓ + γℓ ,

C. Time-varying Rec.

yseq

The maximum step size to reach the nearest breakpoint is
+
−
γℓ = min{γℓ , γℓ }.

Nx1

A. Sequential Rec.

The second scenario causing a breakpoint is an element of xℓ
becoming 0, and the step size for this condition is
−xℓ (j)
= min{
}+ .
j∈S ∂xℓ (j)/∂µ

xpre

MxN

Mx1

Fig. 1.

−
γℓ

Apre

Sometimes it is required to reconstruct a signal from partial
measurements when a reconstructed result from all measurements is known. For instance, a few nodes in the sensor
network may receive bad measurements due to malfunctioning
nodes, packet corruption, or attack. The reconstructed signal
from all measurements is incorrect. In this situation, we
adopt some detection methods (ex. [10]) to identify the bad
measurements. After identifying them, we remove these bad
measurements and reconstruct the correct signal. Firstly, we
separate the measurements ypre and the sensing matrix Apre
each into two sets. They are yrem ∈ Rα×1 and ylef ∈
R(M −α)×1 , denoting the measurements to be removed and the
left measurements. Arem ∈ Rα×N and Alef ∈ R(M −α)×N
are the corresponding sensing matrices. Secondly, WarmL1 is
applied by:
• Substitute A and y with Apre and ypre .
˜
• Substitute A and y with Alef and ylef .
˜

(21)

Because we only change one element of the support at a time,
the Cholesky decomposition needs not be recalculated in each
iteration, and can be updated by rank-1 methods [9].
III. A PPLICATIONS
In this section, we present four applications suitable for
adopting WarmL1 and demonstrate the usage of WarmL1
under various dynamic system modiﬁcations. Fig. 1 illustrates
these applications and the notations used.

C. Time-varying Reconstruction
Several signals vary with time and change slowly, such as
a video sequence. If such signal is sampled by a constant

3

10

2
0
−2
0

2000

4000

6000

8000
10000 12000 14000
Initialization
(N = 16384, number of nonzeros = 102, l2−error = 11.9)

16000

2
0
−2
0

2000

4000

6000
8000
10000 12000 14000
WarmL1 Reconstruction
(N = 16384, M = 1024, lambda = 0.00001, l2−error = 0.000513)

2000

4000

Fig. 2.

6000

8000

10000

12000

14000

3
2

10

StaLasso
GPSR_BB
WarmL1

1
0

5
10
15
20
Number of Added Measurements

Fig. 3. Sequential reconstruction
application with different amounts
of new measurements.

16000

2
0
−2
0

x 10

Computational Cost (Flop)

Computational Cost (Flop)

Original Signal
(N = 16384, number of nonzeros = 167)

4

x 10

3

StaLasso
GPSR_BB
WarmL1

2
1
5
10
15
20
Number of Removed Measurements

Fig. 4.
Removal reconstruction
application with different amounts
of removed measurements.

all parameters ﬁxed (controlled variables) except for one
parameter (independent variable). The initial setting of the
system is as follows. N is 16384, M is 1024 and λ is 1e-5.
The original signal (xpre ∈ RN ×1 ) is generated by a Gaussian
random generator with 1% nonzeros (1% sparsity) and then
normalized to unity l2-norm. The sensing matrix (Apre ) is also
generated by a Gaussian random generator with all columns
having unity l2-norm. Moreover, the measurement vector ypre
is given by ypre = Apre xpre . Fig. 2 shows an example of
WarmL1 reconstruction.
We model all computations in all algorithms as four basic
operations, which are addition, multiplication, inversion and
division, to give a computational cost estimation in terms
of ﬂop. In the ﬁrst two experiments (Sec. IV-A, IV-B), we
compare their computational costs under the same l2-error.
In the other two experiments (Sec. IV-C, IV-D), it takes
considerable time for GPSR BB to achieve the same error as
StaLasso and WarmL1. Therefore, we terminate the calculation
of GPSR BB when the computational cost of GPSR BB
is almost the same as that of WarmL1, and compare their
reconstruction errors.

16000

An example of WarmL1 reconstruction.

sensing matrix, we can start from the reconstructed signal in
the previous time t to compute the next signal at time t + 1.
Let Apre and ypre be the sensing matrix and the measurement
vector of the reconstructed signal at time t. ynex ∈ RM ×1 is
the measurement vector at time t + 1. We can adopt WarmL1
to perform efﬁcient reconstruction with the following steps:
• Substitute A and y with Apre and ypre .
˜
• Substitute A and y with Apre and ynex .
˜
D. Matrix-varying Reconstruction
In applications, such as dictionary learning [11] or deconvolutional networks [12], they repeatedly solve (2) with
different sensing matrices and the same measurement vector.
Asif et al. [7] has presented a method to handle this case.
However, it only works at pretty restricted condition. Let
Apre and ypre be the sensing matrix and the measurement
vector of the reconstructed signal at the previous iteration.
ynex and Anex ∈ RM ×N are the measurement vector and its
corresponding sensing matrix at the next iteration, and ynex is
the same as ypre . With WarmL1, the warm-start reconstruction
is achieved by:
• Substitute A and y with Apre and ypre .
˜
• Substitute A and y with Anex and ynex .
˜
Moreover, it is worth to note that WarmL1 can also handle the
more challenging case both the sensing matrix and the measurement vector changed at the same time. In this situation,
we simply use a ynex different from ypre .

A. Experiments on Sequential Reconstruction
In this experiment, different amounts of new measurements
are given (1, 5, 9, 13, 17 and 20 at a time). Fig. 3 shows
that the computational cost of WarmL1 increases when the
number of new measurements increases. This lies in that with
more measurements added, the available information about
the signal increases, which causes a sparser reconstructed
signal. Because WarmL1 adds/removes one element to/from
the support at each iteration, more support changes between
the two systems requires more iterations and so computation.
WarmL1 is up to 18.6× faster than StaLasso and 3.2× faster
than GPSR BB in this experiment.
B. Experiments on Removal Reconstruction

IV. E XPERIMENTAL R ESULTS AND D ISCUSSIONS

Fig. 4 shows the experimental results of the three algorithms
with respect to different amounts (1, 5, 9, 13, 17 and 20 at
a time) of measurements being removed. With less measurements available, the information about the signal is reduced,
which leads to a denser reconstructed signal and more support
changes between the previous and next system. Therefore, just
as the discussions in the Sec. IV-A, the computational cost of

In this section, we compare WarmL1 to two related algorithms in the four applications mentioned in Sec. III. These
algorithms are the homopopy-based algorithm without warmstart, StaLasso [13], and the gradient-based algorithm with
warm-start, GPSR BB [8]. All the algorithms are implemented
in Matlab and run on a PC equipped with an Intel Xeon
X5680 CPU. The experiments are taken in a manner with

4

x 10

4

StaLasso
GPSR_BB
WarmL1

3
2

the next problem is larger. However, the WarmL1 is still 18.7×
faster than StaLasso and the l2-error of WarmL1 is 1/4800
smaller than that of GPSR BB with the same computational
cost.

0

10
l2−error

Computational Cost (Flop)

10

5

StaLasso
GPSR_BB
WarmL1

−2

10

V. C ONCLUSION

1
0

In this work, a homotopy-based reconstruction algorithm for
sparse signals with warm-start (named WarmL1) is proposed.
WarmL1 provides a general method for quickly reconstructing
a sparse signal under several dynamic system modiﬁcations.
While the system is changed, WarmL1 solves the new problem
by adopting the previous result as the starting point rather
than recalculates the new solution from scratch. We present
a novel formulation to connect the previous and the new
problems, and adopt homotopy continuation principle to develop the algorithm. Four possible applications for WarmL1
are proposed and discussed. Through the experiments, we
demonstrate that WarmL1 outperforms related works in these
applications. WarmL1 achieves up to 37.5× speeding up or
1/5100 l2-error with the same computational cost. Moreover,
WarmL1 can be applied to research ﬁelds such as compressed
sensing and sparse representation.

−4

10

0
5
10
Number of Changed Zero Elements

0
5
10
Number of Changed Zero Elements

(a)

(b)

Fig. 5. Time-varying reconstruction application with different signal changes.
(a) Computational cost. (b) l2-error.
x 10

3

StaLasso
GPSR_BB
WarmL1

2

0

10
l2−error

Computational Cost (Flop)

10

4

StaLasso
GPSR_BB
WarmL1

−2

10

1
0

0
0.1
0.2
0.3
Variance of Gaussian Random Number

(a)

−4

10

0
0.1
0.2
0.3
Variance of Gaussian Random Number

(b)

Fig. 6. Matrix-varying reconstruction application with different amounts of
sensing matrix changes. (a) Computational cost. (b) l2-error.

R EFERENCES
[1] E. Candes, J. Romberg, and T. Tao, “Stable signal recovery from
incomplete and inaccurate measurements,” Comm. Pure Appl. Math.,
vol. 59, pp. 1207–1223, 2005.
[2] D. Donoho, “Compressed sensing,” Information Theory, IEEE Transactions on, vol. 52, no. 4, pp. 1289–1306, April 2006.
[3] J. Yang, K. Yu, Y. Gong, and T. Huang, “Linear spatial pyramid
matching using sparse coding for image classiﬁcation,” in Computer
Vision and Pattern Recognition, IEEE Conference on, June 2009, pp.
1794–1801.
[4] X. Mei and H. Ling, “Robust visual tracking using l1 minimization,” in
Computer Vision, IEEE International Conference on, September 2009,
pp. 1436–1443.
[5] M. Asif and J. Romberg, “Dynamic updating for sparse time varying
signals,” in Information Sciences and Systems, CISS 43rd Annual Conference on, March 2009, pp. 3–8.
[6] P. Garrigues and L. Ghaoui, “An homotopy algorithm for the lasso
with online observations,” in Advances in neural information processing
systems, 2008, pp. 489–496.
[7] M. Asif and J. Romberg, “Sparse signal recovery and dynamic update
of the underdetermined system,” in Signals, Systems and Computers,
Conference Record of the Forty Fourth Asilomar Conference on, Nov.
2010, pp. 798–802.
[8] M. Figueiredo, R. Nowak, and S. Wright, “Gradient projection for sparse
reconstruction: Application to compressed sensing and other inverse
problems,” Selected Topics in Signal Processing, IEEE Journal of, pp.
1–12, 2007.
[9] T. A. Davis and W. W. Hager, “Row modiﬁcations of a sparse cholesky
factorization,” SIAM J. Matrix Anal. Appl., vol. 26, pp. 621–639, March
2005.
[10] H. Kung, T. Lin, and D. Vlah, “Identifying bad measurements in
compressive sensing,” in Computer Communications Workshops, IEEE
Conference on, April 2011, pp. 1054–1059.
[11] M. Aharon, M. Elad, and A. Bruckstein, “K-svd: An algorithm for
designing overcomplete dictionaries for sparse representation,” Signal
Processing, IEEE Transactions on, vol. 54, no. 11, pp. 4311–4322, Nov.
2006.
[12] M. Zeiler, D. Krishnan, G. Taylor, and R. Fergus, “Deconvolutional networks,” in Computer Vision and Pattern Recognition, IEEE Conference
on, June 2010, pp. 2528–2535.
[13] D. Donoho and Y. Tsaig, “Fast solution of l1-norm minimization
problems when the solution may be sparse,” Information Theory, IEEE
Transactions on, vol. 54, no. 11, pp. 4789–4812, Nov. 2008.

WarmL1 increases. In this test, WarmL1 achieves 12.3× and
5.2× speeding up with respect to StaLasso and GPSR BB.
C. Experiments on Time-varying Reconstruction
In this experiment, we model the next signal (xnex ) by the
following steps. We ﬁrstly multiply each nonzero elements
of the previous signal (xpre ) by a Gaussian random number
with mean equal to 1 and variance equal to 0.1. Secondly, we
change different amounts (0, 2, 4, 6, 8 and 10 at a time) of zero
elements in this signal to nonzero values. Therefore, the two
signals act as a time-varying signal at time t and t + 1. The
experimental results are shown in Fig. 5. When the amount
of changed zero elements increases, the support difference
between the systems is larger, which causes the performance
degradation of WarmL1. WarmL1 achieves 37.5× speeding
up with respect to StaLasso, and 1/5100 l2-error comparing to
GPSR BB with the same computational cost.
D. Experiments on Matrix-varying Reconstruction
In this experiment, we consider the challenging scenario
that the sensing matrix and the underlying signal are both
changed. That means Apre and Anex , xpre and xnex , ypre
and ynex are all different. We multiply each nonzero elements
of xpre by a Gaussian random number with mean equal to 1
and variance equal to 0.1 to model xnex . Apre is multiplied by
a Gaussian random number with mean equal to 1 and different
variances (0.01, 0.068, 0.126, 0.184, 0.242 and 0.3 at a time)
to model Anex . ynex is then generated by Anex xnex . Fig. 6
shows the experimental results. In this scenario, we observe
that the performance of WarmL1 decreases when the variance
of the Gaussian random number multiplied with sensing matrix
increases. This lies in that the difference of the previous and

5

