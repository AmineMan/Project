Title:          isit12_CC_v2.dvi
Creator:        www.freepdfconvert.com         
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 11 03:21:32 2012
ModDate:        Tue Jun 19 12:56:00 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      313850 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569564195

Best Rate 1/2 Convolutional Codes for Turbo
Equalization with Severe ISI
John B. Anderson and Mehdi Zeinali
Electrical and Information Tech. Dept. and Strategic Center for High Speed Wireless Communication
Lund University, Box 118, SE-221 00 Lund SWEDEN
Email: anderson@eit.lth.se, mas09mz2@student.lu.se

Abstract—We give a procedure to ﬁnd good convolutional
codes for use with iterative decoding (turbo equalization) of
the coded AWGN intersymbol interference (ISI) channel. The
method is based on the input–output bit error rate characteristic
of the convolutional BCJR module in the iterative decoder, in
combination with the code minimum distance. Both are essential
to ﬁnd best codes. Best feedforward and recursive systematic
codes are listed for memories 2–4, and some individual good codes
at memory 5. Codes are tested over a root raised-cosine faster
than Nyquist channel with severe ISI. Turbo BER performance
reaches 0.8–1.6 dB from capacity with a 4–16 state codes.
Feedforward codes perform better than recursive systematic.

a

Conv.
Encoder

Π

√
± Es

*
v
AWGN

ˆ
a

+−

+
y

CC−BCJR

LLRi

−1

Π

+−

ISI−BCJR

I. I NTRODUCTION

Π

LLR o

Turbo equalization is the name given to iterative decoding of
error-correcting codewords that have passed through a linear
intersymbol interference (ISI) channel. Most often in research
studies, the code is a simple rate 1/2 convolutional code. Figure
1 shows a transmitter–receiver model based on a discretetime AWGN channel. Binary convolutional codeword symbols
√
pass through an interleaver Π and are converted to a ± Es
stream u = u0 , u1 , . . . which convolves with a length-m unitenergy ISI response v = v0 , v1 , . . . , vm . To this discrete-time
sequence the channel adds independent Gaussian variates with
mean zero and variance N0 /2. ISI channels are common in
communication, and interleaving is a standard technique to
break up long ISI error events before they reach the errorcorrecting decoder. What turbo equalization adds is iterative
detection in which the decoding/demodulation is performed
by two cooperating BCJR algorithms, one for the ISI (the ISIBCJR) and one for the code (the CC-BCJR), that feed soft log
likelihood ratio (LLR) information to each other. After a time
the detection converges to a good estimate of the transmitted
data, if the signal-to-noise ratio (SNR) Es /N0 is above a
threshold. Ref. [2] is an excellent survey of recent research
in turbo equalization.
This paper studies how to ﬁnd good convolutional codes.
The iterative decoder places special constraints on the code,
so that the search cannot simply look for the largest Hamming
minimum distance dH . As shown in the ﬁgure, the LLR input
to each BCJR (the “intrinsic information”) is subtracted from
the BCJR’s output LLR before this information is passed to
the other BCJR. This acts to prevent false convergence and
is an effective technique in most applications. However, the
LLR subtraction markedly reduces the CC-BCJR performance

Fig. 1.

Coded ISI transmission with iterative decoding.

as a decoder. Furthermore, both the convergence threshold
and the the required number of iterations depend to some
degree on the code. We will develop an analysis that takes
into account intrinsic subtraction and apply it to ﬁnd good rate
1/2 convolutional codes at memories 2–5. Both feedforward
and recursive systematic (RS) codes are found; unlike the case
with traditional turbo decoding, feedforward are better.
In the earlier turbo equalization literature the RS code
(46,72) has sometimes been studied, e.g., [1], [3], [5].
[We use left-justiﬁed octal generator notation (g1 , g2 ) =
(10011, 11101) → (46, 72), where in the recursive case
the ﬁrst entry is the feedback polynomial; in right-justiﬁed
notation, (46,72) is (23,35)]. Little justiﬁcation is given for
this code; it is perhaps used because of its success in traditional turbo decoding. The most often used code is the
simple memory-2 (7,5) feedforward code, e.g., [4], [6], [8]. It
provides good error performance and a relatively early “soft”
convergence threshold. A few other codes have been studied
[7], but these often show a late, sharp convergence threshold.
What constitutes a good convolutional code depends only
weakly on the ISI response v. The severity of an ISI can
be measured by its state space size (here ≈ exp2 [number of
components of v with signiﬁcant energy]) and by its square
Euclidean minimum distance dmin ; some details are in Section
II. An interesting special case of large state size ISIs are those
with z-plane zeros on or near the unit circle. Most ISIs studied

1

in the literature are “easy”, meaning they have a combination
of large dmin , small state space, and zeros relatively far from
the circle. We intentionally choose a rather hard class called
faster than Nyquist (FTN) ISIs. In these, the spectrum of v is
narrowband and has in theory an entire zero spectral region,
which implies an inﬁnitely long v; in reality v is ﬁnite but
very long.
A long v that stems from spectral zeros means that some
sort of near-optimal but reduced ISI-BCJR needs to be employed in order to keep computation and memory reasonable.
Receiver performance can be lower if the ISI-BCJR is improperly conﬁgured. Reduced ISI-BCJR algorithms are a major
subject and are beyond the scope of this paper. The type in
this paper is based on a white noise model and is described
[4], [6], [8]; another type uses precoding[2].
We have two motivations for choosing our ISI class: We
want to test the convolutional codes with narrowband severe
ISI, and second, coded FTN is an interesting special case,
both in theory and in practice. FTN signals are ordinary linear
modulation signals of the form s(t) =
un h(t − nτ T ),
τ < 1, where the pulse h(t) is orthogonal to T shifts (i.e.,
“Nyquist”) but not to τ T shifts. With uncorrelated symbols
{un }, the average power spectral density (PSD) of s(t) is
proportional to H(f ), the Fourier transform of h(t), but s(t)
carries more symbols per second than orthogonal transmission,
by a factor 1/τ . For a given PSD shape, coded signals can be
constructed by means of the usual Nyquist pulses, but in terms
of bits/Hz-s carried, both the theoretical Shannon capacity and
the performance of practical schemes are markedly worse than
FTN constructions [9]. Uncoded FTN signals can double the
bits/Hz-s carried by OFDM transmission, in a given PSD and
at a given error rate. These facts motivate our interest in coded
FTN.
There are a number of ways a discrete-time v channel model
can be derived from an FTN signal; these are reviewed in [6],
[8]. The FTN model for the code searches in this paper is

in Section IV and some test results with best codes are given
in Section V.
II. E RROR R ATE B EHAVIOR IN I TERATIVE D ECODING
OF ISI
As shown in Fig. 1, each decoder BCJR algorithm produces
LLR values for the convolutional codeword symbols, from
which are subtracted the LLRs that were the input to that
BCJR. The outcome is apriori information for the other BCJR.
The CC-BCJR observes only this input, while the ISI-BCJR
also observes the channel outputs y. What circulates are LLR
values for the codeword symbols, not the data symbols. A
separate CC-BCJR calculation decides the data, presumably
after the last iteration. This calculation is not subject to LLR
subtraction and consequently the data bit error rate BERdat
is generally much lower than the CC-BCJR module’s output
LLR would imply; that is, intrinsic information subtraction
damages BERdat .
The decoder progress can be followed by tracking the
codeword symbol BER, the LLR statistics, or their mutual
information in the form of an EXIT chart. Lee et al. [5]
make a convincing case that symbol BER gives the most
direct and informative description. One plots the input–output
BER relationship as two curves, or “characteristics”, one for
the CC-BCJR and one for the ISI-BCJR. Both are plotted as
log BERi versus log BERo from the point of view of the CCBCJR module, in a picture like Fig. 2 (from the ISI-BCJR
point of view, input and output are reversed). The top lines are
CC-BCJR characteristics and the bottom line is the ISI-BCJR
characteristic. As the code block length grows, a law of large
numbers applies to each module, and the two lines determine
the behavior of the decoder. We will give properties of these
lines and argue for what consitutes a good convolutional code.
But ﬁrst we discuss an important empirical property of the
LLRs, that they are nearly Gaussian. This fact explains and
simpliﬁes many of the decoder properties.
It is often observed in the literature that the LLR inputs are
nearly Gaussian and independent. That they are independent is
a consequence of the interleavers before each BCJR module;
that they are Gaussian is easily veriﬁed by compiling a pdf.
When the codeword symbol BERs are below 0.2, our own
observations of the LLR variate z show good agreement with
the Gaussian pdf

v = .025,.012,-.024,.008,.191,.464,.623,.506,.176,
-.123, -.196,-.075,.060,.080,.013,-.035,-.022

(1)

The model has d2
min = .58 and derives from the pulse
h(τ t), where τ = .35 and h(t) is the standard root raisedcosine (rRC) pulse with 30% excess bandwidth.1 With rate
1/2 coding, the coded FTN signals carry (1/2)/τ T (1/2T ) =
1/τ = 2.86 bits/Hz-s, where bandwidth is taken as the 3dB
positive Hz bandwidth (on this scale, coded QPSK with pulse
h(t) achieves 1 bit/Hz-s). With the simple codes in this paper,
decoders can perform within a dB of the Shannon capacity
that corresponds to the signal PSD shape [8], [9].
In the next section we will review some basic properties of
the iterative decoder in Fig. 1, and then in Section III show
how these lead to a heuristic for choosing good convolutional
codes. The outcome of a search for best short codes is given

0.5
√
exp[(−(z − 4S)/2σ 2 ) + exp(−(z + 4S)/2σ 2 )],
2πσ 2
with σ 2 = 8S

(2)

which becomes more nearly perfect as the LLRs grow. The
quantity S here functions as a pseudo-channel SNR: Exactly
this distribution would occur if ±1 were the input to an AWGN
channel with SNR S and the BCJR observed only the channel
LLR. Consequently, apriori and channel-information LLRs are
almost indistinguishable.
An LLR model is thus as follows. The ISI-BCJR observes

1 This means that the spectrum of h(t) is zero, f > (1 + .3)/2T Hz; the
spectrum of the ﬁnite sequence v follows the RC shape and is more than
40dB below its peak in the stopband f > .35(1.3)/2T = .2275/T .

2

and subtracts zero intrinsic information; it achieves an error
rate near Q( d2 Es /N0 ) (details on this calculation appear
min
in [6], [8], [10]).2 . As the input BER tends to zero, apriori
information is perfect, and it can be shown that the output BER
tends to that of antipodal binary signaling, Q( 2Es /N0 ).
The CC-BCJR BER also displays a linear input–output
characteristic, this time in the loglog domain. With either
Gaussian apriori LLRs or actual turbo decoding, BERs fall on
one of a few straight lines, whose intersections with the ISI
characteristic are set mostly by their slopes.3 The straight line
behavior can be justiﬁed as follows. The Gaussian input LLRs
are equivalent to an AWGN channel with SNR S. A standard
coding gain calculation for the CC-BCJR without intrinsic
information subtraction shows that the input√
symbol BER is
√
converted from about Q( 2S) to about Q( 2dH S), which
√
√
has slope ≈ log[Q( 2S)]/ log[Q( 2dH S)], which is ≈ 1/dH
at all but small S. The intrinsic information subtraction forms
LLRo (n) − LLRi (n); these are correlated variates, but the
differences are again Gaussian, and once interleaved, they are
essentially independent over n. The straight line input–output
behavior remains but the correlation increases the slope to a
less favorable value.
We can now portray the iteration to iteration progress in an
EXIT-like way as shown in Fig. 2. The output of each module
becomes the input of the next until the ISI-BCJR output BER
(denoted BERi ) reaches the cross of the two characteristics.
Where this occurs depends on the CC-BCJR characteristic. If it
is ﬂat enough, the cross will occur at BERi ≈ Q( 2Es /N0 ).
If so, the CC-BCJR data decoder (no intrinsic subtraction)
converts this to the much lower value Q( 2dH Es /N0 ), the
convolutional code performance over an AWGN channel with
Es /N0 . If the characteristic is not ﬂat enough, the iterative
decoder is in the threshold region and will not reach this
performance.
Before ﬁnishing we will make the additional assumption
that the block length is large (the picture shows the average of
10 length-300000 data bit blocks), so that the statistical means
that form the ﬁgure are close to their weak-law asymptotic
values. The decoder in Fig. 2 works well at the 6 dB SNR
with block lengths as short as 2000–4000 data bits, but patterns
from successive blocks only roughly follow the CC and ISI
characteristics. Long block lengths mean the turbo decoder
will almost always stop after the same number of iterations
(5 in the ﬁgure). They are also needed when the SNR is
near the threshold for the decoder. In any case, we will
next base a convolutional code design heuristic on these two
characteristics, and the long block assumption means that the
decoder progress has a well-deﬁned relationship to them. In
effect, we are designing codes for the long block length case.

BERi

CC, (74,54)
& (46,72)

−1

10

CC, (7,5)
ISI,
Es/N0=6dB

1.5e−5
(Dat BER
= 1.3e−6)

1.4e−4
(Dat BER
= 8e−6)

BERo

−2

10

−5

10

−4

−3

10

10

−2

10

−1

10

0

10

Fig. 2. Input–output BER characteristics for 30%rRC τ = .35 ISI mechanism
and convolutional feedforward codes (7,5), (74,54) and (46,72); the last two
have virtually the same line. The intersection sets the ﬁnal BERs. Curves are
essentially the same whether taken from artiﬁcial Gaussian LLR inputs or
actual decoder LLRs. The heavy shaded area is the location of 10 turbo runs
of 300000 data bits.

the nth-symbol LLR
LLR(n) =
=

Pr(+1) exp(−(y − 1)2 Es /N0 )
Pr(−1) exp(−(y + 1)2 Es /N0 )
4yEs /N0 + ln[Pr(+1)/Pr(−1)]
ln

(3)
(4)

where y is the AWGN channel observation and
ln[Pr(+1)/Pr(−1)] is the apriori symbol LLR z, which is
distributed by the form (2). The ISI-BCJR observes both
channel and (independent) apriori LLRs, and these sum to
another Gaussian. The CC-BCJR does not observe channel
outputs, seeing only the apriori LLR term with pdf (2), which
could just as well come from an AWGN channel with SNR S.
LLRi (n), n = 1, 2, . . . , 2L, the LLR input to the CC-BCJR,
is a sequence of independent Gaussians z with pdf of form
(2), and similarly for the output from the CC-BCJR module
LLRo (n). L is the data block length.
We decide symbols in the usual way, namely that the nth
symbol is +1 if LLR(n) > 0 and −1 otherwise. From (2),
√
the model symbol probability of error is then Q( 2S) where
√
∞
Q(α) = α (1/ 2π) exp(u2 /2)du ≈ (1/2) exp(−S).
A plot of actual input–output log BERs observed in tests
for the ISI-BCJR module, including the instrinsic subtraction,
gives the lower line in Fig. 2 when v is (1), one line for
each Es /N0 . A sufﬁcient block length of symbols must be
processed (at least 40000) so that a reliable convergence of
the BERs to their mean occurs. Essentially the same curve
results with real turbo decoding or with artiﬁcial Gaussian
input LLRs. Lee et al. [5] observed that a plot without the logs
taken is nearly a straight line. The loglog curve is in any case
a simple convex one. Its beginning and ending points may be
computed theoretically. When the input BER is 0.5 (no apriori
information), the ISI-BCJR works only from channel outputs

2 Ref. [5] discusses the case where there is pre-processing before the BCJR,
and the ISI-BCJR does not necessarily approach full maximum likelihood
error performance
3 Ref. [5] did not observe this because of insufﬁcient data; the CC-BCJR
output has low BER and to observe 50–100 error events requires ca. 5 millon
symbols. It is not sufﬁcient to average short decoder blocks.

3

given. In all cases, both feedforward and RS, a code with
maximum textbook free distance was found that also had the
minimum slope for the memory and code type.
Within codes of the same distance and CC characteristic,
it is still possible to ﬁnd codes with different weight or bit
error distributions. This does not affect the CC-BCJR input–
output BER characteristic, and therefore does not affect the
iterative decoder progress, but it can change the BER at the
ﬁnal data bit decoding.4 The generators listed below have both
the optimum dH and slope possible in each memory category,
and lead to approximately the best post-threshold data bit BER
as measured by tests over an AWGN channel.

III. D EFINING B EST C ONVOLUTIONAL C ODES
We can conclude from the previous section that a convolutional code has a straight-line characteristic that determines
how it affects iterative decoding at longer block lengths. A
larger free distance dH will lead to a lower data BER at the
end of decoding, but the line’s slope has the larger effect
since it sets how far left along the ISI-BCJR characteristic
the iterations stop. A further-left termination implies a lower
BERi at the convolutional decoder, which in turn leads to a
lower data BER. Both of these parameters, dH and the CC
slope, contribute to error performance; which has the greater
effect depends the location of the ISI-BCJR characteristic,
which is in turn set by the AWGN channel Es /N0 .
We propose the following search scheme at each code
memory.
• Make a short list of codes with the most favorable CCBCJR characteristic. This is easily done by feeding codes
with Gaussian LLRs corresponding to output BERs near
.01 and .08. Decide symbols by the LLR sign. This
quickly yields two reliable BER statistics and because of
the strong straight-line tendency in the loglog plot they
give a good indication of the CC location.
• Investigate the best (lowest) CC slope codes in the list
by a more complete simulation.
• From the codes with best slope, take codes with best dH .
In every case, we have found codes that have both the
the approximate best slope and the known best dH for their
memory. These are listed in the next section. The outcome of
the search does not depend explicitly on the ISI characteristic.
ISI characteristics are roughly similar in shape (see e.g. [5])
and their location up or down in the plot depends chieﬂy on
SNR, so that the best convolutional slope is the same.
Under the Gaussian LLR assumption, the CC slope depends
solely on the correlation of the LLRi and LLRo sequences.
The can be measured statistically, but the procedure seems no
faster than the direct BER method given above. It would be
interesting to ﬁnd a way to derive the correlation directly from
the code generators.

Best Feedforward Codes
Memory 2: Only (7,5) is useful. Slope 1/3.9, dH = 5.
Memory 3: (74,54), (64,54). Slope 1/5.2, dH = 6.
Memory 4: (46,72), (66,62). Slope 1/5.2, dH = 7.
Memory 5: A good code is (62,57). Slope 1/6.5, dH = 8.
Best Recursive Systematic (RS) Codes
Memory 2: Only (7,5) is useful. Slope 1/3.9, dH = 5.
Memory 3: (54,64), (64,74), (64,54), (54,74). Slope 1/4.4,
dH = 6.
Memory 4: (46,56), (56,62), slope 1/5.1; (46,72), slope
1/4.85. dH = 7.
Here are some observations about these results. With feedforward codes no memory 4 code has better slope than the best
memory 3 codes; this is evident in Fig. 2, which shows that
both (74,54) and (46,72) intersect the ISI-BCJR characteristic
at about the same point. Yet (46,72) has dH = 7, while
(74,54) has 6, so when the data symbols are ﬁnally decoded
they should have lower BER. Data BER thus stems from two
primary sources, the CC-BCJR slope and dH . With RS codes,
the memory 4 has better slope than memory 3 — but at each
memory some feedforward code has better (lower) slope than
any RS code. It is interesting to observe that RS code (46,66),
which is the often used “(23,35)” parallel concatenated turbo
convolutional code, has inverse slope 4.8, which is worse
than the best memory-4 feedforward codes with the same dH .
Finally, we observe that there exist codes in a given category
with the best slope but not the best dH , as well as codes with
the best dH but not the best slope.

IV. B EST R ATE 1/2 C ODES
This section lists good codes found by the procedure just
given. We have searched over almost all feedforward and RS
generators with memory 2–4; the skipped codes are those
for which both generators have one or more ﬁnal zeros. A
more limited search was performed at memory 5. Feedforward
generators are considered to be equivalent if g1 and g2 are
exchanged, or if both are reversed [i.e., (6,4) becomes (5,4)].
These operations create codes with the same codeword weight
and bit error weight distributions; that is, the codes have
the same numbers of words at weights dH , dH + 1, . . ., and
the words at each weight lead to the same number of data
bit errors. We found no evidence that codes with the same
distributions lead to different turbo equalizer performance. In
what follows only one member of each equivalent group is

V. S OME E RROR P ERFORMANCE R ESULTS
Figure 3 shows the data BER performance versus Eb /N0 ,
both for turbo decoding with the target ISI model and for convolutional decoding alone over the AWGN channel (here the
energy/data bit Eb = 2Es ). The codes chosen are feedforward
(7,5), (74,54) and (46,72), with one point shown for the 32state code (62,57). The full curves are the convolutional-alone
BER, with turbo equalization shown by certain points. Before
threshold the turbo BER is high, typically 0.2; after threshold it
This Ô can be
moderate
È N Q(BER E /N ),estimatedNquite well atare caused to high SNR by
d
where
bit errors
by codewords of
4

k

k

b

0

k

Hamming weight dk . Typically, 2–3 terms are required.

4

−1

10

BER

−1

10

BER

−2

10
−2

10

−3

10
−3

10

10

(74,54)
Capacity,
30%rRC,τ=.35

−5

10

(46,72)

−5

10

(46,72)

(7,5)

(54,74)

−4

(7,5)
−4

10

(67,51)

E /N
b 0

−6

10

0

1

2

3

(62,57)

4

E /N

−6

5

10

6

Fig. 3. BER versus Eb /N0 in dB, comparing convolutional codes alone to
iterative decoding of coded ISI (τ = .35, 30%rRC FTN). Capacity for FTN
PSD shown for comparison; this is not C for the convolutional codes alone.

b

0

1

2

3

4

0

5

6

Fig. 4. BER versus Eb /N0 in dB for good convolutional codes alone,
comparing RS codes (dashed) to feedforward (solid) with the same generators.

whose spectrum includes null regions. With good codes, BER
performance is .8–1.6 dB from the capacity that applies to
these signals.

closely follows the convolutional-alone BER; we do not show
test results in either of these regions. The ISI-BCJR is the MBCJR method of [6], [8], with the M parameter set to 10–300,
depending on the code and SNR. At the left is the Shannon
BER capacity for the ISI channel.5 Turbo BER performance
approaches within .8–1.6 dB of capacity, depending on which
code is employed (the product of M-BCJR paths and iterations
needed to achieve this is around 3000, and data block length
40000 is required). Note that longer-memory codes do not
approach capacity as closely at larger BER (> 10−3 ), but they
approach progressively more closely at lower BER.
Figure 4 compares the BER performance of some good
feedforward and RS codes with the same (g1 , g2 ), with
convolutional-alone decoding. Representatives of all the best
code categories in Section IV are included here. The results
show that generally speaking RS codes produce somewhat
poorer data BER than feedforward codes, once Eb /N0 lies
above threshold and the iterative decoding BER settles to the
underlying convolutional decoding BER. This is the opposite
of the case with parallel concatenation convolutional turbo
coding.

ACKNOWLEDGMENTS
This work was supported by the Swedish Research Council
(VR) through Grant 621-2003-3210, and by the Swedish
Foundation for Strategic Research (SSF) through its Strategic
Center for High Speed Wireless Communication at Lund.
R EFERENCES
[1] C. Douillard et al., “Iterative correction of intersymbol interference: Turbo
equalization,” Eur. Trans. Telecomm., 6, pp. 507–511, Sept./Oct. 1995.
[2] M. T¨ chler, A.C. Singer, “Turbo equalization: An overview,” IEEE Trans.
u
Information Theory, 57, Feb. 2011, pp. 920–952.
[3] G. Colavolpe, G. Ferrari, R. Raheli, “Reduced-state BCJR type algorithms,” IEEE J. Sel. Areas Communs., 19, pp. 848–859, May 2001.
[4] D. Fertonani, A. Barbieri, G. Colavolpe, “Reduced-complexity BCJR
algorithm for turbo equalization,” IEEE Trans. Communs., 55, pp. 2279–
2287, Dec. 2007.
[5] S.-J. Lee, A.C. Singer, N.R. Shanbhag, “Linear turbo equalization analysis
via BER transfer and EXIT charts,” IEEE Trans. Signal Proc., 53, pp.
2883–2897, Aug. 2005.
[6] J.B. Anderson, A. Prlja, “Turbo equalization and an M-BCJR algorithm
for strongly narrowband intersymbol interference,” Proc., Int. Symp.
Information Theory and Its Applications, Taichung, Taiwan, pp. 261–266,
Oct. 2010.
[7] M. McGuire, M. Sinha, “Discrete time faster-than-Nyquist signaling,”
Proc., IEEE Global Communs. Conf., Miami, Dec. 2010.
[8] A. Prlja, J.B. Anderson, “Reduced-complexity receivers for strongly
narrowband intersymbol interference introduced by faster than Nyquist
signaling,” to appear, IEEE Trans. Communs., 2012.
[9] F. Rusek, J.B. Anderson, “Constrained capacities for faster-than-Nyquist
signaling,” IEEE Trans. Information Theory, 55, pp. 764–775, Feb. 2009.
[10] J.B. Anderson, A. Svensson, Coded Modulation Systems, KluwerPlenum, New York, 2003.
[11] J.B. Anderson, F. Rusek, “The Shannon bit error limit for linear coded
modulation,” Proc., Int. Symp. Information Theory and Its Applications,
Parma, Italy, Oct. 2004, pp. 9–11.

VI. C ONCLUSION
We have used the near-Gaussian nature of LLRs to create an
efﬁcient procedure to ﬁnd good short-memory convolutional
codes for use in turbo equalization. What codes are best is
strongly driven by the intrinsic information subtraction in the
iterative decoding. Consequently, decoder BER depends not
only on code minimum distance, but also on an input–output
BER slope parameter. The codes are derived for and tested
against severe ISI derived from faster than Nyquist signals,
5 This is the location of the constrained capacity for systems with the ISI
PSD, at the BER and Eb /N0 combination. How to calculate such capacities
is shown in [11]; capacities at a given PSD are discussed in [8], [9]. Plotting
the traditional “brickwall” spectrum capacity instead of the true PSD capacity
will produce a curve to the right that is too optimistic.

5

