Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Sun May 13 17:04:59 2012
ModDate:        Tue Jun 19 12:55:34 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      438227 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565887

Joint Source-Channel Coding for Cribbing Models
Eliron Amir and Yossef Steinberg
Dept. of EE, Technion, Israel
{eliron@tx,ysteinbe@ee}.technion.ac.il

Abstract— In this work we study problems of joint source-channel
coding for the multiple access channel with cribbing encoders. These
problems are motivated by modern communication scenarios such as
an uplink channel for cellular users. Provided that the users are close
enough to each other, they can causally crib to the output signals
of their neighbours, thus obtaining some measure of cooperation.
Three scenarios are considered in this work: (i) the symmetric model,
where both encoders crib strictly causally at each other’s output, (ii)
the model where encoder 1 cribs strictly causally at the output of
encoder 2, and encoder 2 cribs causally at the output of encoder
1, and (iii) the model where only one encoder cribs, in a causal
or non-causal manner. For the symmetric case, model (i), sufﬁcient
conditions are derived for lossless transmission of a correlated pair
source (U, V ) via the multiple access channel (MAC). For the nonsymmetric scenarios (ii) and (iii), necessary and sufﬁcient conditions
are derived, for transmissibilty of the pair source via the MAC. The
main focus of this work is on lossless transmission, however, for case
(iii) we allow distortion in one of the source components.

•

Strictly causal cribbing by encoder 2:
m
f1 : U n → X1
i−1
f2,i : V n × X1 → X2,i

•

m
f1 : U n → X1
i
f2,i : V n × X1 → X2,i
•

m
f1 : U n → X1
m
f2,i : V × X1 → X2,i

f2,i : V ×
•

A discrete memoryless multiple access channel (MAC) is a
quadruple X1 , X2 , PY |X1 ,X2 , Y where X1 , X2 are the input
alphabets, Y the output alphabet, and PY |X1 ,X2 a transition probability matrix from X1 × X2 to Y. For shorthand notation, we
will refer to the MAC by PY |X1 ,X2 . A source pair is a triplet
{U, V, PU,V } where PU,V is a distribution on U × V. We use
the following notation: random variables are denoted by uppercase
letters. Sequence of n letters from alphabet X is denoted by xn ,
and the substring xi , xi+1 , . . . , xj as xj . When the dimension n
i
is understood from the context, n-vectors are denoted by boldface
letters: xn = x. Similar notation holds for random variables and
random vectors, e.g., V , U, U n , etc. The channel and source are
assumed memoryless, thus probabilities of n-sequences are given
by

(2)

i = 1, . . . , m

(3)

Strictly causal cribbing by both encoders:
i−1
f1,i : U n × X2 → X1,i

I. N OTATION

i−1
X1

→ X2,i

i = 1, . . . , m
i = 1, . . . , m

(4)

Strictly causal cribbing by encoder 1 and Causal cribbing by
encoder 2:
i−1
f1,i : U n × X2 → X1,i
n

f2,i : V ×

i
X1

→ X2,i

i = 1, . . . , m
i = 1, . . . , m

(5)

An (n, m, ) joint source-channel code for lossless transmission of
the source PU,V via the channel PY |X1 ,X2 consists of a pair of
encoding functions, according to one of the models (1), (2), (3),
(4), or (5), and a decoding function
dm : Y m → U n × V n
such that the probability of error is bounded from above by :
Pe = P r {(U n , V n ) = dm (Y m )} ≤ .
The rate of the code is deﬁned as ρ = n/m. A source pair (U, V )
is said to be transmissible via the MAC PY |X1 ,X2 with strictly
causal cribbing at rate ρ if for every > 0, δ > 0, and sufﬁciently
large n there exists an (n, n/(ρ − δ), ) code for the source pair
and the channel.
We are interested also in the case where one of the sources is
ˆ
transmitted with distortion. Thus let V be the reproduction alphabet
ˆ → [0, ∞) be a single letter
of the source V , and let d : V × V
distortion function. Distortion between sequences is deﬁnes as the
normalized sum of single letter distortions:

n

PY |X1 ,X2 (yi |x1,i , x2,i )
i=1
n

PU,V (u, v) =

i = 1, . . . , m

Non causal cribbing by encoder 2:
n

•

(1)

Causal cribbing by encoder 2:

n

PY |X1 ,X2 (y|x1 , x2 ) =

i = 1, . . . , m

PU,V (ui , vi )
i=1

II. P ROBLEM DEFINITION AND PREVIOUS RESULTS
We investigate a joint source-channel coding schemes for the
discrete memoryless MAC with cribbing encoders. The sources,
U and V , deliver their output to two separate encoders 1 and 2.
Several cribbing models will be discussed, each of them deﬁned
by a set of encoding functions:

d(v n , v n ) =
ˆ

1
n

n

d(vi , vi ).
ˆ

(6)

i=1

For simplicity of notation it is understood in the sequel that the
source U is transmitted without distortion. An (n, m, D, ) joint

This research was supported by the ISAREL SCIENCE FOUNDATION (grant
No. 684/11)

1

PX1 ,X2 (x1 , x2 ) PY |X1 ,X2 (y | x1 , x2 )

source channel code with non-causal cribbing by encoder 2 consists
of a pair of encoders (3) and a pair of decoders
dm : Y m → U n
u
ˆ
dm : Y m → V n
v

Theorem 2. Slepian & Wolf [4]: For the distributed lossless
source coding problem with source (U, V ) drawn i.i.d ∼ PU,V ,
the achievable rate region is given by:

(7)

such that the probability of error in decoding U does not exceed
:

H (U | V ) ≤ R1

(11)

H (V | U ) ≤ R2

(u)
Pe = P r {U n = dm (Y m )} ≤ .
u

(12)

H (U, V ) ≤ R1 + R2

and the average distortion in decoding V is at most D:

(13)

III. S TATEMENT OF RESULTS

Ed(V n , dm (Y m )) ≤ D
v

Our ﬁrst result states sufﬁcient conditions for transmissibility
when the two encoders crib causally, that is, when the encoding
functions are given by (4).

Transmissibility conditions for the communication models deﬁned by encoders (1) and (2) were presented in [1]. We are
interested in transmissibility conditions for models (3), (4) and (5).
We start by stating previous relevant results, on capacity regions of
MAC with cribbing ([5]), and on distributed source coding ([4]).
We cite only the results that are directly relevant to us; the reference
to these works is by no means comprehensive.

Theorem 3. A source (U, V ) ∼ PU,V is transmissible via the
MAC PY |X1 ,X2 with strictly causal cribbing by both encoders at
rate ρ = 1 if
H (U | V ) ≤ H (X1 | W, V )

(14)

H (V | U ) ≤ H (X2 | W, U )

(15)

H (U, V ) ≤ I (Y ; X1 , X2 )

Theorem 1. Willems & van der Meulen [5]: The capacity region
for causal and non-causal cribbing by encoder 2 is given by the
collection of all rate pairs (R1 , R2 ) such that

(16)

for some distribution of the form

R1 ≤ H (X1 )

(8a)

R2 ≤ I (X2 ; Y | X1 )

(8b)

PU,V,W,X1 ,X2 ,Y (u, v, w, x1 , x2 , y) =

(8c)

PU,V (u, v) PW (w) PX1 |W,U (x1 | w, u) ·

R1 + R2 ≤ I (X1 , X2 ; Y )

PX2 |W,V (x2 | w, v) PY |X1 ,X2 (y | x1 , x2 )

for some distribution of the form

where W is an external random variable whose size can be
bounded as |W| ≤ min {|X1 | · |X2 | + 1, |Y| + 2}.

PX1 ,X2 ,Y (x1 , x2 , y) =
PX1 ,X2 (x1 , x2 ) PY |X1 ,X2 (y | x1 , x2 )

The proof of Theorem 3 is omitted due to space considerations.
We proceed now to state necessary and sufﬁcient conditions for
transmission according to the model (5).

The capacity region for strictly causal cribbing by both encoders
is given by the collection of all rate pairs (R1 , R2 ) such that
R1 ≤ H(X1 | W )

(9a)

R2 ≤ H(X2 | W )

(9b)

R1 + R2 ≤ I (X1 , X2 ; Y )

Theorem 4. A source (U, V ) ∼ PU,V is transmissible via the
MAC PY |X1 ,X2 with strictly causal cribbing by the ﬁrst encoder
and causal cribbing by the second encoder at rate ρ if and only if
there exists a joint input distribution PX1 ,X2 such that:

(9c)

for some distribution of the form

ρH (U | V ) ≤ H (X1 )

PW (w) PX1 |W (x1 | w) ·

(18)

ρH (U, V ) ≤ I (Y ; X1 , X2 )

PX2 |W (x2 | w) PY |X1 ,X2 (y | x1 , x2 )

R1 ≤ H(X1 )

(10a)

R2 ≤ H(X2 | X1 )

(19)

The proof of the achievability part of Theorem (4) is based on
separation and resembles the proof for causal cribbing only by
encoder 2, presented in [1]. It is omitted due to space considerations. The proof of the converse part is given in section V. We turn
next to the case of non-causal cribbing by encoder 2, that is, the
transmission model described by (3). Here U is decoded losslessly,
and we allow distortion for the source V .

where W is an external random variable whose size can be
bounded as |W| ≤ min {|X1 | · |X2 | + 1, |Y| + 2}.
The capacity region for strictly causal cribbing by encoder 1
and causal cribbing by encoder 2 is given by the collection of all
rate pairs (R1 , R2 ) such that
(10b)

R1 + R2 ≤ I (X1 , X2 ; Y )

(17)

ρH (V | U ) ≤ H (X2 | X1 )

PW,X1 ,X2 ,Y (w, x1 , x2 , y) =

Theorem 5. A source (U, V ) ∼ i p (ui , vi ) can be sent with
arbitrarily small probability of error Pe for U and distortion D
for V over a multiple access channel, with causal or non-causal
cribbing by the second encoder at rate ρ = 1 if and only if

(10c)

for some distribution of the form

H (U | V ) ≤ H (X1 )

PX1 ,X2 ,Y (x1 , x2 , y) =

2

(20)

(21)

H (U ) + I (V ; W | U ) ≤ I (Y ; X1 , X2 )
Ed (V, W ) ≤ D

= I (V n ; Y m | U n ) + H (V n | Y m , U n )

(22)

I (V ; W | U ) ≤ I (Y ; X2 | X1 )

≤ I (V n ; Y m | U n ) + mδ (Pe )
≤

(23)

=

for some

≤

PU,V,W,X1 ,X2 ,Y (w, u, v, x1 , x2 , y) =
PU,V (u, v) PW |U,V (w | u, v) PX1 ,X2 (x1 , x2 ) PY |X1 ,X2 (y | x1 , x2 )

=

where W is a random variable deﬁned on the reconstruction
ˆ
alphabet V.

=

An outline of the proof of Theorem 5 for non-causal cribbing
is given in sections VI and VII.

m
I (V , X2 ; Y m | U n ) + mδ (Pe )
m
I (X2 ; Y m | U n ) + mδ (Pe )
m
H (X2 | U n ) + mδ (Pe )
m
i−1
H X2i | U n , X2
+ mδ (Pe )
i=1
m
i−1
H X2i | U n , X2 , X1i + mδ (Pe )
i=1
m

≤

IV. D ISCUSSION
Theorem 3 provides an achievability scheme in which the
channel code is based on the source messages. This scheme does
not use the separation principle. Theorem 4 implies that a source
PU,V is transmissible via the MAC PY |X1 ,X2 with cribbing model
deﬁned by (5) at rate ρ if and only if the Slepian-Wolf region
scaled by ρ intersects the capacity region of the MAC with the
same cribbing model. This implies that for model (5), a separation
strategy, where we use a Slepian-Wolf code to compress the source
and a channel code to transmit the source codewords via the MAC,
is optimal.
Theorem 5 reﬂects a weaker form of separation. The encoders
construct the channel codebooks independent of the sources. However, encoder 2 must decode encoder’s 1 message in order to
choose the appropriate compressed word w. Thus, separation is
preformed only in the code design stage but not operatively.

(29)

n

H (X2i | X1i ) + mδ (Pe )

(30)

(31)
(32)

i=1

Here (29) follows from Fano inequality, (30) follows from the
m
m
Markovity of (U n , V n ) → (X1 , X2 ) → Y m , and (31) follows
from the encoding equations (5).
n

H (Ui , Vi ) = H (U n , V n )
i=1

= I (U n , V n ; Y m ) + H (U n , V n | Y m )
≤ I (U n , V n ; Y m ) + mδ (Pe )

(33)

m

I U n , V n ; Yi | Y i−1 + mδ (Pe )

=
i=1
m

I U n , V n , X1i , X2i ; Yi | Y i−1

≤
i=1

+mδ (Pe )
m

V. P ROOF OF THE CONVERSE PART OF T HEOREM 4
Proof: Suppose that for an i.i.d source PU,V (u, v) and a MAC
PY |X1 ,X2 with a cribbing model deﬁned in (5), there exists a
(n, n/ρ, )-code. We then have the following inequalities:

I (U n , V n , X1i , X2i ; Yi ) + mδ (Pe )

≤
i=1
m

=

n

I (X1i , X2i ; Yi ) + mδ (Pe )

(34)

i=1

H (Ui | Vi ) = H (U n | V n )

Here (33) follows from Fano inequality, and (34) is due to the
Markovity of (U n , V n ) → (X1i , X2i ) → Yi

i=1
n

m

| V ) + H (U | Y , V )

n

m

| V n ) + mδ (Pe )

= I (U ; Y
≤ I (U ; Y
≤
=
≤
=

n

n

n

m

m
I (U , X1 ; Y m | V n ) + mδ (Pe )
m
I (X1 ; Y m | V n ) + mδ (Pe )
m
H (X1 | V n ) + mδ (Pe )
m
i−1
H X1i | X1 , V n + mδ (Pe )
i=1
m

≤

H (X1i ) + mδ (Pe )

n

(24)

PU,V,X1 ,X2 ,Y (ui , vi , x1i , x2i , yi )

(25)

= PU,V (ui , vi ) PX1 ,X2 |U,V (x1i , x2i | ui , vi ) ·

(26)

PY |X1 ,X2 ,U,V (yi | x1i , x2i , ui , vi )
= PU,V (ui , vi ) PX1 ,X2 |U,V (x1i , x2i | ui , vi ) ·

(35)

PY |X1 ,X2 ,U,V (yi | x1i , x2i )
Observe that the right hand side of (27),(32),(34) (that is, the
"channel coding part") is independent of the source and the reconstruction, (Ui , Vi , Wi ). Therefore it is possible to choose X1 , X2
independent of Ui , Vi , Wi (35), without affecting the constraints.
Deﬁning X1 = (X1Q , Q) , X2 = (X2Q , Q) , Y = (YQ , Q),
enables us to use the standard time-sharing argument, resulting in
the ﬁnal conditions of Theorem (4). Thus, the converse is proved.

(27)

i=1

Here (24) follows from Fano inequality, where:
(Pe log (|U| |V| − 1) + H (Pe ))
(28)
n
n
n
and (26) follows from the Markovity of (U n , V n ) → (X1 , X2 ) →
n
Y .
δ (Pe )

n

VI. P ROOF OF THE ACHIEVABILITY PART OF T HEOREM 5

H (Vi | Ui ) = H (V n | U n )

Proof: We will use ρ = 1 throughout the proof.

i=1

3

1) P r {E } < : Due to the AEP.
2) P r {E v } < : By classical rate-distortion arguments.
3) P r E u :

A. Codebook Generation
Fix PW |U,V (w | u, v) , PX1 (x1 ), PX2 |X1 (x2 | x1 ) satisfying the
conditions of Theorem 5. Let RV |U = I (V ; W | U ) + δ for some
δ > 0.
1) Generate 22nRV |U codewords w of length n, sampled iid
from a marginal distributions PW |U (w | u) and assign them
to a bin denoted by B (u).
2) For every pair (u, v) ﬁnd a sequence w ∈ B (u) such that
(u, v, w) ∈ A (U, V, W ). If no sequence is found, choose
randomly. Label it w (v | u) .
3) For every sequence u generate a sequence x1 with probabiln
ity P r {x1 } = i=1 p (x1i ) . Label it x1 (u).
4) For every pair (w (v | u) , x1 ) generate a sequence x2
n
with probability P r {x2 } =
i=1 p (x2i | x1i ) . Label it
x2 (w, x1 ).

= P r {∃u = u : x1 (u ) = x1 (u)}
P r {x1 (u ) = x1 (u)}

=
u =u:
(u ,v)∈A

2−nH(X1 )

≤
(u

≤ 2nH(U |V ) 2−nH(X1 )
4) P r E uw :
= P r {∃w = w : (u, w , x1 (u) , x2 (w , x1 ) , y)
∈ A (U, W, X1 , X2 , Y ) , w ∈ B(u)}

B. Encoding:
Given a source sequences u, encoder 1 transmits the codeword
ˆ
x1 (u). Encoder 2 estimates u by cribbing into x1 , and then
ˆ
transmits x2 (w (v | u) , x1 ).

=

C. Decoding:
Given a channel output sequence y the decoder looks for a
ˆ
source sequence u and a “compressed” sequence w such that
ˆ

≤

∈ A (X1 , X2 , Y )}
w =w:

= 2nI(V ;W |U ) 2−nI(Y ;X2 |X1 )
where we used,
P r {(x1 (u) , x2 (w , x1 ) , y)

D. Error Probability:
Suppose that (u, v) was the source output pair, then an error is
made if
1) (u, w, x1 (u), x2 (w, x1 ), y) ∈ A (U, W, X1 , X2 , Y )
/
2) There exist some (u , v ) = (u, v) such that

∈ A (X1 , X2 , Y )}
(x1 ,x2 ,y)∈A (X1 ,X2 ,Y )

(x1 ,x2 ,y)∈A (X1 ,X2 ,Y )

≤ 2nH(X1 ,X2 ,Y ) 2−nH(X1 ,Y ) 2−nH(X2 |X1 )
= 2−nI(X2 ;Y |X1 )

The probabilities of the error events are as follows:

Eu w
Eu w

2−nH(X1 ,Y ) 2−nH(X2 |X1 )

≤

w ∈ B(u )

E uw

P r {x1 , y} P r {x2 | x1 }

=

(u , w , x1 (u ), x2 (w , x1 ), y) ∈ A (U, W, X1 , X2 , Y )

Eu

2−I(Y ;X2 |X1 ) ≤ |{w }| 2−nI(Y ;X2 |X1 )
(u,w )∈A

ˆ
and w ∈ B (ˆ ). If no such sequences are found, the decoder
u
declares an error and incurs a distortion equal to Dmax .

Ev

P r {(x1 (u) , x2 (w , x1 ) , y)
w =w:
(u,w )∈A

ˆ
(ˆ , w, x1 (ˆ ) , x2 (w, x1 ) , y) ∈ A (U, W, X1 , X2 , Y )
u ˆ
u

Events
E

u =u:
,v)∈A

u
u
5) P r Eb w = P r Eb w : Where the equality above is
due to binning. That is, if the decoder falsely decodes u then it
must also falsely decode w since w ∈ B (u) .
u
6) P r Eb w :

Description
(u, w, x1 (u), x2 (w, x1 ), y)
∈ A (U, W, X1 , X2 , Y )
/
No sequence w ∈ B (u) was found
such that (u, v, w) ∈ A (U, V, W )
An error was made by encoder 2
in decoding u.
An error was made by the decoder
in decoding w.
An error was made by the decoder
in decoding u.
An error was made by the decoder in decoding
both w and u.

= P r {∃u = u, w = w : (u , w , x1 (u ) , x2 (w , x1 ) , y)
∈ A (U, W, X1 , X2 , Y ) , w ∈ B(u )}
P r {(x1 (u ) , x2 (w , x1 ) , y)

=
u =u,w =w:
(w ,u )∈A

∈ A (X1 , X2 , Y )}
2−nI(Y ;X1 ,X2 )

≤
u =u,w =w:
(w ,u )∈A

≤ |{w }| |{u }| 2−nI(Y ;X1 ,X2 )

The event that an error was made at the decoder E, is the union
of the above events:

= 2nI(V ;W |U ) 2nH(U ) 2−nI(Y ;X1 ,X2 )

E = E ∪ E v ∪ E u ∪ E uw ∪ E u w ∪ E u w

where we used
P r {(x1 (u ) , x2 (w , x1 ) , y)

We denote the total probability of error with P e .

4

∈ A (X1 , X2 , Y )}
n

P r {x1 , x2 } P r {y}

=

H (Ui ) + I (Vi ; Wi | Ui )

(x1 ,x2 ,y)∈A (X1 ,X2 ,Y )

i=1

2−nH(X1 ,X2 ) 2−nH(Y )

≤

n
n

i=1
n

nH(X1 ,X2 ,Y ) −nH(X1 ,X2 ) −nH(Y )

≤2

2

2

= 2−nI(Y ;X1 ,X2 )

≤ H (U n ) +

It follows that for n large enough, P e <
satisﬁes the conditions of Theorem 5.

≤ H (U n ) + I (V n ; Y m | U n )
= H (U n | Y m ) + I (V n , U n ; Y m )
≤ I (V n , U n ; Y m ) + mδ (Pe )

i=1
n

Proof: Suppose that for an i.i.d source PU,V (u, v) and a
MAC PY |X1 ,X2 with a cribbing model deﬁned in (3), there
exists a (n, n/ρ, )-code. Deﬁne Wi
φi (Y m ), where φi is a
deterministic function of Y m . We have the following inequalities:
H (Ui | Vi ) ≤

i=1
m

i=1
m

H (X1i ) + nδ (Pe )

=

i=1

n

H (Vi | Ui ) −

i=1

H (Vi | Ui , Wi )

I (Vi ; Wi | Ui ) ≤
n

H Vi | U , W

n

, V1i−1

i=1

i=1

= I (V n ; W n | U n ) ≤ I (V n ; Y m | U n )

i=1

m

i=1
m

(37)

i=1
m

[1] E. Amir, Y. Steinberg, “The Multiple Access Channel with Correlated Sources
and Cribbing Encoders",” to appear at the 2012 International Zurich Seminar
on Communications, Zurich, Switzerland, Feb. 29 - Mar. 2, 2012.
[2] T. M. Cover, A. A. E. Gamal, and M. Salehi, “Multiple access channels with
arbitrarily correlated sources,” IEEE Trans. Inform. Theory, vol. 26, No. 6,
pp. 648–657, November 1980.
[3] I. Csiszár and J. Körner, Information Theory: Coding Theorems for Discrete
Memoryless Systems, Academic Press, London, 1981.
[4] D. Slepian and J. K. Wolf, “A coding theorem for multiple access channels
with correlated sources,” Bell Syst. Tech. J., vol. 52, pp. 1037- 1076, Sept.
1973.
[5] Frans M.J. Willems and Edward C. van der Meulen, “The discrete memoryless
multiple access channel with cribbing encoders”. IEEE Trans. Inform. Theory,
vol. 31, No. 3, pp. 313-327, May 1985.

i=1
m
m
I (V n , X2i ; Yi | U n , X1 )
i=1
m

m
m
I (X2i ; Yi | U n , X1 ) ≤

=
i=1

I (X2i ; Yi | X1i )

I (X1i , X2i ; Yi ) + mδ (Pe )
i=1

R EFERENCES

m
I V n , X2i ; Yi | U n , Y i−1 , X1

≤

m

As for the joint distribution, the Markov conditions are satisﬁed.
We omit the details due to space considerations. Thus, it is possible
to choose an arbitrary input joint distribution PX1 ,X2 . By choosing
ρ = 1 and using the time-sharing argument we get the conditions
of Theorem 5.

I V n ; Yi | U n , Y i−1
m
I V n ; Yi | U n , Y i−1 , X1

I (X2i ; Yi | X1i ) + mδ (Pe ) (42)

H (Ui ) + I (Vi ; Wi | Ui ) ≤
(36)

(41)

i=1

n

= H (V n | U n ) − H (V n | U n , W n )

≤

H (X1i ) + mδ (Pe )
i=1
m

n

i=1
n

H (Vi | Ui ) −

m

H (Ui | Vi ) ≤

n

i=1
n

(40)

where (39) is due to Fano inequality, and (40) due to the Markovity
of (U n , V n ) → (X1i , X2i ) → Yi . Therefore we obtained:

I (Vi ; Wi | Ui )

=

I (X1i , X2i ; Y ) + mδ (Pe )
i=1

n

=

I (V n , U n , X1i , X2i ; Yi ) + mδ (Pe )

≤

see (27).

i=1
n

I V n , U n , X1i , X2i ; Yi | Y i−1 + mδ (Pe )

≤

m

i=1

I V n , U n ; Yi | Y i−1 + mδ (Pe )

=

VII. P ROOF OF THE CONVERSE PART OF T HEOREM 5

n

(39)

m

ˆ
we have D ≤ D + .

i=1

i=1

= H (U n ) + I (V n ; W n | U n )

ˆ
D ≤ (1 − Pe ) D + Pe Dmax .

≤

H Vi | U n , W n , V1i−1

H (Vi | Ui ) −

= H (U n ) + H (V n | U n ) − H (V n | U n , W n )

The distortion of the proposed scheme is bounded by

=

H (Vi | Ui , Wi )
i=1
n

i=1

if the source PU,V

E. Distortion:

Since Pe ≤

H (Vi | Ui ) −

= H (U ) +

(x1 ,x2 ,y)∈A (X1 ,X2 ,Y )

n

(38)

i=1

where (36) follows from the deﬁnition of Wi
φi (Y m ), (37)
from the encoding equations (3), and (38) from the Markovity of
(U n , V n ) → (X1i , X2i ) → Yi .

5

