Title:          ga_isit12-diff-LN.dvi
Creator:        dvips(k) 5.991 Copyright 2011 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Tue May 15 10:44:30 2012
ModDate:        Tue Jun 19 12:55:39 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      345699 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566917

Message-passing sequential detection of multiple
change points in networks
2

XuanLong Nguyen1, Arash Ali Amini1 and Ram Rajagopal2
1 Department of Statistics, University of Michigan
Department of Civil and Environmental Engineering, Stanford University
detection delay can be expressed in terms of Kullback-Leibler
divergences deﬁned along edges of the network structure.
We provide simulations that demonstrate both statistical and
computational efﬁciency of our approach.
Related Work. The rich statistical literature on sequential
analysis tends to focus almost entirely on the inference of a
single change point variable [1]. There are recent formulations
for sequential diagnosis of a single change point, which may
be associated with multiple causes [7], or multiple sequences
[8]. Another approach taken in [9] considers a change propagating in a Markov fashion across an array of sensors. These
are interesting directions but the focus is still on detecting
the onset of a single event. Graphical models have been
considered for distributed learning and decentralized detection
before, but not in the sequential setting [10], [11]. This paper
follows the line of work of [5], [12], but our formulation
based on graphical models is more general, and we impose
less severe constraints on the amount of information that can
be exchanged across network sites.

Abstract—We propose a probabilistic formulation that enables
sequential detection of multiple change points in a network
setting. We present a class of sequential detection rules for functionals of change points, and prove their asymptotic optimality
properties in terms of expected detection delay time. Drawing
from graphical model formalism, the sequential detection rules
can be implemented by a computationally efﬁcient messagepassing protocol which may scale up linearly in network size and
in waiting time. The effectiveness of our exact and approximate
inference algorithms are demonstrated by simulations.

I. I NTRODUCTION
Classical sequential detection is the problem of detecting
changes in the distribution of data collected sequentially over
time [1]. In a decentralized network setting, the decentralized
sequential detection problem concerns with data sequences
aggregaged over the network, while sequential detection rules
are constrained to the network structure (see, e.g., [2], [3],
[4]). The focus was still on a single change point variable
taking values in (discrete) time. In this paper, our interests lie
in sequential detection in a network setting, where multiple
change point variables may be simultaneously present.
As an example, quickest detection of trafﬁc jams concerns
with multiple potential hotspots (i.e., change points) spatially
located across a highway network. A simplistic approach is
to treat each change point variables independently, so that the
sequential analysis of individual change points can be applied
separately. However, it has been shown that accounting for the
statistical dependence among the change point variables can
provide signiﬁcant improvement in reducing both false alarm
probability and detection delay time [5].
This paper proposes a general probablistic formulation for
the multiple change point problem in a network setting,
adopting the perspective of probabilistic graphical models for
multivariate data [6]. We consider estimating functionals of
multiple change points deﬁned globally and locally across the
network. The probablistic formulation enables the borrowing
of statistical strengh from one network site (associated with
a change point variable) to another. We propose a class of
sequential detection rules, which can be implemented in a
message-passing and distributed fashion across the network.
The computation of the proposed sequential rules scales up
linearly in both network size and in waiting time, while an
approximate version scales up constantly in waiting time.
The proposed detection rules are shown to be asymptotically
optimal in a Bayesian setting. Interestingly, the expected

II. G RAPHICAL

MODEL FOR MULTIPLE CHANGE POINTS

In this section, we shall formulate the multiple change
point detection problem, where the change point variables and
observed data are linked using a graphical model. Consider a
sensor network with d sensors, each of which is associated
with a random variable λj ∈ N, for j ∈ [d] := {1, 2, . . . , d},
representing a change point, the time at which a sensor fails to
function properly. We are interested in detecting these change
points as accurately and as early as possible, using the data
that are associated with (e.g., observed by) the sensors. Taking
a Bayesian approach, each λj is independently endowed with
a prior distribution πj (·).
A central ingredient in our formalism is the notion of a
statistical graph, denoted as G = (V, E), which speciﬁes the
probabilistic linkage between the change point variables and
observed data collected in the network. The vertex set of the
graph, V = [d] represents the indices of the change point
variables λj . The edge set E represents pairings of change
point variables, E = {e = {s1 , s2 } | s1 , s2 ∈ V }. With each
vertex and each edge, we associate a sequence of observation
variables,
1
2
Xj = (Xj , Xj , . . . ),

1

j ∈ V,

(1)

1
2
Xe = (Xe , Xe , . . . ),

e ∈ E,

(2)

where the superscript denotes the time index. The Xj models
the private information of node j, while Xe models the
shared information of nodes connected by e. We will use the
1
n
notation Xn = (Xj , . . . , Xj ) and similarly for Xn ; notice
e
j
n
the distinction between Xj , the observation at time n, versus
bold Xn , the observations up to time n, both at node j. The
j
aggregate of all the observations in the network is denoted as
X∗ = (Xj , j ∈ V, Xe , e ∈ E). Similarly, Xn represents all the
∗
observations up to time n. We will also use λ∗ = (λj , j ∈ V ).
The joint distribution of λ∗ and Xn is given by a graphical
∗
model,
P (λ∗ , Xn ) =
∗

P (Xn |λj )
j

πj (λj )
j∈V

would be an interesting direction to study our multiple change
point model in such settings.
B. Communication graph and message passing (MP)
Another ingredient of our formalism is the notion of a
communication graph representing constraints under which the
data can be transmitted across network to compute a particular
stopping rule, say τj . In general, such a rule depends on all
the aggregated data Xn . We are primarily interested in those
∗
rules that can be implemented in a distributed fashion by
passing messages from one sensor only to its neighbors in the
communication graph. Although, conceptually, the statistical
graph and communication graphs play two distinct roles,
they usually coincide in practice and this will be assumed
throughout this paper. See Fig. 1 for an illustration.

P (Xn |λs1 , λs2 ).
e
e ∈E

j∈V

(3)
k−1
1
Given λj = k, we assume Xj , . . . , Xj
to be i.i.d. with
k+1
k
density gj and Xj , Xj , . . . to be i.i.d. with density fj .
Given (λs1 , λs2 ), we assume that the distribution of Xn only
e
depends on λe := λs1 ∧ λs2 , the minimum of the two
change points; hence we often write P (Xn |λe ) instead of
e
1
k−1
are i.i.d. with
P (Xn |λs1 , λs2 ). Given λe = k, Xe , . . . , Xe
e
k
k+1
density ge and Xe , Xe , . . . are i.i.d. with density fe . All the
densities are assumed to be with respect to some underlying
measure µ. These speciﬁcations can be summarized as,
t
fj (Xj )

t
gj (Xj )
t=1

(4)

t=k

and similarly for P (Xn |λe ). We will assume the prior on λj
e
to be geometric with parameter ρj ∈ (0, 1), i.e. πj (k) := (1 −
ρj )k−1 ρj , for k ∈ N. Note that these change point variables
are dependent a posteriori, despite being independent a priori.

n
γS [n] := P(λS ≤ n | Xn ) =
∗

n
γS (k).

(8)

k=1
n
We propose to stop at the ﬁrst time γS [n] goes above a
threshold,
n
τS = inf{n ∈ N : γS [n] ≥ 1 − α}

(9)

where α is the maximum tolerable false alarm. It is easily
veriﬁed that these rules have a false alarm at most α.

(5)

for some subset S ⊂ [d]. Examples include a single change
point S = {j}, the earliest among a pair S = {i, j} and
the earliest in the entire network S = [d]. Let Fn = σ(Xn )
∗
be the σ-algebra induced by the sequence Xn . A sequential
∗
detection rule for φ is formally a stopping time τ with respect
to ﬁltration (Fn )n≥0 . To emphasize the subset S, we will use
τS to denote a rule when the functional φ = λS . For example
τ1 is a detection rule for λ1 and τ12 is a rule for λ12 = λ1 ∧λ2 .
In choosing τ , there is a trade-off between the false alarm
probability P(τ ≤ φ) and the detection delay E(τ −φ)+ . Here,
we adopt the Neyman-Pearson setting to consider all stopping
rules for φ, having false alarm at most α,
∆φ (α) := {τ : P(τ ≤ φ) ≤ α},

(7)
n

Although our primary interest is in sequential estimation of
the change points λ∗ = (λj ), we are in general interested in
the following functionals,
j∈S

MP RULES

n
γS (k) := P(λS = k | Xn ),
∗

A. Sequential rules and optimality

φ := φ(λ∗ ) := λS := min λj .

OPTIMAL

We suspect that it is not feasible to derive strictly optimal
sequential stopping rules in closed from (say by stochastic
dynamic programming) for the multiple change point problem
introduced earlier. More crucially, even if such rules are
obtained, they are not computationally tractable for large
networks, due to the exponential complexity of the state-space.
In this section, we shall present a class of detection rules
that scale linearly in the size of the network, d, and can be
implemented in a distributed fashion by message passing.
Consider the following posterior probabilities

n

k−1

P (Xn |λj ) =
j

III. A SYMPTOTICALLY

Lemma 1. For φ = λS , the rule τS ∈ ∆φ (α).
More interestingly, we will show that τS is asymptotically
optimal for detecting λS . To do so, let us extend the edge
set to E := E ∪ {{j} : j ∈ V }. This allows us to treat the
private data associated with node j, i.e. Xj , as (shared) data
associated with a self-loop in the graph (V, E). For any e ∈ E,
let Ie := fe log fe dµ be the KL divergence between fe and
ge
ge . For φ = λS , let
Iφ :=

Ie

(10)

e⊂S

where the sum runs over all e ∈ E which are subsets of S.
For example, for a chain graph on {1, 2, 3} with node 2 in
the middle, E = {{1, 2}, {2, 3}, {1}, {2}, {3}} and we have
Iλ12 := I1 + I2 + I12 while Iλ13 := I1 + I3 . (Here, we abuse
notation to write I12 instead of I{1,2} and so on.)

(6)

and pick a rule in ∆φ that has minimum detection delay. It
is worth mentioning that there are non-Bayesian optimality
criteria for the single change point problem, e.g. [13], and it

2

X23

λ3

X23
λ3

λ1

λ1

λ2

λ2
λ4

λ5

X12

λ4

mn
12

λ1

X34 X45

λ2

λ3
mn
32
X34 X45

mn
24

X12

λ5

mn λ5
45

λ4

Fig. 1. Left panel illustrates a statistical graph, which induces a graphical model in the middle panel. Right panel illustrates statistical messages passed at
time n along some edges in a communication graph (which coincides with statistical graph in this case).

Recall the geometric prior on λj (with parameter ρj ) and
the deﬁnition of φ = λS as the minimum of λj , j ∈ S.
Then, φ is geometrically distributed a priori with parameter
1 − e−qφ := 1 − j∈S (1 − ρj ). We can now state our main
result on asymptotic optimality.

Lemma 2. Let {i1 , i2 , . . . , ir } ⊂ [d] be a distinct collection
of indices. The function

Theorem 1. Assume | log fe | ≤ M for all e ∈ E. Then, τS
ge
is asymptotically optimal for φ = λS ; more speciﬁcally, as
α → 0,

The algorithm is invoked at each time step n, by passing
messages between nodes according to the following protocol:
a node sends a message to one of its neighbors (in G) when
and only when it has received messages from all its other
neighbors. Message passing continues until any node can be
linked to any other node by a chain of messages, assuming
a connected graph. For a tree, this is usually achieved by
designating a node as root and passing messages from the
root to the leaves and then backwards.
The message that node j sends to its neighbor i, at time n,
is denoted as mn = [mn (1), . . . , mn (n + 1)] ∈ Rn+1 and
ji
ji
ji
computed as

(k1 , k2 , . . . , kr ) → P (Xn |λi1 = k1 , λi2 = k2 , . . . , λir = kr )
∗
is constant over {n + 1, n + 2, . . . }r .

| log α|
(1 + o(1))
qφ + Iφ
= inf E τ − φ | τ ≥ φ .

E τS − φ | τS ≥ φ =

τ ∈∆φ (α)

Remark 1. A notable feature of this result is the decomposition (10) of information along the edges of the graph. For
example, in the case of a paired delay φ = λ12 , for which the
information Iφ = I1 + I2 + I12 1{{1,2}∈E} increases (hence the
asymptotic delay decreases) if there is an edge between nodes
1 and 2. This has no counterpart in the classical theory where
one looks at change points independently.
Remark 2. Another feature of the result is observed for a single
delay, say φ = λ1 , where one has Iφ = I1 regardless of
whether there is an edge between nodes 1 and 2. Thus, the
asymptotic delay for the threshold rule which bases its decision
on the posterior probability of λ1 given all the data in the
network (Xn ) is the same as the one which bases its decision
∗
on the posterior given only private data of node 1 (Xn ). Al1
though this rather counter-intuitive result holds asymptotically,
the simulations show that even for moderately low values of α,
having access to extra information in Xn does indeed improve
12
performance as one expects. (cf. Section VI).
IV. E XACT

n+1

πj (k ′ )P (Xn |k ′ )P (Xn |k ∧ k ′ )
j
ij

mn (k) =
ji
k′ =1

mn (k ′ )
rj

r∈∂j\{i}

for k ∈ [n + 1], where πj (k) := πj (k) for k ∈ [n] and πj (n +
∞
1) := πj [n]c = k=n+1 πj (k), and ∂j is the neighborhood
n
n
set of j. Once the message passing ends, γj and γij are readily
available. We have
n
γj (k) ∝ πj (k)P (Xn |k)
j

mn (k),
rj

k ∈ [n].

(11)

r∈∂j
n
It also holds for k = n + 1 if the LHS is interpreted as γj [n]c .
Similarly, when {i, j} ∈ E, P (λi = k1 , λj = k2 |Xn ) is
∗
proportional to

πi (k1 )πj (k2 )P (Xn |k1 )P (Xn |k2 )P (Xn |k1 ∧ k2 )
ij
i
j
mn (k1 )
ri

×
MESSAGE PASSING ALGORITHM

r∈∂j\{i}

r∈∂i\{j}
2

for (k1 , k2 ) ∈ [n] , from which

It is relatively simple to adapt the well-established belief
propagation algorithm, also known as sum-product, to the
graphical model (3). The algorithm produces exact values of
n
the posterior γS , as deﬁned in (7), in the cases where G is a
polytree (and provides a reasonable estimate otherwise.) In this
section, we provide the details for S = {j} or S = {i, j} ∈ E.
One issue in adapting the algorithm is the possible inﬁnite
n
support of γS . Thanks to a “constancy” property of the
likelihood, it is possible to lump all the states after n when
n
computing γS [n].

mn (k2 )
rj

n
γij

can be computed.

Lemma 3. When G is a tree, the message passing algorithm
n
n
produces correct values of γj and γij at time step n, with
computational complexity O((|V | + |E|)n).
V. FAST APPROXIMATION ALGORITHM
We now turn to an approximate message passing algorithm which, at each time step, has computational complexity
O(|V | + |E|). Let us deﬁne binary variables
n
Zj = 1{λj ≤ n},

3

n
n
n
Z∗ = (Z1 , . . . , Zd ).

(12)

1

1

0.8

0.8

2.6

SINGLE
MP
APPROX
LIMIT

j=1
2.4

2.6

SINGLE
MP
APPROX
LIMIT

j=2
2.4

0.6

0.6

0.4

0.4

2.2

0.2

0.2

2

2

1.8

1.8

1.6

1.6

0

MP
APPROX
10

20

30

40

50

60

70

0

MP
APPROX
10

20

30

40

50

60

70

Fig. 2. Examples of posterior paths, n → γ n [n], obtained by exact (MP)
and approximate (APPROX) message passing.

2.2

1.4

1.4
5

10

15

20

2.6

j=3

n
n
n−1
P (Z∗ , X∗ |X∗ ) =

j∈V

2.2

1.6
1.4
5

10

15

20

25

30

SINGLE
MP
APPROX
LIMIT

2

5

10

15

20

e = (3,2)
2

30

1.5

1

25

SINGLE
MP
APPROX
LIMIT

2.5

1.5

1

0.5

0.5
5

10

15

20

25

30

5

10

15

20

25

30

1
Fig. 3.
Plots of the slope − log α E[τS − φ|τS ≥ φ] against − log α
for message-passing algorithm (MP), approximate algorithm (APPROX) and
SINGLE algorithm which disregards shared information. The graph is the
star graph of 4 nodes with node 2 in the center. Estimates of both single and
paired change points (λj and λij ) are shown together with theoretical limit of
Theorem 1. (The case e = (4, 2) is omitted to conserve space; it looks very
similar to e = (1, 2), (3, 2)). False alarm tolerance α ranges in [0.5, 10−13 ].

n−1
n
ν(Zj ; γj [n]),
j∈V

(14)
where ν(z; β) := β z (1 − β)1−z . By constancy Lemma 2,
Bayes rule and algebra, we get the recursion
n−1
γj [n] =

30

2

e = (1,2)

j∈V

25

SINGLE
MP
APPROX
LIMIT

1.8

2.5

n
n−1
P (Zj |X∗ ) =

20

2.2

1.4

Let ue (z; ξ) := [ge (ξ)]1−z [fe (ξ])z for e ∈ E, z ∈ {0, 1}.
n
n
n
n
n
n
n
Then, P (Xj |Zj ) = uj (Zj ; Xj ), and P (Xij |Zi , Zj ) =
n
n
n
n
n−1
uij (Zi ∨ Zj ; Xij ). It remains to express P (Z∗ |X∗ ) in
n−1
n−1
terms of P (Z∗ |X∗ ). This is possible at a cost of O(2|V | ),
but we omit the details for brevity. To obtain a fast algorithm
(i.e., O(poly(|V |))), we instead approximate

15

j=4

1.6

(13)

10

2.4

1.8

{i,j}∈E

n
n−1
P (Z∗ |X∗ ) ≈

5
2.6

2

n
n
n
n
n−1
P (Xij |Zi , Zj ) P (Z∗ |X∗ ).

n
n
P (Xj |Zj )

30

SINGLE
MP
APPROX
LIMIT

2.4

n
n
n
n−1
The idea is to compute P (Z∗ |Xn ) = P (Z∗ |X∗ , X∗ )
∗
n−1
n−1
recursively based on P (Z∗ |X∗ ). The former is proporn
n
n−1
n
tional (in Z∗ ) to P (Z∗ , X∗ |X∗ ) and we have

25

πj (n)
πj [n]c
+
γ n−1 [n − 1].
πj [n − 1]c
πj [n − 1]c j

three methods: the message-passing algorithm of Section IV
(MP), approximate algorithm of Section V (APPROX) and
the method which bases its inference on posteriors calculated
based only on each node’s private information (SINGLE).
This latter method estimates a single change point λj by
τj := inf{n : P (λj ≤ n|Xn ) ≥ 1 − α} and a paired
j
λij = λi ∧ λj by τi ∧ τj . Also shown in the ﬁgure is the
limiting value of the normalized expected delay as predicted by
Theorem 1. All plots are generated by Monte Carlo simulation
over 5000 realizations.
In estimating single change points, MP, which takes shared
information into account, has a clear advantage over SINGLE,
for high to relatively low false alarm values (even, say, around
α ≈ e−5 ); though, both methods seem to converge to the same
slope in the α → 0 limit, as suggested by Theorem 1. (The
particular value is (− log 0.9 + 0.5)−1 = 1.6519.) Also note
that the advantage of MP over SINGLE is more emphasized
for node 2, as expected by its access to shared information
from all the three nodes. We also note that APPROX does
a reasonable job at approximating MP, with delays between
those of SINGLE and MP, getting closer to MP as α → 0.
For paired change points, the advantage of MP and APPROX over SINGLE is more emphasized. It is also interesting
to note that while MP seems to converge to the expected
theoretical limit (−2 log 0.9 + 3 · 0.5)−1 = 0.5845, SINGLE

Thus, at time n, the RHS of (14) is known based on values
0
computed at time n − 1 (with initial value γj [0] = 0, j ∈ V ).
n
n−1
Inserting this RHS into (13) in place of P (Z∗ |X∗ ), we
n
obtain a graphical model in variables Z∗ (instead of λ∗ ) which
n−1
n
has the same form as (3) with ν(Zj ; γj [n]) playing the role
of the prior π(λj ).
Hence, we can apply a message passing algorithm similar to
that described in Section IV, to marginalize this approximate
n
n
n−1
version of P (Z∗ , X∗ |X∗ ), providing approximate values
n
n
n
of γj [n] = P (Zj = 1|Xn ) and γij [n]. The message update
∗
equations are similar to those of Section IV and are omitted.
The difference is that the messages are now binary and do
not grow in size with n. Fig. 2 shows examples of posterior
tracking by approximate algorithm. Theoretical analysis of the
algorithm will appear in a longer version of this paper.
VI. S IMULATIONS
We present simulation results as depicted in Fig. 3. The
setting is that of graphical model (3) on d = 4 nodes, where
the statistical graph is a star with node 2 in the middle. Conditioned on λ∗ , all the data sequences, X∗ , are assumed Gaussian
of variance 1, with pre-change mean 1 and post-change mean
zero. All priors are geometric with parameters ρj = 0.1. Fig. 3
shows plots of expected delay over | log α|, against | log α|, for

4

k
Let us focus on the case φ = λ12 . For k < ∞, πφ (k1 , k2 ) ∝
where ρj := 1 − ρj . Let

seems to converge to a higher slope (with a reasonable guess
being 1.6519 as in the case of single change points).
In regard to false alarm probability, nonzero values were
only observed for the ﬁrst few values of α considered here,
and those were either below or very close to the speciﬁed
tolerance.

ρk1 ρk2 1{k1 ∧k2 =k}
1 2

n,n
n
T1 := Rk {1}[Sk+1 {2} + Q2,n ],
n
k
where Q2,n :=
k2 >n πφ (k, k2 ) ∝ ρ2 (symmetrically
for T2 with roles of 1 and 2 reversed) and T0 :=
k
n
n
k
n
πφ (k, k)Rk {1}Rk {2}. Then, Dφ (Xn ) = Rk {12} 2 Ti .
∗
i=0
Now, condition on λ1 = k and λ2 = r ≥ k so that λ12 = k.
(The other case with roles of 1 and 2 reversed follows by
symmetry.) By (15),

A PPENDIX A
P ROOF SKETCH OF T HEOREM 1
We provide a proof sketch for the case d = 2 here (the full
proof is quite technical, and can be found in [14]). Fix some
φ = τS and consider the likelihood ratio
k
Dφ (Xn )
∗

ε

k
n
n−1 log Dφ (Xn ) ≍ n−1 log Rk {12} + max n−1 log Ti
∗
i=0,1,2

P (Xn | φ = k)
∗
:=
.
P (Xn | φ = ∞)
∗

−1

n
Consider T1 and note that n log T1 ≍ n−1 log Rk {1} +
ε
n,n
1
max{ n log Sk+1 {2}, log ρ2 } where the ﬁrst term is ≍ I1
n,n
−1
w.h.p. by (16). Similarly, n log Sk+1 {2} equals

Let Pk = P( · | φ = k). Our asymptotic analysis hinges on
φ
1
k
the asymptotic behavior of n log Dφ (Xn ), as n → ∞, under
∗
k
probability measure Pφ . In particular, building on the results
of [15], it is straightforward to derive the following sufﬁcient
,m
condition. Let Pm1,λ2 2 be the probability measure conditioned
λ1
k
on λ1 = m1 and λ2 = m2 . Also, let πφ (m1 , m2 ) :=
k
Pφ (λ1 = m1 , λ2 = m2 ). Suppose that for all (m1 , m2 )
k
with positive probability under πφ (m1 , m2 ), we can show the
“concentration inequality”
1
,m
k
Pm1,λ2 2
log Dφ (Xn ) − Iφ > ε ≤ q(n) exp(−c1 nε2 )
∗
λ1
n
for all n ≥ 1 p(m1 , m2 , k) for polynomials p(·) and q(·). Then,
ε
k
if both πφ (·, ·) and P(φ = ·) have ﬁnite polynomial moments,
which is the case for our geometric priors, conclusions of
k
Theorem 1 hold for φ. We say that |n−1 log Dφ (Xn )−Iφ | ≤ ε
∗
holds with high probability, abbreviated w.h.p.
n
fe (Xe )
n
n
Next, we deﬁne Rp (e) := Rp (Xe ) := t=p ge (Xe ) if e ∈
n
E and p ≤ n, and Rp (Xe ) = 1 otherwise. Similarly let Ie be
deﬁned as in (10) if e ∈ E and Ie = 0 otherwise. We note
that
k
n
n
n
k1 ,k2 πφ (k1 , k2 ) Rk1{1} Rk1{2} Rk1 ∧k2{12}
k
,
Dφ (Xn ) =
∗
n
n
∞
n
k1 ,k2 πφ (k1 , k2 ) Rk1{1} Rk1{2} Rk1 ∧k2{12}

ε

r,r
n,n
n
n−1 log Sk+1 {2}Rr {2} + Sr+1 {2} ≍ I2
ε

and since log ρ1 < 0, we have n−1 log T1 ≍ I1 + I2 . Other
terms are dealt with similarly, and we get, w.h.p.
1
ε
k
log Dφ (Xn ) ≍ I12 + max{I1 + I2 , I2 + I1 , I1 + I2 }.
∗
n
The case φ = λ1 follows along similar lines.
R EFERENCES
[1] T. L. Lai, “Sequential analysis: Some classical problems and new
challenges (with discussion),” Statist. Sinica, vol. 11, pp. 303–408, 2001.
[2] V. V. Veeravalli, T. Basar, and H. V. Poor, “Decentralized sequential
detection with a fusion center performing the sequential test,” IEEE
Trans. Info. Theory, vol. 39, no. 2, pp. 433–442, 1993.
[3] Y. Mei, “Asymptotic optimality theory for decentralized sequential hypothesis testing in sensor networks,” IEEE Transactions on Information
Theory, vol. 54, no. 5, pp. 2072–2089, 2008.
[4] X. Nguyen, M. J. Wainwright, and M. I. Jordan, “On optimal quantization rules in some problems in sequential decentralized detection,” IEEE
Transactions on Information Theory, vol. 54(7), pp. 3285–3295, 2008.
[5] R. Rajagopal, X. Nguyen, S. Ergen, and P. Varaiya, “Distributed online
simultaneous fault detection for multiple sensors,” in Proc. of 7th Int’l
Conf. on Info. Proc. in Sensor Networks (IPSN), April 2008.
[6] M. I. Jordan, “Graphical models,” Statistical Science, vol. 19, pp. 140–
155, 2004.
[7] S. Dayanik, C. Goulding, and H. V. Poor, “Bayesian sequential change
diagnosis,” Mathematics of Operations Research, vol. 33, no. 2, pp.
475–496, 2008.
[8] Y. Xie and D. Siegmund, “Sequential multi-sensor change-point detection,” in Joint Statistical Meeting, 2011.
[9] V. Raghavan and V. V. Veeravalli, “Quickest change detection of a
markov process across a sensor array,” IEEE Transactions on Information Theory, vol. 56(4), pp. 1961–1981, 2010.
[10] M. Cetin, L. Chen, J. W. Fisher III, A. Ihler, R. Moses, M. Wainwright,
and A. Willsky, “Distributed fusion in sensor networks: A graphical
models perspective,” IEEE Signal Processing Magazine, vol. July, pp.
42–55, 2006.
[11] O. P. Kreidl and A. Willsky, “Inference with minimum communication:
a decision-theoretic variational approach,” in NIPS, 2007.
[12] R. Rajagopal, X. Nguyen, S. Ergen, and P. Varaiya,
“Simultaneous sequential detection of multiple interacting faults,”
http://arxiv.org/abs/1012.1258, 2010.
[13] A. G. Tartakovsky and M. Pollak, “Nearly minimax changepoint detection procedures,” in ISIT, 2011.
[14] A. A. Amini and X. Nguyen, “Sequential detection of multiple change
points in networks: A graphical model based approach,” Department of
Statistics, University of Michigan, Tech. Rep., 2012.
[15] A. G. Tartakovsky and V. V. Veeravalli, “General asymptotic bayesian
theory of quickest change detection,” Theory Probab. Appl., vol. 49,
no. 3, pp. 458–497, 2005.

with some abuse of notation. In addition, for a collection E =
{e1 , . . . , eJ }, deﬁne
q

Ae−βp

q,n
Sm (E) =
p=m

n
Rp (e)
e∈E

for some A, β > 0, and let IE := e∈E Ie .
ε
0
We will say that an ≍ bn if |an − bn | ≤ ε for n ≥ cε . This
relation is transitive and stable under addition, subtraction and
taking maximum. Furthermore,
ε

n−1 log(an + bn ) ≍ max{n−1 log an , n−1 log bn }.

ε

(15)

At the heart of the proof are two concentration inequalities:
1
1
ε
ε
n
n,n
log Rm (e) ≍ Ie ,
log Sm (E) ≍ IE ,
(16)
n
n
where the ﬁrst holds conditioned on λe ≤ m w.h.p. and the
second conditioned on λe ≤ m for all e ∈ E, w.h.p.

5

